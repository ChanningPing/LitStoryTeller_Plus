0,"Probabilistic Latent Semantic Indexing is a novel approach to automated document indexing which is based on a statistical latent class model for factor analysis of count data"
0,"Fitted from a training corpus of text documents by a generalization of the Expectation Maximization algorithm the utilized model is able to deal with domainspecic synonymy as well as with polysemous words"
1,"In contrast to standard Latent Semantic Indexing LSI by Singular Value Decomposition the probabilistic variant has a solid statistical foundation and denes a proper generative data model"
1,"Retrieval experiments on a number of test collections indicate substantial performance gains over direct term matching methods as well as over LSI"
1,"In particular the combination of models with dierent dimensionalities has proven to be advantageous"
1,"The performance of PLSI has been systematically compared with the standard term matching method based on the raw term frequencies tf and their combination with the inverse document frequencies tdf as well as with LSI"
0,"We have utilized the following four mediumsized standard document collection i MED 1033 document abstracts from the National Library of Medicine ii CRAN 1400 document abstracts on aeronautics from the Craneld Institute of Technology iii CACM 3204 abstracts from the CACM journal and iv CISI 1460 abstracts in library science from the Institute for Scientic Information"
0,"The condensed results in terms of average precision recall at the 9 recall levels 10 ô€€€ 90 are summarized in Table 3"
0,"A selection of average precision recall curves can be found in Figure 3"
0,"Here are some details of the experimental setup PLSA models at K  32 48 64 80 128 have been trained by TEM for each data set with 10 heldout data"
0,"For PLSIU PLSIQ we report the best result obtained by any of these models for LSI we report the best result obtained for the optimal dimension exploring 32512 dimensions at a step size of 8"
0,"The combination weight  with the cosine baseline score has been coarsely optimized by hand MED CRAN   12 CACM CISI   23 in general slightly smaller weights have been utilized for the combined models"
1,"The experiments consistently validate the advantages of PLSI over LSI"
1,"Substantial performance gains have been achieved for all 4 data sets and both term weighting schemes"
1,"In particular PLSIQPLSIQ work particularly well on the raw term frequencies where LSI on the other hand may even fail completely in accordance with the results 56 reported in 1"
1,"We explain this by the fact that large frequencies dominate the squared error deviation used in SVD and a dampening eg by idf weighting is necessary to get a reasonable decomposition of the termdocument matrix"
1,"Since PLSIQ can not take much advantage from the term weighting scheme PLSIUPLSIU performs slightly better in this case"
1,"We suspect that even better results could be achieved by an improved integration of term weights in PLSIQ"
0,"The benets of model combination are also very substantial"
1,"In all cases the uniformly combined model performed better than the best single model"
0,"As a sighte ect model averaging also deliberates from selecting the optimal model dimensionality"
1,"In terms of computational complexity despite of the iterative nature of EM the computing time for TEM model tting at K  128 was roughly comparable to SVD in a standard implementation"
0,"For larger data sets one may also consider speeding up TEM by online learning 11"
1,"Notice that the PLSIQ scheme has the advantage that documents can be represented in a lowdimensional vector space as in LSI while PLSIU requires the calculation of the high dimensional multinomials Pwjd which oers advantages in terms of the space requirements for the indexing information that has to be stored"
0,"Finally we have also performed an experiment to stress the importance of tempered EM over standard EMbased model tting"
0,"Figure 4 plots the performance of a 128 factor model trained on CRAN in terms of perplexity and in terms of precision as a function of "
0,"It can be seen that it is crucial to control the generalization performance of the model since the precision is inversely correlated with the perplexity"
0,"In particular notice that the model obtained by maximum likelihood estimation at   1 actually deteriorates the retrieval performance"
0,"Abstract"
0,"We present a method that learns word embedding for Twitter sentiment classification in this paper."
1,"Most existing algorithms for learning continuous word representations typically only model the syntactic context of words but ignore the sentiment of text."
1,"Experiments on applying SSWE to a benchmark Twitter sentiment classification dataset in SemEval 2013 show that (1) the SSWE feature performs comparably with hand-crafted features in the top-performed system; (2) the performance is further improved by concatenating SSWE with existing feature set."
0,"1 Introduction"
1,"In the task of predicting positive/negative polarity of tweets, our method yields 84.89% in macro-F1 by only using SSWE as feature, which is comparable to the top-performed system based on hand-crafted features (84.70%)."
1,"After concatenating the SSWE feature with existing feature set, we push the state-of-the-art to 86.58% in macro-F1."
0,"The quality of SSWE is also directly evaluated by measuring the word similarity in the embedding space for sentiment lexicons."
1,"In the accuracy of polarity consistency between each sentiment word and its top N closest words, SSWE outperforms existing word embedding learning algorithms."
0,"The major contributions of the work presented in this paper are as follows."
0,"We develop three neural networks to learn sentiment-specific word embedding (SSWE) from massive distant-supervised tweets without any manual annotations;"
1,"To our knowledge, this is the first work that exploits word embedding for Twitter sentiment classification."
1,"We report the results that the SSWE feature performs comparably with hand-crafted features in the top-performed system in SemEval 2013;"
0,"We release the sentiment-specific word embedding learned from 10 million tweets, which can be adopted off-the-shell in other sentiment analysis tasks."
0,"2 Related Work"
1,"Unlike Maas et al.(2011) that follow the probabilistic document model (Blei et al., 2003) and give an sentiment predictor function to each word, we develop neural networks and map each ngram to the sentiment polarity of sentence."
1,"Unlike Socher et al.(2011c) that utilize manually labeled texts to learn the meaning of phrase (or sentence) through compositionality, we focus on learning the meaning of word, namely word embedding, from massive distant-supervised tweets."
1,"Unlike Labutov and Lipson (2013) that produce task-specific embedding from an existing word embedding, we learn sentiment-specific word embedding from scratch."
0,"3 Sentiment-Specific Word Embedding"
0,"for Twitter Sentiment Classification In this section, we present the details of learning sentiment-specific word embedding (SSWE) for Twitter sentiment classification."
0,"We propose incorporating the sentiment information of sentences to learn continuous representations for words and phrases."
0,"1560 Method Macro-F1 DistSuper + unigram 61.74 DistSuper + uni/bi/tri-gram 63.84 SVM + unigram 74.50 SVM + uni/bi/tri-gram 75.06 NBSVM 75.28 RAE 75.12 NRC (Top System in SemEval) 84.73 NRC - ngram 84.17 SSWEu 84.98 SSWEu+NRC 86.58 SSWEu+NRC-ngram 86.48 Table 2: Macro-F1 on positive/negative classification of tweets."
1,"a big gap in comparison with the NRC and SSWEbased methods."
0,"The reason is that RAE and NBSVM learn the representation of tweets from the small-scale manually annotated training set, which cannot well capture the comprehensive linguistic phenomenons of words."
1,"NRC implements a variety of features and reaches 84.73% in macro-F1, verifying the importance of a better feature representation for Twitter sentiment classification."
0,"We achieve 84.98% by using only SSWEu as features without borrowing any sentiment lexicons or hand-crafted rules."
0,"The results indicate that SSWEu automatically learns discriminative features from massive tweets and performs comparable with the state-of-the-art manually designed features."
1,"After concatenating SSWEu with the feature set of NRC, the performance is further improved to 86.58%."
0,"We also compare SSWEu with the ngram feature by integrating SSWE into NRC-ngram."
1,"The concatenated features SSWEu+NRC-ngram (86.48%) outperform the original feature set of NRC (84.73%)."
0,"As a reference, we apply SSWEu on subjective classification of tweets, and obtain 72.17% in macro-F1 by using only SSWEu as feature."
1,"After combining SSWEu with the feature set of NRC, we improve NRC from 74.86% to 75.39% for subjective classification."
0,"When such word embeddings are fed as features to a Twitter sentiment classifier, the discriminative ability of sentiment words are weakened thus the classification performance is affected."
0,"Sentiment-specific word em- 4Available at https://code.google.com/p/word2vec/."
1,"We utilize the Skip-gram model because it performs better than CBOW in our experiments."
0,"5MVSA and ReEmb are not suitable for learning bigram and trigram embedding because their sentiment predictor functions only utilize the unigram embedding."
1,"1561 beddings (SSWEh, SSWEr, SSWEu) effectively distinguish words with opposite sentiment polarity and perform best in three settings."
1,"SSWE outperforms MVSA by exploiting more contextual information in the sentiment predictor function."
1,"SSWE outperforms ReEmb by leveraging more sentiment information from massive distant-supervised tweets."
1,"Among three sentiment-specific word embeddings, SSWEu captures more context information and yields best performance."
1,"SSWEh and SSWEr obtain comparative results."
1,"From each row of Table 3, we can see that the bigram and trigram embeddings consistently improve the performance of Twitter sentiment classi- fication."
0,"The underlying reason is that a phrase, which cannot be accurately represented by unigram embedding, is directly encoded into the ngram embedding as an idiomatic unit."
0,"Results of positive/negative classification of tweets on our development set are given in Figure 3."
0,"1 2 3 4 5 6 7 8 9 10 11 12 x 106 0.77 0.78 0.79 0.8 0.81 0.82 0.83 0.84 # of distantsupervised tweets MacroF1 SSWEu Figure 3: Macro-F1 of SSWEu with different size of distant-supervised data on our development set."
1,"We can see that when more distant-supervised tweets are added, the accuracy of SSWEu consistently improves."
1,"The underlying reason is that when more tweets are incorporated, the word embedding is better estimated as the vocabulary size is larger and the context and sentiment information are richer."
1,"When we have 10 million distantsupervised tweets, the SSWEu feature increases the macro-F1 of positive/negative classification of tweets to 82.94% on our development set."
0,"When we have more than 10 million tweets, the performance remains stable as the contexts of words have been mostly covered."
0,"4.2 Word Similarity of Sentiment Lexicons"
0,"The distribution of the lexicons used in this paper is listed in Table 4."
0,"Lexicon Positive Negative Total HL 1,331 2,647 3,978 MPQA 1,932 2,817 4,749 Joint 1,051 2,024 3,075 Table 4: Statistics of the sentiment lexicons."
0,"Joint stands for the words that occur in both HL and MPQA with the same sentiment polarity."
0,"Results."
0,"Table 5 shows our results compared to other word embedding learning algorithms."
0,"The accuracy of random result is 50% as positive and negative words are randomly occurred in the nearest neighbors of each word."
1,"Sentiment-specific word embeddings (SSWEh, SSWEr, SSWEu) outperform existing neural models (C&W, word2vec) by large margins."
1,"SSWEu performs best in three lexicons."
1,"SSWEh and SSWEr have comparable performances."
0,"Experimental results further demonstrate that sentiment-specific word embeddings are able to capture the sentiment information of texts and distinguish words with opposite sentiment polarity, which are not well solved in traditional neural Embedding HL MPQA Joint Random 50.00 50.00 50.00 C&W 63.10 58.13 62.58 Word2vec 66.22 60.72 65.59 ReEmb(C&W) 64.81 59.76 64.09 ReEmb(w2v) 67.16 61.81 66.39 WVSA 68.14 64.07 67.12 SSWEh 74.17 68.36 74.03 SSWEr 73.65 68.02 73.14 SSWEu 77.30 71.74 77.33 Table 5: Accuracy of the polarity consistency of words in different sentiment lexicons."
0,"models like C&W and word2vec."
1,"SSWE outperforms MVSA and ReEmb by exploiting more context information of words and sentiment information of sentences, respectively."