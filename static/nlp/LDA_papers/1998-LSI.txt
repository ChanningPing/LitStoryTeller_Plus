Latent Semantic Indexing: A Probabilistic Analysis
CH Papadimitriou
1998
#Abstract
Latent semantic indexing (LX) is an information retrieval technique based on the spectral analysis of the term-document matrix, whose empirical success had heretofort been without rigorous prediction and explanation. We prove that, under certain conditions, LSI does succeed in capturing the underlying semantics of the corpus and achieves improved retrieval performance. We also propose the technique of random projection as a way of speeding up LSI. We complement our theorems with encouraging experimental results. We also argue that our results may be viewed in a more general framework, as a theoretical basis for the use of spectral methods in a wider class of applications such as collaborative filtering. 
#1 Introduction 
The field of information retrieval has traditionally been considered outside the scope of database theory. While database theory deals with queries that are precise predicates (the so-called “employee-manager-salary paradigm”), in information retrieval we have the rather nebulous and ill-defined concept of “relevance”, which depends in intricate ways on the intent of the user and the nature of the corpus. Evidently, very little theory can be built on this basis. See retrieval, inclu 6 ing [lo, 211 for surveys on information discussions of the technique that is the focus of this paper., from database and theoretical points of view, respectrvely; [22, 231 are classical texts on the subject of information retrieval.) However, the field of information retrieval has been evolving in directions that bring it closer to databases. Information retrieval systems are increasingly being built on relational (or object-relational) database systems, rather than on flat text and index files. Another important change is the dramatic expansion of the scope of information retrieval. with the advent of multimedia, the internet, and giobalized information; database concepts and some theory have started to find fertile ground there (see for example [9,3,18], as well a record number of information retrieval papers in the 1997 SIGMOD Proceedings)’ Secondly, the techniques employed in information retrieval have become more mathematical and sophisticated, more plausibly amenable to analytical treatment. The present paper is an attempt to treat rigorously one such technique, latent semantic indexing @XI), introduced next. Thirdly, information retrieval systems are increasingly being built on relational (or object-relational) database systems (rather than on flat text and index files). Finally, the advent of the web has enabled powerful new applications such as collaborativefiltering (also known as target or personalized recommendation systems) that can be tackled using techniques inspired in part by information retrieval [4]; more on this in Section 6. IR and LSI 
The complexity of information retrieval is best illustrated by the two nasty classical problems of synonymy (missing documents with references to “automobile” when querying on “car”) and polysemy (retrieving documents about the internet when querying on “surfing”). To deal with these two and other similar problems, we would ideally like to represent documents (and queries) not by terms (as in conventiomd vector-based methods), but by the underlying (latent, hidden] concepts referred to by the terms. This hidden structure is not a fixed manyto-many mapping between terms and concepts, but depends critically on the corpus (document collection) in hand, and the term correlations it embodies. Latent Semantic Indexing [6] is an information retrieval method which attempts to capture this hidden structure by using techniques from linear algebra. Briefly (see the next section for a more detailed description), vectors representing the documents are projected in a new, low-dimensional space obtained by singular value &composition of the term-document matrix A (see the next subsection). This low-dimensional space is spanned by the cigenvcctors of ATA that correspond to the few largest eigcnvalues -and thus, presumably, to the few most striking correlations between terms. Queries are also projected and processed in thii low-dimensional space, This results not only in great savings in storage and query time (at the expense of some considerable preprocessing), but also, according to empirical evidence reported in the literature, to improued information retrieval [l, 7, 81. Indeed, it has been repeatedly reported that LSI outperforms, with regard to precision and recall in standard collections and query workloads, more conventional vector-based methods, and that it does address the problems of polysemy and synonymy. There is very little in the literature in the way of a mathematical theory that predicts this improved performance, An interesting mathematical fact due to E&art and Young (stated below as Theorem l), often cited as an explanation of the improved performance of LSI, states, informally, that LSI retains as much as possible the relative position of the document vectors. Thii may only provide an explanation of why LSI does not deteriorate too much in performance over conventional vector-space methods; it fails to justify the observed improvement in precision and recall.
This paper is a first attempt at using mathematical techniques to rigorously explain the empirically observed improved performance of LSI. Since LSI seems to exploit and reveal the statistical properties of a corpus, WC must start with a riaorous nrobabilistic model of the corpus (that is to say; a maihematical model of how corpora arc generated); we do this in Section 3. Briefly, we model topics as probability distributions on terms. A document is then a probability distribution that is the convex combination of a small number of topics. WC also include in our framework style of authorship, which we model by a stochastic matrix that modifies the term distribution. A corpus is then a collection of documents obtained by repeatedly drawing sample documents. (In Section 6 we brieflv discuss an alternative probabiliitic model, motivated ;n part by applications to collaborative filtering.)
Once we have a corpus model, we would like to determine under what conditions LSI results in enhanced retrieval, We would like to prove a theorem stating essentially that if the corpus is a reasonablyfocused collection of meaningfully correlated documents, then LSI performs urell. The problem is to define these terms 50 that (1) there is a reasonably close correspondence with what they mean intuitively and in practice, and (2) the theorem can be proved. In Section 4 we prove results that, although not quite as general as we would have liked, definitely point to this direction. In particular, we show that in the special case in which (a) there is no style modifier; (b) each document is on a single topic; and (c) the terms are partitioned among the top its so that each topic distribution has high probability on its own terms, and low probability on all others; then LSI, projecting to a subspace of dimension equal to the number of topics, wilI discover these topics exactly, with high probability (Theorem 2). In Section 5 we show that, if we project the termdocument matrix on a completely random low-dimensional subspace, then with high probability we have a distancepreservation property akin to that enjoyed by LSI. This suggests that random projection may yield an interesting improvement on LSI: we can perform the LSI precomputation not on the original term-document matrix, but on a low-dimensional projection, at great computational savings and no great loss of accuracy (Theorem 4).
Random projection can be seen as an alternative to (and a justification of) somplingin LSI. Reports on LSI experiments in the literature seem to suggest that LSI is often done not on the entire corpus, but on a randomly selected subcorpus (both terms and documents may be sampled, although it appears that most often documents are). There is verv little non-emnirical evidence of the a&racy of such Isampling, Our iesult suggests a different and more elaborate (and computationally intensive) approach -projection on a random lowdimensional subspace- which can be rigorously proved to be accurate.
#2 LSI background
A corpusis a collection of documents. Each document is a collection of terms from a universe of n terms. Each document can thus be represented as a vector in !I?“‘ where each axis represents a term. The ith coordinate of a vector represents some function of the number of times the ith term occurs in the document represented by the vector. There are several candidates for the right function to be used here (O-l, frequency, etc.), and the precise choice does not affect our results. Let A be an n x m matrix of rank r whose rows rep resent terms and columns represent documents. Let the singular values of A (the eigenvalues of AAT) be u1 2 c72 2 -*. 2 Us (not necessarily distinct). The singular ualue decomposition of A expresses A as the product of three matrices A = UDV*, where D = diag(gl, . . . , CT?) is an rxr matrix, U =(ul,...,u+) is an n xr matrix whose columns are orthonormal, and V = (VI,. . . , v,) is an m x r matrix which is also column-orthonormal. LSI works by omitting all but the I; largest singular values in the above decomposition, for some appropriate L; here 5 is the dimension of the low-dimensional space alluded to in the informal description on page 2. It should be small enough to enable fast retrieval, and large enough to adequately capture the structure of the corpus (in practice, b is in the few hundreds, compared Al, = vkDk@ hi a matrix of rank k, which is our approximation of A. The rows of l$Db above are then used to represent the documents. In other words, the column vectors of A (documents) are projected to the k-dimensional space spanned by the column vectors of u,V; we sometimes call this space the LSI space of A. How good is this approximation? The following well-known theorem gives us some idea (the subscript F denotes the Frobenius norm), Tlrcorom 1 (I%kaTt and Young, see [15/) Among all n x m matrices 0 o ’ rank at most k, Ak is the one that minimizes /iA - C((, = Ci,, (Ai,j - Ci,J)2- Therefore, LSI preserves (to the extent possible) the relative distances (and hence, presumably, the retrieval capabilities) in the term-document matrix while projecting it to a lower-dimensional space. It remains to be seen in what way it improves these retrieval capabilities.
#3 The probabilistic corpus model
There are many useful formal models of IR in the literature, and probability plays a major role in many of them -see for instance the surveys and comparisons in [13, 22, 241. The approach in this body of work is to formulate information retrieval as a problem of learning the concept of “relevance” that relates documents and queries, The corpus and its correlations plays no centrnl role In contrast, our focus is on the probabilistic properties of the corpus.
Since LSI is supposed to exploit and bring out the structure of the corpus, it will fare well in a meaningful collection of strongly correlated documents, and will produce noise in a random set of unrelated documents. In order to study the dependence of the performance of LSI on the statistical properties of the corpus, we must start with a probabilistic model of a corpus. We state now our basic probabilistic model, which we will use for much of this paper,
Let the universe of all terms be U. A topic is a probability distribution on U. A meaningful topic is very different from the uniform distribution on U, and is concentrated on terms that might be used to talk nbout a particular subject. For example, the topic of “space travel” might favor the terms “galaxy” and “starship”, while rarely mentioning ‘misery” or “spider”. A possible criticism against this model is that it does not take into account correlations of terms within the same topic (for example, a document on the topic “internet” is much more likely to contain the term “search” if it also contains the term “engine”). The structure of documents is also heavily affected by authorship style. We model style as a [VI x lU[ stochastic matrix (a matrix with nonnegative entries and row sums equal to l), denoting the way whereby style modifies the frequency of terms. For example, a “formal” style may map &car” often to “automobile” and “vehicle,” and seldom to %a?’ - and almost never to %heeIs.” Admittedly, this is not a comprehensive treatment of style; for example, it makes the assumption - not alwavs valid - that thii influence is indenendent of the und&lying topic.
A corpus model C is a quadruple C = (U, 7, S, D), where U is the universe of terms, I is a set of topics, and S a set of styles, and D a probability distribution onFxSxZ+, where by Z? we denote the set of all convex combinations of topics in ‘/, by 3 the set of all convex combinations of styles in S, and by Zt the set of positive integers (the integers represent the lengths of documents). That is, a corpus model is a probability distribution on topic combinations (intuitively, favoring combinations of a few related topics), style combinations, and document lengths (total number of term occurrences in a document). A document is generated from a corpus model C = (U, T,S, D) through the following two-step sampling process. In the first step, a convex combination of topics 5!’ from 9, a convex combination of styles 3 from 3, and a positive integer 1 are sampled according to dii tribution D. Then terms are sampled e times to form a document, each time according to distribution E!‘.!?. A corpus of size m is a collection of m documents generated from C by repeating this two-step sampling process m times.
#4 An analysis of LSI
Does LSI indeed bring together semantically related documents? And does it deal effectively with the problem of synonymy? We present below theoretical evidence that it does. Our results assume the corpus model has a particularly simple structure. We show that, in thii case, LSI does discover the structure of the corpus, and handles synonymy well. These results should be taken only as indications of the kinds of results that can be proved; our hope is that the present work will lead to more elaborate techniques, so that LSI can be proved to work well under more realistic assumptions. We will first need a useful lemma, which formalizes the following intuition: if the k largest singular values of a matrix A are well-separated from the remaining singular values, then the subspace spanned by the corresponding singular vectors is preserved well when a small perturbation is added to A. Lemma 1 Let A be an n x m matriz of rank r with singular value decomposition A = UDVT, where D =diag(cn,..., ut). Suppose that, for some k, 1 5 h < r, U);/u):+:+1 > cul/uk for sufficiently large constant c. Let F be an arbitrary n x m matrix with llPllz 5 c, where E is a sufficiently small positive constant, Let A’ = A + F and let U’D’V’T be its aingularvalue decomposition. Let Uk and l.$ be n x k matrices consisting of the first h columns of U and U’ reapectiuelv. Then, l$ = UkR+G for some k x k orthonormal matrix R and some n x k matrix G with 110112 2 O(c). The proof of this lemma, given in the appendix, relies on a theorem of Stewart [16] about perturbing a symmetric matrix.
Let C = (U, T, D) be a corpus model. We call C pure if each document involves only a single topic. We call C c-separable, where 0 ,< E < 1, if a set of terms UT is associated with each topic T E 7 so that (1) UT arc mutually disjoint and (2) for each T, the total probability T assigns to the terms in UT is at least 1 -E. we call UT the primary set of terms of topic T. The assumption that a corpus model is style-free and pure is probably too strong and its elimination should be addressed in future investigations. On the other hand, the assumption that a corpus is c-separable for some small value of 6 may be reasonably realistic, since documents are usually preprocessed to eliminate commonlyoccurring stop-words.
Let C be a pure corpus model and let L = 171 denote the number of topics in C. Since C is pure, each document generated from C is in fact generated from some single topic T: we say that the document belongs to the topic T. Let C be a corpus generated from C and, for each document d E C, let Vd denote the vector assigned to d by the rank-k LSI performed on C. We say that the rank-H LSI is C-skewed on the corpus instance C if, for each pair of documents d and d’, z)d’2)& < 611vdll[lV&ll if d and d’ belong to different topics and Tad avd/ 2 1 - 6ll&3lll]vd,ll if they belong to the same topic. Informally, the rank-l LSI is &skewed on a corpus (for small 6), if it assigns nearly orthogonal vectors to two documents from different topics and nearly parallel vectors to two documents from a single topic: LSI does a particularly good job of classifying documents when applied to such a corpus. The following theorem states that a large enough corpus (specifically, when the number of documents is greater than the number of terms) generated from our restricted corpus model indeed has this nice property with high probability. Theorem 2 Let C be a pure, c-separable corpus model with h topics such that the probability each topic assigns to each term is at most r, where r > 0 is a sufficiently small constant, Let C be a corpus of m documents generated from C, Then, the rant-k LSI is O(c)-skewed on C with probability 1 - O(m-‘).
Proof, Let Ci denote the subset of the generated corpus C consisting of documents belonging to topic Ti, 1 ,< i < k. To see the main idea, let us first assume that E = 0. Then, each document of Ci consists only of terms in Ui, the primary set of terms associated with topic T<. Thus, the term-document matrix A representing corpus C consists of blocks .&, 1 5 i 2 k: the rows of B; correspond to terms in U, and columns of Bi correspond to documents in Ci; the entire matrix A can have non-zero entries in these rows and columns only within Bi. Therefore, ATA is block-diagonal with blocks BTBi, 1 < i _< h. Now focus on a particular block BTBi and let Xi and Xi denote the largest and the second largest eigenvalues of B’Bi. Intuitively, the matrix BTBi is essentially the adjacency matrix of a random bipartite multigraph and then, from the standard theory of spectra of graphs[S], we have that X:/Xi -+ 0 with probability 1 a~ r -+ 0 and IC,l -+ 00. Below we give a formal justification of this by showing that a quantity that captures this property, the conductance [20] (equivalently, expansion) of BTBi is high. The conductance of an undirected edge-weighted graph G=(V,E)is mm ‘. ,~s,,p4~J3 A min{lSl, ISI}
Let X1,x2,... , x* be random documents picked from the topic Ti. Then we will show that the conductance is n(a), where lzl is the number of terms in the topic T<. Let G be the graph induced by the adjacency matrix BFB;. For any subset S of the vertices (documents), = (C x’) - (C 2’). ig.S 3s
Assume w.l.0.g. that ISI 5 131. Let ps be the probability of the ath term in Ti. Then we can estimate, for each term, &TX: 1 min{p,/2,p, - E) with probability at least 1 - Z$ using the independence of the x1’s via a simple application of Chernoff-Hoeffding bound [l?]. Using this we lower bound the weight of the cut (S,‘s>: i&S j8 iES which is Q(,!$) with high probability by a second application of the Chernoff-Hoeffding bound. The desired bound on the conductance follows from this. Thus, if the sample size m = ICI is sufficiently large, and the maximum term probability r is sufficiently small (note this implies that the size of the primary set of terms for each topic is sufficiently large), the JZ largest eigenvalues of ATA are Xi, 1 5 i 5 I;, with high probability. Suppose now that our sample C indeed enjoys this property, Let zii denote the eigenvector of B’Bi corresponding to eigenvalue Xi (in the space where coordinates are indexed bv the terms in T;\ and let zli be its extension to the full berm space, obtaiged by padding zero entries for terms not in Ti. Then, the k-dimensional LSI-space for corpus C is spanned by the mutually orthO&Onid vectors Ui, 1 s i 5 I;. When a vector od representing a document d E Ci is projected into this space, the projection is a scalar multiple of ui, because ~JUJ is orthogonal to Uj for every j # i. When E > 0, the term-document matrix A can be written as A = B + F, where B consists of blocks Bi as above and F is a matrix with small ]]L]]z-norm (not exceeding c by much, with high probability). As observed in the above analysis for the case E = 0, the invariant subspace w~; of BTB corresponding to its largest h eigenvalues is an ideal representation space for representing documents according to their topics. Our hope is that the small perturbation F does not prevent LSI from identifying WI; with small errors. This is where we apply Lemma 1. Let kVi denote the k-dimensional space the rank-l; LSI identifies. The c-separability of the corpus model implies that the twonorm of the perturbation to the document-term matrix is O(r) and, therefore by the lemma, the two-norm of the difference between the matrix representations of klfj; and Wi is O(L). Since WI is a small perturbation of WI;, pro ecting I’ a vector representing a document in Cl into WL. yields a vector close, in its direction, to a< (the dominating eigenvector of BFBi). Therefore, the LSI renresentations of two documents are almost in the same direction if they belong to the same topic and are nearly orthogonal if they belong to different topics. A quantitative analysis (Lemma 5) shows that the rank-l; LSI is indeed O(c)-skewed on C with high probability.
Even though Theorem 2 gives an asymptotic result and only claims that the probability approaches 1 as the size parameters grow, the phenomenon it indicates can be observed in corpora of modest sizes, as is seen in the following experiment. We generated 1000 documents (each 50 to 100 terms long) from a corpus model with 2000 terms and 20 topics. Each topic is assigned a dii joint set of 100 terms as its primary set. The probability distribution for each topic is such that 0.95 of its probability density is equally distributed among terms from the primary set, and the remaining 0.05 is equally distributed among all the 2000 terms. Thus this corpus model is 0.05-separable. We measured the angle (not some function of the angle such as the cosine) between all pairs of documents in the original space and in the rank 20 LSI space. The following is a typical result; similar results are obtained from repeated trials. Call n pair of documents intro-topic if the two documents are generated from the same topic and inter-topic otherwise.
Here, angles are measured in radians. It can be seen that the angles of intra-topic pairs are dramatically reduced in the LSI space. Although the minimum intertopic angle is rather small, indicating that some intertopic pairs can be close enough to be confused, the average and the standard deviation show that such pairs are extremely rare. Results from experiments with different size-parameters are also similar in spirit. In this and the other experiments reported here, we used SVDPACKC [2] for singuIar value decomposition.
We end this section with a brief discussion of synonymy in the context of LSI. Let us consider a simple model in which two terms have identical co-occurrences (this generalizes synonymy, as it also applies to pairs of terms such as supply-demand and war-peace). Furthermore, these two terms have each a small occurrence probability. Then, in the term-term autocorrelation matrix AAT, the two rows and columns corresponding to these terms will be nearly identical. Therefore, there is a very small eigenvalue corresponding to this pair - presumably the smallest eigenvalue of AAT. The corresponding eigenvector will be a vector with 1 and -1 at these terms - that is to say, the dijference of these terms. Intuitively then, this version of LSI will “project out” a very weak eigenvector that corresponds to the presumabIy insignificant semantic differences between the two synonymous terms. This is exactly what one would expect from a method that claims to bring out the hidden semantics of the corpus.
#5 LSI by random projection
A result by Johnson and Lindenstrauss [ll, 191 states that if points in a vector space are projected to a random subspace of suitably high dimension, then the distances between the points are approximately preserved. Although such a random projection can be used to reduce the dimension of the document space, it does not bring together semanticallv related documents. LSI on the . other hand seems to achieve the latter, but its computation time is a bottleneck. This naturally suggests the following two-step approach:
1. Apply a random projection to the initial corpus to I dimensions, for some small 1 > k, to obtain, with high probability, a much smaller representation, which is still very close (in terms of distances and angles) to the original corpus.
2, Apply rank O(k) LSI (because of the random projection, the number of singular values kept may have to be increased a little).
In this section we establish that the above approach works, in the sense that the final representation is very close to what, we would get by directly applying LSI.
Another way to view this result is that random projection gives us a fast way to approximate the eigenspace (eigenvalues, eigenvectors) of a matrix.
We first state the Johnson-Lindenstrauss lemma. Lemma 2 r(Johnson and Lindenstrauss, see 111, 191.) Let v E R be a unit vector, let H be a random Idimensional subspace through the origin, and let the random variable X denote the square of the length of the projection of v onto H. Suppose 0 < E < 3, and 24106 n < I< fi. Then, E[X] = l/n, and Pr(lX - l/n1 > d/n) < 2&e’(‘-‘)c1’4. Using the above result, we can infer that with high probability, all pairwise Euclidean distances are approximately maintained under projection to a random subspace, By choosing 1 to be Q(y) in Lemma 2, with high probability the projected vectors, after scaling by a factor &ji, {vi), satisfy Ilvi - V,ll2(1 - C) < 114 - 74112 I 11% - Vj[IZ(l+ E)- Similarly inner products are also preserved approximately: 2Vi * VI satisfy = vp + vj” - (Vi - v,)?. So the projected vectors 2Vi ‘Vi < (VT +VT)(l+ E) - (Vi -Vj)‘(l -t) Therefore, v~*v: 5 vi*vj(l-~)+c(v:+~,‘). In particular, if the vi’s are all of length at most 1, then any inner product vi - v, changes by at most 26. Consider again the term-document, matrix A generated by our corpus model. Let R be a random columnorthonormal matrix with n rows and 1 columns, used to project A down to an l-dimensional space. Let B = In RTA be the matrix after random projection and scaling, where, iel i=l are the SVD’s of A and B respectively. Lemma 3 Let c be an arbitrary positive constant. If 1 2 cw for a sufficiently large constant c then, for p=l,*..,~ .- r=l J=l The proof of this lemma is given in the appendix. As a corollary we have: Corollary 3 p=l Now our rank 2t approximation to the original matrix as B2k = Acbib: i=l From thii we get. the main result of this section (whose proof also appears in the appendix): Theorem 4 II-4 - BZJ& I IIA - Asll’F t W4ll3
The measure IlA - Akll~ tells us how much of the original matrix is recovered by direct LSI. In other words, the theorem says that the matrix obtained by random projection followed by LSI (expanded to twice the rank) recovers almost as much as the matrix obtained by direct LSI.
What are the computational savings achieved by the twvo-ster, method? Let A be an n x m matrix. The time to corn&e LSI is O(mnc) if A is sparse with about c non-zero entries per column (i.e., c is the average number of terms in a document). The time needed to compute the random projection to 2 dimensions is O(mc1). After the projection, the time to compute LSI is O(m12). So the total time is O(ml(l+ c)). To obtain an E ap proximation we need 1 to be 0( 9). Thus the running time of the two-step method is asymptotically superior: O(m(log’ n + clog n)) compared to O(mnc). In [12], Frieze, Kannan and Vempala propose an alternative way to speed up LSI. They give fast algorithms for finding low rank approximations to an m x n matrix A. They compute an approximate Singular Value Decompositibn from a randomly chosen p& of submatrices of A. For anv eiven fi.e.6. their Monte Carlo algorithm finds the dekgption bfg katrix D of rank at most I; so that IIA - DIIF I IIA - AAIF + WIIF holds with probability at least 1 - 6. (llAll$ is the sum of squares of all the entries of the matrix, and Ak is the best rank t approximation with this measure.) The algorithm takes time polynomial in fi, l/c, log( l/6) only, i.e. independent of m, n. The matrix D can be explicitly constructed from its description in O(nmn) time.
#6 Conclusion and further work
Our corpus model is, from a practical standpoint, rather weak; what would it take to establish theorems for “more realistic” models? Our present work has relied on studying the spectra of the perturbations of block matrices; more generally, we require tools to study the eigenvectom of the linear superposition of several matrices (under some assumptions about the individual matrices). Alternatively, we may think of the spectral properties of random graphs with (a small set of) varying edge probabilities, This appears be the technical bottleneck to furthering our results.
A theoretician’s first reaction to an unexpected (positive or negative) empirical phenomenon is to understand it in terms of mathematical models and rigorously proved theorems; this is precisely what we have tried to do, with substantial if partial success. What we have been able to prove should be seen as a mere indication of what might hold; we expect the true positive properties of LSI to go far beyond the theorems we are proving Jlcrc,
There are several specific technical issues to be pursued, Can Theorem 2 be extended to a model where documents could belong to several topics, or to one whcrc term occurrences are not independent? Also, does LSI address polysemy as spectral techniques of slightly different kind to, see \ IS])? We have seen some evidence that it does handle synonymy.
Theory should ideally go beyond the ez post facto justification of methods and explanation of positive phenomcna, it should point the way to new ways of exploiting them and improving them. Section 5, in which we propose a random projection technique as a way of speeding up LSI (and possibly as an alternative to it), is an attempt in this direction.
Another important role of theory is to unify and generalize, Spectral techniques are not confined to the vector-apace model, neither to the strict context of information retrievah Furthermore, spectral analysis of a similar graph-theoretic model of the world-wide web has been shown experimentally to succeed in identifying topics and to substantially increase precision and recall in web searches [18], as well as in databases of law decisions, service logs and patents [4]. Finally, it is becoming clear that spectral techniques and their theoretical analysis may prove to be key methodologies in many other domains of current interest, such as data mining (using spectral techniques to discover correlations in relational databases [14]) and collaborative filtering (personalizing subscriber preferences and interests) [4]. The rows and columns of A could in general be, mstead of terms and documents, consumers and products, viewers and movies, or components and systems.
We conclude this section with a brief description of a promising alternative, graph-theoretic, corpus model. Suppose that documents are nodes in a graph, and weights on the edges capture conceptual proximity of two documents (for example, this distance matrix could be derived from, or in fact coincide with, AAT). Then a topic is defined implicitly as a subgraph with high conductance [ZO], a concept of connectivity which seems very appropriate in this context. We can prove that, under an assumption similar to c-separability, spectral analysis of the graph can identify the topics in thii model as well (proof omitted from this abstract): Theorem 5 If the corpus consists of k disjoint subgraphs with high conductance, and joined with edges with total weight bounded from above by an E fraction of the distance matrix, then rank-k LSI will discover the subgraphs. 