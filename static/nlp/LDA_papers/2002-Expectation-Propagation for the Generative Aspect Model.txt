Expectation-Propagation for the Generative Aspect Model
T Minka
2002
#Abstract
The generative aspect model is an extension of the multinomial model for text that allows word probabilities to vary stochastically across docu- ments. Previous results with aspect models have been promising, but hindered by the computa- tional difficulty of carrying out inference and learning. This paper demonstrates that the sim- ple variational methods of Blei et al. (2001) can lead to inaccurate inferences and biased learning for the generative aspect model. We develop an alternative approach that leads to higher accuracy at comparable cost. An extension of Expectation- Propagation is used for inference and then em- bedded in an EM algorithm for learning. Exper- imental results are presented for both synthetic and real data sets.
#1 Introduction
Approximate inference techniques, such as variational methods, are increasingly being used to tackle advanced data models. When learning and inference are intractable, approximation can make the difference between a useful model and an impractical model. However, if applied in- discriminately, approximation can change the qualitative behavior of a model, leading to unpredictable and unde- sirable results.
The generative aspect model introduced by Blei et al. (2001) is a promising model for discrete data, and provides an interesting example of the need for good approxima- tion strategies. When applied to text, the model explic- itly accounts for the intuition that a document may have several subtopics or “aspects,” making it an attractive tool for several applications in text processing and information retrieval. As an example, imagine that rather than sim- ply returning a list of documents that are “relevant” to a given topic, a search engine could automatically determine the different aspects of the topic that are treated by each document in the list, and reorder the documents to effi- ciently cover these different aspects. The TREC interac- tive track (Over, 2001) has been set up to help investigate precisely this task, but from the point of view of users in- teracting with the search engine. As an example of the judgements made in this task, for the topic “electric au- tomobiles” (number 247i), the human assessors identified eleven aspects among the documents that were judged to be relevant, having descriptions such as “government fund- ing of electric car development programs,” “industrial de- velopment of hybrid electric cars,” and “increased use of aluminum bodies.”
This paper examines computation in the generative as- pect model, proposing new algorithms for approximate in- ference and learning that are based on the Expectation- Propagation framework of Minka (2001b). Hofmann’s original aspect model involved a large number of param- eters and heuristic procedures to avoid overfitting (Hof- mann, 1999). Blei et al. (2001) introduced a modified model with a proper generative semantics and used vari- ational methods to carry out inference and learning. It is found that the variational methods can lead to inac- curate inferences and biased learning, while Expectation- Propagation gives results that are more true to the model. Besides providing a practical new algorithm for a useful model, we hope that this result will shed light on the ques- tion of which approximations are appropriate for which problems.
The following section presents the generative aspect model, briefly discussing some of the properties that make it at- tractive for modeling documents, and stating the infer- ence and learning problems to be addressed. After a brief overview of Expectation-Propagation in Section 3, a new algorithm for approximate inference in the genera- tive aspect model is presented in Section 4. Separate from Expectation-Propagation, a new algorithm for approximate learning in the generative aspect model is presented in Section 5. Brief descriptions of the corresponding proce- dures with variational methods are included for complete- ness. Section 6 describes experiments on synthetic and real data. Section 6.1 presents a synthetic data experiment us-ing low dimensional multinomials, which clearly demon- strates how variational methods can result in inaccurate inferences compared to Expectation-Propagation. In Sec- tions 6.2 and 6.3 the methods are then compared using doc- ument collections taken from TREC data, where it is seen that Expectation-Propagation attains lower test set perplex- ity. Section 7 summarizes the results of the paper.
#2 The Generative Aspect Model
A simple generative model for documents is the multino- mial model, which assumes that words are drawn one at a time and independently from a fixed word distribution p(w). The probability of a document d having word counts nw is thus
This family is very restrictive, in that a document of length
n is expected to have np(w) occurrences of word w, with little variation away from this number. Even within a ho- mogeneous set of documents, such as machine learning papers, there is typically far more variation in the word counts. One way to accommodate this is to allow the word probabilities p to vary across documents, leading to a hier- archical multinomial model.
This requires us to specify a distribution on p itself, con- sidered as a vector of numbers which sum to one. One natural choice is the Dirichlet distribution, which is conju- gate to the multinomial. Unfortunately, while the Dirichlet can capture variation in the p(w)’s, it cannot capture co- variation, the tendency for some probabilities to move up and down together. At the other extreme, we can sample p from a finite set, corresponding to a finite mixture of multi- nomials. This model can capture co-variation, but at great expense, since a new mixture component is needed for ev- ery distinct choice of word probabilities.
In the generative aspect model, it is assumed that there are A underlying aspects, each represented as a multinomial distribution over the words in the vocabulary. A document is generated by the following process. First, λ is sampled from a Dirichlet distribution D (λ | α), so that  a λa = 1. This determines mixing weights for the aspects, yielding a word probability vector:
The document is then sampled from a multinomial distri- bution with these probabilities. Instead of a finite mixture, this distribution might be called a simplicial mixture, since the word probability vector ranges over a simplex with cor- ners p(w|a = 1),...,p(w|a = A). The probability of a document is where the parameters θ are the Dirichlet parameters αa and the multinomial models p(· | a); ∆ denotes the (A − 1)- dimensional simplex, the sample space of the Dirichlet D (· | α). Because λ is sampled for each document, differ- ent documents can exhibit the aspects in different propor- tions. However, the integral in (3) does not simplify and must be approximated, which is the main complication in using this model.
The two basic computational tasks for this model are:
Inference: Evaluate the probability of a document; i.e., the integral in (3).
Learning: For a set of training documents, find the pa- rameter values θ = (p(· | a), α) which maximize the likelihood; i.e., maximize the value of the integral in (3).
#3 Expectation-Propagation
Expectation-Propagation is an algorithm for approximating integrals over functions that factor into simple terms. The general form for such integrals in our setting is
In previous work each count nw was assumed to be 1 (Minka, 2001b). Here we present a slight generalization to allow real-valued powers on the terms. Expectation- Propagation approximates each term tw(λ) by a simpler term t ̃w(λ), giving a simpler integral
The algorithm proceeds by iteratively applying “dele- tion/inclusion” steps. One of the approximate terms is deleted from q(λ), giving the partial function q\w(λ) = Then a new approximation for tw (λ) is com- puted so that tw (λ) q (λ) is similar to tw (λ) q (λ), in the sense of having the same integral and the same set of specified moments. The moments used in this paper are the mean and variance. The partial function q\w(λ) thus acts as context for the approximation. Unlike variational bounds, this approximation is global, not local, and conse- quently the estimate of the integral is more accurate.
A fixed point of this algorithm always exists, but we may not always reach one. The approximation may oscillate or enter a region where the integral is undefined. We uti- lize two techniques to prevent this. First, the updates are “damped” so that t ̃w(λ) cannot oscillate. Second, if a deletion-inclusion step leads to an undefined integral, the step is undone and the algorithm continues with the next term.
#4 Inference
This section describes two algorithms for approximating the integral in (3): variational inference used by Blei et al. (2001) and Expectation-Propagation.
#4.1 Variational inference
To approximate the integral of a function, variational in- ference lower bounds the function and then integrates the lower bound. A simple lower bound for (3) comes from Jensen’s inequality. The bound is parameterized by a vec- tor q(a | w): or “responsibility” of word w to the aspects. Given bound parameters q(a | w) for all a and w, the integral is now an- alytic:
Laplace’s method using a softmax transformation, vari- ational inference, and two different Monte Carlo algo- rithms.
EP gives an integral estimate as well as an approximate posterior for the mixture weights. For the generative aspect model, the approximate posterior will be Dirichlet, and the integrand will be factored into terms of the form which resembles a Dirichlet with parameters βwa. Thus, the approximate posterior is given by
The best bound parameters are found by maximizing the value of the bound. A convenient way to do this is with EM. The “parameter” in the algorithm is q(a | w) and the “hidden variable” is λ:
E-step: γa = αa+ nwq(a|w) (10) w
M-step: q(a|w) ∝ p(w|a)exp(Ψ(γa)) (11)
where Ψ is the digamma function. Note that this is the same as the variational algorithm for mixture weights given by Minka (2000). The variables γa used in this algorithm can be interpreted as defining an approximate Dirichlet poste- rior on λ: D (λ | γ).
#4.2 Expectation-Propagation
As mentioned above, the aspect model integral is the same as marginalizing the weights of a mixture model whose components are known. This problem was studied in depth by Minka (2001b) and it was found that Expectation- Propagation (EP) provides the best results, compared to
loop w = 1,...,W:
(a) Deletion. Remove t ̃w from the posterior to get an “old”
(d) Inclusion. Incorporate t ̃w back into q(λ) by scaling the change in β:
γ = γold+n(β −βold) (21) a a w wa wa
This preserves the invariant (16). If any γa < 0, undo all changes and skip this word.
In our experience, words are skipped only on the first few iterations, before EP has settled into a decent approxima- tion. It can be shown that the safest stepsize for (c) is μ = 1/nw, which makes γ = γ′. This is the value used in the experiments, though for faster convergence a larger μ is often acceptable.
After convergence, the approximate posterior gives the fol- lowing estimate for the likelihood of the document, thus approximating the integral (3): estimates from the previous section and (2) a new approach based on approximative EM. The decision between these two approaches is separate from the decision of using vari- ational inference versus Expectation-Propagation.
#5 Learning
Given a set of documents C = {di,i = 1,...,n}, with word counts denoted niw, the learning problem is to max- imize the likelihood as a function of the parameters θ = (p(· | a), α); the likelihood is given by Notice that each document has its own integral over λ. It is tempting to use EM for this problem, where we regard λ as a hidden variable for each document. However, the E-step requires expectations over the posterior for λ, p(λ | d , θ), which is an intractable distribution. This section describes two alternative approaches: (1) maximizing the likelihood estimates from the previous section and (2) a new approach based on approximative EM. The decision between these two approaches is separate from the decision of using vari- ational inference versus Expectation-Propagation.
#5.1 Maximizing the estimate
Given that we can estimate the likelihood function for each document, it seems natural to try to maximize the value of the estimate. This is the approach taken by Blei et al. (2001). For the variational bound (8), the maximum with respect to the parameters is obtained at 
Of course, once the aspect parameters are changed, the op- timal bound parameters q(a | w) also change, so Blei et al. (2001) alternate between optimizing the bound and apply- ing these updates. This can be understood as an EM algo- rithm where both λ and the ‘aspect assignments’ are hidden variables. The aspect parameters at convergence will result in the largest possible variational estimate of the likelihood. The same approach could be taken with EP, where we find the parameters that result in the largest possible EP esti- mate of the likelihood. However, this does not seem to be as simple as in the variational approach. It also seems mis- guided, because an approximation which is close to the true likelihood in an average sense need not have its maximum close to the true maximum.
#5.2 Approximative EM
The second approach is to use an approximative EM algo- rithm, sometimes called “variational EM,” where we use expectations over an approximate posterior for λ, call it qi(λ). The inference algorithms in the previous section conveniently give such an approximate posterior. The E- step will compute qi(λ) for each document, and the M- step will maximize the following lower bound to the log- likelihood:
This decouples into separate maximization problems for α and p(w | a). Given that qi (λ) is Dirichlet with parameters γia, the optimization problem for α is to maximize
which is the standard Dirichlet maximum-likelihood prob- lem (Minka, 2001a).
By zeroing the derivative with respect to p(w | a), we obtain the M-step
This requires approximating another integral over λ. Up- date (27) is equivalent to assuming that λa is constant, at the value exp(Ψ(γa)) (from (11)). A more accurate ap- proximation can be obtained by Taylor expansion, as de- scribed in the appendix. The resulting update is
#6 Experimental Results
This section presents the result of experiments carried out on synthetic and real data. The first experiments involve a “toy” data set where the aspects are multinomials over a two word vocabulary. Later experiments use documents from two TREC collections.
#6.1 Results on Synthetic Data
This section elucidates the difference between variational inference (VB) and Expectation Propagation (EP) using simple, controlled datasets. The algorithms mainly differ in how they approximate (3), thus it is helpful to consider two extremes: (Exact) the exact value of (3) versus (Max) approximating by the maximum over λ: only serve to restrict the domain of the word probabilities p(w) = a λap(w | a). To maximize likelihood, we would want the domain to be as large as possible—the as- pects as extreme and distinct as possible. However, when using the exact value of (3), all choices of λ contribute, which favors a domain that only includes word probabili- ties matching the frequencies in the documents. In exper- iments, we find that VB behaves like the Max approxima- tion while EP behaves like the exact value.
This update can be used with the γ’s found by either VB or EP. When running EP with the new parameter values, the β’s can be started from their previous values, so that only a few EP iterations are required.
Consider a simple scenario in which there are only two words in the vocabulary, w=1 and w=2. This allows each aspect to be represented by one parameter p(w=1 | a), since p(w=2 | a) = 1 − p(w=1 | a). Let there be two aspects, a=1 and a=2, with α1=α2=1, so that D(λ|α) is uni- form. This means that the probability of word 1 in the doc- ument collection varies uniformly between p(w=1|a=1) and p(w=1 | a=2). Learning the aspects from data amounts to estimating the endpoints of this variation.
Let p(w=1 | a=2) = 1 so that the only free parameter is p(w=1 | a=1). Ten training documents of length 10 are generated from the model with p(w=1 | a=1) = 0.5. Figure 1 (top) shows the typical result. When we ap- ply the Max approximation, each document i wants to choose p(w=1) (between p(w=1 | a=1) and p(w=1 | a=2)) to match its frequency of word 1: ni1/ni. Any choice of (p(w=1|a=1),p(w=1|a=2)) which spans these frequen- cies will maximize likelihood, e.g. p(w=1 | a=1) = 0, p(w=1 | a=2) = 1. The exact likelihood, by contrast, peaks near the true value of p(w=1 | a=1). As the number of train- ing documents increases, the exact likelihood gets sharper around the true value, but the Max approximation gets far- ther away from the truth, because the observed frequencies exhibit more variance.
As shown in Figure 1 (bottom), VB behaves similarly to Max. The solid curve is the exact likelihood, generated by computing the probability of the training documents for all p’s on a fine grid. The dashed curve is the VB estimate of the likelihood, scaled up to make its shape visible on the plot, for the same p’s. The dot-dashed curve is the EP estimate of the likelihood for the same p’s. EP clearly gives a better approximation.
The parameter estimates are indicated by vertical lines. The dashed vertical line corresponds to Blei et al.’s algorithm, which as expected converges to the maximum of the VB curve. The solid line is the result of VB combined with the EM update of Section 5.2; as expected, it is closer to the true maximum. The dot-dash vertical line is the result of applying EM using EP and is closest to the true maximum.
To demonstrate the difference between the algorithms in the multidimensional case, 100 documents of length 100 were generated from a simple multinomial model with five words having equal probability. A generative aspect model was fit using three aspects with α1=α2=α3=1. The EP so- lution correctly chose all aspects to be similar to the gener- ating multinomial; all probabilities were between 0.15 and 0.24. The VB solution is quite different; it chose the ex- treme parameters shown in the following table (rounded to the tenths place):
Resampling the training documents gives similar results. In terms of convergence rate, learning typically converged after 150 parameter updates with EP, while over 1,000 up- dates were required with VB.
Interestingly, on an independent set of 1000 test docu- ments, the perplexity of the model learned by EP is 5.0 while the VB model’s is 5.1, a seemingly trivial differ- ence. This is because the perplexity measure, as used by Blei et al., focuses on per-word prediction rather than per- document prediction. As long as there exists a mixture of the aspects which matches the word probabilities in the document, the perplexity will be low. Indeed, if the above VB aspects are evenly mixed, the correct word distribution is produced.
To show that there really is a difference between the mod- els, a synthetic classification problem was constructed. One class had documents sampled from a uniform multinomial over five words. The other class had documents sampled from a multinomial with word probabilities [1 2 3 4 5]/15. There were 50 documents of length 50 in each class. A three-aspect model was trained on each class and test doc- uments (of the same length) were classified according to highest class-conditional probability. The EP models com- mitted 76/2000 errors while the VB models committed 163/2000, which is both statistically and practically signif- icant. As above, EP learned the correct models while VB chose extreme probabilities.
#6.2 Controlled TREC Data
In order to compare variational inference and Expectation- Propagation on more realistic data, a corpus was created by mixing together TREC documents on known topics. From the 1989 AP data on TREC disks 1 and 2, we extracted all of the documents that were judged to be relevant to one of the following six topics:
Synthetic documents were created by first drawing three topics randomly, with replacement, from the above list of six. A random document from each topic was then se- lected, and the three documents were concatenated together to form a synthetic document containing either one, two or three different “aspects.” A total of 200 documents were generated in this manner. used to train aspect models using both EP and VB, fixing the number of aspects at six; 75% of the data was used for training, and the remaining 25% was used as test data.
Figure 2 shows the top words for each aspect for the EP- trained model. Because likelihood is used as the objective function, the common, “content-free” words take up a sig- nificant portion of the probability mass—a fact that is of- ten not acknowledged in descriptions of aspect models. As seen in this figure, the aspects model variations across doc- uments in the distribution of common words such as SAID, FOR, and WAS. After filtering out the common words from the list, by not displaying words having a unigram proba- bility larger than a threshold of 0.001, the most probable words that remain clearly indicate that the true underlying aspects have been captured, though some more cleanly than others. For example, aspect 1 corresponds to topic 142, and aspect 5 corresponds to topic 59.
The models are compared quantitatively using test set per- plexity, exp(−( i log p(di))/  i |di|); lower perplexity is better. The probability function (3) cannot be computed analytically, and we do not want to favor either of the two approximations, so we use importance sampling to com- pute perplexity. In particular, we sample λ from the approximate posterior D (λ | γ) obtained from EP.
Figure 3 shows the test set perplexities for VB and EP; the perplexity for the EP-trained model is consistently lower than the perplexity of the VB-trained model. Based on the results of Section 6.1, we anticipate that for VB the as- pects will be more extreme and specialized. This would make the Dirichlet weights αa smaller for the specialized aspects, which are used infrequently, and larger for the as- pects that are used in different topics or that are devoted to the common words. Plots of the Dirichlet parameters (Fig- ure 3, center and right) show that VB results in αas that are indeed more spread out towards these extremes, compared with those obtained using EP.
#6.3 TREC Interactive Data
To compare VB and EP on real data having a mixture of aspects, this section considers documents from the TREC interactive collection (Over, 2001). The data used for this track is interesting for studying aspect models because the relevant documents have been hand labeled according to the specific aspects of a topic that they cover. Here we simply evaluate perplexities of the models.
We extracted all of the relevant documents for each of the six topics that the collection has relevance judgements for, resulting in a set of 772 documents. The average docu- ment length is 594 tokens, and the total vocabulary size is 26,319 words. As above, 75% of the data was used for training, and the remaining 25% was used for evaluating perplexities. In these experiments the speed of VB and EP are comparable.
Figure 3 shows the test set perplexity and Dirichlet param- eters αa for both EP and VB, trained using A = 10 aspects. As for the controlled TREC data, EP achieves a lower per- plexity, and has aspects that are more balanced compared to those obtained using VB. We suspect that the perplex- ity difference on both the TREC interactive and controlled TREC data is small because the true aspects have little overlap, and thus the posterior of the mixing weights is sharply peaked.
#7 Conclusions
The generative aspect model provides an attractive ap- proach to modeling the variation of word probabilities across documents, making the model well suited to in- formation retrieval and other text processing applications. This paper studied the problem of approximation meth- ods for learning and inference in the generative aspect model, and proposed an algorithm based on Expectation- Propagation as an alternative to the variational method adopted by Blei et al. (2001). Experiments on synthetic data showed that simple variational inference can lead to in- accurate inferences and biased learning, while Expectation- Propagation can lead to more accurate inferences. Exper- iments on TREC data show that Expectation-Propagation achieves lower test set perplexity. We attribute this to the fact that the Jensen bound used by the variational method is inadequate for representing how ‘peaky’ versus ‘spread out’ is the posterior on λ, which happens to be crucial for good parameter estimates. Because there is a separate λ for each document, this deficiency is not minimized by ad- ditional documents, but rather compounded.
This synthetic collection thus simulates a set of retrieved documents to answer a query, which we then wish to ana- lyze in order to extract the aspect structure. The data was