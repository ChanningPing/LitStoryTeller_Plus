Probabilistic Latent Semantic Indexing
T Hofmann
1999
#Abstract
Probabilistic Latent Semantic Indexing is a novel approachto automated document indexing which is based on a statisticallatent class model for factor analysis of count data.Fitted from a training corpus of text documents by a generalizationof the Expectation Maximization algorithm, theutilized model is able to deal with domain{specic synonymyas well as with polysemous words. In contrast to standardLatent Semantic Indexing (LSI) by Singular Value Decomposition,the probabilistic variant has a solid statistical foundationand denes a proper generative data model. Retrievalexperiments on a number of test collections indicate substantialperformance gains over direct term matching methodsas well as over LSI. In particular, the combination ofmodels with dierent dimensionalities has proven to be advantageous.
#1 Introduction
With the advent of digital databases and communicationnetworks, huge repositories of textual data have becomeavailable to a large public. Today, it is one of the greatchallenges in the information sciences to develop intelligentinterfaces for human{machine interaction which supportcomputer users in their quest for relevant information.Although the use of elaborate ergonomic elements likecomputer graphics and visualization has proven to be extremelyfruitful to facilitate and enhance information access,progress on the more fundamental question of machine intelligenceis ultimately necessary to ensure substantial progresson this issue. In order for computers to interact more nat- ural ly with humans, one has to deal with the potential ambivalence,impreciseness, or even vagueness of user requests,and has to recognize the dierence between what a usermight say or do and what she or he actually meant or intended.One typical scenario of human{machine interaction in informationretrieval is by natural language queries: the userformulates a request, e.g., by providing a number of key- words or some free-form text, and expects the system to return the relevant data in some amenable representation, e.g., in form of a ranked list of relevant documents. Manyretrieval methods are based on simple word matching strategiesto determine the rank of relevance of a document withrespect to a query. Yet, it is well known that literal termmatching has severe drawbacks, mainly due to the ambivalenceof words and their unavoidable lack of precision as wellas due to personal style and individual dierences in wordusage.
One typical scenario of human{machine interaction in informationretrieval is by natural language queries: the userformulates a request, e.g., by providing a number of key- words or some free-form text, and expects the system toreturn the relevant data in some amenable representation,e.g., in form of a ranked list of relevant documents. Manyretrieval methods are based on simple word matching strategiesto determine the rank of relevance of a document withrespect to a query. Yet, it is well known that literal termmatching has severe drawbacks, mainly due to the ambivalenceof words and their unavoidable lack of precision as wellas due to personal style and individual dierences in wordusage.
Latent Semantic Analysis (LSA) [1] is an approach toautomatic indexing and information retrieval that attemptsto overcome these problems by mapping documents as wellas terms to a representation in the so{called latent semanticspace. LSA usually takes the (high dimensional) vectorspace representation of documents based on term frequencies[14] as a starting point and applies a dimensionreducing linear pro jection. The specic form of this mappingis determined by a given document collection and isbased on a Singular Value Decomposition (SVD) of the correspondingterm/document matrix. The general claim isthat similarities between documents or between documentsand queries can be more reliably estimated in the reducedlatent space representation than in the original representation.The rationale is that documents which share frequentlyco-occurring terms will have a similar representation in thelatent space, even if they have no terms in common. LSAthus performs some sort of noise reduction and has the potentialbenet to detect synonyms as well as words that referto the same topic. In many applications this has proven toresult in more robust word processing.
Although LSA has been applied with remarkable successin dierent domains including automatic indexing (LatentSemantic Indexing, LSI) [1, 3], it has a number of decits,mainly due to its unsatisfactory statistical foundation. Theprimary goal of this paper is to present a novel approachto LSA and factor analysis { called Probabilistic Latent SemanticAnalysis (PLSA) { that has a solid statistical foundation,since it is based on the likelihood principle and de-nes a proper generative model of the data. This implies inparticular that standard techniques from statistics can beapplied for questions like model tting, model combination,and complexity control. In addition, the factor representationobtained by PLSA allows to deal with polysemous words and to explicitly distinguish between dierent meaningsand dierent types of word usage.
#2 The Aspect Model
The core of PLSA is a statistical model which has been called aspect model [7, 15]. The latter is a latent variable model for general co-occurrence data which associates an unobserved class variable z 2 Z = fz1 ;:::;zKg with each observation, i.e., with each occurrence of a word w 2W = fw1;:::;wMg in a document d2D=fd1 ;:::;dN g. In terms of a generative model it can be dened in the following way:
 select a document d with probability P (d),
 pick a latent class z with probability P (zjd),
 generate a word w with probability P (wjz).
As a result one obtains an observed pair (d; w), while the latent class variable z is discarded.
Translating this process into a joint probability model results in the expression.
Essentially, to derive (2) one has to sum over the possible choices of z which could have generated the observation. The aspect model is a statistical mixture model [9] which is based on two independence assumptions: First, observation pairs (d; w) are assumed to be generated independently; this essentially corresponds to the `bag{of{words' approach. Secondly, the conditional independence assumption is made that conditioned on the latent class z, words w are generated independently of the specic document identity d. Given that the number of states is smaller than the number of documents (K  N), z acts as a bottleneck variable in predicting w conditioned on d. 
Notice that in contrast to document clustering models document{specic word distributions P (wjd) are obtained by a convex combination of the aspects or factors P (wjz). Documents are not assigned to clusters, they are characterized by a specic mixture of factors with weights P (zjd). These mixing weights oer more modeling power and are conceptually very dierent from posterior probabilities in clustering models and (unsupervised) naive Bayes models (cf. [7]).
Following the likelihood principle, one determines P (d), P (zjd), and P (wjz) by maximization of the log{likelihood function 
where n(d; w) denotes the term frequency, i.e., the number of times w occurred in d. It is worth noticing that an equivalent symmetric version of the model can be obtained by inverting the conditional probability P (zjd) with the help of Bayes' rule, which results in:
This is just a re-parameterized version of the generative model described by (1), (2).
#3 Model Fitting with Tempered EM
The standard procedure for maximum likelihood estimation in latent variable models is the Expectation Maximization (EM) algorithm [2]. EM alternates two steps: (i) an expectation (E) step where posterior probabilities are computed for the latent variables z, based on the current estimates of the parameters, (ii) an maximization (M) step, where parameters are updated for given posterior probabilities computed in the previous E{step.
For the aspect model in the symmetric parameterization Bayes' rule yields the E{step which is the probability that a word w in a particular docu- ment or context d is explained by the factor corresponding to z. By standard calculations one arrives at the following M{step re-estimation equations.
Alternating (5) with (6){(8) denes a convergent procedure that approaches a local maximum of the log{likelihood in (3). 
So far we have focused on maximum likelihood estimation or, equivalently, word perplexity reduction. One has, however, to distinguish between the predictive performance of the model on training data and the expected performance on unseen test data. In particular, it is to naive to assume that a model will generalize well on new data just based on the fact that it might achieve low perplexity on training data. To derive conditions under which generalization on unseen data can be guaranteed is actually the fundamental problem of statistical learning theory. Here, we propose a generalization of maximum likelihood for mixture models { called tempered EM (TEM) { which is based on entropic regularization and is closely related to a method known as deterministic annealing [13]. 
Since a principled derivation of TEM is beyond the scope of this paper (the interested reader is referred to [12, 7]), we will present the necessary modication of standard EM in an ad hoc manner. Essentially, one introduces a control parameter  (inverse computational temperature) and modies the E-step in (5) according to Notice that  = 1 results in the standard E{step, while for  < 1 the likelihood part in Bayes' formula is discounted (additively on the log{scale). 
It can be shown, that TEM minimizes an ob jective function known as the free energy [11] and hence denes a convergent algorithm. While temperature{based generalizations of EM and related algorithms for optimization are often used as a homotopy or continuation method to avoid unfavorable local extrema, the main advantage of TEM in our context is to avoid overtting. Somewhat contrary to the spirit of annealing as a continuation method we propose to utilize (9) to temper EM by \heating". In order to determine the optimal value of  we propose to make use of some held{out portion of the data. This idea can be implemented by the following scheme:
i. Set  1 and perform EM until the performance on held{out data deteriorates (early stopping).
ii. Decrease , e.g., by setting   with some rate parameter  < 1.
iii. As long as the performance on held-out data improves continue TEM iterations at this value of .
iv. Stop on , i.e., stop when decreasing  does not yield further improvements, otherwise goto step (ii).
v. Perform some nal iterations using both, training and held-out data.
In our experiments, the typical number of iterations TEM performed starting from randomized initial conditions was , where each iteration requires one pass through the data, i.e., of the order of R  K arithmetical operations.
#4 Probabilistic Latent Semantic Analysis
#4.1 Latent Semantic Analysis
As mentioned in the introduction, the key idea of LSA [1] is to map documents (and by symmetry terms) to a vector space of reduced dimensionality, the latent semantic space. This mapping is computed by decomposing the term/document matrix N with SVD, N = UVt , where U and V are orthogonal matrices UtU = VtV = I and the diagonal matrix  contains the singular values of N. The LSA approximation of N is computed by thresholding all but the largest K singular values in  to zero (= ~ ), which is rank K optimal in the sense of the L2 -matrix norm as is well-known from linear algebra, i.e., one obtains the approximation N~ = UV~ t  UVt = N. Note that the L2{norm approximation does not prohibit entries of N~ to be negative.
#4.2 Geometry of the Aspect Model
Now consider the class-conditional multinomial distributions P (jz) over the vocabulary in the aspect model which can be represented as points on the M 1 dimensional simplex of all possible multinomials. Via its convex hull, this set of K points denes a K 1 dimensional sub-simplex. The modeling assumption expressed by (2) is that all conditional distributions P (jd) are approximated byamultinomial representable as a convex combination of the class-conditionals P (jz). In this geometrical view, the mixing weights P (zjd) correspond exactly to the coordinates of a document in that sub-simplex. A simple sketch of the geometry is shown in Figure 1. This demonstrates that despite of the discreteness of the latent variables introduced in the aspect model, a continuous latent space is obtained within the space of all multinomial distributions. Since the dimensionality of the sub-simplex is K 1 as opposed to M 1 for the complete probability simplex, this can also be thought of in terms of dimensionality reduction and the sub-simplex can be identi ed with a probabilistic latent semantic space.
#4.3 Mixture Decomposition vs. Singular Value Decomposition
To stress this point and to clarify the relation to LSA, let us rewrite the aspect model as parameterized by (4) in matrix notation. Hence dene matrices by U^ = (P (di jzk))i;k , V^ = (P (wjjzk ))j;k , and ^ = diag(P (zk ))k . The joint probability model P can then be written as a matrix product P = U^ ^V^ t . By comparing this decomposition with the SVD decomposition in LSA, one can point out the following re-interpretation of concepts of linear algebra:
i. The weighted sum over outer products between rows of U^ and V^ reects conditional independence in PLSA.
ii. The left/right eigenvectors in SVD are seen to correspond to the factors P (wjz) and the component distribution P (djz) of the aspect model.
iii. The mixing proportions P (z) in PLSA substitute the singular values of the SVD in LSA.
Despite this similarity, there is also a fundamental difference between PLSA and LSA, which is the ob jective function utilized to determine the optimal decomposition/approximation. In LSA, this is the L2{norm or Frobenius norm, which corresponds to an implicit additive Gaussian noise assumption on counts. In contrast, PLSA relies on the likelihood function of multinomial sampling and aims at an explicit maximization of the predictive power of the model. On the modeling side this oers important advantages, for example, the mixture approximation P of the cooccurrence table is a well-dened probability distribution and factors have a clear probabilistic meaning in terms of mixture component distributions.
#4.4 Kullback{Leibler Projection vs. Orthogonal Projection
Returning to the geometrical view of the aspect model as sketched in Figure 1, it is interesting to reveal the pro jection principle which is implicitly used in the aspect model. Rewriting the log{likelihood in (3) one arrives at The rst term in brackets corresponds to the negative Kullback{Leibler (KL) divergence (or cross{entropy) between the empirical distribution of words in a document P^(wjd)  n(d; w)=n(d) and the model distribution P (wjd). For xed factors P (wjz) maximizing the log{likelihood w.r.t the mixing proportions P (zjd) thus amounts to projecting P^ (wjd) on the subspace spanned by the factors based on the KL{divergence. This is very dierent from any type of squared deviation which would result in an orthogonal pro jection (cf. [10] for more details on the geometry of statistical models).
#4.5 Factor Representation: An Example
In order to visualize the factor solution found by PLSA we present an elucidating example. We have performed experiments with the TDT-1 collection, which contains 15,862 documents of broadcast news stories [8].1 Stop words have been eliminated by a standard stop word list, no stemming or further preprocessing has been performed. Table 1 shows a reduced representation of 4 factors from a 128 factor solution.
The rst two factors have been selected as the ones with the highest probablity to generate the word \ight", the last two factors have the highest probability to generate the word \love". It is interesting to see that the rst two factors indeed capture two dierent types of usage for the term \ight": ights with planes and ights with space ships/shuttles. Similarly the last two factors capture two distinguishable contexts in which the word \love" occurs in the TDT-1 collection: real love in the context of family life as opposed to staged love in the sense of \Hollywood".
#4.6 Folding-In Queries
Folding-in refers to the problem of computing a representation for a document or query that was not contained in the original training collection. In the LSA approach, this is simply done by a linear mapping that eectively represents a document or query by the center of its constituent terms (with an appropriate term weighting) [1]. In PLSA, mixing proportions can be computed by EM iteration, where the factors are xed such that only the mixing proportions P (zjq) are adapted in each M{step.
Table 2 shows some more factors for the TDT-1 collection which clearly reect the vocabulary dealing with certain events: the war in Bosnia and Iraq, the crisis in Rwanda, and the earthquake in Kobe. Based on this four factors, we have computed a representation for a test query consisting of the terms \aid", \food", \medical", \people", \UN", and \war". Figure 2 visualizes the evolution of the posterior probabilities and the mixing proportions in the course of the EM procedure. The query has been designed such that only the \Rwanda" factor is matching all query terms (e.g., the UN was not involved in the Kobe earthquake, there was no medical aid provided for the Iraq during the Gulf war, etc.). As can be seen this factor has indeed the highest weight after the rst iteration, but notice that the other factors still account for more than half of the probability. However this changes after some EM iterations, since the aspect model introduces feedback between the terms. For example, although a term like \UN" would by itself be best explained by the \Bosnia" factor, the context of the other query terms drastically increases the probability that this particular occurrence of \UN" is related to the events in Rwanda. The same mechanism is able to detect \true" polysems [6].
#5 Probabilistic Latent Semantic Indexing
#5.1 Vector-Space Models and LSI
One of the most popular families of information retrieval techniques is based on the Vector{Space Model (VSM) for documents [14]. A VSM variant is characterized by three ingredients: (i) a transformation function (also called local term weight), (ii) a term weighting scheme (also called global term weight), and (iii) a similarity measure. In our experi- ments we have utilized (i) a representation based on the (untransformed) term frequencies (tf ) n(d; w) which has been combined with (ii) the popular inverse document frequency (idf ) term weights, and the (iii) standard cosine matching function. The same representation applies to queries q such that the matching function for the baseline methods can be written as s(d; q) = P w n^(d; w)^n(q; w) pP w n^(d; w)2pP w n^(q; w)2 ; (11) where ^n(d; w) = idf(w)  n(d; w) are the weighted word frequencies.
In latent semantic indexing, the original vector space representation of documents is replaced by a representation in the low{dimensional latent space and the similarity is computed based on that representation. Queries or documents which were not part of the original collection can be folded in by a simple matrix multiplication (cf. [1] for details). In our experiments, we have actually considered linear combinations of the original similarity score (11) (weight ) and the one derived from the latent space representation (weight 1 ), as suggested in [3] (cf. [16] for a more detailed empirical investigation of linear combination schemes for information retrieval systems).
#5.2 Variants of Probabilistic Latent Semantic Indexing
Two dierent schemes to exploit PLSA for indexing have been investigated: (i) as a context{dependent unigram model to smoothen the empirical word distributions in docu- ments (PLSI-U), (ii) as a latent space model which provides a low{dimensional document/query representation (PLSIQ):
PLSI-U For each document d in the collection, PLSA provides a multinomial distribution P (wjd) over the vocabulary as given by (2). This distribution will in general be a smooth version of the empirical distribution P^(wjd) = n(d; w)=n(d). We propose to utilize P (wjd) (thought of as a document vector) in order to compute a matching score between a docu- ment an a query. Notice that P (wjd) is a representation in the original (word) space obtained by back{pro jection from the probabilistic latent space. The vector P (jd) can (optionally) be weighted with the inverse document frequencies and compared with the (weighted) query by the cosine.2 We have considered two ways of combining PLSAU with the standard VSM: (i) by linearly combining the cosine similarities as discussed above for LSI, and (ii) by additively combining the multinomials like in interpolation methods for language modeling, i.e., by using the representation P~(wjd) = P^(wjd) + (1 )P (wjd). Both methods have empirically shown almost identical performance and we will only report results of variant (i), because this scheme has also been used in the case of LSI.
PLSI-Q In this scheme we use the low{dimensional representation P (zjd) and P (zjq) to evaluate similarities. Therefore, queries have to be folded in, which is done by xing the P (wjz) parameters and calculating weights P (zjq) by TEM. How to optimally take into account (global) term weights in PLSI-Q is an only partially resolved problem. We have used the ad hoc approach to reweight the dierent model components by the quantities P w P (wjz)  idf(w), but this may not make optimal use of the term weight priors.
One advantage of using statistical models vs. SVD techniques is that it allows us to systematically combine dierent models. While this should optimally be done according to a Bayesian model combination scheme, we have utilized a much simpler approach in our experiments which has nevertheless shown excellent performance and robustness. In the PLSI-U we have combined the probability estimates P (wjd) for models with dierent number of components K additively with uniform weights. In the PLSI-Q scheme, we have simply combined the cosine scores of all models with a uniform weight. The resulting methods are referred to as PLSI-U and PLSI-Q , respectively. Empirically we have found the performance to be very robust w.r.t. dierent (non-uniform) weights and also w.r.t. the {weight used in combination with the original cosine score. This is due to the noise reducing benets of (model) averaging. Notice that LSA representations for dierent K form a nested sequence, which is not true for the statistical models which are expected to capture a larger variety of reasonable decompositions.
#6 Experimental Results
The performance of PLSI has been systematically compared with the standard term matching method based on the raw term frequencies (tf ) and their combination with the inverse document frequencies (tdf ), as well as with LSI. We have utilized the following four medium{sized standard document collection: (i) MED (1033 document abstracts from the National Library of Medicine), (ii) CRAN (1400 document abstracts on aeronautics from the Craneld Institute of Technology), (iii) CACM (3204 abstracts from the CACM journal), and (iv) CISI (1460 abstracts in library science from the Institute for Scientic Information). The condensed results in terms of average precision recall (at the 9 recall levels 10% 90%) are summarized in Table 3. A selection of average precision recall curves can be found in Figure 3.
Here are some details of the experimental setup: PLSA models at K = 32; 48; 64; 80; 128 have been trained by TEM for each data set with 10% held{out data. For PLSIU/PLSI-Q we report the best result obtained by any of these models, for LSI we report the best result obtained for the optimal dimension (exploring 32{512 dimensions at a step size of 8). The combination weight  with the cosine baseline score has been coarsely optimized by hand, MED, CRAN:  = 1=2, CACM, CISI:  = 2=3; in general slightly smaller weights have been utilized for the combined models.
The experiments consistently validate the advantages of PLSI over LSI. Substantial performance gains have been achieved for all 4 data sets and both term weighting schemes. In particular, PLSI-Q/PLSI-Q work particularly well on the raw term frequencies, where LSI on the other hand may even fail completely (in accordance with the results reported in [1]). We explain this by the fact that large frequencies dominate the squared error deviation used in SVD and a dampening (e.g., by idf weighting) is necessary to get a reasonable decomposition of the term/document matrix. Since PLSI-Q can not take much advantage from the term weighting scheme, PLSI-U/PLSI-U performs slightly better in this case. We suspect that even better results could be achieved by an improved integration of term weights in PLSI-Q. The benets of model combination are also very substantial. In all cases the (uniformly) combined model performed better than the best single model. As a sighte ect, model averaging also deliberates from selecting the \optimal" model dimensionality.
In terms of computational complexity, despite of the iterative nature of EM, the computing time for TEM model tting at K = 128 was roughly comparable to SVD in a standard implementation. For larger data sets one may also consider speeding up TEM by on-line learning [11]. Notice that the PLSI-Q scheme has the advantage that documents can be represented in a low{dimensional vector space (as in LSI), while PLSI-U requires the calculation of the high{ dimensional multinomials P (wjd) which oers advantages in terms of the space requirements for the indexing information that has to be stored.
Finally, we have also performed an experiment to stress the importance of tempered EM over standard EM{based model tting. Figure 4 plots the performance of a 128 factor model trained on CRAN in terms of perplexity and in terms of precision as a function of . It can be seen that it is crucial to control the generalization performance of the model, since the precision is inversely correlated with the perplexity. In particular, notice that the model obtained by maximum likelihood estimation (at  = 1) actually deteriorates the retrieval performance.
#7 Conclusion and Outlook
We have presented a novel method for automated indexing based on a statistical latent class model. This approach has important theoretical advantages over standard LSI, since it is based on the likelihood principle, denes a generative data model, and directly minimizes word perplexity. It can also take advantage of statistical standard methods for model tting, overtting control, and model combination. The empirical evaluation has clearly conrmed the benets of Probabilistic Latent Semantic Indexing which achieves signi cant gains in precision over both, standard term matching and LSI. Further investigation is needed to take full advantage of the prior information provided by term weighting schemes. Recent work has also shown that the benets of PLSA extend beyond document indexing and that a similar approach can be utilized, e.g., for language modeling [4] and collaborative ltering [5].