{"paragraph_scenes_info": [{"x": 0, "text": "Experiment Setup and Datasets We utilize the widely-used sentiment lexicons, namely MPQA (Wilson et al., 2005) and HL (Hu and Liu, 2004), to evaluate the quality of word embedding.For each lexicon, we remove the words that do not appear in the lookup table of word embedding.We only use unigram embedding in this section because these sentiment lexicons do not contain phrases.The distribution of the lexicons used in this paper is listed in Table 4.Lexicon Positive Negative Total HL 1,331 2,647 3,978 MPQA 1,932 2,817 4,749 Joint 1,051 2,024 3,075 Table 4: Statistics of the sentiment lexicons.Joint stands for the words that occur in both HL and MPQA with the same sentiment polarity."}, {"x": 1, "text": "Results.Table 5 shows our results compared to other word embedding learning algorithms.The accuracy of random result is 50% as positive and negative words are randomly occurred in the nearest neighbors of each word.Sentiment-specific word embeddings (SSWEh, SSWEr, SSWEu) outperform existing neural models (C&W, word2vec) by large margins.SSWEu performs best in three lexicons.SSWEh and SSWEr have comparable performances.Experimental results further demonstrate that sentiment-specific word embeddings are able to capture the sentiment information of texts and distinguish words with opposite sentiment polarity, which are not well solved in traditional neural Embedding HL MPQA Joint Random 50.00 50.00 50.00 C&W 63.10 58.13 62.58 Word2vec 66.22 60.72 65.59 ReEmb(C&W) 64.81 59.76 64.09 ReEmb(w2v) 67.16 61.81 66.39 WVSA 68.14 64.07 67.12 SSWEh 74.17 68.36 74.03 SSWEr 73.65 68.02 73.14 SSWEu 77.30 71.74 77.33 Table 5: Accuracy of the polarity consistency of words in different sentiment lexicons.models like C&W and word2vec.SSWE outperforms MVSA and ReEmb by exploiting more context information of words and sentiment information of sentences, respectively."}, {"x": 3, "text": "In this paper, we propose learning continuous word representations as features for Twitter sentiment classification under a supervised learning framework.We show that the word embedding learned by traditional neural networks are not effective enough for Twitter sentiment classification.These methods typically only model the context information of words so that they cannot distinguish words with similar context but opposite sentiment polarity (e.g.good and bad).We learn sentiment-specific word embedding (SSWE) by integrating the sentiment information into the loss functions of three neural networks.We train SSWE with massive distant-supervised tweets selected by positive and negative emoticons.The effectiveness of SSWE has been implicitly evaluated by using it as features in sentiment classification on the benchmark dataset in SemEval 2013, and explicitly verified by measuring word similarity in the embedding space for sentiment lexicons.Our unified model combining syntactic context of words and sentiment information of sentences yields the best performance in both experiments."}], "scenes": [["Lexicon"], ["SSWE", "Country_music", "Word2vec"], ["SSWE"], ["SSWE"], ["Randomness", "Country_music", "Word2vec", "SSWE", "Embedding"], ["Country_music", "Word2vec"], ["SSWE"], ["Twitter"], ["Twitter"], ["SSWE"], ["SSWE"], ["SSWE", "SemEval"]], "chapters": [{"text": "5 Conclusion", "sentence_id": "s_15", "sentence_rank": "15", "paragraph_id": "p_2", "paragraph_rank": 2}], "all_paragraphs": [{"paragraph_info": {"end": 687, "start": 0, "text": "Experiment Setup and Datasets We utilize the widely-used sentiment lexicons, namely MPQA (Wilson et al., 2005) and HL (Hu and Liu, 2004), to evaluate the quality of word embedding.For each lexicon, we remove the words that do not appear in the lookup table of word embedding.We only use unigram embedding in this section because these sentiment lexicons do not contain phrases.The distribution of the lexicons used in this paper is listed in Table 4.Lexicon Positive Negative Total HL 1,331 2,647 3,978 MPQA 1,932 2,817 4,749 Joint 1,051 2,024 3,075 Table 4: Statistics of the sentiment lexicons.Joint stands for the words that occur in both HL and MPQA with the same sentiment polarity.", "rank": 0, "paragraph_comparative_number": 2, "entities": [], "id": "p_0"}, "sentences": [{"end": 180, "text": "Experiment Setup and Datasets We utilize the widely-used sentiment lexicons, namely MPQA (Wilson et al., 2005) and HL (Hu and Liu, 2004), to evaluate the quality of word embedding.", "rank": 0, "start": 0, "IsComparative": "1", "id": "st_0"}, {"end": 275, "text": "For each lexicon, we remove the words that do not appear in the lookup table of word embedding.", "rank": 1, "start": 180, "IsComparative": "0", "id": "st_1"}, {"end": 377, "text": "We only use unigram embedding in this section because these sentiment lexicons do not contain phrases.", "rank": 2, "start": 275, "IsComparative": "1", "id": "st_2"}, {"end": 450, "text": "The distribution of the lexicons used in this paper is listed in Table 4.", "rank": 3, "start": 377, "IsComparative": "0", "id": "st_3"}, {"end": 596, "text": "Lexicon Positive Negative Total HL 1,331 2,647 3,978 MPQA 1,932 2,817 4,749 Joint 1,051 2,024 3,075 Table 4: Statistics of the sentiment lexicons.", "rank": 4, "start": 450, "IsComparative": "0", "id": "st_4"}, {"end": 687, "text": "Joint stands for the words that occur in both HL and MPQA with the same sentiment polarity.", "rank": 5, "start": 596, "IsComparative": "0", "id": "st_5"}]}, {"paragraph_info": {"end": 1849, "start": 687, "text": "Results.Table 5 shows our results compared to other word embedding learning algorithms.The accuracy of random result is 50% as positive and negative words are randomly occurred in the nearest neighbors of each word.Sentiment-specific word embeddings (SSWEh, SSWEr, SSWEu) outperform existing neural models (C&W, word2vec) by large margins.SSWEu performs best in three lexicons.SSWEh and SSWEr have comparable performances.Experimental results further demonstrate that sentiment-specific word embeddings are able to capture the sentiment information of texts and distinguish words with opposite sentiment polarity, which are not well solved in traditional neural Embedding HL MPQA Joint Random 50.00 50.00 50.00 C&W 63.10 58.13 62.58 Word2vec 66.22 60.72 65.59 ReEmb(C&W) 64.81 59.76 64.09 ReEmb(w2v) 67.16 61.81 66.39 WVSA 68.14 64.07 67.12 SSWEh 74.17 68.36 74.03 SSWEr 73.65 68.02 73.14 SSWEu 77.30 71.74 77.33 Table 5: Accuracy of the polarity consistency of words in different sentiment lexicons.models like C&W and word2vec.SSWE outperforms MVSA and ReEmb by exploiting more context information of words and sentiment information of sentences, respectively.", "rank": 1, "paragraph_comparative_number": 4, "entities": [], "id": "p_1"}, "sentences": [{"end": 695, "text": "Results.", "rank": 6, "start": 687, "IsComparative": "0", "id": "st_6"}, {"end": 774, "text": "Table 5 shows our results compared to other word embedding learning algorithms.", "rank": 7, "start": 695, "IsComparative": "0", "id": "st_7"}, {"end": 902, "text": "The accuracy of random result is 50% as positive and negative words are randomly occurred in the nearest neighbors of each word.", "rank": 8, "start": 774, "IsComparative": "0", "id": "st_8"}, {"end": 1026, "text": "Sentiment-specific word embeddings (SSWEh, SSWEr, SSWEu) outperform existing neural models (C&W, word2vec) by large margins.", "rank": 9, "start": 902, "IsComparative": "1", "id": "st_9"}, {"end": 1064, "text": "SSWEu performs best in three lexicons.", "rank": 10, "start": 1026, "IsComparative": "1", "id": "st_10"}, {"end": 1109, "text": "SSWEh and SSWEr have comparable performances.", "rank": 11, "start": 1064, "IsComparative": "1", "id": "st_11"}, {"end": 1687, "text": "Experimental results further demonstrate that sentiment-specific word embeddings are able to capture the sentiment information of texts and distinguish words with opposite sentiment polarity, which are not well solved in traditional neural Embedding HL MPQA Joint Random 50.00 50.00 50.00 C&W 63.10 58.13 62.58 Word2vec 66.22 60.72 65.59 ReEmb(C&W) 64.81 59.76 64.09 ReEmb(w2v) 67.16 61.81 66.39 WVSA 68.14 64.07 67.12 SSWEh 74.17 68.36 74.03 SSWEr 73.65 68.02 73.14 SSWEu 77.30 71.74 77.33 Table 5: Accuracy of the polarity consistency of words in different sentiment lexicons.", "rank": 12, "start": 1109, "IsComparative": "0", "id": "st_12"}, {"end": 1716, "text": "models like C&W and word2vec.", "rank": 13, "start": 1687, "IsComparative": "0", "id": "st_13"}, {"end": 1849, "text": "SSWE outperforms MVSA and ReEmb by exploiting more context information of words and sentiment information of sentences, respectively.", "rank": 14, "start": 1716, "IsComparative": "1", "id": "st_14"}]}, {"paragraph_info": {"end": 1861, "start": 1849, "text": "5 Conclusion", "rank": 2, "paragraph_comparative_number": 0, "entities": [], "id": "p_2"}, "sentences": [{"end": 1861, "text": "5 Conclusion", "rank": 15, "start": 1849, "IsComparative": "0", "id": "st_15"}]}, {"paragraph_info": {"end": 2954, "start": 1861, "text": "In this paper, we propose learning continuous word representations as features for Twitter sentiment classification under a supervised learning framework.We show that the word embedding learned by traditional neural networks are not effective enough for Twitter sentiment classification.These methods typically only model the context information of words so that they cannot distinguish words with similar context but opposite sentiment polarity (e.g.good and bad).We learn sentiment-specific word embedding (SSWE) by integrating the sentiment information into the loss functions of three neural networks.We train SSWE with massive distant-supervised tweets selected by positive and negative emoticons.The effectiveness of SSWE has been implicitly evaluated by using it as features in sentiment classification on the benchmark dataset in SemEval 2013, and explicitly verified by measuring word similarity in the embedding space for sentiment lexicons.Our unified model combining syntactic context of words and sentiment information of sentences yields the best performance in both experiments.", "rank": 3, "paragraph_comparative_number": 4, "entities": [], "id": "p_3"}, "sentences": [{"end": 2015, "text": "In this paper, we propose learning continuous word representations as features for Twitter sentiment classification under a supervised learning framework.", "rank": 16, "start": 1861, "IsComparative": "0", "id": "st_16"}, {"end": 2148, "text": "We show that the word embedding learned by traditional neural networks are not effective enough for Twitter sentiment classification.", "rank": 17, "start": 2015, "IsComparative": "0", "id": "st_17"}, {"end": 2312, "text": "These methods typically only model the context information of words so that they cannot distinguish words with similar context but opposite sentiment polarity (e.g.", "rank": 18, "start": 2148, "IsComparative": "0", "id": "st_18"}, {"end": 2326, "text": "good and bad).", "rank": 19, "start": 2312, "IsComparative": "1", "id": "st_19"}, {"end": 2466, "text": "We learn sentiment-specific word embedding (SSWE) by integrating the sentiment information into the loss functions of three neural networks.", "rank": 20, "start": 2326, "IsComparative": "1", "id": "st_20"}, {"end": 2563, "text": "We train SSWE with massive distant-supervised tweets selected by positive and negative emoticons.", "rank": 21, "start": 2466, "IsComparative": "0", "id": "st_21"}, {"end": 2812, "text": "The effectiveness of SSWE has been implicitly evaluated by using it as features in sentiment classification on the benchmark dataset in SemEval 2013, and explicitly verified by measuring word similarity in the embedding space for sentiment lexicons.", "rank": 22, "start": 2563, "IsComparative": "1", "id": "st_22"}, {"end": 2954, "text": "Our unified model combining syntactic context of words and sentiment information of sentences yields the best performance in both experiments.", "rank": 23, "start": 2812, "IsComparative": "1", "id": "st_23"}]}], "sentence_scenes_info": [{"x": 3, "text": "The distribution of the lexicons used in this paper is listed in Table 4."}, {"x": 9, "text": "Sentiment-specific word embeddings (SSWEh, SSWEr, SSWEu) outperform existing neural models (C&W, word2vec) by large margins."}, {"x": 10, "text": "SSWEu performs best in three lexicons."}, {"x": 11, "text": "SSWEh and SSWEr have comparable performances."}, {"x": 12, "text": "Experimental results further demonstrate that sentiment-specific word embeddings are able to capture the sentiment information of texts and distinguish words with opposite sentiment polarity, which are not well solved in traditional neural Embedding HL MPQA Joint Random 50.00 50.00 50.00 C&W 63.10 58.13 62.58 Word2vec 66.22 60.72 65.59 ReEmb(C&W) 64.81 59.76 64.09 ReEmb(w2v) 67.16 61.81 66.39 WVSA 68.14 64.07 67.12 SSWEh 74.17 68.36 74.03 SSWEr 73.65 68.02 73.14 SSWEu 77.30 71.74 77.33 Table 5: Accuracy of the polarity consistency of words in different sentiment lexicons."}, {"x": 13, "text": "models like C&W and word2vec."}, {"x": 14, "text": "SSWE outperforms MVSA and ReEmb by exploiting more context information of words and sentiment information of sentences, respectively."}, {"x": 16, "text": "In this paper, we propose learning continuous word representations as features for Twitter sentiment classification under a supervised learning framework."}, {"x": 17, "text": "We show that the word embedding learned by traditional neural networks are not effective enough for Twitter sentiment classification."}, {"x": 20, "text": "We learn sentiment-specific word embedding (SSWE) by integrating the sentiment information into the loss functions of three neural networks."}, {"x": 21, "text": "We train SSWE with massive distant-supervised tweets selected by positive and negative emoticons."}, {"x": 22, "text": "The effectiveness of SSWE has been implicitly evaluated by using it as features in sentiment classification on the benchmark dataset in SemEval 2013, and explicitly verified by measuring word similarity in the embedding space for sentiment lexicons."}], "characters": [{"name": "Randomness", "offsets": [1373], "paragraph_occurrences": [1], "sentence_occurrences": [12], "affiliation": "light", "frequency": 1, "id": "Randomness"}, {"name": "Lexicon", "offsets": [450], "paragraph_occurrences": [0], "sentence_occurrences": [3], "affiliation": "light", "frequency": 1, "id": "Lexicon"}, {"name": "Word2vec", "offsets": [999, 1707, 1420], "paragraph_occurrences": [1, 1, 1], "sentence_occurrences": [9, 13, 12], "affiliation": "light", "frequency": 3, "id": "Word2vec"}, {"name": "Twitter Inc.", "offsets": [1944, 2115], "paragraph_occurrences": [3, 3], "sentence_occurrences": [16, 17], "affiliation": "light", "frequency": 2, "id": "Twitter"}, {"name": "SemEval", "offsets": [2699], "paragraph_occurrences": [3], "sentence_occurrences": [22], "affiliation": "light", "frequency": 1, "id": "SemEval"}, {"name": "Embedding", "offsets": [1349], "paragraph_occurrences": [1], "sentence_occurrences": [12], "affiliation": "light", "frequency": 1, "id": "Embedding"}, {"name": "Country", "offsets": [994, 1398, 1453, 1699], "paragraph_occurrences": [1, 1, 1, 1], "sentence_occurrences": [9, 12, 12, 13], "affiliation": "light", "frequency": 4, "id": "Country_music"}, {"name": "SSWE", "offsets": [], "paragraph_occurrences": [1, 3], "sentence_occurrences": [9, 10, 11, 12, 14, 20, 21, 22], "affiliation": "light", "frequency": 13, "id": "SSWE"}]}