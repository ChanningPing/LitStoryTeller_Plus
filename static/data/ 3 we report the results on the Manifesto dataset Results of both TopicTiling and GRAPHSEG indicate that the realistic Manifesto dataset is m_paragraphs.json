{"paragraph_scenes_info": [{"x": 0, "text": "Unsupervised Text Segmentation Using Semantic Relatedness Graphs"}, {"x": 2, "text": "Segmenting text into semantically coherent fragments improves readability of text and facilitates tasks like text summarization and passage retrieval.In this paper, we present a novel unsupervised algorithm for linear text segmentation (TS) that exploits word embeddings and a measure of semantic relatedness of short texts to construct a semantic relatedness graph of the document.Semantically coherent segments are then derived from maximal cliques of the relatedness graph.The algorithm performs competitively on a standard synthetic dataset and outperforms the best-performing method on a real-world (i.e., non-artificial) dataset of political manifestos."}, {"x": 4, "text": "Despite the fact that in mainstream natural language processing (NLP) and information retrieval (IR) texts are modeled as bags of unordered words, texts are sequences of semantically coherent segments, designed (often very thoughtfully) to ease readability and understanding of the ideas conveyed by the authors.Although authors may explicitly define coherent segments (e.g., as paragraphs), many texts, especially on the web, lack any explicit segmentation."}, {"x": 5, "text": "Linear text segmentation aims to represent texts as sequences of semantically coherent segments.Besides improving readability and understandability of texts for readers, automated text segmentation is beneficial for NLP and IR tasks such as text summarization (Angheluta et al., 2002; Dias et al., 2007) and passage retrieval (Huang et al., 2003; Dias et al., 2007).Whereas early approaches to unsupervised text segmentation measured the co herence of segments via raw term overlaps between sentences (Hearst, 1997; Choi, 2000), more recent methods (Misra et al., 2009; Riedl and Biemann, 2012) addressed the issue of sparsity of term-based representations by replacing term-vectors with vectors of latent topics."}, {"x": 6, "text": "A topical representation of text is, however, merely a vague approximation of its meaning.Considering that the goal of TS is to identify semantically coherent segments, we propose a TS algorithm aiming to directly capture the semantic relatedness between segments, instead of approximating it via topical similarity.We employ word embeddings (Mikolov et al., 2013) and a measure of semantic relatedness of short texts (Sari  c et al.,  2012) to construct a relatedness graph of the text in which nodes denote sentences and edges are added between semantically related sentences.We then derive segments using the maximal cliques of such similarity graphs."}, {"x": 7, "text": "The proposed algorithm displays competitive performance on the artifically-generated benchmark TS dataset (Choi, 2000) and, more importantly, outperforms the best-performing topic modeling-based TS method on a real-world dataset of political manifestos."}, {"x": 9, "text": "Automated text segmentation received a lot of attention in NLP and IR communities due to its usefulness for text summarization and text indexing.Text segmentation can be performed in two different ways, namely (1) with the goal of obtaining linear segmentations (i.e. detecting the sequence of different segments in a text) , or (2) in order to obtain hierarchical segmentations (i.e. defining a structure of subtopics between the detected segments).Like the majority of TS methods (Hearst, 1994; Brants et al., 2002; Misra et al., 2009; Riedl and Biemann, 2012), in this work we focus on linear segmentation of text, but there is also a solid body of work on hierarchical TS, where each toplevel segment is further broken down (Yaari, 1997; Eisenstein, 2009)."}, {"x": 10, "text": "Hearst (1994) introduced TextTiling, one of the first unsupervised algorithms for linear text segmentation.She exploits the fact that words tend to be repeated in coherent segments and measures the similarity between paragraphs by comparing their sparse term-vectors.Choi (2000) introduced the probabilistic algorithm using matrix-based ranking and clustering to determine similarities between segments.Galley et al.(2003) combined contentbased information with acoustic cues in order to detect discourse shifts whereas Utiyama and Isahara (2001) and Fragkou et al.(2004) minimized different segmentation cost functions with dynamic programming."}, {"x": 11, "text": "The first segmentation approach based on topic modeling (Brants et al., 2002) employed the probabilistic latent semantic analysis (pLSA) to derive latent representations of segments and determined the segmentation based on similarities of segments latent vectors.More recent models (Misra et al., 2009; Riedl and Biemann, 2012) employed the latent Dirichlet allocation (LDA) (Blei et al., 2003) to compute the latent topics and displayed superior performance to previous models on standard synthetic datasets (Choi, 2000; Galley et al., 2003).Misra et al.(2009) used dynamic programming to find globally optimal segmentation over the set of LDA-based segment representations, whereas Riedl and Biemann (2012) introduced TopicTiling, an LDA-driven extension of Hearsts TextTiling algorithm where segments are, represented as dense vectors of dominant topics of terms they contain (instead of as sparse term vectors).Riedl and Biemann (2012) show that TopicTiling outperforms at-that-time state-of-the-art methods for unsupervised linear segmentation (Choi, 2000; Utiyama and Isahara, 2001; Galley et al., 2003; Fragkou et al., 2004; Misra et al., 2009) and that it is also faster than other LDA-based methods (Misra et al., 2009)."}, {"x": 12, "text": "In the most closely related work to ours, Malioutov and Barzilay (2006) proposed a graphbased TS approach in which they first construct the fully connected graph of sentences, with edges weighted via the cosine similarity between bagof-words sentence vectors, and then run the minimum normalized multiway cut algorithm to obtain the segments.Similarly, Ferret (2007) builds the similarity graph, only between words instead of between sentences, using sparse co-occurrence vectors as semantic representations for words.He then identifies topics by clustering the word similarity graph via the Shared Nearest Neighbor algorithm (Ertoz et al., 2004)  .Unlike these works, we use the dense semantic representations of words and sentences (i.e., embeddings), which have been shown to outperform sparse semantic vectors on a range of NLP tasks.Also, instead of looking for minimal cuts in the relatedness graph, we exploit the maximal cliques of the relatedness graph between sentences to obtain the topic segments."}, {"x": 13, "text": "3 Text Segmentation Algorithm"}, {"x": 14, "text": "Our TS algorithm, dubbed GRAPHSEG, builds a semantic relatedness graph in which nodes denote sentences and edges are created for pairs of semantically related sentences.We then determine the coherent segments by finding maximal cliques of the relatedness graph.The novelty of GRAPHSEG is in the fact that it directly exploits the semantics of text instead of approximating the meaning with topicality."}, {"x": 15, "text": "3.1 Semantic Relatedness of Sentences"}, {"x": 16, "text": "The measure of semantic relatedness between sentences we use is an extension of a salient greedy lemma alignment feature proposed in a supervised model by Sari  c et al.(2012).They greedily align  content words between sentences by the similarity of their distributional vectors and then sum the similarity scores of aligned word pairs.However, such greedily obtained alignment is not necessarily optimal.In contrast, we compute the optimal alignment by (1) creating a weighted complete bipartite graph between the sets of content words of the two sentences (i.e., each word from one sentence is connected with a relatedness edge to all of the words in the other sentence) and (2) running a bipartite graph matching algorithm known as the Hungarian method (Kuhn, 1955) that has the polynomial complexity.The similarities of content words between sentences (i.e., the weights of the bipartite graph) are computed as the cosine of the angle between their corresponding embedding vectors (Mikolov et al., 2013)."}, {"x": 17, "text": "Let A be the set of word pairs in the optimal alignment between the content-word sets of the two 126 sentences S1 and S2, i.e., A = <(w1, w2) | w1  S1  w2  S2>.We then compute the semantic relatedness for two given sentences S1 and S2 as follows: sr (S1, S2) =X (w1,w2)A cos(v1, v2)  min(ic(w1), ic(w2)) where vi is the embedding vector of the word wi and ic(w) is the information content (IC) of the word w, computed based on the relative frequency of w in some large corpus C: ic(w) =  log freq(w) + 1 |C| + P w0C freq(w0) ."}, {"x": 18, "text": "We utilize the IC weighting of embedding similarity because we assume that matches between less frequent words (e.g., guitar and ukulele) contribute more to sentence relatedness than pairs of similar but frequent words (e.g., do and make).We used Google Books Ngrams (Michel et al., 2011) as a large corpus C for estimating relative frequencies of words in a language."}, {"x": 22, "text": "For each pair of sentences for which the semantic relatedness is above some treshold value  we add an edge between the corresponding nodes of G. Next, we employ the Bron-Kerbosch algorithm (Bron and Kerbosch, 1973) to compute the set Q of all maximal cliques of G.We then create the initial set of segments SG by merging adjacent sentences found in at least one maximal clique Q  Q of graph G. Next, we merge the adjacent segments sgi and sgi+1 for which there is at least one clique Q  Q containing at least one sentence from sgi and one sentence from sgi+1.Finally, given the Step Sets Cliques Q <1, 2, 6>, <2, 4, 7>, <3, 4, 5>, <1, 8, 9> Init.seg.<1, 2>, <3, 4, 5>, <6>, <7> <8, 9> Merge seg.<1, 2, 3, 4, 5>, <6>, <7>, <8, 9> Merge small <1, 2, 3, 4, 5>, <6, 7>, <8, 9> Table 1: Creating segments from graph cliques (n = 2).In the third step we merge segments <1, 2, 3> and <4, 5> because the second clique contains sentences 2 (from the left segment) and 4 (from the right segment).In the final step we merge single sentence segments (assuming segs(<1, 2, 3, 4, 5>, <6>) < segs(<6>, <7>) and segs(<7>, <8, 9>) < segs(<6>, <7>)).minimal segment size n, we merge segments sgi with less than n sentences with the semantically more related of the two adjacent segments  sgi1 or sgi+1.The relatedness between two adjacent segments (sgr (sgi , sgi+1)) is computed as the average relatedness between their respective sentences: sgr (SG1, SG2) = 1 |SG1||SG2| X S1SG1 S2SG2 rel(S1, S2)."}, {"x": 23, "text": "We exemplify the creation of segments from maximal cliques in Table 1.The complete segmentation algorithm is fleshed out in Algorithm 1.1"}, {"x": 27, "text": "Unsupervised methods for text segmentation have most often been evaluated on synthetic datasets with segments from different sources being concatenated in artificial documents (Choi, 2000; Galley et al., 2003).Segmenting such artificial texts is easier than segmenting real-world documents.This is why besides on the artificial Choi dataset we also evaluate GRAPHSEG on a real-world dataset of political texts from the Manifesto Project,2,3 manually labeled by domain experts with segments of seven different topics (e.g., economy and welfare, quality of life, foreign affairs).The selected manifestos contain between 1000 and 2500 sentences, with segments ranging in length from 1 to 78 sentences, which is in sharp contrast to the Choi dataset where all segments are of similar size."}, {"x": 28, "text": "4.2 Experimental Setting"}, {"x": 29, "text": "To allow for comparison with previous work, we evaluate GRAPHSEG on four subsets of the Choi dataset, differing in number of sentences the seg- 2008, and 2012 U.S.elections ments contain.For the evaluation on the Choi dataset, the GRAPHSEG algorithm made use of the publicly available word embeddings built from a Google News dataset.4"}, {"x": 30, "text": "Both LDA-based models (Misra et al., 2009; Riedl and Biemann, 2012) and GRAPHSEG rely on corpus-derived word representations.Thus, we evaluated on the Manifesto dataset both the domainadapted and domain-unadapted variants of these methods.The domain-adapted variants of the models used the unlabeled domain corpus  a test set of 466 unlabeled political manifestos  to train the domain-specific word representations.This means that we obtain (1) in-domain topics for the LDAbased TopicTiling model of Riedl and Biemann (2012) and (2) domain-specific embeddings for the GRAPHSEG algorithm.On the Manifesto dataset we also evaluate a baseline that randomly (50% chance) starts a new segment at points m sentences apart, with m being set to half of the average length of gold segments."}, {"x": 31, "text": "We evaluate the performance using two standard TS evaluation metrics  Pk (Beeferman et al., 1999) and WindowDiff (WD) (Pevzner and Hearst, 2002).Pk is the probability that two randomly drawn sentences mutually k sentences apart are classified incorrectly  either as belonging to the same segment when they are in different gold segments or as being in different segments when they are in the same gold segment.Following Riedl and Biemann (2012), we set k to half of the document length divided by the number of gold segments.WindowDiff is a stricter version of Pk as, instead of only checking if the randomly chosen sentences are in the same predicted segment or not, it compares the exact number of segments between the sentences in the predicted segmentation with the number of segments in between the same sentences in the gold standard.Lower scores indicate better performance for both these metrics."}, {"x": 35, "text": "In Table 2 we report the performance of GRAPHSEG and prominent TS methods on the synthetic Choi dataset.GRAPHSEG performs competitively, outperforming all methods but (Fragkou et al., 2004) and domain-adapted versions of LDA-based models (Misra et al., 2009; Riedl and Biemann, 2012).However, the approach by (Fragkou et al., 2004) uses the gold standard information  the average gold segment size  as input.On the other hand, the LDA-based models adapt their topic models on parts of the Choi dataset itself.Despite the fact that they use different documents for training the topic models from those used for evaluating segmentation quality, the evaluation is still tainted because snippets from the original documents appear in multiple artificial documents  some of which belong to the the training set and others to the test set, as admitted by Riedl and Biemann (2012) and this is why their reported performance on this dataset is overestimated."}, {"x": 36, "text": "In Table 3 we report the results on the Manifesto dataset.Results of both TopicTiling and GRAPHSEG indicate that the realistic Manifesto dataset is much more difficult to segment than the artificial Choi dataset.The GRAPHSEG algorithm significantly outperforms the TopicTiling method (p < 0.05, Students t-test).In-domain training of word representations, topics for TopicTiling and word embeddings for GraphSeg, does not signifi- cantly improve the performance for neither of the two models.This result contrasts previous findings (Misra et al., 2009; Riedl and Biemann, 2012) in which the performance boost was credited to the indomain trained topics and supports our hypothesis that the performance boost of the LDA-based methods with in-domain trained topics originates from information leakage between different portions of the synthetic Choi dataset."}, {"x": 38, "text": "In this work we presented GRAPHSEG, a novel graph-based algorithm for unsupervised text segmentation.GRAPHSEG employs word embeddings and extends a measure of semantic relatedness to construct a relatedness graph with edges established between semantically related sentences.The segmentation is then determined by the maximal cliques of the relatedness graph and improved by semantic comparison of adjacent segments.GRAPHSEG displays competitive performance compared to best-performing LDA-based methods on a synthetic dataset.However, we identify and discuss evaluation issues pertaining to LDA-based TS on this dataset.We also performed an evaluation on the real-world dataset of political manifestos and showed that in a realistic setting GRAPHSEG significantly outperforms the state-of-the-art LDAbased TS model."}], "chapters": [{"text": "Abstract", "sentence_id": "s_1", "sentence_rank": "1", "paragraph_id": "p_1", "paragraph_rank": 1}, {"text": "1 Introduction", "sentence_id": "s_6", "sentence_rank": "6", "paragraph_id": "p_3", "paragraph_rank": 3}, {"text": "2 Related Work", "sentence_id": "s_17", "sentence_rank": "17", "paragraph_id": "p_8", "paragraph_rank": 8}, {"text": "3 Text Segmentation Algorithm", "sentence_id": "s_37", "sentence_rank": "37", "paragraph_id": "p_13", "paragraph_rank": 13}, {"text": "3.1 Semantic Relatedness of Sentences", "sentence_id": "s_41", "sentence_rank": "41", "paragraph_id": "p_15", "paragraph_rank": 15}, {"text": "3.2 Graph-Based Segmentation", "sentence_id": "s_54", "sentence_rank": "54", "paragraph_id": "p_20", "paragraph_rank": 20}, {"text": "4 Evaluation", "sentence_id": "s_69", "sentence_rank": "69", "paragraph_id": "p_24", "paragraph_rank": 24}, {"text": "4.1 Datasets", "sentence_id": "s_72", "sentence_rank": "72", "paragraph_id": "p_26", "paragraph_rank": 26}, {"text": "4.2 Experimental Setting", "sentence_id": "s_77", "sentence_rank": "77", "paragraph_id": "p_28", "paragraph_rank": 28}, {"text": "4.3 Results and Discussion", "sentence_id": "s_95", "sentence_rank": "95", "paragraph_id": "p_34", "paragraph_rank": 34}, {"text": "5 Conclusion", "sentence_id": "s_106", "sentence_rank": "106", "paragraph_id": "p_37", "paragraph_rank": 37}], "scenes": [["Semantic_similarity", "Text_segmentation"], ["Text_segmentation"], ["Natural_language_processing", "Information_retrieval"], ["Hearst_Corporation", "Natural_language_processing", "Information_retrieval"], ["Text_segmentation"], ["Text_segmentation"], ["Hearst_Corporation", "Natural_language_processing", "Information_retrieval", "Text_segmentation"], ["Galley"], ["Probabilistic_latent_semantic_analysis", "David_Blei", "Galley", "Latent_Dirichlet_allocation"], ["Natural_language_processing", "Nearest_neighbor", "Text_segmentation"], ["Text_segmentation"], ["Text_segmentation"], ["Semantic_similarity"], ["Hungarian_algorithm"], ["Integrated_circuit"], ["Integrated_circuit", "Google", "N-gram"], ["Stargate_SG-1", "Bron\u2013Kerbosch_algorithm", "Clique_(graph_theory)", "SG_postcode_area"], ["Algorithm"], ["Galley"], ["Experimental_music"], ["Google_Search"], ["Latent_Dirichlet_allocation"], ["Pakistan"], ["Latent_Dirichlet_allocation"], ["Latent_Dirichlet_allocation"], ["Latent_Dirichlet_allocation"]], "characters": [{"name": "Text segmentation", "offsets": [13, 6482, 309, 2035, 2098, 2665, 2765, 3308, 3510, 5565, 6513, 2982], "paragraph_occurrences": [0, 13, 2, 6, 6, 7, 7, 9, 9, 12, 14, 9], "sentence_occurrences": [0, 37, 3, 13, 13, 16, 16, 20, 20, 32, 38, 18], "affiliation": "light", "frequency": 12, "id": "Text_segmentation"}, {"name": "n-gram", "offsets": [8741], "paragraph_occurrences": [18], "sentence_occurrences": [51], "affiliation": "light", "frequency": 1, "id": "N-gram"}, {"name": "Integrated circuit", "offsets": [8345, 8496], "paragraph_occurrences": [17, 18], "sentence_occurrences": [49, 50], "affiliation": "light", "frequency": 2, "id": "Integrated_circuit"}, {"name": "David M. Blei", "offsets": [4618], "paragraph_occurrences": [11], "sentence_occurrences": [28], "affiliation": "light", "frequency": 1, "id": "David_Blei"}, {"name": "Stargate SG-1", "offsets": [9783, 10906, 10921, 10949], "paragraph_occurrences": [22, 22, 22, 22], "sentence_occurrences": [58, 66, 66, 66], "affiliation": "light", "frequency": 4, "id": "Stargate_SG-1"}, {"name": "Clique", "offsets": [10064], "paragraph_occurrences": [22], "sentence_occurrences": [59], "affiliation": "light", "frequency": 1, "id": "Clique_(graph_theory)"}, {"name": "Information retrieval", "offsets": [842, 1427, 2904], "paragraph_occurrences": [4, 5, 9], "sentence_occurrences": [7, 10, 18], "affiliation": "light", "frequency": 3, "id": "Information_retrieval"}, {"name": "Semantic similarity", "offsets": [37, 6914], "paragraph_occurrences": [0, 15], "sentence_occurrences": [0, 41], "affiliation": "light", "frequency": 2, "id": "Semantic_similarity"}, {"name": "Natural language processing", "offsets": [810, 1419, 2896, 6299], "paragraph_occurrences": [4, 5, 9, 12], "sentence_occurrences": [7, 10, 18, 35], "affiliation": "light", "frequency": 4, "id": "Natural_language_processing"}, {"name": "Probabilistic latent semantic analysis", "offsets": [4373], "paragraph_occurrences": [11], "sentence_occurrences": [27], "affiliation": "light", "frequency": 1, "id": "Probabilistic_latent_semantic_analysis"}, {"name": "Bron\u2013Kerbosch algorithm", "offsets": [9641], "paragraph_occurrences": [22], "sentence_occurrences": [57], "affiliation": "light", "frequency": 1, "id": "Bron\u2013Kerbosch_algorithm"}, {"name": "Hungarian algorithm", "offsets": [7686], "paragraph_occurrences": [16], "sentence_occurrences": [46], "affiliation": "light", "frequency": 1, "id": "Hungarian_algorithm"}, {"name": "Algorithm", "offsets": [11081], "paragraph_occurrences": [23], "sentence_occurrences": [68], "affiliation": "light", "frequency": 1, "id": "Algorithm"}, {"name": "Galley", "offsets": [4000, 4764, 5331, 11655], "paragraph_occurrences": [10, 11, 11, 27], "sentence_occurrences": [23, 28, 31, 73], "affiliation": "light", "frequency": 4, "id": "Galley"}, {"name": "Latent Dirichlet allocation", "offsets": [4583, 4612, 4883, 4978, 5432, 12615, 15158, 15368, 16602, 17241, 17347], "paragraph_occurrences": [11, 11, 11, 11, 11, 30, 35, 35, 36, 38, 38], "sentence_occurrences": [28, 28, 30, 30, 31, 81, 97, 99, 105, 110, 111], "affiliation": "light", "frequency": 11, "id": "Latent_Dirichlet_allocation"}, {"name": "Nearest neighbor", "offsets": [6070], "paragraph_occurrences": [12], "sentence_occurrences": [34], "affiliation": "light", "frequency": 1, "id": "Nearest_neighbor"}, {"name": "Pakistan", "offsets": [13461, 13536, 13952], "paragraph_occurrences": [31, 31, 31], "sentence_occurrences": [86, 86, 89], "affiliation": "light", "frequency": 3, "id": "Pakistan"}, {"name": "Google", "offsets": [8728], "paragraph_occurrences": [18], "sentence_occurrences": [51], "affiliation": "light", "frequency": 1, "id": "Google"}, {"name": "Experimental music", "offsets": [12255], "paragraph_occurrences": [28], "sentence_occurrences": [77], "affiliation": "light", "frequency": 1, "id": "Experimental_music"}, {"name": "Hearst Corporation", "offsets": [1705, 3320, 3597], "paragraph_occurrences": [5, 9, 9], "sentence_occurrences": [11, 20, 20], "affiliation": "light", "frequency": 3, "id": "Hearst_Corporation"}, {"name": "Google Search", "offsets": [12589], "paragraph_occurrences": [29], "sentence_occurrences": [80], "affiliation": "light", "frequency": 1, "id": "Google_Search"}, {"name": "SG postcode area", "offsets": [10911, 10926, 10953], "paragraph_occurrences": [22, 22, 22], "sentence_occurrences": [66, 66, 66], "affiliation": "light", "frequency": 3, "id": "SG_postcode_area"}], "all_paragraphs": [{"paragraph_info": {"end": 64, "start": 0, "text": "Unsupervised Text Segmentation Using Semantic Relatedness Graphs", "rank": 0, "paragraph_comparative_number": 0, "entities": [], "id": "p_0"}, "sentences": [{"end": 64, "text": "Unsupervised Text Segmentation Using Semantic Relatedness Graphs", "rank": 0, "start": 0, "IsComparative": "0", "id": "st_0"}]}, {"paragraph_info": {"end": 72, "start": 64, "text": "Abstract", "rank": 1, "paragraph_comparative_number": 1, "entities": [], "id": "p_1"}, "sentences": [{"end": 72, "text": "Abstract", "rank": 1, "start": 64, "IsComparative": "1", "id": "st_1"}]}, {"paragraph_info": {"end": 731, "start": 72, "text": "Segmenting text into semantically coherent fragments improves readability of text and facilitates tasks like text summarization and passage retrieval.In this paper, we present a novel unsupervised algorithm for linear text segmentation (TS) that exploits word embeddings and a measure of semantic relatedness of short texts to construct a semantic relatedness graph of the document.Semantically coherent segments are then derived from maximal cliques of the relatedness graph.The algorithm performs competitively on a standard synthetic dataset and outperforms the best-performing method on a real-world (i.e., non-artificial) dataset of political manifestos.", "rank": 2, "paragraph_comparative_number": 1, "entities": [], "id": "p_2"}, "sentences": [{"end": 222, "text": "Segmenting text into semantically coherent fragments improves readability of text and facilitates tasks like text summarization and passage retrieval.", "rank": 2, "start": 72, "IsComparative": "0", "id": "st_2"}, {"end": 454, "text": "In this paper, we present a novel unsupervised algorithm for linear text segmentation (TS) that exploits word embeddings and a measure of semantic relatedness of short texts to construct a semantic relatedness graph of the document.", "rank": 3, "start": 222, "IsComparative": "1", "id": "st_3"}, {"end": 548, "text": "Semantically coherent segments are then derived from maximal cliques of the relatedness graph.", "rank": 4, "start": 454, "IsComparative": "0", "id": "st_4"}, {"end": 731, "text": "The algorithm performs competitively on a standard synthetic dataset and outperforms the best-performing method on a real-world (i.e., non-artificial) dataset of political manifestos.", "rank": 5, "start": 548, "IsComparative": "0", "id": "st_5"}]}, {"paragraph_info": {"end": 745, "start": 731, "text": "1 Introduction", "rank": 3, "paragraph_comparative_number": 0, "entities": [], "id": "p_3"}, "sentences": [{"end": 745, "text": "1 Introduction", "rank": 6, "start": 731, "IsComparative": "0", "id": "st_6"}]}, {"paragraph_info": {"end": 1203, "start": 745, "text": "Despite the fact that in mainstream natural language processing (NLP) and information retrieval (IR) texts are modeled as bags of unordered words, texts are sequences of semantically coherent segments, designed (often very thoughtfully) to ease readability and understanding of the ideas conveyed by the authors.Although authors may explicitly define coherent segments (e.g., as paragraphs), many texts, especially on the web, lack any explicit segmentation.", "rank": 4, "paragraph_comparative_number": 0, "entities": [], "id": "p_4"}, "sentences": [{"end": 1057, "text": "Despite the fact that in mainstream natural language processing (NLP) and information retrieval (IR) texts are modeled as bags of unordered words, texts are sequences of semantically coherent segments, designed (often very thoughtfully) to ease readability and understanding of the ideas conveyed by the authors.", "rank": 7, "start": 745, "IsComparative": "0", "id": "st_7"}, {"end": 1203, "text": "Although authors may explicitly define coherent segments (e.g., as paragraphs), many texts, especially on the web, lack any explicit segmentation.", "rank": 8, "start": 1057, "IsComparative": "0", "id": "st_8"}]}, {"paragraph_info": {"end": 1916, "start": 1203, "text": "Linear text segmentation aims to represent texts as sequences of semantically coherent segments.Besides improving readability and understandability of texts for readers, automated text segmentation is beneficial for NLP and IR tasks such as text summarization (Angheluta et al., 2002; Dias et al., 2007) and passage retrieval (Huang et al., 2003; Dias et al., 2007).Whereas early approaches to unsupervised text segmentation measured the co herence of segments via raw term overlaps between sentences (Hearst, 1997; Choi, 2000), more recent methods (Misra et al., 2009; Riedl and Biemann, 2012) addressed the issue of sparsity of term-based representations by replacing term-vectors with vectors of latent topics.", "rank": 5, "paragraph_comparative_number": 2, "entities": [], "id": "p_5"}, "sentences": [{"end": 1299, "text": "Linear text segmentation aims to represent texts as sequences of semantically coherent segments.", "rank": 9, "start": 1203, "IsComparative": "1", "id": "st_9"}, {"end": 1569, "text": "Besides improving readability and understandability of texts for readers, automated text segmentation is beneficial for NLP and IR tasks such as text summarization (Angheluta et al., 2002; Dias et al., 2007) and passage retrieval (Huang et al., 2003; Dias et al., 2007).", "rank": 10, "start": 1299, "IsComparative": "0", "id": "st_10"}, {"end": 1916, "text": "Whereas early approaches to unsupervised text segmentation measured the co herence of segments via raw term overlaps between sentences (Hearst, 1997; Choi, 2000), more recent methods (Misra et al., 2009; Riedl and Biemann, 2012) addressed the issue of sparsity of term-based representations by replacing term-vectors with vectors of latent topics.", "rank": 11, "start": 1569, "IsComparative": "1", "id": "st_11"}]}, {"paragraph_info": {"end": 2570, "start": 1916, "text": "A topical representation of text is, however, merely a vague approximation of its meaning.Considering that the goal of TS is to identify semantically coherent segments, we propose a TS algorithm aiming to directly capture the semantic relatedness between segments, instead of approximating it via topical similarity.We employ word embeddings (Mikolov et al., 2013) and a measure of semantic relatedness of short texts (Sari  c et al.,  2012) to construct a relatedness graph of the text in which nodes denote sentences and edges are added between semantically related sentences.We then derive segments using the maximal cliques of such similarity graphs.", "rank": 6, "paragraph_comparative_number": 3, "entities": [], "id": "p_6"}, "sentences": [{"end": 2006, "text": "A topical representation of text is, however, merely a vague approximation of its meaning.", "rank": 12, "start": 1916, "IsComparative": "1", "id": "st_12"}, {"end": 2232, "text": "Considering that the goal of TS is to identify semantically coherent segments, we propose a TS algorithm aiming to directly capture the semantic relatedness between segments, instead of approximating it via topical similarity.", "rank": 13, "start": 2006, "IsComparative": "1", "id": "st_13"}, {"end": 2494, "text": "We employ word embeddings (Mikolov et al., 2013) and a measure of semantic relatedness of short texts (Sari  c et al.,  2012) to construct a relatedness graph of the text in which nodes denote sentences and edges are added between semantically related sentences.", "rank": 14, "start": 2232, "IsComparative": "1", "id": "st_14"}, {"end": 2570, "text": "We then derive segments using the maximal cliques of such similarity graphs.", "rank": 15, "start": 2494, "IsComparative": "0", "id": "st_15"}]}, {"paragraph_info": {"end": 2823, "start": 2570, "text": "The proposed algorithm displays competitive performance on the artifically-generated benchmark TS dataset (Choi, 2000) and, more importantly, outperforms the best-performing topic modeling-based TS method on a real-world dataset of political manifestos.", "rank": 7, "paragraph_comparative_number": 0, "entities": [], "id": "p_7"}, "sentences": [{"end": 2823, "text": "The proposed algorithm displays competitive performance on the artifically-generated benchmark TS dataset (Choi, 2000) and, more importantly, outperforms the best-performing topic modeling-based TS method on a real-world dataset of political manifestos.", "rank": 16, "start": 2570, "IsComparative": "0", "id": "st_16"}]}, {"paragraph_info": {"end": 2837, "start": 2823, "text": "2 Related Work", "rank": 8, "paragraph_comparative_number": 0, "entities": [], "id": "p_8"}, "sentences": [{"end": 2837, "text": "2 Related Work", "rank": 17, "start": 2823, "IsComparative": "0", "id": "st_17"}]}, {"paragraph_info": {"end": 3597, "start": 2837, "text": "Automated text segmentation received a lot of attention in NLP and IR communities due to its usefulness for text summarization and text indexing.Text segmentation can be performed in two different ways, namely (1) with the goal of obtaining linear segmentations (i.e. detecting the sequence of different segments in a text) , or (2) in order to obtain hierarchical segmentations (i.e. defining a structure of subtopics between the detected segments).Like the majority of TS methods (Hearst, 1994; Brants et al., 2002; Misra et al., 2009; Riedl and Biemann, 2012), in this work we focus on linear segmentation of text, but there is also a solid body of work on hierarchical TS, where each toplevel segment is further broken down (Yaari, 1997; Eisenstein, 2009).", "rank": 9, "paragraph_comparative_number": 2, "entities": [], "id": "p_9"}, "sentences": [{"end": 2982, "text": "Automated text segmentation received a lot of attention in NLP and IR communities due to its usefulness for text summarization and text indexing.", "rank": 18, "start": 2837, "IsComparative": "1", "id": "st_18"}, {"end": 3287, "text": "Text segmentation can be performed in two different ways, namely (1) with the goal of obtaining linear segmentations (i.e. detecting the sequence of different segments in a text) , or (2) in order to obtain hierarchical segmentations (i.e. defining a structure of subtopics between the detected segments).", "rank": 19, "start": 2982, "IsComparative": "0", "id": "st_19"}, {"end": 3597, "text": "Like the majority of TS methods (Hearst, 1994; Brants et al., 2002; Misra et al., 2009; Riedl and Biemann, 2012), in this work we focus on linear segmentation of text, but there is also a solid body of work on hierarchical TS, where each toplevel segment is further broken down (Yaari, 1997; Eisenstein, 2009).", "rank": 20, "start": 3287, "IsComparative": "1", "id": "st_20"}]}, {"paragraph_info": {"end": 4242, "start": 3597, "text": "Hearst (1994) introduced TextTiling, one of the first unsupervised algorithms for linear text segmentation.She exploits the fact that words tend to be repeated in coherent segments and measures the similarity between paragraphs by comparing their sparse term-vectors.Choi (2000) introduced the probabilistic algorithm using matrix-based ranking and clustering to determine similarities between segments.Galley et al.(2003) combined contentbased information with acoustic cues in order to detect discourse shifts whereas Utiyama and Isahara (2001) and Fragkou et al.(2004) minimized different segmentation cost functions with dynamic programming.", "rank": 10, "paragraph_comparative_number": 1, "entities": [], "id": "p_10"}, "sentences": [{"end": 3704, "text": "Hearst (1994) introduced TextTiling, one of the first unsupervised algorithms for linear text segmentation.", "rank": 21, "start": 3597, "IsComparative": "0", "id": "st_21"}, {"end": 3864, "text": "She exploits the fact that words tend to be repeated in coherent segments and measures the similarity between paragraphs by comparing their sparse term-vectors.", "rank": 22, "start": 3704, "IsComparative": "1", "id": "st_22"}, {"end": 4000, "text": "Choi (2000) introduced the probabilistic algorithm using matrix-based ranking and clustering to determine similarities between segments.", "rank": 23, "start": 3864, "IsComparative": "0", "id": "st_23"}, {"end": 4013, "text": "Galley et al.", "rank": 24, "start": 4000, "IsComparative": "0", "id": "st_24"}, {"end": 4162, "text": "(2003) combined contentbased information with acoustic cues in order to detect discourse shifts whereas Utiyama and Isahara (2001) and Fragkou et al.", "rank": 25, "start": 4013, "IsComparative": "0", "id": "st_25"}, {"end": 4242, "text": "(2004) minimized different segmentation cost functions with dynamic programming.", "rank": 26, "start": 4162, "IsComparative": "0", "id": "st_26"}]}, {"paragraph_info": {"end": 5471, "start": 4242, "text": "The first segmentation approach based on topic modeling (Brants et al., 2002) employed the probabilistic latent semantic analysis (pLSA) to derive latent representations of segments and determined the segmentation based on similarities of segments latent vectors.More recent models (Misra et al., 2009; Riedl and Biemann, 2012) employed the latent Dirichlet allocation (LDA) (Blei et al., 2003) to compute the latent topics and displayed superior performance to previous models on standard synthetic datasets (Choi, 2000; Galley et al., 2003).Misra et al.(2009) used dynamic programming to find globally optimal segmentation over the set of LDA-based segment representations, whereas Riedl and Biemann (2012) introduced TopicTiling, an LDA-driven extension of Hearsts TextTiling algorithm where segments are, represented as dense vectors of dominant topics of terms they contain (instead of as sparse term vectors).Riedl and Biemann (2012) show that TopicTiling outperforms at-that-time state-of-the-art methods for unsupervised linear segmentation (Choi, 2000; Utiyama and Isahara, 2001; Galley et al., 2003; Fragkou et al., 2004; Misra et al., 2009) and that it is also faster than other LDA-based methods (Misra et al., 2009).", "rank": 11, "paragraph_comparative_number": 1, "entities": [], "id": "p_11"}, "sentences": [{"end": 4505, "text": "The first segmentation approach based on topic modeling (Brants et al., 2002) employed the probabilistic latent semantic analysis (pLSA) to derive latent representations of segments and determined the segmentation based on similarities of segments latent vectors.", "rank": 27, "start": 4242, "IsComparative": "0", "id": "st_27"}, {"end": 4785, "text": "More recent models (Misra et al., 2009; Riedl and Biemann, 2012) employed the latent Dirichlet allocation (LDA) (Blei et al., 2003) to compute the latent topics and displayed superior performance to previous models on standard synthetic datasets (Choi, 2000; Galley et al., 2003).", "rank": 28, "start": 4505, "IsComparative": "0", "id": "st_28"}, {"end": 4797, "text": "Misra et al.", "rank": 29, "start": 4785, "IsComparative": "0", "id": "st_29"}, {"end": 5157, "text": "(2009) used dynamic programming to find globally optimal segmentation over the set of LDA-based segment representations, whereas Riedl and Biemann (2012) introduced TopicTiling, an LDA-driven extension of Hearsts TextTiling algorithm where segments are, represented as dense vectors of dominant topics of terms they contain (instead of as sparse term vectors).", "rank": 30, "start": 4797, "IsComparative": "1", "id": "st_30"}, {"end": 5471, "text": "Riedl and Biemann (2012) show that TopicTiling outperforms at-that-time state-of-the-art methods for unsupervised linear segmentation (Choi, 2000; Utiyama and Isahara, 2001; Galley et al., 2003; Fragkou et al., 2004; Misra et al., 2009) and that it is also faster than other LDA-based methods (Misra et al., 2009).", "rank": 31, "start": 5157, "IsComparative": "0", "id": "st_31"}]}, {"paragraph_info": {"end": 6480, "start": 5471, "text": "In the most closely related work to ours, Malioutov and Barzilay (2006) proposed a graphbased TS approach in which they first construct the fully connected graph of sentences, with edges weighted via the cosine similarity between bagof-words sentence vectors, and then run the minimum normalized multiway cut algorithm to obtain the segments.Similarly, Ferret (2007) builds the similarity graph, only between words instead of between sentences, using sparse co-occurrence vectors as semantic representations for words.He then identifies topics by clustering the word similarity graph via the Shared Nearest Neighbor algorithm (Ertoz et al., 2004)  .Unlike these works, we use the dense semantic representations of words and sentences (i.e., embeddings), which have been shown to outperform sparse semantic vectors on a range of NLP tasks.Also, instead of looking for minimal cuts in the relatedness graph, we exploit the maximal cliques of the relatedness graph between sentences to obtain the topic segments.", "rank": 12, "paragraph_comparative_number": 0, "entities": [], "id": "p_12"}, "sentences": [{"end": 5813, "text": "In the most closely related work to ours, Malioutov and Barzilay (2006) proposed a graphbased TS approach in which they first construct the fully connected graph of sentences, with edges weighted via the cosine similarity between bagof-words sentence vectors, and then run the minimum normalized multiway cut algorithm to obtain the segments.", "rank": 32, "start": 5471, "IsComparative": "0", "id": "st_32"}, {"end": 5989, "text": "Similarly, Ferret (2007) builds the similarity graph, only between words instead of between sentences, using sparse co-occurrence vectors as semantic representations for words.", "rank": 33, "start": 5813, "IsComparative": "0", "id": "st_33"}, {"end": 6120, "text": "He then identifies topics by clustering the word similarity graph via the Shared Nearest Neighbor algorithm (Ertoz et al., 2004)  .", "rank": 34, "start": 5989, "IsComparative": "0", "id": "st_34"}, {"end": 6309, "text": "Unlike these works, we use the dense semantic representations of words and sentences (i.e., embeddings), which have been shown to outperform sparse semantic vectors on a range of NLP tasks.", "rank": 35, "start": 6120, "IsComparative": "0", "id": "st_35"}, {"end": 6480, "text": "Also, instead of looking for minimal cuts in the relatedness graph, we exploit the maximal cliques of the relatedness graph between sentences to obtain the topic segments.", "rank": 36, "start": 6309, "IsComparative": "0", "id": "st_36"}]}, {"paragraph_info": {"end": 6509, "start": 6480, "text": "3 Text Segmentation Algorithm", "rank": 13, "paragraph_comparative_number": 0, "entities": [], "id": "p_13"}, "sentences": [{"end": 6509, "text": "3 Text Segmentation Algorithm", "rank": 37, "start": 6480, "IsComparative": "0", "id": "st_37"}]}, {"paragraph_info": {"end": 6910, "start": 6509, "text": "Our TS algorithm, dubbed GRAPHSEG, builds a semantic relatedness graph in which nodes denote sentences and edges are created for pairs of semantically related sentences.We then determine the coherent segments by finding maximal cliques of the relatedness graph.The novelty of GRAPHSEG is in the fact that it directly exploits the semantics of text instead of approximating the meaning with topicality.", "rank": 14, "paragraph_comparative_number": 3, "entities": [], "id": "p_14"}, "sentences": [{"end": 6678, "text": "Our TS algorithm, dubbed GRAPHSEG, builds a semantic relatedness graph in which nodes denote sentences and edges are created for pairs of semantically related sentences.", "rank": 38, "start": 6509, "IsComparative": "1", "id": "st_38"}, {"end": 6770, "text": "We then determine the coherent segments by finding maximal cliques of the relatedness graph.", "rank": 39, "start": 6678, "IsComparative": "1", "id": "st_39"}, {"end": 6910, "text": "The novelty of GRAPHSEG is in the fact that it directly exploits the semantics of text instead of approximating the meaning with topicality.", "rank": 40, "start": 6770, "IsComparative": "1", "id": "st_40"}]}, {"paragraph_info": {"end": 6947, "start": 6910, "text": "3.1 Semantic Relatedness of Sentences", "rank": 15, "paragraph_comparative_number": 0, "entities": [], "id": "p_15"}, "sentences": [{"end": 6947, "text": "3.1 Semantic Relatedness of Sentences", "rank": 41, "start": 6910, "IsComparative": "0", "id": "st_41"}]}, {"paragraph_info": {"end": 7955, "start": 6947, "text": "The measure of semantic relatedness between sentences we use is an extension of a salient greedy lemma alignment feature proposed in a supervised model by Sari  c et al.(2012).They greedily align  content words between sentences by the similarity of their distributional vectors and then sum the similarity scores of aligned word pairs.However, such greedily obtained alignment is not necessarily optimal.In contrast, we compute the optimal alignment by (1) creating a weighted complete bipartite graph between the sets of content words of the two sentences (i.e., each word from one sentence is connected with a relatedness edge to all of the words in the other sentence) and (2) running a bipartite graph matching algorithm known as the Hungarian method (Kuhn, 1955) that has the polynomial complexity.The similarities of content words between sentences (i.e., the weights of the bipartite graph) are computed as the cosine of the angle between their corresponding embedding vectors (Mikolov et al., 2013).", "rank": 16, "paragraph_comparative_number": 2, "entities": [], "id": "p_16"}, "sentences": [{"end": 7116, "text": "The measure of semantic relatedness between sentences we use is an extension of a salient greedy lemma alignment feature proposed in a supervised model by Sari  c et al.", "rank": 42, "start": 6947, "IsComparative": "0", "id": "st_42"}, {"end": 7123, "text": "(2012).", "rank": 43, "start": 7116, "IsComparative": "1", "id": "st_43"}, {"end": 7283, "text": "They greedily align  content words between sentences by the similarity of their distributional vectors and then sum the similarity scores of aligned word pairs.", "rank": 44, "start": 7123, "IsComparative": "1", "id": "st_44"}, {"end": 7352, "text": "However, such greedily obtained alignment is not necessarily optimal.", "rank": 45, "start": 7283, "IsComparative": "0", "id": "st_45"}, {"end": 7751, "text": "In contrast, we compute the optimal alignment by (1) creating a weighted complete bipartite graph between the sets of content words of the two sentences (i.e., each word from one sentence is connected with a relatedness edge to all of the words in the other sentence) and (2) running a bipartite graph matching algorithm known as the Hungarian method (Kuhn, 1955) that has the polynomial complexity.", "rank": 46, "start": 7352, "IsComparative": "0", "id": "st_46"}, {"end": 7955, "text": "The similarities of content words between sentences (i.e., the weights of the bipartite graph) are computed as the cosine of the angle between their corresponding embedding vectors (Mikolov et al., 2013).", "rank": 47, "start": 7751, "IsComparative": "0", "id": "st_47"}]}, {"paragraph_info": {"end": 8481, "start": 7955, "text": "Let A be the set of word pairs in the optimal alignment between the content-word sets of the two 126 sentences S1 and S2, i.e., A = <(w1, w2) | w1  S1  w2  S2>.We then compute the semantic relatedness for two given sentences S1 and S2 as follows: sr (S1, S2) =X (w1,w2)A cos(v1, v2)  min(ic(w1), ic(w2)) where vi is the embedding vector of the word wi and ic(w) is the information content (IC) of the word w, computed based on the relative frequency of w in some large corpus C: ic(w) =  log freq(w) + 1 |C| + P w0C freq(w0) .", "rank": 17, "paragraph_comparative_number": 2, "entities": [], "id": "p_17"}, "sentences": [{"end": 8115, "text": "Let A be the set of word pairs in the optimal alignment between the content-word sets of the two 126 sentences S1 and S2, i.e., A = <(w1, w2) | w1  S1  w2  S2>.", "rank": 48, "start": 7955, "IsComparative": "1", "id": "st_48"}, {"end": 8481, "text": "We then compute the semantic relatedness for two given sentences S1 and S2 as follows: sr (S1, S2) =X (w1,w2)A cos(v1, v2)  min(ic(w1), ic(w2)) where vi is the embedding vector of the word wi and ic(w) is the information content (IC) of the word w, computed based on the relative frequency of w in some large corpus C: ic(w) =  log freq(w) + 1 |C| + P w0C freq(w0) .", "rank": 49, "start": 8115, "IsComparative": "1", "id": "st_49"}]}, {"paragraph_info": {"end": 8849, "start": 8481, "text": "We utilize the IC weighting of embedding similarity because we assume that matches between less frequent words (e.g., guitar and ukulele) contribute more to sentence relatedness than pairs of similar but frequent words (e.g., do and make).We used Google Books Ngrams (Michel et al., 2011) as a large corpus C for estimating relative frequencies of words in a language.", "rank": 18, "paragraph_comparative_number": 0, "entities": [], "id": "p_18"}, "sentences": [{"end": 8720, "text": "We utilize the IC weighting of embedding similarity because we assume that matches between less frequent words (e.g., guitar and ukulele) contribute more to sentence relatedness than pairs of similar but frequent words (e.g., do and make).", "rank": 50, "start": 8481, "IsComparative": "0", "id": "st_50"}, {"end": 8849, "text": "We used Google Books Ngrams (Michel et al., 2011) as a large corpus C for estimating relative frequencies of words in a language.", "rank": 51, "start": 8720, "IsComparative": "0", "id": "st_51"}]}, {"paragraph_info": {"end": 9251, "start": 8849, "text": "Because there will be more aligned pairs between longer sentences, the relatedness score will be larger for longer sentences merely because of their length (regardless of their actual similarity).Thus, we normalize the sr(S1, S2) score first with the length of S1 and then with the length S2 and we finally average these two normalized scores: rel(S1, S2) = 1 2   sr (S1, S2) |S1| + sr (S1, S2) |S2|  .", "rank": 19, "paragraph_comparative_number": 1, "entities": [], "id": "p_19"}, "sentences": [{"end": 9045, "text": "Because there will be more aligned pairs between longer sentences, the relatedness score will be larger for longer sentences merely because of their length (regardless of their actual similarity).", "rank": 52, "start": 8849, "IsComparative": "1", "id": "st_52"}, {"end": 9251, "text": "Thus, we normalize the sr(S1, S2) score first with the length of S1 and then with the length S2 and we finally average these two normalized scores: rel(S1, S2) = 1 2   sr (S1, S2) |S1| + sr (S1, S2) |S2|  .", "rank": 53, "start": 9045, "IsComparative": "0", "id": "st_53"}]}, {"paragraph_info": {"end": 9279, "start": 9251, "text": "3.2 Graph-Based Segmentation", "rank": 20, "paragraph_comparative_number": 0, "entities": [], "id": "p_20"}, "sentences": [{"end": 9279, "text": "3.2 Graph-Based Segmentation", "rank": 54, "start": 9251, "IsComparative": "0", "id": "st_54"}]}, {"paragraph_info": {"end": 9476, "start": 9279, "text": "All sentences in a text become nodes of the relatedness graph G.We then compute the semantic similarity, as described in the previous subsection, between all pairs of sentences in a given document.", "rank": 21, "paragraph_comparative_number": 0, "entities": [], "id": "p_21"}, "sentences": [{"end": 9343, "text": "All sentences in a text become nodes of the relatedness graph G.", "rank": 55, "start": 9279, "IsComparative": "0", "id": "st_55"}, {"end": 9476, "text": "We then compute the semantic similarity, as described in the previous subsection, between all pairs of sentences in a given document.", "rank": 56, "start": 9343, "IsComparative": "0", "id": "st_56"}]}, {"paragraph_info": {"end": 10957, "start": 9476, "text": "For each pair of sentences for which the semantic relatedness is above some treshold value  we add an edge between the corresponding nodes of G. Next, we employ the Bron-Kerbosch algorithm (Bron and Kerbosch, 1973) to compute the set Q of all maximal cliques of G.We then create the initial set of segments SG by merging adjacent sentences found in at least one maximal clique Q  Q of graph G. Next, we merge the adjacent segments sgi and sgi+1 for which there is at least one clique Q  Q containing at least one sentence from sgi and one sentence from sgi+1.Finally, given the Step Sets Cliques Q <1, 2, 6>, <2, 4, 7>, <3, 4, 5>, <1, 8, 9> Init.seg.<1, 2>, <3, 4, 5>, <6>, <7> <8, 9> Merge seg.<1, 2, 3, 4, 5>, <6>, <7>, <8, 9> Merge small <1, 2, 3, 4, 5>, <6, 7>, <8, 9> Table 1: Creating segments from graph cliques (n = 2).In the third step we merge segments <1, 2, 3> and <4, 5> because the second clique contains sentences 2 (from the left segment) and 4 (from the right segment).In the final step we merge single sentence segments (assuming segs(<1, 2, 3, 4, 5>, <6>) < segs(<6>, <7>) and segs(<7>, <8, 9>) < segs(<6>, <7>)).minimal segment size n, we merge segments sgi with less than n sentences with the semantically more related of the two adjacent segments  sgi1 or sgi+1.The relatedness between two adjacent segments (sgr (sgi , sgi+1)) is computed as the average relatedness between their respective sentences: sgr (SG1, SG2) = 1 |SG1||SG2| X S1SG1 S2SG2 rel(S1, S2).", "rank": 22, "paragraph_comparative_number": 3, "entities": [], "id": "p_22"}, "sentences": [{"end": 9740, "text": "For each pair of sentences for which the semantic relatedness is above some treshold value  we add an edge between the corresponding nodes of G. Next, we employ the Bron-Kerbosch algorithm (Bron and Kerbosch, 1973) to compute the set Q of all maximal cliques of G.", "rank": 57, "start": 9476, "IsComparative": "0", "id": "st_57"}, {"end": 10035, "text": "We then create the initial set of segments SG by merging adjacent sentences found in at least one maximal clique Q  Q of graph G. Next, we merge the adjacent segments sgi and sgi+1 for which there is at least one clique Q  Q containing at least one sentence from sgi and one sentence from sgi+1.", "rank": 58, "start": 9740, "IsComparative": "0", "id": "st_58"}, {"end": 10122, "text": "Finally, given the Step Sets Cliques Q <1, 2, 6>, <2, 4, 7>, <3, 4, 5>, <1, 8, 9> Init.", "rank": 59, "start": 10035, "IsComparative": "0", "id": "st_59"}, {"end": 10126, "text": "seg.", "rank": 60, "start": 10122, "IsComparative": "0", "id": "st_60"}, {"end": 10171, "text": "<1, 2>, <3, 4, 5>, <6>, <7> <8, 9> Merge seg.", "rank": 61, "start": 10126, "IsComparative": "0", "id": "st_61"}, {"end": 10303, "text": "<1, 2, 3, 4, 5>, <6>, <7>, <8, 9> Merge small <1, 2, 3, 4, 5>, <6, 7>, <8, 9> Table 1: Creating segments from graph cliques (n = 2).", "rank": 62, "start": 10171, "IsComparative": "0", "id": "st_62"}, {"end": 10462, "text": "In the third step we merge segments <1, 2, 3> and <4, 5> because the second clique contains sentences 2 (from the left segment) and 4 (from the right segment).", "rank": 63, "start": 10303, "IsComparative": "1", "id": "st_63"}, {"end": 10608, "text": "In the final step we merge single sentence segments (assuming segs(<1, 2, 3, 4, 5>, <6>) < segs(<6>, <7>) and segs(<7>, <8, 9>) < segs(<6>, <7>)).", "rank": 64, "start": 10462, "IsComparative": "1", "id": "st_64"}, {"end": 10760, "text": "minimal segment size n, we merge segments sgi with less than n sentences with the semantically more related of the two adjacent segments  sgi1 or sgi+1.", "rank": 65, "start": 10608, "IsComparative": "0", "id": "st_65"}, {"end": 10957, "text": "The relatedness between two adjacent segments (sgr (sgi , sgi+1)) is computed as the average relatedness between their respective sentences: sgr (SG1, SG2) = 1 |SG1||SG2| X S1SG1 S2SG2 rel(S1, S2).", "rank": 66, "start": 10760, "IsComparative": "1", "id": "st_66"}]}, {"paragraph_info": {"end": 11094, "start": 10957, "text": "We exemplify the creation of segments from maximal cliques in Table 1.The complete segmentation algorithm is fleshed out in Algorithm 1.1", "rank": 23, "paragraph_comparative_number": 0, "entities": [], "id": "p_23"}, "sentences": [{"end": 11027, "text": "We exemplify the creation of segments from maximal cliques in Table 1.", "rank": 67, "start": 10957, "IsComparative": "0", "id": "st_67"}, {"end": 11094, "text": "The complete segmentation algorithm is fleshed out in Algorithm 1.1", "rank": 68, "start": 11027, "IsComparative": "0", "id": "st_68"}]}, {"paragraph_info": {"end": 11106, "start": 11094, "text": "4 Evaluation", "rank": 24, "paragraph_comparative_number": 0, "entities": [], "id": "p_24"}, "sentences": [{"end": 11106, "text": "4 Evaluation", "rank": 69, "start": 11094, "IsComparative": "0", "id": "st_69"}]}, {"paragraph_info": {"end": 11454, "start": 11106, "text": "In this section, we first introduce the two evaluation datasets that we use one being the commonly used synthetic dataset and the other a realistic dataset of politi- cal manifestos.Following, we present the experimental setting and finally describe and discuss the results achieved by our GRAPHSEG algorithm and how it compares to other TS models.", "rank": 25, "paragraph_comparative_number": 2, "entities": [], "id": "p_25"}, "sentences": [{"end": 11288, "text": "In this section, we first introduce the two evaluation datasets that we use one being the commonly used synthetic dataset and the other a realistic dataset of politi- cal manifestos.", "rank": 70, "start": 11106, "IsComparative": "1", "id": "st_70"}, {"end": 11454, "text": "Following, we present the experimental setting and finally describe and discuss the results achieved by our GRAPHSEG algorithm and how it compares to other TS models.", "rank": 71, "start": 11288, "IsComparative": "1", "id": "st_71"}]}, {"paragraph_info": {"end": 11466, "start": 11454, "text": "4.1 Datasets", "rank": 26, "paragraph_comparative_number": 0, "entities": [], "id": "p_26"}, "sentences": [{"end": 11466, "text": "4.1 Datasets", "rank": 72, "start": 11454, "IsComparative": "0", "id": "st_72"}]}, {"paragraph_info": {"end": 12251, "start": 11466, "text": "Unsupervised methods for text segmentation have most often been evaluated on synthetic datasets with segments from different sources being concatenated in artificial documents (Choi, 2000; Galley et al., 2003).Segmenting such artificial texts is easier than segmenting real-world documents.This is why besides on the artificial Choi dataset we also evaluate GRAPHSEG on a real-world dataset of political texts from the Manifesto Project,2,3 manually labeled by domain experts with segments of seven different topics (e.g., economy and welfare, quality of life, foreign affairs).The selected manifestos contain between 1000 and 2500 sentences, with segments ranging in length from 1 to 78 sentences, which is in sharp contrast to the Choi dataset where all segments are of similar size.", "rank": 27, "paragraph_comparative_number": 1, "entities": [], "id": "p_27"}, "sentences": [{"end": 11676, "text": "Unsupervised methods for text segmentation have most often been evaluated on synthetic datasets with segments from different sources being concatenated in artificial documents (Choi, 2000; Galley et al., 2003).", "rank": 73, "start": 11466, "IsComparative": "0", "id": "st_73"}, {"end": 11756, "text": "Segmenting such artificial texts is easier than segmenting real-world documents.", "rank": 74, "start": 11676, "IsComparative": "0", "id": "st_74"}, {"end": 12044, "text": "This is why besides on the artificial Choi dataset we also evaluate GRAPHSEG on a real-world dataset of political texts from the Manifesto Project,2,3 manually labeled by domain experts with segments of seven different topics (e.g., economy and welfare, quality of life, foreign affairs).", "rank": 75, "start": 11756, "IsComparative": "0", "id": "st_75"}, {"end": 12251, "text": "The selected manifestos contain between 1000 and 2500 sentences, with segments ranging in length from 1 to 78 sentences, which is in sharp contrast to the Choi dataset where all segments are of similar size.", "rank": 76, "start": 12044, "IsComparative": "1", "id": "st_76"}]}, {"paragraph_info": {"end": 12275, "start": 12251, "text": "4.2 Experimental Setting", "rank": 28, "paragraph_comparative_number": 0, "entities": [], "id": "p_28"}, "sentences": [{"end": 12275, "text": "4.2 Experimental Setting", "rank": 77, "start": 12251, "IsComparative": "0", "id": "st_77"}]}, {"paragraph_info": {"end": 12610, "start": 12275, "text": "To allow for comparison with previous work, we evaluate GRAPHSEG on four subsets of the Choi dataset, differing in number of sentences the seg- 2008, and 2012 U.S.elections ments contain.For the evaluation on the Choi dataset, the GRAPHSEG algorithm made use of the publicly available word embeddings built from a Google News dataset.4", "rank": 29, "paragraph_comparative_number": 2, "entities": [], "id": "p_29"}, "sentences": [{"end": 12438, "text": "To allow for comparison with previous work, we evaluate GRAPHSEG on four subsets of the Choi dataset, differing in number of sentences the seg- 2008, and 2012 U.S.", "rank": 78, "start": 12275, "IsComparative": "1", "id": "st_78"}, {"end": 12462, "text": "elections ments contain.", "rank": 79, "start": 12438, "IsComparative": "0", "id": "st_79"}, {"end": 12610, "text": "For the evaluation on the Choi dataset, the GRAPHSEG algorithm made use of the publicly available word embeddings built from a Google News dataset.4", "rank": 80, "start": 12462, "IsComparative": "1", "id": "st_80"}]}, {"paragraph_info": {"end": 13391, "start": 12610, "text": "Both LDA-based models (Misra et al., 2009; Riedl and Biemann, 2012) and GRAPHSEG rely on corpus-derived word representations.Thus, we evaluated on the Manifesto dataset both the domainadapted and domain-unadapted variants of these methods.The domain-adapted variants of the models used the unlabeled domain corpus  a test set of 466 unlabeled political manifestos  to train the domain-specific word representations.This means that we obtain (1) in-domain topics for the LDAbased TopicTiling model of Riedl and Biemann (2012) and (2) domain-specific embeddings for the GRAPHSEG algorithm.On the Manifesto dataset we also evaluate a baseline that randomly (50% chance) starts a new segment at points m sentences apart, with m being set to half of the average length of gold segments.", "rank": 30, "paragraph_comparative_number": 4, "entities": [], "id": "p_30"}, "sentences": [{"end": 12735, "text": "Both LDA-based models (Misra et al., 2009; Riedl and Biemann, 2012) and GRAPHSEG rely on corpus-derived word representations.", "rank": 81, "start": 12610, "IsComparative": "1", "id": "st_81"}, {"end": 12849, "text": "Thus, we evaluated on the Manifesto dataset both the domainadapted and domain-unadapted variants of these methods.", "rank": 82, "start": 12735, "IsComparative": "0", "id": "st_82"}, {"end": 13025, "text": "The domain-adapted variants of the models used the unlabeled domain corpus  a test set of 466 unlabeled political manifestos  to train the domain-specific word representations.", "rank": 83, "start": 12849, "IsComparative": "1", "id": "st_83"}, {"end": 13197, "text": "This means that we obtain (1) in-domain topics for the LDAbased TopicTiling model of Riedl and Biemann (2012) and (2) domain-specific embeddings for the GRAPHSEG algorithm.", "rank": 84, "start": 13025, "IsComparative": "1", "id": "st_84"}, {"end": 13391, "text": "On the Manifesto dataset we also evaluate a baseline that randomly (50% chance) starts a new segment at points m sentences apart, with m being set to half of the average length of gold segments.", "rank": 85, "start": 13197, "IsComparative": "1", "id": "st_85"}]}, {"paragraph_info": {"end": 14295, "start": 13391, "text": "We evaluate the performance using two standard TS evaluation metrics  Pk (Beeferman et al., 1999) and WindowDiff (WD) (Pevzner and Hearst, 2002).Pk is the probability that two randomly drawn sentences mutually k sentences apart are classified incorrectly  either as belonging to the same segment when they are in different gold segments or as being in different segments when they are in the same gold segment.Following Riedl and Biemann (2012), we set k to half of the document length divided by the number of gold segments.WindowDiff is a stricter version of Pk as, instead of only checking if the randomly chosen sentences are in the same predicted segment or not, it compares the exact number of segments between the sentences in the predicted segmentation with the number of segments in between the same sentences in the gold standard.Lower scores indicate better performance for both these metrics.", "rank": 31, "paragraph_comparative_number": 2, "entities": [], "id": "p_31"}, "sentences": [{"end": 13536, "text": "We evaluate the performance using two standard TS evaluation metrics  Pk (Beeferman et al., 1999) and WindowDiff (WD) (Pevzner and Hearst, 2002).", "rank": 86, "start": 13391, "IsComparative": "0", "id": "st_86"}, {"end": 13801, "text": "Pk is the probability that two randomly drawn sentences mutually k sentences apart are classified incorrectly  either as belonging to the same segment when they are in different gold segments or as being in different segments when they are in the same gold segment.", "rank": 87, "start": 13536, "IsComparative": "1", "id": "st_87"}, {"end": 13916, "text": "Following Riedl and Biemann (2012), we set k to half of the document length divided by the number of gold segments.", "rank": 88, "start": 13801, "IsComparative": "1", "id": "st_88"}, {"end": 14231, "text": "WindowDiff is a stricter version of Pk as, instead of only checking if the randomly chosen sentences are in the same predicted segment or not, it compares the exact number of segments between the sentences in the predicted segmentation with the number of segments in between the same sentences in the gold standard.", "rank": 89, "start": 13916, "IsComparative": "0", "id": "st_89"}, {"end": 14295, "text": "Lower scores indicate better performance for both these metrics.", "rank": 90, "start": 14231, "IsComparative": "0", "id": "st_90"}]}, {"paragraph_info": {"end": 14337, "start": 14295, "text": "The GRAPHSEG algorithm has two parameters:", "rank": 32, "paragraph_comparative_number": 1, "entities": [], "id": "p_32"}, "sentences": [{"end": 14337, "text": "The GRAPHSEG algorithm has two parameters:", "rank": 91, "start": 14295, "IsComparative": "1", "id": "st_91"}]}, {"paragraph_info": {"end": 14911, "start": 14337, "text": "(1) the sentence similarity treshold  which is used when creating edges of the sentence relatedness graph and (2) the minimal segment size n, which we utilize to merge adjacent segments that are too small.In all experiments we use grid-search in a folded cross-validation setting to jointly optimize both parameters.In view of comparison with other models, the parameter optimization is justified be cause other models, e.g., TopicTiling (Riedl and Biemann, 2012), also have parameters (e.g., number of topics for the topic model) which are optimized using cross-validation.", "rank": 33, "paragraph_comparative_number": 0, "entities": [], "id": "p_33"}, "sentences": [{"end": 14542, "text": "(1) the sentence similarity treshold  which is used when creating edges of the sentence relatedness graph and (2) the minimal segment size n, which we utilize to merge adjacent segments that are too small.", "rank": 92, "start": 14337, "IsComparative": "0", "id": "st_92"}, {"end": 14653, "text": "In all experiments we use grid-search in a folded cross-validation setting to jointly optimize both parameters.", "rank": 93, "start": 14542, "IsComparative": "0", "id": "st_93"}, {"end": 14911, "text": "In view of comparison with other models, the parameter optimization is justified be cause other models, e.g., TopicTiling (Riedl and Biemann, 2012), also have parameters (e.g., number of topics for the topic model) which are optimized using cross-validation.", "rank": 94, "start": 14653, "IsComparative": "0", "id": "st_94"}]}, {"paragraph_info": {"end": 14937, "start": 14911, "text": "4.3 Results and Discussion", "rank": 34, "paragraph_comparative_number": 1, "entities": [], "id": "p_34"}, "sentences": [{"end": 14937, "text": "4.3 Results and Discussion", "rank": 95, "start": 14911, "IsComparative": "1", "id": "st_95"}]}, {"paragraph_info": {"end": 15887, "start": 14937, "text": "In Table 2 we report the performance of GRAPHSEG and prominent TS methods on the synthetic Choi dataset.GRAPHSEG performs competitively, outperforming all methods but (Fragkou et al., 2004) and domain-adapted versions of LDA-based models (Misra et al., 2009; Riedl and Biemann, 2012).However, the approach by (Fragkou et al., 2004) uses the gold standard information  the average gold segment size  as input.On the other hand, the LDA-based models adapt their topic models on parts of the Choi dataset itself.Despite the fact that they use different documents for training the topic models from those used for evaluating segmentation quality, the evaluation is still tainted because snippets from the original documents appear in multiple artificial documents  some of which belong to the the training set and others to the test set, as admitted by Riedl and Biemann (2012) and this is why their reported performance on this dataset is overestimated.", "rank": 35, "paragraph_comparative_number": 2, "entities": [], "id": "p_35"}, "sentences": [{"end": 15041, "text": "In Table 2 we report the performance of GRAPHSEG and prominent TS methods on the synthetic Choi dataset.", "rank": 96, "start": 14937, "IsComparative": "1", "id": "st_96"}, {"end": 15221, "text": "GRAPHSEG performs competitively, outperforming all methods but (Fragkou et al., 2004) and domain-adapted versions of LDA-based models (Misra et al., 2009; Riedl and Biemann, 2012).", "rank": 97, "start": 15041, "IsComparative": "0", "id": "st_97"}, {"end": 15345, "text": "However, the approach by (Fragkou et al., 2004) uses the gold standard information  the average gold segment size  as input.", "rank": 98, "start": 15221, "IsComparative": "0", "id": "st_98"}, {"end": 15446, "text": "On the other hand, the LDA-based models adapt their topic models on parts of the Choi dataset itself.", "rank": 99, "start": 15345, "IsComparative": "1", "id": "st_99"}, {"end": 15887, "text": "Despite the fact that they use different documents for training the topic models from those used for evaluating segmentation quality, the evaluation is still tainted because snippets from the original documents appear in multiple artificial documents  some of which belong to the the training set and others to the test set, as admitted by Riedl and Biemann (2012) and this is why their reported performance on this dataset is overestimated.", "rank": 100, "start": 15446, "IsComparative": "0", "id": "st_100"}]}, {"paragraph_info": {"end": 16743, "start": 15887, "text": "In Table 3 we report the results on the Manifesto dataset.Results of both TopicTiling and GRAPHSEG indicate that the realistic Manifesto dataset is much more difficult to segment than the artificial Choi dataset.The GRAPHSEG algorithm significantly outperforms the TopicTiling method (p < 0.05, Students t-test).In-domain training of word representations, topics for TopicTiling and word embeddings for GraphSeg, does not signifi- cantly improve the performance for neither of the two models.This result contrasts previous findings (Misra et al., 2009; Riedl and Biemann, 2012) in which the performance boost was credited to the indomain trained topics and supports our hypothesis that the performance boost of the LDA-based methods with in-domain trained topics originates from information leakage between different portions of the synthetic Choi dataset.", "rank": 36, "paragraph_comparative_number": 3, "entities": [], "id": "p_36"}, "sentences": [{"end": 15945, "text": "In Table 3 we report the results on the Manifesto dataset.", "rank": 101, "start": 15887, "IsComparative": "0", "id": "st_101"}, {"end": 16099, "text": "Results of both TopicTiling and GRAPHSEG indicate that the realistic Manifesto dataset is much more difficult to segment than the artificial Choi dataset.", "rank": 102, "start": 15945, "IsComparative": "1", "id": "st_102"}, {"end": 16199, "text": "The GRAPHSEG algorithm significantly outperforms the TopicTiling method (p < 0.05, Students t-test).", "rank": 103, "start": 16099, "IsComparative": "0", "id": "st_103"}, {"end": 16379, "text": "In-domain training of word representations, topics for TopicTiling and word embeddings for GraphSeg, does not signifi- cantly improve the performance for neither of the two models.", "rank": 104, "start": 16199, "IsComparative": "1", "id": "st_104"}, {"end": 16743, "text": "This result contrasts previous findings (Misra et al., 2009; Riedl and Biemann, 2012) in which the performance boost was credited to the indomain trained topics and supports our hypothesis that the performance boost of the LDA-based methods with in-domain trained topics originates from information leakage between different portions of the synthetic Choi dataset.", "rank": 105, "start": 16379, "IsComparative": "1", "id": "st_105"}]}, {"paragraph_info": {"end": 16755, "start": 16743, "text": "5 Conclusion", "rank": 37, "paragraph_comparative_number": 0, "entities": [], "id": "p_37"}, "sentences": [{"end": 16755, "text": "5 Conclusion", "rank": 106, "start": 16743, "IsComparative": "0", "id": "st_106"}]}, {"paragraph_info": {"end": 17571, "start": 16755, "text": "In this work we presented GRAPHSEG, a novel graph-based algorithm for unsupervised text segmentation.GRAPHSEG employs word embeddings and extends a measure of semantic relatedness to construct a relatedness graph with edges established between semantically related sentences.The segmentation is then determined by the maximal cliques of the relatedness graph and improved by semantic comparison of adjacent segments.GRAPHSEG displays competitive performance compared to best-performing LDA-based methods on a synthetic dataset.However, we identify and discuss evaluation issues pertaining to LDA-based TS on this dataset.We also performed an evaluation on the real-world dataset of political manifestos and showed that in a realistic setting GRAPHSEG significantly outperforms the state-of-the-art LDAbased TS model.", "rank": 38, "paragraph_comparative_number": 3, "entities": [], "id": "p_38"}, "sentences": [{"end": 16856, "text": "In this work we presented GRAPHSEG, a novel graph-based algorithm for unsupervised text segmentation.", "rank": 107, "start": 16755, "IsComparative": "0", "id": "st_107"}, {"end": 17030, "text": "GRAPHSEG employs word embeddings and extends a measure of semantic relatedness to construct a relatedness graph with edges established between semantically related sentences.", "rank": 108, "start": 16856, "IsComparative": "1", "id": "st_108"}, {"end": 17171, "text": "The segmentation is then determined by the maximal cliques of the relatedness graph and improved by semantic comparison of adjacent segments.", "rank": 109, "start": 17030, "IsComparative": "0", "id": "st_109"}, {"end": 17282, "text": "GRAPHSEG displays competitive performance compared to best-performing LDA-based methods on a synthetic dataset.", "rank": 110, "start": 17171, "IsComparative": "0", "id": "st_110"}, {"end": 17376, "text": "However, we identify and discuss evaluation issues pertaining to LDA-based TS on this dataset.", "rank": 111, "start": 17282, "IsComparative": "1", "id": "st_111"}, {"end": 17571, "text": "We also performed an evaluation on the real-world dataset of political manifestos and showed that in a realistic setting GRAPHSEG significantly outperforms the state-of-the-art LDAbased TS model.", "rank": 112, "start": 17376, "IsComparative": "1", "id": "st_112"}]}]}