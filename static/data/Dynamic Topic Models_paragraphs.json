{"paragraph_scenes_info": [{"x": 1, "text": "A family of probabilistic time series models is developed to analyze the time evolution of topics in large document collections.The approach is to use state space models on the natural parameters of the multinomial distributions that represent the topics.Variational approximations based on Kalman filters and nonparametric wavelet regression are developed to carry out approximate posterior inference over the latent topics.In addition to giving quantitative, predictive models of a sequential corpus, dynamic topic models provide a qualitative window into the contents of a large document collection.The models are demonstrated by analyzing the OCRed archives of the journal Science from 1880 through 2000."}, {"x": 3, "text": "Managing the explosion of electronic document archives requires new tools for automatically organizing, searching, indexing, and browsing large collections.Recent research in machine learning and statistics has developed new techniques for finding patterns of words in document collections using hierarchical probabilistic models (Blei et al., 2003; McCallum et al., 2004; Rosen-Zvi et al., 2004; Griffiths and Steyvers, 2004; Buntine and Jakulin, 2004; Blei and Lafferty, 2006).These models are called topic models because the discovered patterns often reflect the underlying topics which combined to form the documents.Such hierarchical probabilistic models are easily generalized to other kinds of data; for example, topic models have been used to analyze images (Fei-Fei and Perona, 2005; Sivic et al., 2005), biological data (Pritchard et al., 2000), and survey data (Erosheva, 2002).In an exchangeable topic model, the words of each document are assumed to be independently drawn from a mixture of multinomials.The mixing proportions are randomly drawn for each document; the mixture components, or topics, are shared by all documents.Thus, each document reflects the components with different proportions.These models are a powerful method of dimensionality reduction for large collections of unstructured documents.Moreover, posterior inference at the document level is useful for information retrieval, classification, and topic-directed browsing."}, {"x": 4, "text": "Treating words exchangeably is a simplification that it is consistent with the goal of identifying the semantic themes within each document.For many collections of interest, however, the implicit assumption of exchangeable documents is inappropriate.Document collections such as scholarly journals, email, news articles, and search query logs all reflect evolving content.For example, the Science article The Brain of Professor Laborde may be on the same scientific path as the article Reshaping the Cortical MotorMap by Unmasking Latent Intracortical Connections, but the study of neuroscience looked much different in 1903 than it did in 1991.The themes in a document collection evolve over time, and it is of interest to explicitly model the dynamics of the underlying topics."}, {"x": 5, "text": "In this paper, we develop a dynamic topic model which captures the evolution of topics in a sequentially organized corpus of documents.We demonstrate its applicability by analyzing over 100 years of OCRed articles from the journal Science, which was founded in 1880 by Thomas Edison and has been published through the present.Under this model, articles are grouped by year, and each years articles arise from a set of topics that have evolved from the last years topics."}, {"x": 9, "text": "First, we review the underlying statistical assumptions of a static topic model, such as latent Dirichlet allocation (LDA) (Blei et al., 2003).Let \f1:K be K topics, each of which is a distribution over a fixed vocabulary.In a static topic model, each document is assumed drawn from the following generative process:"}, {"x": 10, "text": "1.Choose topic proportions  from a distribution over the (K  1)-simplex, such as a Dirichlet."}, {"x": 13, "text": "For a K-component model with V terms, let \ft,k denote the V -vector of natural parameters for topic k in slice t. The usual representation of a multinomial distribution is by its mean parameterization.If we denote the mean parameter of a V -dimensional multinomial by , the ith component of the natural parameter is given by the mapping \fi = log(i/V ).In typical language modeling applications, Dirichlet distributions are used to model uncertainty about the distributions over words.However, the Dirichlet is not amenable to sequential modeling.Instead, we chain the natural parameters of each topic \ft,k in a state space model that evolves with Gaussian noise; the simplest version of such a model is \ft,k | \ft1,k  N(\ft1,k, 2I) .(1)"}, {"x": 14, "text": "Our approach is thus to model sequences of compositional random variables by chaining Gaussian distributions in a dynamic model and mapping the emitted values to the simplex.This is an extension of the logistic normal distribution (Aitchison, 1982) to time-series simplex data (West and Harrison, 1997)."}, {"x": 15, "text": "In LDA, the document-specific topic proportions  are drawn from a Dirichlet distribution.In the dynamic topic model, we use a logistic normal with mean \u000b to express uncertainty over proportions.The sequential structure between models is again captured with a simple dynamic model"}, {"x": 16, "text": "For simplicity, we do not model the dynamics of topic correlation, as was done for static models by Blei and Lafferty (2006)."}, {"x": 20, "text": "For clarity of presentation, we now focus on a model with K dynamic topics evolving as in (1), and where the topic proportion model is fixed at a Dirichlet.The technical issues associated with modeling the topic proportions in a time series as in (2) are essentially the same as those for chaining the topics together."}, {"x": 22, "text": "Working with time series over the natural parameters enables the use of Gaussian models for the time dynamics; however, due to the nonconjugacy of the Gaussian and multinomial models, posterior inference is intractable.In this section, we present a variational method for approximate posterior inference.We use variational methods as deterministic alternatives to stochastic simulation, in order to handle the large data sets typical of text analysis."}, {"x": 23, "text": "While Gibbs sampling has been effectively used for static topic models (Griffiths and Steyvers, 2004), nonconjugacy makes sampling methods more difficult for this dynamic model."}, {"x": 24, "text": "The idea behind variational methods is to optimize the free parameters of a distribution over the latent variables so that the distribution is close in Kullback-Liebler (KL) divergence to the true posterior; this distribution can then be used as a substitute for the true posterior.In the dynamic topic model, the latent variables are the topics \ft,k, mixture proportions t,d, and topic indicators zt,d,n.The variational distribution reflects the group structure of the latent variables."}, {"x": 26, "text": "In the commonly used mean-field approximation, each latent variable is considered independently of the others.In the variational distribution of <\fk,1, ..., \fk,T >, however, we retain the sequential structure of the topic by positing a dynamic model with Gaussian variational observations <  \fk,1, ...,  \fk,T >.These parameters are fit to minimize the KL divergence between the resulting posterior, which is Gaussian, and the true posterior, which is not Gaussian.(A similar technique for Gaussian processes is described in Snelson and Ghahramani, 2006.)"}, {"x": 27, "text": "The variational distribution of the document-level latent variables follows the same form as in Blei et al.(2003).Each proportion vector t,d is endowed with a free Dirichlet parameter  t,d, each topic indicator zt,d,n is endowed with a free multinomial parameter t,d,n, and optimization proceeds by coordinate ascent.The updates for the documentlevel variational parameters have a closed form; we use the conjugate gradient method to optimize the topic-level variational observations.The resulting variational approximation for the natural topic parameters <\fk,1, ..., \fk,T > incorporates the time dynamics; we describe one approximation based on a Kalman filter, and a second based on wavelet regression."}, {"x": 29, "text": "The view of the variational parameters as outputs is based on the symmetry properties of the Gaussian density, f,(x) = fx,(), which enables the use of the standard forward-backward calculations for linear state space models.The graphical model and its variational approximation are shown in Figure 2.Here the triangles denote variational parameters; they can be thought of as hypothetical outputs of the Kalman filter, to facilitate calculation."}, {"x": 31, "text": "The variational parameters are  \ft and t. Using standard Kalman filter calculations (Kalman, 1960), the forward mean and variance of the variational posterior are given by"}, {"x": 33, "text": "with initial conditions emT = mT and eVT = VT .We approximate the posterior p(\f1:T |w1:T ) using the state space posterior q(\f1:T |  \f1:T ).From Jensens inequality, the loglikelihood is bounded from below as"}, {"x": 35, "text": "Variational Wavelet Regression"}, {"x": 36, "text": "The variational Kalman filter can be replaced with variational wavelet regression; for a readable introduction standard wavelet methods, see Wasserman (2006).We rescale time so it is between 0 and 1.For 128 years of Science we take n = 2J and J = 7.To be consistent with our earlier"}, {"x": 38, "text": "where t  N(0, 1).Our variational wavelet regression algorithm estimates <  \ft>, which we view as observed data, just as in the Kalman filter method, as well as the noise level ."}, {"x": 39, "text": "For concreteness, we illustrate the technique using the Haar wavelet basis; Daubechies wavelets are used in our actual examples.The model is then  where xt = t/n, (x) = 1 for 0  x  1, and  jk(x) = 2j/2 (2jx  k).Our variational estimate for the posterior mean becomes where \u000b = n1Pn t=1  \ft, and Djk are obtained by thresholding the coefficients"}, {"x": 40, "text": "To estimate  \ft we use gradient ascent, as for the Kalman filter approximation, requiring the derivatives @ emt/@  \ft. If soft thresholding is used, then we have that"}, {"x": 45, "text": "We analyzed a subset of 30,000 articles from Science, 250 from each of the 120 years between 1881 and 1999.Our data were collected by JSTOR (www.jstor.org), a notfor- profit organization that maintains an online scholarly archive obtained by running an optical character recognition (OCR) engine over the original printed journals.JSTOR indexes the resulting text and provides online access to the scanned images of the original content through keyword search."}, {"x": 46, "text": "Our corpus is made up of approximately 7.5 million words.We pruned the vocabulary by stemming each term to its root, removing function terms, and removing terms that occurred fewer than 25 times.The total vocabulary size is 15,955.To explore the corpus and its themes, we estimated a 20-component dynamic topic model.Posterior inference took approximately 4 hours on a 1.5GHZ PowerPC Macintosh laptop.Two of the resulting topics are illustrated in Figure 4, showing the top several words from those topics in each decade, according to the posterior mean number of occurrences as estimated using the Kalman filter variational approximation.Also shown are example articles which exhibit those topics through the decades.As illustrated, the model captures different scientific themes, and can be used to inspect trends of word usage within them."}, {"x": 47, "text": "To validate the dynamic topicmodel quantitatively, we consider the task of predicting the next year of Science given all the articles from the previous years.We compare the predictive power of three 20-topic models: the dynamic topic model estimated from all of the previous years, a static topic model estimated from all of the previous years, and a static topic model estimated from the single previous year.All the models are estimated to the same convergence criterion.The topic model estimated from all the previous data and dynamic topic model are initialized at the same point."}, {"x": 50, "text": "We have developed sequential topic models for discrete data by using Gaussian time series on the natural parameters of the multinomial topics and logistic normal topic proportion models.We derived variational inference algorithms that exploit existing techniques for sequential data; we demonstrated a novel use of Kalman filters and wavelet regression as variational approximations.Dynamic topic models can give a more accurate predictive model, and also offer new ways of browsing large, unstructured document collections."}, {"x": 51, "text": "There are many ways that the work described here can be extended.One direction is to use more sophisticated state space models.We have demonstrated the use of a simple Gaussian model, but it would be natural to include a drift term in a more sophisticated autoregressive model to explicitly capture the rise and fall in popularity of a topic, or in the use of specific terms.Another variant would allow for heteroscedastic time series."}, {"x": 52, "text": "Perhaps the most promising extension to the methods presented here is to incorporate a model of how new topics in the collection appear or disappear over time, rather than assuming a fixed number of topics.One possibility is to use a simple Galton-Watson or birth-death process for the topic population.While the analysis of birth-death or branching processes often centers on extinction probabilities, here a goal would be to find documents that may be responsible for spawning new themes in a collection."}], "chapters": [{"text": "Abstract", "sentence_id": "s_0", "sentence_rank": "0", "paragraph_id": "p_0", "paragraph_rank": 0}, {"text": "Introduction", "sentence_id": "s_6", "sentence_rank": "6", "paragraph_id": "p_2", "paragraph_rank": 2}, {"text": "Dynamic Topic Models", "sentence_id": "s_27", "sentence_rank": "27", "paragraph_id": "p_7", "paragraph_rank": 7}, {"text": "Approximate Inference", "sentence_id": "s_69", "sentence_rank": "69", "paragraph_id": "p_21", "paragraph_rank": 21}, {"text": "Variational Kalman Filtering", "sentence_id": "s_97", "sentence_rank": "97", "paragraph_id": "p_28", "paragraph_rank": 28}, {"text": "Variational Wavelet Regression", "sentence_id": "s_112", "sentence_rank": "112", "paragraph_id": "p_35", "paragraph_rank": 35}, {"text": "Analysis of Science", "sentence_id": "s_131", "sentence_rank": "131", "paragraph_id": "p_44", "paragraph_rank": 44}, {"text": "Discussion", "sentence_id": "s_150", "sentence_rank": "150", "paragraph_id": "p_49", "paragraph_rank": 49}], "scenes": [["Science"], ["David_Blei"], ["Brain", "Professor", "Cerebral_cortex", "Science"], ["Science", "Thomas_Edison"], ["David_Blei", "Latent_Dirichlet_allocation"], ["Dirichlet_distribution"], ["Dirichlet_distribution", "Normal_distribution"], ["Normal_distribution", "Longitude"], ["Dirichlet_distribution", "Latent_Dirichlet_allocation"], ["David_Blei"], ["Dirichlet_distribution"], ["Normal_distribution"], ["Gibbs_sampling"], ["Kullback\u2013Leibler_divergence"], ["Normal_distribution", "Kullback\u2013Leibler_divergence"], ["Kalman_filter", "Dirichlet_distribution", "Cretaceous\u2013Paleogene_extinction_event"], ["Kalman_filter", "Normal_distribution"], ["Kalman_filter"], ["Vermont", "Montana"], ["Wavelet"], ["Science_(journal)", "Kalman_filter"], ["Kalman_filter"], ["Haar_wavelet", "Daubechies_wavelet"], ["Kalman_filter"], ["Science_(journal)", "Optical_character_recognition", "JSTOR"], ["Kalman_filter", "Mac_OS", "PowerPC"], ["Science_(journal)"], ["Normal_distribution"], ["Normal_distribution"], ["Galton\u2013Watson_process"]], "characters": [{"name": "Science", "offsets": [12163, 13962, 15322], "paragraph_occurrences": [36, 45, 47], "sentence_occurrences": [115, 132, 143], "affiliation": "light", "frequency": 3, "id": "Science_(journal)"}, {"name": "Optical character recognition", "offsets": [14201], "paragraph_occurrences": [45], "sentence_occurrences": [133], "affiliation": "light", "frequency": 1, "id": "Optical_character_recognition"}, {"name": "Galton\u2013Watson process", "offsets": [17344], "paragraph_occurrences": [52], "sentence_occurrences": [159], "affiliation": "light", "frequency": 1, "id": "Galton\u2013Watson_process"}, {"name": "Daubechies wavelet", "offsets": [12506], "paragraph_occurrences": [39], "sentence_occurrences": [120], "affiliation": "light", "frequency": 1, "id": "Daubechies_wavelet"}, {"name": "JSTOR", "offsets": [14051, 14248], "paragraph_occurrences": [45, 45], "sentence_occurrences": [133, 133], "affiliation": "light", "frequency": 2, "id": "JSTOR"}, {"name": "Kalman filter", "offsets": [10371, 10859, 11395, 11963, 12380, 12825, 14976], "paragraph_occurrences": [27, 29, 31, 36, 38, 40, 46], "sentence_occurrences": [96, 100, 105, 113, 119, 123, 140], "affiliation": "light", "frequency": 7, "id": "Kalman_filter"}, {"name": "David M. Blei", "offsets": [1059, 1182, 4460, 6702], "paragraph_occurrences": [3, 3, 9, 16], "sentence_occurrences": [8, 8, 30, 53], "affiliation": "light", "frequency": 4, "id": "David_Blei"}, {"name": "Brain", "offsets": [2589], "paragraph_occurrences": [4], "sentence_occurrences": [19], "affiliation": "light", "frequency": 1, "id": "Brain"}, {"name": "Dirichlet distribution", "offsets": [4734, 5681, 5783, 7660, 6389, 9886], "paragraph_occurrences": [10, 13, 13, 20, 15, 27], "sentence_occurrences": [34, 44, 45, 67, 50, 91], "affiliation": "light", "frequency": 6, "id": "Dirichlet_distribution"}, {"name": "Montana", "offsets": [11682], "paragraph_occurrences": [33], "sentence_occurrences": [108], "affiliation": "light", "frequency": 1, "id": "Montana"}, {"name": "Vermont", "offsets": [11695], "paragraph_occurrences": [33], "sentence_occurrences": [108], "affiliation": "light", "frequency": 1, "id": "Vermont"}, {"name": "Science", "offsets": [685, 2573, 3194], "paragraph_occurrences": [1, 4, 5], "sentence_occurrences": [5, 19, 22], "affiliation": "light", "frequency": 3, "id": "Science"}, {"name": "Professor", "offsets": [2602], "paragraph_occurrences": [4], "sentence_occurrences": [19], "affiliation": "light", "frequency": 1, "id": "Professor"}, {"name": "Longitude", "offsets": [6298], "paragraph_occurrences": [14], "sentence_occurrences": [49], "affiliation": "light", "frequency": 1, "id": "Longitude"}, {"name": "Haar wavelet", "offsets": [12486], "paragraph_occurrences": [39], "sentence_occurrences": [120], "affiliation": "light", "frequency": 1, "id": "Haar_wavelet"}, {"name": "Mac OS", "offsets": [14761], "paragraph_occurrences": [46], "sentence_occurrences": [139], "affiliation": "light", "frequency": 1, "id": "Mac_OS"}, {"name": "Latent Dirichlet allocation", "offsets": [4425, 4454, 6326], "paragraph_occurrences": [9, 9, 15], "sentence_occurrences": [30, 30, 50], "affiliation": "light", "frequency": 3, "id": "Latent_Dirichlet_allocation"}, {"name": "Thomas Edison", "offsets": [3232], "paragraph_occurrences": [5], "sentence_occurrences": [22], "affiliation": "light", "frequency": 1, "id": "Thomas_Edison"}, {"name": "Cerebral cortex", "offsets": [2684], "paragraph_occurrences": [4], "sentence_occurrences": [19], "affiliation": "light", "frequency": 1, "id": "Cerebral_cortex"}, {"name": "Cretaceous\u2013Paleogene extinction event", "offsets": [10292], "paragraph_occurrences": [27], "sentence_occurrences": [96], "affiliation": "light", "frequency": 1, "id": "Cretaceous\u2013Paleogene_extinction_event"}, {"name": "PowerPC", "offsets": [14753], "paragraph_occurrences": [46], "sentence_occurrences": [139], "affiliation": "light", "frequency": 1, "id": "PowerPC"}, {"name": "Normal distribution", "offsets": [5933, 7925, 8004, 9423, 9576, 9623, 9657, 6106, 10548, 16213, 16836], "paragraph_occurrences": [13, 22, 22, 26, 26, 26, 26, 14, 29, 50, 51], "sentence_occurrences": [46, 70, 70, 83, 87, 87, 88, 48, 98, 151, 156], "affiliation": "light", "frequency": 11, "id": "Normal_distribution"}, {"name": "Kullback\u2013Leibler divergence", "offsets": [8633, 8651, 9520], "paragraph_occurrences": [24, 24, 26], "sentence_occurrences": [74, 74, 87], "affiliation": "light", "frequency": 3, "id": "Kullback\u2013Leibler_divergence"}, {"name": "Gibbs sampling", "offsets": [8310], "paragraph_occurrences": [23], "sentence_occurrences": [73], "affiliation": "light", "frequency": 1, "id": "Gibbs_sampling"}, {"name": "Wavelet", "offsets": [11929], "paragraph_occurrences": [35], "sentence_occurrences": [112], "affiliation": "light", "frequency": 1, "id": "Wavelet"}], "all_paragraphs": [{"paragraph_info": {"end": 8, "start": 0, "text": "Abstract", "rank": 0, "paragraph_comparative_number": 1, "entities": [], "id": "p_0"}, "sentences": [{"end": 8, "text": "Abstract", "rank": 0, "start": 0, "IsComparative": "1", "id": "st_0"}]}, {"paragraph_info": {"end": 716, "start": 8, "text": "A family of probabilistic time series models is developed to analyze the time evolution of topics in large document collections.The approach is to use state space models on the natural parameters of the multinomial distributions that represent the topics.Variational approximations based on Kalman filters and nonparametric wavelet regression are developed to carry out approximate posterior inference over the latent topics.In addition to giving quantitative, predictive models of a sequential corpus, dynamic topic models provide a qualitative window into the contents of a large document collection.The models are demonstrated by analyzing the OCRed archives of the journal Science from 1880 through 2000.", "rank": 1, "paragraph_comparative_number": 2, "entities": [], "id": "p_1"}, "sentences": [{"end": 136, "text": "A family of probabilistic time series models is developed to analyze the time evolution of topics in large document collections.", "rank": 1, "start": 8, "IsComparative": "1", "id": "st_1"}, {"end": 263, "text": "The approach is to use state space models on the natural parameters of the multinomial distributions that represent the topics.", "rank": 2, "start": 136, "IsComparative": "0", "id": "st_2"}, {"end": 433, "text": "Variational approximations based on Kalman filters and nonparametric wavelet regression are developed to carry out approximate posterior inference over the latent topics.", "rank": 3, "start": 263, "IsComparative": "0", "id": "st_3"}, {"end": 610, "text": "In addition to giving quantitative, predictive models of a sequential corpus, dynamic topic models provide a qualitative window into the contents of a large document collection.", "rank": 4, "start": 433, "IsComparative": "1", "id": "st_4"}, {"end": 716, "text": "The models are demonstrated by analyzing the OCRed archives of the journal Science from 1880 through 2000.", "rank": 5, "start": 610, "IsComparative": "0", "id": "st_5"}]}, {"paragraph_info": {"end": 728, "start": 716, "text": "Introduction", "rank": 2, "paragraph_comparative_number": 1, "entities": [], "id": "p_2"}, "sentences": [{"end": 728, "text": "Introduction", "rank": 6, "start": 716, "IsComparative": "1", "id": "st_6"}]}, {"paragraph_info": {"end": 2184, "start": 728, "text": "Managing the explosion of electronic document archives requires new tools for automatically organizing, searching, indexing, and browsing large collections.Recent research in machine learning and statistics has developed new techniques for finding patterns of words in document collections using hierarchical probabilistic models (Blei et al., 2003; McCallum et al., 2004; Rosen-Zvi et al., 2004; Griffiths and Steyvers, 2004; Buntine and Jakulin, 2004; Blei and Lafferty, 2006).These models are called topic models because the discovered patterns often reflect the underlying topics which combined to form the documents.Such hierarchical probabilistic models are easily generalized to other kinds of data; for example, topic models have been used to analyze images (Fei-Fei and Perona, 2005; Sivic et al., 2005), biological data (Pritchard et al., 2000), and survey data (Erosheva, 2002).In an exchangeable topic model, the words of each document are assumed to be independently drawn from a mixture of multinomials.The mixing proportions are randomly drawn for each document; the mixture components, or topics, are shared by all documents.Thus, each document reflects the components with different proportions.These models are a powerful method of dimensionality reduction for large collections of unstructured documents.Moreover, posterior inference at the document level is useful for information retrieval, classification, and topic-directed browsing.", "rank": 3, "paragraph_comparative_number": 1, "entities": [], "id": "p_3"}, "sentences": [{"end": 884, "text": "Managing the explosion of electronic document archives requires new tools for automatically organizing, searching, indexing, and browsing large collections.", "rank": 7, "start": 728, "IsComparative": "1", "id": "st_7"}, {"end": 1207, "text": "Recent research in machine learning and statistics has developed new techniques for finding patterns of words in document collections using hierarchical probabilistic models (Blei et al., 2003; McCallum et al., 2004; Rosen-Zvi et al., 2004; Griffiths and Steyvers, 2004; Buntine and Jakulin, 2004; Blei and Lafferty, 2006).", "rank": 8, "start": 884, "IsComparative": "0", "id": "st_8"}, {"end": 1349, "text": "These models are called topic models because the discovered patterns often reflect the underlying topics which combined to form the documents.", "rank": 9, "start": 1207, "IsComparative": "0", "id": "st_9"}, {"end": 1617, "text": "Such hierarchical probabilistic models are easily generalized to other kinds of data; for example, topic models have been used to analyze images (Fei-Fei and Perona, 2005; Sivic et al., 2005), biological data (Pritchard et al., 2000), and survey data (Erosheva, 2002).", "rank": 10, "start": 1349, "IsComparative": "0", "id": "st_10"}, {"end": 1745, "text": "In an exchangeable topic model, the words of each document are assumed to be independently drawn from a mixture of multinomials.", "rank": 11, "start": 1617, "IsComparative": "0", "id": "st_11"}, {"end": 1869, "text": "The mixing proportions are randomly drawn for each document; the mixture components, or topics, are shared by all documents.", "rank": 12, "start": 1745, "IsComparative": "0", "id": "st_12"}, {"end": 1940, "text": "Thus, each document reflects the components with different proportions.", "rank": 13, "start": 1869, "IsComparative": "0", "id": "st_13"}, {"end": 2051, "text": "These models are a powerful method of dimensionality reduction for large collections of unstructured documents.", "rank": 14, "start": 1940, "IsComparative": "0", "id": "st_14"}, {"end": 2184, "text": "Moreover, posterior inference at the document level is useful for information retrieval, classification, and topic-directed browsing.", "rank": 15, "start": 2051, "IsComparative": "0", "id": "st_15"}]}, {"paragraph_info": {"end": 2963, "start": 2184, "text": "Treating words exchangeably is a simplification that it is consistent with the goal of identifying the semantic themes within each document.For many collections of interest, however, the implicit assumption of exchangeable documents is inappropriate.Document collections such as scholarly journals, email, news articles, and search query logs all reflect evolving content.For example, the Science article The Brain of Professor Laborde may be on the same scientific path as the article Reshaping the Cortical MotorMap by Unmasking Latent Intracortical Connections, but the study of neuroscience looked much different in 1903 than it did in 1991.The themes in a document collection evolve over time, and it is of interest to explicitly model the dynamics of the underlying topics.", "rank": 4, "paragraph_comparative_number": 3, "entities": [], "id": "p_4"}, "sentences": [{"end": 2324, "text": "Treating words exchangeably is a simplification that it is consistent with the goal of identifying the semantic themes within each document.", "rank": 16, "start": 2184, "IsComparative": "1", "id": "st_16"}, {"end": 2434, "text": "For many collections of interest, however, the implicit assumption of exchangeable documents is inappropriate.", "rank": 17, "start": 2324, "IsComparative": "0", "id": "st_17"}, {"end": 2556, "text": "Document collections such as scholarly journals, email, news articles, and search query logs all reflect evolving content.", "rank": 18, "start": 2434, "IsComparative": "1", "id": "st_18"}, {"end": 2829, "text": "For example, the Science article The Brain of Professor Laborde may be on the same scientific path as the article Reshaping the Cortical MotorMap by Unmasking Latent Intracortical Connections, but the study of neuroscience looked much different in 1903 than it did in 1991.", "rank": 19, "start": 2556, "IsComparative": "1", "id": "st_19"}, {"end": 2963, "text": "The themes in a document collection evolve over time, and it is of interest to explicitly model the dynamics of the underlying topics.", "rank": 20, "start": 2829, "IsComparative": "0", "id": "st_20"}]}, {"paragraph_info": {"end": 3433, "start": 2963, "text": "In this paper, we develop a dynamic topic model which captures the evolution of topics in a sequentially organized corpus of documents.We demonstrate its applicability by analyzing over 100 years of OCRed articles from the journal Science, which was founded in 1880 by Thomas Edison and has been published through the present.Under this model, articles are grouped by year, and each years articles arise from a set of topics that have evolved from the last years topics.", "rank": 5, "paragraph_comparative_number": 1, "entities": [], "id": "p_5"}, "sentences": [{"end": 3098, "text": "In this paper, we develop a dynamic topic model which captures the evolution of topics in a sequentially organized corpus of documents.", "rank": 21, "start": 2963, "IsComparative": "0", "id": "st_21"}, {"end": 3289, "text": "We demonstrate its applicability by analyzing over 100 years of OCRed articles from the journal Science, which was founded in 1880 by Thomas Edison and has been published through the present.", "rank": 22, "start": 3098, "IsComparative": "1", "id": "st_22"}, {"end": 3433, "text": "Under this model, articles are grouped by year, and each years articles arise from a set of topics that have evolved from the last years topics.", "rank": 23, "start": 3289, "IsComparative": "0", "id": "st_23"}]}, {"paragraph_info": {"end": 3954, "start": 3433, "text": "In the subsequent sections, we extend classical state space models to specify a statistical model of topic evolution.We then develop efficient approximate posterior inference techniques for determining the evolving topics from a sequential collection of documents.Finally, we present qualitative results that demonstrate how dynamic topic models allow the exploration of a large document collection in new ways, and quantitative results that demonstrate greater predictive accuracy when compared with static topic models.", "rank": 6, "paragraph_comparative_number": 3, "entities": [], "id": "p_6"}, "sentences": [{"end": 3550, "text": "In the subsequent sections, we extend classical state space models to specify a statistical model of topic evolution.", "rank": 24, "start": 3433, "IsComparative": "1", "id": "st_24"}, {"end": 3697, "text": "We then develop efficient approximate posterior inference techniques for determining the evolving topics from a sequential collection of documents.", "rank": 25, "start": 3550, "IsComparative": "1", "id": "st_25"}, {"end": 3954, "text": "Finally, we present qualitative results that demonstrate how dynamic topic models allow the exploration of a large document collection in new ways, and quantitative results that demonstrate greater predictive accuracy when compared with static topic models.", "rank": 26, "start": 3697, "IsComparative": "1", "id": "st_26"}]}, {"paragraph_info": {"end": 3974, "start": 3954, "text": "Dynamic Topic Models", "rank": 7, "paragraph_comparative_number": 1, "entities": [], "id": "p_7"}, "sentences": [{"end": 3974, "text": "Dynamic Topic Models", "rank": 27, "start": 3954, "IsComparative": "1", "id": "st_27"}]}, {"paragraph_info": {"end": 4336, "start": 3974, "text": "While traditional time series modeling has focused on continuous data, topic models are designed for categorical data.Our approach is to use state space models on the natural parameter space of the underlying topic multinomials, as well as on the natural parameters for the logistic normal distributions used for modeling the document-specific topic proportions.", "rank": 8, "paragraph_comparative_number": 1, "entities": [], "id": "p_8"}, "sentences": [{"end": 4092, "text": "While traditional time series modeling has focused on continuous data, topic models are designed for categorical data.", "rank": 28, "start": 3974, "IsComparative": "0", "id": "st_28"}, {"end": 4336, "text": "Our approach is to use state space models on the natural parameter space of the underlying topic multinomials, as well as on the natural parameters for the logistic normal distributions used for modeling the document-specific topic proportions.", "rank": 29, "start": 4092, "IsComparative": "1", "id": "st_29"}]}, {"paragraph_info": {"end": 4651, "start": 4336, "text": "First, we review the underlying statistical assumptions of a static topic model, such as latent Dirichlet allocation (LDA) (Blei et al., 2003).Let \f1:K be K topics, each of which is a distribution over a fixed vocabulary.In a static topic model, each document is assumed drawn from the following generative process:", "rank": 9, "paragraph_comparative_number": 0, "entities": [], "id": "p_9"}, "sentences": [{"end": 4479, "text": "First, we review the underlying statistical assumptions of a static topic model, such as latent Dirichlet allocation (LDA) (Blei et al., 2003).", "rank": 30, "start": 4336, "IsComparative": "0", "id": "st_30"}, {"end": 4557, "text": "Let \f1:K be K topics, each of which is a distribution over a fixed vocabulary.", "rank": 31, "start": 4479, "IsComparative": "0", "id": "st_31"}, {"end": 4651, "text": "In a static topic model, each document is assumed drawn from the following generative process:", "rank": 32, "start": 4557, "IsComparative": "0", "id": "st_32"}]}, {"paragraph_info": {"end": 4744, "start": 4651, "text": "1.Choose topic proportions  from a distribution over the (K  1)-simplex, such as a Dirichlet.", "rank": 10, "paragraph_comparative_number": 2, "entities": [], "id": "p_10"}, "sentences": [{"end": 4653, "text": "1.", "rank": 33, "start": 4651, "IsComparative": "1", "id": "st_33"}, {"end": 4744, "text": "Choose topic proportions  from a distribution over the (K  1)-simplex, such as a Dirichlet.", "rank": 34, "start": 4653, "IsComparative": "1", "id": "st_34"}]}, {"paragraph_info": {"end": 4833, "start": 4744, "text": "2.For each word: (a) Choose a topic assignment Z  Mult ().(b) Choose a word W  Mult (\fz).", "rank": 11, "paragraph_comparative_number": 1, "entities": [], "id": "p_11"}, "sentences": [{"end": 4746, "text": "2.", "rank": 35, "start": 4744, "IsComparative": "1", "id": "st_35"}, {"end": 4802, "text": "For each word: (a) Choose a topic assignment Z  Mult ().", "rank": 36, "start": 4746, "IsComparative": "0", "id": "st_36"}, {"end": 4833, "text": "(b) Choose a word W  Mult (\fz).", "rank": 37, "start": 4802, "IsComparative": "0", "id": "st_37"}]}, {"paragraph_info": {"end": 5286, "start": 4833, "text": "This process implicitly assumes that the documents are drawn exchangeably from the same set of topics.For many collections, however, the order of the documents reflects an evolving set of topics.In a dynamic topic model, we suppose that the data is divided by time slice, for example by year.We model the documents of each slice with a K- component topic model, where the topics associated with slice t evolve from the topics associated with slice t  1.", "rank": 12, "paragraph_comparative_number": 1, "entities": [], "id": "p_12"}, "sentences": [{"end": 4935, "text": "This process implicitly assumes that the documents are drawn exchangeably from the same set of topics.", "rank": 38, "start": 4833, "IsComparative": "0", "id": "st_38"}, {"end": 5028, "text": "For many collections, however, the order of the documents reflects an evolving set of topics.", "rank": 39, "start": 4935, "IsComparative": "1", "id": "st_39"}, {"end": 5125, "text": "In a dynamic topic model, we suppose that the data is divided by time slice, for example by year.", "rank": 40, "start": 5028, "IsComparative": "0", "id": "st_40"}, {"end": 5286, "text": "We model the documents of each slice with a K- component topic model, where the topics associated with slice t evolve from the topics associated with slice t  1.", "rank": 41, "start": 5125, "IsComparative": "0", "id": "st_41"}]}, {"paragraph_info": {"end": 6020, "start": 5286, "text": "For a K-component model with V terms, let \ft,k denote the V -vector of natural parameters for topic k in slice t. The usual representation of a multinomial distribution is by its mean parameterization.If we denote the mean parameter of a V -dimensional multinomial by , the ith component of the natural parameter is given by the mapping \fi = log(i/V ).In typical language modeling applications, Dirichlet distributions are used to model uncertainty about the distributions over words.However, the Dirichlet is not amenable to sequential modeling.Instead, we chain the natural parameters of each topic \ft,k in a state space model that evolves with Gaussian noise; the simplest version of such a model is \ft,k | \ft1,k  N(\ft1,k, 2I) .(1)", "rank": 13, "paragraph_comparative_number": 3, "entities": [], "id": "p_13"}, "sentences": [{"end": 5487, "text": "For a K-component model with V terms, let \ft,k denote the V -vector of natural parameters for topic k in slice t. The usual representation of a multinomial distribution is by its mean parameterization.", "rank": 42, "start": 5286, "IsComparative": "1", "id": "st_42"}, {"end": 5638, "text": "If we denote the mean parameter of a V -dimensional multinomial by , the ith component of the natural parameter is given by the mapping \fi = log(i/V ).", "rank": 43, "start": 5487, "IsComparative": "0", "id": "st_43"}, {"end": 5770, "text": "In typical language modeling applications, Dirichlet distributions are used to model uncertainty about the distributions over words.", "rank": 44, "start": 5638, "IsComparative": "0", "id": "st_44"}, {"end": 5832, "text": "However, the Dirichlet is not amenable to sequential modeling.", "rank": 45, "start": 5770, "IsComparative": "0", "id": "st_45"}, {"end": 6017, "text": "Instead, we chain the natural parameters of each topic \ft,k in a state space model that evolves with Gaussian noise; the simplest version of such a model is \ft,k | \ft1,k  N(\ft1,k, 2I) .", "rank": 46, "start": 5832, "IsComparative": "1", "id": "st_46"}, {"end": 6020, "text": "(1)", "rank": 47, "start": 6017, "IsComparative": "1", "id": "st_47"}]}, {"paragraph_info": {"end": 6323, "start": 6020, "text": "Our approach is thus to model sequences of compositional random variables by chaining Gaussian distributions in a dynamic model and mapping the emitted values to the simplex.This is an extension of the logistic normal distribution (Aitchison, 1982) to time-series simplex data (West and Harrison, 1997).", "rank": 14, "paragraph_comparative_number": 2, "entities": [], "id": "p_14"}, "sentences": [{"end": 6194, "text": "Our approach is thus to model sequences of compositional random variables by chaining Gaussian distributions in a dynamic model and mapping the emitted values to the simplex.", "rank": 48, "start": 6020, "IsComparative": "1", "id": "st_48"}, {"end": 6323, "text": "This is an extension of the logistic normal distribution (Aitchison, 1982) to time-series simplex data (West and Harrison, 1997).", "rank": 49, "start": 6194, "IsComparative": "1", "id": "st_49"}]}, {"paragraph_info": {"end": 6602, "start": 6323, "text": "In LDA, the document-specific topic proportions  are drawn from a Dirichlet distribution.In the dynamic topic model, we use a logistic normal with mean \u000b to express uncertainty over proportions.The sequential structure between models is again captured with a simple dynamic model", "rank": 15, "paragraph_comparative_number": 0, "entities": [], "id": "p_15"}, "sentences": [{"end": 6412, "text": "In LDA, the document-specific topic proportions  are drawn from a Dirichlet distribution.", "rank": 50, "start": 6323, "IsComparative": "0", "id": "st_50"}, {"end": 6517, "text": "In the dynamic topic model, we use a logistic normal with mean \u000b to express uncertainty over proportions.", "rank": 51, "start": 6412, "IsComparative": "0", "id": "st_51"}, {"end": 6602, "text": "The sequential structure between models is again captured with a simple dynamic model", "rank": 52, "start": 6517, "IsComparative": "0", "id": "st_52"}]}, {"paragraph_info": {"end": 6727, "start": 6602, "text": "For simplicity, we do not model the dynamics of topic correlation, as was done for static models by Blei and Lafferty (2006).", "rank": 16, "paragraph_comparative_number": 0, "entities": [], "id": "p_16"}, "sentences": [{"end": 6727, "text": "For simplicity, we do not model the dynamics of topic correlation, as was done for static models by Blei and Lafferty (2006).", "rank": 53, "start": 6602, "IsComparative": "0", "id": "st_53"}]}, {"paragraph_info": {"end": 7100, "start": 6727, "text": "By chaining together topics and topic proportion distributions, we have sequentially tied a collection of topic models.The generative process for slice t of a sequential corpus is thus as follows: 1.Draw topics \ft | \ft1  N(\ft1, 2I).2.Draw \u000bt | \u000bt1  N(\u000bt1, 2I).3.For each document: (a) Draw   N(\u000bt, a2I) (b) For each word: i. Draw Z  Mult (()).ii.Draw Wt,d,n  Mult ((\ft,z)).", "rank": 17, "paragraph_comparative_number": 2, "entities": [], "id": "p_17"}, "sentences": [{"end": 6846, "text": "By chaining together topics and topic proportion distributions, we have sequentially tied a collection of topic models.", "rank": 54, "start": 6727, "IsComparative": "0", "id": "st_54"}, {"end": 6926, "text": "The generative process for slice t of a sequential corpus is thus as follows: 1.", "rank": 55, "start": 6846, "IsComparative": "0", "id": "st_55"}, {"end": 6959, "text": "Draw topics \ft | \ft1  N(\ft1, 2I).", "rank": 56, "start": 6926, "IsComparative": "0", "id": "st_56"}, {"end": 6961, "text": "2.", "rank": 57, "start": 6959, "IsComparative": "1", "id": "st_57"}, {"end": 6987, "text": "Draw \u000bt | \u000bt1  N(\u000bt1, 2I).", "rank": 58, "start": 6961, "IsComparative": "0", "id": "st_58"}, {"end": 6989, "text": "3.", "rank": 59, "start": 6987, "IsComparative": "1", "id": "st_59"}, {"end": 7070, "text": "For each document: (a) Draw   N(\u000bt, a2I) (b) For each word: i. Draw Z  Mult (()).", "rank": 60, "start": 6989, "IsComparative": "0", "id": "st_60"}, {"end": 7073, "text": "ii.", "rank": 61, "start": 7070, "IsComparative": "0", "id": "st_61"}, {"end": 7100, "text": "Draw Wt,d,n  Mult ((\ft,z)).", "rank": 62, "start": 7073, "IsComparative": "0", "id": "st_62"}]}, {"paragraph_info": {"end": 7213, "start": 7100, "text": "Note that  maps the multinomial natural parameters to the mean parameters, (\fk,t)w = exp(\fk,t,w) Pw exp(\fk,t,w) .", "rank": 18, "paragraph_comparative_number": 1, "entities": [], "id": "p_18"}, "sentences": [{"end": 7213, "text": "Note that  maps the multinomial natural parameters to the mean parameters, (\fk,t)w = exp(\fk,t,w) Pw exp(\fk,t,w) .", "rank": 63, "start": 7100, "IsComparative": "1", "id": "st_63"}]}, {"paragraph_info": {"end": 7514, "start": 7213, "text": "The graphical model for this generative process is shown in Figure 1.When the horizontal arrows are removed, breaking the time dynamics, the graphical model reduces to a set of independent topic models.With time dynamics, the kth topic at slice t has smoothly evolved from the kth topic at slice t  1.", "rank": 19, "paragraph_comparative_number": 1, "entities": [], "id": "p_19"}, "sentences": [{"end": 7282, "text": "The graphical model for this generative process is shown in Figure 1.", "rank": 64, "start": 7213, "IsComparative": "0", "id": "st_64"}, {"end": 7415, "text": "When the horizontal arrows are removed, breaking the time dynamics, the graphical model reduces to a set of independent topic models.", "rank": 65, "start": 7282, "IsComparative": "1", "id": "st_65"}, {"end": 7514, "text": "With time dynamics, the kth topic at slice t has smoothly evolved from the kth topic at slice t  1.", "rank": 66, "start": 7415, "IsComparative": "0", "id": "st_66"}]}, {"paragraph_info": {"end": 7832, "start": 7514, "text": "For clarity of presentation, we now focus on a model with K dynamic topics evolving as in (1), and where the topic proportion model is fixed at a Dirichlet.The technical issues associated with modeling the topic proportions in a time series as in (2) are essentially the same as those for chaining the topics together.", "rank": 20, "paragraph_comparative_number": 0, "entities": [], "id": "p_20"}, "sentences": [{"end": 7670, "text": "For clarity of presentation, we now focus on a model with K dynamic topics evolving as in (1), and where the topic proportion model is fixed at a Dirichlet.", "rank": 67, "start": 7514, "IsComparative": "0", "id": "st_67"}, {"end": 7832, "text": "The technical issues associated with modeling the topic proportions in a time series as in (2) are essentially the same as those for chaining the topics together.", "rank": 68, "start": 7670, "IsComparative": "0", "id": "st_68"}]}, {"paragraph_info": {"end": 7853, "start": 7832, "text": "Approximate Inference", "rank": 21, "paragraph_comparative_number": 1, "entities": [], "id": "p_21"}, "sentences": [{"end": 7853, "text": "Approximate Inference", "rank": 69, "start": 7832, "IsComparative": "1", "id": "st_69"}]}, {"paragraph_info": {"end": 8304, "start": 7853, "text": "Working with time series over the natural parameters enables the use of Gaussian models for the time dynamics; however, due to the nonconjugacy of the Gaussian and multinomial models, posterior inference is intractable.In this section, we present a variational method for approximate posterior inference.We use variational methods as deterministic alternatives to stochastic simulation, in order to handle the large data sets typical of text analysis.", "rank": 22, "paragraph_comparative_number": 0, "entities": [], "id": "p_22"}, "sentences": [{"end": 8072, "text": "Working with time series over the natural parameters enables the use of Gaussian models for the time dynamics; however, due to the nonconjugacy of the Gaussian and multinomial models, posterior inference is intractable.", "rank": 70, "start": 7853, "IsComparative": "0", "id": "st_70"}, {"end": 8157, "text": "In this section, we present a variational method for approximate posterior inference.", "rank": 71, "start": 8072, "IsComparative": "0", "id": "st_71"}, {"end": 8304, "text": "We use variational methods as deterministic alternatives to stochastic simulation, in order to handle the large data sets typical of text analysis.", "rank": 72, "start": 8157, "IsComparative": "0", "id": "st_72"}]}, {"paragraph_info": {"end": 8481, "start": 8304, "text": "While Gibbs sampling has been effectively used for static topic models (Griffiths and Steyvers, 2004), nonconjugacy makes sampling methods more difficult for this dynamic model.", "rank": 23, "paragraph_comparative_number": 0, "entities": [], "id": "p_23"}, "sentences": [{"end": 8481, "text": "While Gibbs sampling has been effectively used for static topic models (Griffiths and Steyvers, 2004), nonconjugacy makes sampling methods more difficult for this dynamic model.", "rank": 73, "start": 8304, "IsComparative": "0", "id": "st_73"}]}, {"paragraph_info": {"end": 8968, "start": 8481, "text": "The idea behind variational methods is to optimize the free parameters of a distribution over the latent variables so that the distribution is close in Kullback-Liebler (KL) divergence to the true posterior; this distribution can then be used as a substitute for the true posterior.In the dynamic topic model, the latent variables are the topics \ft,k, mixture proportions t,d, and topic indicators zt,d,n.The variational distribution reflects the group structure of the latent variables.", "rank": 24, "paragraph_comparative_number": 1, "entities": [], "id": "p_24"}, "sentences": [{"end": 8763, "text": "The idea behind variational methods is to optimize the free parameters of a distribution over the latent variables so that the distribution is close in Kullback-Liebler (KL) divergence to the true posterior; this distribution can then be used as a substitute for the true posterior.", "rank": 74, "start": 8481, "IsComparative": "0", "id": "st_74"}, {"end": 8886, "text": "In the dynamic topic model, the latent variables are the topics \ft,k, mixture proportions t,d, and topic indicators zt,d,n.", "rank": 75, "start": 8763, "IsComparative": "0", "id": "st_75"}, {"end": 8968, "text": "The variational distribution reflects the group structure of the latent variables.", "rank": 76, "start": 8886, "IsComparative": "1", "id": "st_76"}]}, {"paragraph_info": {"end": 9168, "start": 8968, "text": "There are variational parameters for each topics sequence of multinomial parameters, and variational parameters for each of the document-level latent variables.The approximate variational posterior is", "rank": 25, "paragraph_comparative_number": 1, "entities": [], "id": "p_25"}, "sentences": [{"end": 9128, "text": "There are variational parameters for each topics sequence of multinomial parameters, and variational parameters for each of the document-level latent variables.", "rank": 77, "start": 8968, "IsComparative": "1", "id": "st_77"}, {"end": 9168, "text": "The approximate variational posterior is", "rank": 78, "start": 9128, "IsComparative": "0", "id": "st_78"}]}, {"paragraph_info": {"end": 9722, "start": 9168, "text": "In the commonly used mean-field approximation, each latent variable is considered independently of the others.In the variational distribution of <\fk,1, ..., \fk,T >, however, we retain the sequential structure of the topic by positing a dynamic model with Gaussian variational observations <  \fk,1, ...,  \fk,T >.These parameters are fit to minimize the KL divergence between the resulting posterior, which is Gaussian, and the true posterior, which is not Gaussian.(A similar technique for Gaussian processes is described in Snelson and Ghahramani, 2006.)", "rank": 26, "paragraph_comparative_number": 1, "entities": [], "id": "p_26"}, "sentences": [{"end": 9278, "text": "In the commonly used mean-field approximation, each latent variable is considered independently of the others.", "rank": 79, "start": 9168, "IsComparative": "0", "id": "st_79"}, {"end": 9321, "text": "In the variational distribution of <\fk,1, .", "rank": 80, "start": 9278, "IsComparative": "0", "id": "st_80"}, {"end": 9322, "text": ".", "rank": 81, "start": 9321, "IsComparative": "0", "id": "st_81"}, {"end": 9323, "text": ".", "rank": 82, "start": 9322, "IsComparative": "0", "id": "st_82"}, {"end": 9467, "text": ", \fk,T >, however, we retain the sequential structure of the topic by positing a dynamic model with Gaussian variational observations <  \fk,1, .", "rank": 83, "start": 9323, "IsComparative": "1", "id": "st_83"}, {"end": 9468, "text": ".", "rank": 84, "start": 9467, "IsComparative": "0", "id": "st_84"}, {"end": 9469, "text": ".", "rank": 85, "start": 9468, "IsComparative": "0", "id": "st_85"}, {"end": 9479, "text": ",  \fk,T >.", "rank": 86, "start": 9469, "IsComparative": "0", "id": "st_86"}, {"end": 9632, "text": "These parameters are fit to minimize the KL divergence between the resulting posterior, which is Gaussian, and the true posterior, which is not Gaussian.", "rank": 87, "start": 9479, "IsComparative": "0", "id": "st_87"}, {"end": 9722, "text": "(A similar technique for Gaussian processes is described in Snelson and Ghahramani, 2006.)", "rank": 88, "start": 9632, "IsComparative": "0", "id": "st_88"}]}, {"paragraph_info": {"end": 10427, "start": 9722, "text": "The variational distribution of the document-level latent variables follows the same form as in Blei et al.(2003).Each proportion vector t,d is endowed with a free Dirichlet parameter  t,d, each topic indicator zt,d,n is endowed with a free multinomial parameter t,d,n, and optimization proceeds by coordinate ascent.The updates for the documentlevel variational parameters have a closed form; we use the conjugate gradient method to optimize the topic-level variational observations.The resulting variational approximation for the natural topic parameters <\fk,1, ..., \fk,T > incorporates the time dynamics; we describe one approximation based on a Kalman filter, and a second based on wavelet regression.", "rank": 27, "paragraph_comparative_number": 4, "entities": [], "id": "p_27"}, "sentences": [{"end": 9829, "text": "The variational distribution of the document-level latent variables follows the same form as in Blei et al.", "rank": 89, "start": 9722, "IsComparative": "1", "id": "st_89"}, {"end": 9836, "text": "(2003).", "rank": 90, "start": 9829, "IsComparative": "1", "id": "st_90"}, {"end": 10039, "text": "Each proportion vector t,d is endowed with a free Dirichlet parameter  t,d, each topic indicator zt,d,n is endowed with a free multinomial parameter t,d,n, and optimization proceeds by coordinate ascent.", "rank": 91, "start": 9836, "IsComparative": "0", "id": "st_91"}, {"end": 10206, "text": "The updates for the documentlevel variational parameters have a closed form; we use the conjugate gradient method to optimize the topic-level variational observations.", "rank": 92, "start": 10039, "IsComparative": "0", "id": "st_92"}, {"end": 10287, "text": "The resulting variational approximation for the natural topic parameters <\fk,1, .", "rank": 93, "start": 10206, "IsComparative": "1", "id": "st_93"}, {"end": 10288, "text": ".", "rank": 94, "start": 10287, "IsComparative": "0", "id": "st_94"}, {"end": 10289, "text": ".", "rank": 95, "start": 10288, "IsComparative": "0", "id": "st_95"}, {"end": 10427, "text": ", \fk,T > incorporates the time dynamics; we describe one approximation based on a Kalman filter, and a second based on wavelet regression.", "rank": 96, "start": 10289, "IsComparative": "1", "id": "st_96"}]}, {"paragraph_info": {"end": 10455, "start": 10427, "text": "Variational Kalman Filtering", "rank": 28, "paragraph_comparative_number": 0, "entities": [], "id": "p_28"}, "sentences": [{"end": 10455, "text": "Variational Kalman Filtering", "rank": 97, "start": 10427, "IsComparative": "0", "id": "st_97"}]}, {"paragraph_info": {"end": 10900, "start": 10455, "text": "The view of the variational parameters as outputs is based on the symmetry properties of the Gaussian density, f,(x) = fx,(), which enables the use of the standard forward-backward calculations for linear state space models.The graphical model and its variational approximation are shown in Figure 2.Here the triangles denote variational parameters; they can be thought of as hypothetical outputs of the Kalman filter, to facilitate calculation.", "rank": 29, "paragraph_comparative_number": 0, "entities": [], "id": "p_29"}, "sentences": [{"end": 10679, "text": "The view of the variational parameters as outputs is based on the symmetry properties of the Gaussian density, f,(x) = fx,(), which enables the use of the standard forward-backward calculations for linear state space models.", "rank": 98, "start": 10455, "IsComparative": "0", "id": "st_98"}, {"end": 10755, "text": "The graphical model and its variational approximation are shown in Figure 2.", "rank": 99, "start": 10679, "IsComparative": "0", "id": "st_99"}, {"end": 10900, "text": "Here the triangles denote variational parameters; they can be thought of as hypothetical outputs of the Kalman filter, to facilitate calculation.", "rank": 100, "start": 10755, "IsComparative": "0", "id": "st_100"}]}, {"paragraph_info": {"end": 11338, "start": 10900, "text": "To explain the main idea behind this technique in a simpler setting, consider the model where unigram models \ft (in the natural parameterization) evolve over time.In this model there are no topics and thus no mixing parameters.The calculations are simpler versions of those we need for the more general latent variable models, but exhibit the essential features.Our state space model is and we form the variational state space model where", "rank": 30, "paragraph_comparative_number": 2, "entities": [], "id": "p_30"}, "sentences": [{"end": 11063, "text": "To explain the main idea behind this technique in a simpler setting, consider the model where unigram models \ft (in the natural parameterization) evolve over time.", "rank": 101, "start": 10900, "IsComparative": "1", "id": "st_101"}, {"end": 11127, "text": "In this model there are no topics and thus no mixing parameters.", "rank": 102, "start": 11063, "IsComparative": "0", "id": "st_102"}, {"end": 11262, "text": "The calculations are simpler versions of those we need for the more general latent variable models, but exhibit the essential features.", "rank": 103, "start": 11127, "IsComparative": "0", "id": "st_103"}, {"end": 11338, "text": "Our state space model is and we form the variational state space model where", "rank": 104, "start": 11262, "IsComparative": "1", "id": "st_104"}]}, {"paragraph_info": {"end": 11509, "start": 11338, "text": "The variational parameters are  \ft and t. Using standard Kalman filter calculations (Kalman, 1960), the forward mean and variance of the variational posterior are given by", "rank": 31, "paragraph_comparative_number": 0, "entities": [], "id": "p_31"}, "sentences": [{"end": 11509, "text": "The variational parameters are  \ft and t. Using standard Kalman filter calculations (Kalman, 1960), the forward mean and variance of the variational posterior are given by", "rank": 105, "start": 11338, "IsComparative": "0", "id": "st_105"}]}, {"paragraph_info": {"end": 11652, "start": 11509, "text": "with initial conditions specified by fixed m0 and V0.The backward recursion then calculates the marginal mean and variance of \ft given  \f1:T as", "rank": 32, "paragraph_comparative_number": 0, "entities": [], "id": "p_32"}, "sentences": [{"end": 11562, "text": "with initial conditions specified by fixed m0 and V0.", "rank": 106, "start": 11509, "IsComparative": "0", "id": "st_106"}, {"end": 11652, "text": "The backward recursion then calculates the marginal mean and variance of \ft given  \f1:T as", "rank": 107, "start": 11562, "IsComparative": "0", "id": "st_107"}]}, {"paragraph_info": {"end": 11859, "start": 11652, "text": "with initial conditions emT = mT and eVT = VT .We approximate the posterior p(\f1:T |w1:T ) using the state space posterior q(\f1:T |  \f1:T ).From Jensens inequality, the loglikelihood is bounded from below as", "rank": 33, "paragraph_comparative_number": 0, "entities": [], "id": "p_33"}, "sentences": [{"end": 11699, "text": "with initial conditions emT = mT and eVT = VT .", "rank": 108, "start": 11652, "IsComparative": "0", "id": "st_108"}, {"end": 11792, "text": "We approximate the posterior p(\f1:T |w1:T ) using the state space posterior q(\f1:T |  \f1:T ).", "rank": 109, "start": 11699, "IsComparative": "0", "id": "st_109"}, {"end": 11859, "text": "From Jensens inequality, the loglikelihood is bounded from below as", "rank": 110, "start": 11792, "IsComparative": "0", "id": "st_110"}]}, {"paragraph_info": {"end": 11917, "start": 11859, "text": "Details of optimizing this bound are given in an appendix.", "rank": 34, "paragraph_comparative_number": 0, "entities": [], "id": "p_34"}, "sentences": [{"end": 11917, "text": "Details of optimizing this bound are given in an appendix.", "rank": 111, "start": 11859, "IsComparative": "0", "id": "st_111"}]}, {"paragraph_info": {"end": 11947, "start": 11917, "text": "Variational Wavelet Regression", "rank": 35, "paragraph_comparative_number": 1, "entities": [], "id": "p_35"}, "sentences": [{"end": 11947, "text": "Variational Wavelet Regression", "rank": 112, "start": 11917, "IsComparative": "1", "id": "st_112"}]}, {"paragraph_info": {"end": 12229, "start": 11947, "text": "The variational Kalman filter can be replaced with variational wavelet regression; for a readable introduction standard wavelet methods, see Wasserman (2006).We rescale time so it is between 0 and 1.For 128 years of Science we take n = 2J and J = 7.To be consistent with our earlier", "rank": 36, "paragraph_comparative_number": 2, "entities": [], "id": "p_36"}, "sentences": [{"end": 12105, "text": "The variational Kalman filter can be replaced with variational wavelet regression; for a readable introduction standard wavelet methods, see Wasserman (2006).", "rank": 113, "start": 11947, "IsComparative": "0", "id": "st_113"}, {"end": 12146, "text": "We rescale time so it is between 0 and 1.", "rank": 114, "start": 12105, "IsComparative": "0", "id": "st_114"}, {"end": 12196, "text": "For 128 years of Science we take n = 2J and J = 7.", "rank": 115, "start": 12146, "IsComparative": "1", "id": "st_115"}, {"end": 12229, "text": "To be consistent with our earlier", "rank": 116, "start": 12196, "IsComparative": "1", "id": "st_116"}]}, {"paragraph_info": {"end": 12253, "start": 12229, "text": "notation, we assume that", "rank": 37, "paragraph_comparative_number": 0, "entities": [], "id": "p_37"}, "sentences": [{"end": 12253, "text": "notation, we assume that", "rank": 117, "start": 12229, "IsComparative": "0", "id": "st_117"}]}, {"paragraph_info": {"end": 12430, "start": 12253, "text": "where t  N(0, 1).Our variational wavelet regression algorithm estimates <  \ft>, which we view as observed data, just as in the Kalman filter method, as well as the noise level .", "rank": 38, "paragraph_comparative_number": 1, "entities": [], "id": "p_38"}, "sentences": [{"end": 12270, "text": "where t  N(0, 1).", "rank": 118, "start": 12253, "IsComparative": "0", "id": "st_118"}, {"end": 12430, "text": "Our variational wavelet regression algorithm estimates <  \ft>, which we view as observed data, just as in the Kalman filter method, as well as the noise level .", "rank": 119, "start": 12270, "IsComparative": "1", "id": "st_119"}]}, {"paragraph_info": {"end": 12774, "start": 12430, "text": "For concreteness, we illustrate the technique using the Haar wavelet basis; Daubechies wavelets are used in our actual examples.The model is then  where xt = t/n, (x) = 1 for 0  x  1, and  jk(x) = 2j/2 (2jx  k).Our variational estimate for the posterior mean becomes where \u000b = n1Pn t=1  \ft, and Djk are obtained by thresholding the coefficients", "rank": 39, "paragraph_comparative_number": 1, "entities": [], "id": "p_39"}, "sentences": [{"end": 12558, "text": "For concreteness, we illustrate the technique using the Haar wavelet basis; Daubechies wavelets are used in our actual examples.", "rank": 120, "start": 12430, "IsComparative": "0", "id": "st_120"}, {"end": 12641, "text": "The model is then  where xt = t/n, (x) = 1 for 0  x  1, and  jk(x) = 2j/2 (2jx  k).", "rank": 121, "start": 12558, "IsComparative": "0", "id": "st_121"}, {"end": 12774, "text": "Our variational estimate for the posterior mean becomes where \u000b = n1Pn t=1  \ft, and Djk are obtained by thresholding the coefficients", "rank": 122, "start": 12641, "IsComparative": "1", "id": "st_122"}]}, {"paragraph_info": {"end": 12940, "start": 12774, "text": "To estimate  \ft we use gradient ascent, as for the Kalman filter approximation, requiring the derivatives @ emt/@  \ft. If soft thresholding is used, then we have that", "rank": 40, "paragraph_comparative_number": 0, "entities": [], "id": "p_40"}, "sentences": [{"end": 12940, "text": "To estimate  \ft we use gradient ascent, as for the Kalman filter approximation, requiring the derivatives @ emt/@  \ft. If soft thresholding is used, then we have that", "rank": 123, "start": 12774, "IsComparative": "0", "id": "st_123"}]}, {"paragraph_info": {"end": 13115, "start": 12940, "text": "Note also that |Zjk| >  if and only if |Djk| > 0.These derivatives can be computed using off-the-shelf software for the wavelet transform in any of the standard wavelet bases.", "rank": 41, "paragraph_comparative_number": 2, "entities": [], "id": "p_41"}, "sentences": [{"end": 12989, "text": "Note also that |Zjk| >  if and only if |Djk| > 0.", "rank": 124, "start": 12940, "IsComparative": "1", "id": "st_124"}, {"end": 13115, "text": "These derivatives can be computed using off-the-shelf software for the wavelet transform in any of the standard wavelet bases.", "rank": 125, "start": 12989, "IsComparative": "1", "id": "st_125"}]}, {"paragraph_info": {"end": 13628, "start": 13115, "text": "Sample results of running this and the Kalman variational algorithm to approximate a unigram model are given in Figure 3.Both variational approximations smooth out the local fluctuations in the unigram counts, while preserving the sharp peaks that may indicate a significant change of content in the journal.While the fit is similar to that obtained using standard wavelet regression to the (normalized) counts, the estimates are obtained by minimizing the KL divergence as in standard variational approximations.", "rank": 42, "paragraph_comparative_number": 0, "entities": [], "id": "p_42"}, "sentences": [{"end": 13236, "text": "Sample results of running this and the Kalman variational algorithm to approximate a unigram model are given in Figure 3.", "rank": 126, "start": 13115, "IsComparative": "0", "id": "st_126"}, {"end": 13423, "text": "Both variational approximations smooth out the local fluctuations in the unigram counts, while preserving the sharp peaks that may indicate a significant change of content in the journal.", "rank": 127, "start": 13236, "IsComparative": "0", "id": "st_127"}, {"end": 13628, "text": "While the fit is similar to that obtained using standard wavelet regression to the (normalized) counts, the estimates are obtained by minimizing the KL divergence as in standard variational approximations.", "rank": 128, "start": 13423, "IsComparative": "0", "id": "st_128"}]}, {"paragraph_info": {"end": 13898, "start": 13628, "text": "In the dynamic topic model of Section 2, the algorithms are essentially the same as those described above.However, rather than fitting the observations from true observed counts, we fit them from expected counts under the document-level variational distributions in (3).", "rank": 43, "paragraph_comparative_number": 0, "entities": [], "id": "p_43"}, "sentences": [{"end": 13734, "text": "In the dynamic topic model of Section 2, the algorithms are essentially the same as those described above.", "rank": 129, "start": 13628, "IsComparative": "0", "id": "st_129"}, {"end": 13898, "text": "However, rather than fitting the observations from true observed counts, we fit them from expected counts under the document-level variational distributions in (3).", "rank": 130, "start": 13734, "IsComparative": "0", "id": "st_130"}]}, {"paragraph_info": {"end": 13917, "start": 13898, "text": "Analysis of Science", "rank": 44, "paragraph_comparative_number": 0, "entities": [], "id": "p_44"}, "sentences": [{"end": 13917, "text": "Analysis of Science", "rank": 131, "start": 13898, "IsComparative": "0", "id": "st_131"}]}, {"paragraph_info": {"end": 14377, "start": 13917, "text": "We analyzed a subset of 30,000 articles from Science, 250 from each of the 120 years between 1881 and 1999.Our data were collected by JSTOR (www.jstor.org), a notfor- profit organization that maintains an online scholarly archive obtained by running an optical character recognition (OCR) engine over the original printed journals.JSTOR indexes the resulting text and provides online access to the scanned images of the original content through keyword search.", "rank": 45, "paragraph_comparative_number": 3, "entities": [], "id": "p_45"}, "sentences": [{"end": 14024, "text": "We analyzed a subset of 30,000 articles from Science, 250 from each of the 120 years between 1881 and 1999.", "rank": 132, "start": 13917, "IsComparative": "1", "id": "st_132"}, {"end": 14248, "text": "Our data were collected by JSTOR (www.jstor.org), a notfor- profit organization that maintains an online scholarly archive obtained by running an optical character recognition (OCR) engine over the original printed journals.", "rank": 133, "start": 14024, "IsComparative": "1", "id": "st_133"}, {"end": 14377, "text": "JSTOR indexes the resulting text and provides online access to the scanned images of the original content through keyword search.", "rank": 134, "start": 14248, "IsComparative": "1", "id": "st_134"}]}, {"paragraph_info": {"end": 15219, "start": 14377, "text": "Our corpus is made up of approximately 7.5 million words.We pruned the vocabulary by stemming each term to its root, removing function terms, and removing terms that occurred fewer than 25 times.The total vocabulary size is 15,955.To explore the corpus and its themes, we estimated a 20-component dynamic topic model.Posterior inference took approximately 4 hours on a 1.5GHZ PowerPC Macintosh laptop.Two of the resulting topics are illustrated in Figure 4, showing the top several words from those topics in each decade, according to the posterior mean number of occurrences as estimated using the Kalman filter variational approximation.Also shown are example articles which exhibit those topics through the decades.As illustrated, the model captures different scientific themes, and can be used to inspect trends of word usage within them.", "rank": 46, "paragraph_comparative_number": 4, "entities": [], "id": "p_46"}, "sentences": [{"end": 14434, "text": "Our corpus is made up of approximately 7.5 million words.", "rank": 135, "start": 14377, "IsComparative": "1", "id": "st_135"}, {"end": 14572, "text": "We pruned the vocabulary by stemming each term to its root, removing function terms, and removing terms that occurred fewer than 25 times.", "rank": 136, "start": 14434, "IsComparative": "1", "id": "st_136"}, {"end": 14608, "text": "The total vocabulary size is 15,955.", "rank": 137, "start": 14572, "IsComparative": "0", "id": "st_137"}, {"end": 14694, "text": "To explore the corpus and its themes, we estimated a 20-component dynamic topic model.", "rank": 138, "start": 14608, "IsComparative": "1", "id": "st_138"}, {"end": 14778, "text": "Posterior inference took approximately 4 hours on a 1.5GHZ PowerPC Macintosh laptop.", "rank": 139, "start": 14694, "IsComparative": "1", "id": "st_139"}, {"end": 15016, "text": "Two of the resulting topics are illustrated in Figure 4, showing the top several words from those topics in each decade, according to the posterior mean number of occurrences as estimated using the Kalman filter variational approximation.", "rank": 140, "start": 14778, "IsComparative": "0", "id": "st_140"}, {"end": 15095, "text": "Also shown are example articles which exhibit those topics through the decades.", "rank": 141, "start": 15016, "IsComparative": "0", "id": "st_141"}, {"end": 15219, "text": "As illustrated, the model captures different scientific themes, and can be used to inspect trends of word usage within them.", "rank": 142, "start": 15095, "IsComparative": "0", "id": "st_142"}]}, {"paragraph_info": {"end": 15803, "start": 15219, "text": "To validate the dynamic topicmodel quantitatively, we consider the task of predicting the next year of Science given all the articles from the previous years.We compare the predictive power of three 20-topic models: the dynamic topic model estimated from all of the previous years, a static topic model estimated from all of the previous years, and a static topic model estimated from the single previous year.All the models are estimated to the same convergence criterion.The topic model estimated from all the previous data and dynamic topic model are initialized at the same point.", "rank": 47, "paragraph_comparative_number": 1, "entities": [], "id": "p_47"}, "sentences": [{"end": 15377, "text": "To validate the dynamic topicmodel quantitatively, we consider the task of predicting the next year of Science given all the articles from the previous years.", "rank": 143, "start": 15219, "IsComparative": "1", "id": "st_143"}, {"end": 15629, "text": "We compare the predictive power of three 20-topic models: the dynamic topic model estimated from all of the previous years, a static topic model estimated from all of the previous years, and a static topic model estimated from the single previous year.", "rank": 144, "start": 15377, "IsComparative": "0", "id": "st_144"}, {"end": 15692, "text": "All the models are estimated to the same convergence criterion.", "rank": 145, "start": 15629, "IsComparative": "0", "id": "st_145"}, {"end": 15803, "text": "The topic model estimated from all the previous data and dynamic topic model are initialized at the same point.", "rank": 146, "start": 15692, "IsComparative": "0", "id": "st_146"}]}, {"paragraph_info": {"end": 16134, "start": 15803, "text": "The dynamic topic model performs well; it always assigns higher likelihood to the next years articles than the other two models (Figure 5).It is interesting that the predictive power of each of the models declines over the years.We can tentatively attribute this to an increase in the rate of specialization in scientific language.", "rank": 48, "paragraph_comparative_number": 2, "entities": [], "id": "p_48"}, "sentences": [{"end": 15942, "text": "The dynamic topic model performs well; it always assigns higher likelihood to the next years articles than the other two models (Figure 5).", "rank": 147, "start": 15803, "IsComparative": "1", "id": "st_147"}, {"end": 16032, "text": "It is interesting that the predictive power of each of the models declines over the years.", "rank": 148, "start": 15942, "IsComparative": "0", "id": "st_148"}, {"end": 16134, "text": "We can tentatively attribute this to an increase in the rate of specialization in scientific language.", "rank": 149, "start": 16032, "IsComparative": "1", "id": "st_149"}]}, {"paragraph_info": {"end": 16144, "start": 16134, "text": "Discussion", "rank": 49, "paragraph_comparative_number": 1, "entities": [], "id": "p_49"}, "sentences": [{"end": 16144, "text": "Discussion", "rank": 150, "start": 16134, "IsComparative": "1", "id": "st_150"}]}, {"paragraph_info": {"end": 16668, "start": 16144, "text": "We have developed sequential topic models for discrete data by using Gaussian time series on the natural parameters of the multinomial topics and logistic normal topic proportion models.We derived variational inference algorithms that exploit existing techniques for sequential data; we demonstrated a novel use of Kalman filters and wavelet regression as variational approximations.Dynamic topic models can give a more accurate predictive model, and also offer new ways of browsing large, unstructured document collections.", "rank": 50, "paragraph_comparative_number": 2, "entities": [], "id": "p_50"}, "sentences": [{"end": 16330, "text": "We have developed sequential topic models for discrete data by using Gaussian time series on the natural parameters of the multinomial topics and logistic normal topic proportion models.", "rank": 151, "start": 16144, "IsComparative": "0", "id": "st_151"}, {"end": 16527, "text": "We derived variational inference algorithms that exploit existing techniques for sequential data; we demonstrated a novel use of Kalman filters and wavelet regression as variational approximations.", "rank": 152, "start": 16330, "IsComparative": "1", "id": "st_152"}, {"end": 16668, "text": "Dynamic topic models can give a more accurate predictive model, and also offer new ways of browsing large, unstructured document collections.", "rank": 153, "start": 16527, "IsComparative": "1", "id": "st_153"}]}, {"paragraph_info": {"end": 17103, "start": 16668, "text": "There are many ways that the work described here can be extended.One direction is to use more sophisticated state space models.We have demonstrated the use of a simple Gaussian model, but it would be natural to include a drift term in a more sophisticated autoregressive model to explicitly capture the rise and fall in popularity of a topic, or in the use of specific terms.Another variant would allow for heteroscedastic time series.", "rank": 51, "paragraph_comparative_number": 2, "entities": [], "id": "p_51"}, "sentences": [{"end": 16733, "text": "There are many ways that the work described here can be extended.", "rank": 154, "start": 16668, "IsComparative": "0", "id": "st_154"}, {"end": 16795, "text": "One direction is to use more sophisticated state space models.", "rank": 155, "start": 16733, "IsComparative": "1", "id": "st_155"}, {"end": 17043, "text": "We have demonstrated the use of a simple Gaussian model, but it would be natural to include a drift term in a more sophisticated autoregressive model to explicitly capture the rise and fall in popularity of a topic, or in the use of specific terms.", "rank": 156, "start": 16795, "IsComparative": "1", "id": "st_156"}, {"end": 17103, "text": "Another variant would allow for heteroscedastic time series.", "rank": 157, "start": 17043, "IsComparative": "0", "id": "st_157"}]}, {"paragraph_info": {"end": 17609, "start": 17103, "text": "Perhaps the most promising extension to the methods presented here is to incorporate a model of how new topics in the collection appear or disappear over time, rather than assuming a fixed number of topics.One possibility is to use a simple Galton-Watson or birth-death process for the topic population.While the analysis of birth-death or branching processes often centers on extinction probabilities, here a goal would be to find documents that may be responsible for spawning new themes in a collection.", "rank": 52, "paragraph_comparative_number": 2, "entities": [], "id": "p_52"}, "sentences": [{"end": 17309, "text": "Perhaps the most promising extension to the methods presented here is to incorporate a model of how new topics in the collection appear or disappear over time, rather than assuming a fixed number of topics.", "rank": 158, "start": 17103, "IsComparative": "1", "id": "st_158"}, {"end": 17406, "text": "One possibility is to use a simple Galton-Watson or birth-death process for the topic population.", "rank": 159, "start": 17309, "IsComparative": "0", "id": "st_159"}, {"end": 17609, "text": "While the analysis of birth-death or branching processes often centers on extinction probabilities, here a goal would be to find documents that may be responsible for spawning new themes in a collection.", "rank": 160, "start": 17406, "IsComparative": "1", "id": "st_160"}]}]}