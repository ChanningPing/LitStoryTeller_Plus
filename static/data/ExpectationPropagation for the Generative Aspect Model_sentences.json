{"paragraph_scenes_info": [{"x": 1, "text": "The generative aspect model is an extension of the multinomial model for text that allows word probabilities to vary stochastically across docu- ments.Previous results with aspect models have been promising, but hindered by the computa- tional difficulty of carrying out inference and learning.This paper demonstrates that the sim- ple variational methods of Blei et al.(2001) can lead to inaccurate inferences and biased learning for the generative aspect model.We develop an alternative approach that leads to higher accuracy at comparable cost.An extension of Expectation- Propagation is used for inference and then em- bedded in an EM algorithm for learning.Exper- imental results are presented for both synthetic and real data sets."}, {"x": 4, "text": "The generative aspect model introduced by Blei et al.(2001) is a promising model for discrete data, and provides an interesting example of the need for good approxima- tion strategies.When applied to text, the model explic- itly accounts for the intuition that a document may have several subtopics or aspects, making it an attractive tool for several applications in text processing and information retrieval.As an example, imagine that rather than sim- ply returning a list of documents that are relevant to a given topic, a search engine could automatically determine the different aspects of the topic that are treated by each document in the list, and reorder the documents to effi- ciently cover these different aspects.The TREC interac- tive track (Over, 2001) has been set up to help investigate precisely this task, but from the point of view of users in- teracting with the search engine.As an example of the judgements made in this task, for the topic electric au- tomobiles (number 247i), the human assessors identified eleven aspects among the documents that were judged to be relevant, having descriptions such as government fund- ing of electric car development programs, industrial de- velopment of hybrid electric cars, and increased use of aluminum bodies."}, {"x": 5, "text": "This paper examines computation in the generative as- pect model, proposing new algorithms for approximate in- ference and learning that are based on the Expectation- Propagation framework of Minka (2001b).Hofmanns original aspect model involved a large number of param- eters and heuristic procedures to avoid overfitting (Hof- mann, 1999).Blei et al.(2001) introduced a modified model with a proper generative semantics and used vari- ational methods to carry out inference and learning.It is found that the variational methods can lead to inac- curate inferences and biased learning, while Expectation- Propagation gives results that are more true to the model.Besides providing a practical new algorithm for a useful model, we hope that this result will shed light on the ques- tion of which approximations are appropriate for which problems."}, {"x": 6, "text": "The following section presents the generative aspect model, briefly discussing some of the properties that make it at- tractive for modeling documents, and stating the infer- ence and learning problems to be addressed.After a brief overview of Expectation-Propagation in Section 3, a new algorithm for approximate inference in the genera- tive aspect model is presented in Section 4.Separate from Expectation-Propagation, a new algorithm for approximate learning in the generative aspect model is presented in Section 5.Brief descriptions of the corresponding proce- dures with variational methods are included for complete- ness.Section 6 describes experiments on synthetic and real data.Section 6.1 presents a synthetic data experiment us-ing low dimensional multinomials, which clearly demon- strates how variational methods can result in inaccurate inferences compared to Expectation-Propagation.In Sec- tions 6.2 and 6.3 the methods are then compared using doc- ument collections taken from TREC data, where it is seen that Expectation-Propagation attains lower test set perplex- ity.Section 7 summarizes the results of the paper."}, {"x": 7, "text": "2 The Generative Aspect Model"}, {"x": 11, "text": "This requires us to specify a distribution on p itself, con- sidered as a vector of numbers which sum to one.One natural choice is the Dirichlet distribution, which is conju- gate to the multinomial.Unfortunately, while the Dirichlet can capture variation in the p(w)s, it cannot capture co- variation, the tendency for some probabilities to move up and down together.At the other extreme, we can sample p from a finite set, corresponding to a finite mixture of multi- nomials.This model can capture co-variation, but at great expense, since a new mixture component is needed for ev- ery distinct choice of word probabilities."}, {"x": 12, "text": "In the generative aspect model, it is assumed that there are A underlying aspects, each represented as a multinomial distribution over the words in the vocabulary.A document is generated by the following process.First,  is sampled from a Dirichlet distribution D ( | ), so that  a a = 1.This determines mixing weights for the aspects, yielding a word probability vector:"}, {"x": 13, "text": "The document is then sampled from a multinomial distri- bution with these probabilities.Instead of a finite mixture, this distribution might be called a simplicial mixture, since the word probability vector ranges over a simplex with cor- ners p(w|a = 1),...,p(w|a = A).The probability of a document is where the parameters  are the Dirichlet parameters a and the multinomial models p( | a);  denotes the (A  1)- dimensional simplex, the sample space of the Dirichlet D ( | ).Because  is sampled for each document, differ- ent documents can exhibit the aspects in different propor- tions.However, the integral in (3) does not simplify and must be approximated, which is the main complication in using this model."}, {"x": 17, "text": "3 Expectation-Propagation"}, {"x": 19, "text": "In previous work each count nw was assumed to be 1 (Minka, 2001b).Here we present a slight generalization to allow real-valued powers on the terms.Expectation- Propagation approximates each term tw() by a simpler term t w(), giving a simpler integral"}, {"x": 23, "text": "This section describes two algorithms for approximating the integral in (3): variational inference used by Blei et al.(2001) and Expectation-Propagation."}, {"x": 26, "text": "Laplaces method using a softmax transformation, vari- ational inference, and two different Monte Carlo algo- rithms."}, {"x": 27, "text": "EP gives an integral estimate as well as an approximate posterior for the mixture weights.For the generative aspect model, the approximate posterior will be Dirichlet, and the integrand will be factored into terms of the form which resembles a Dirichlet with parameters wa.Thus, the approximate posterior is given by"}, {"x": 28, "text": "The best bound parameters are found by maximizing the value of the bound.A convenient way to do this is with EM.The parameter in the algorithm is q(a | w) and the hidden variable is :"}, {"x": 32, "text": "4.2 Expectation-Propagation"}, {"x": 33, "text": "As mentioned above, the aspect model integral is the same as marginalizing the weights of a mixture model whose components are known.This problem was studied in depth by Minka (2001b) and it was found that Expectation- Propagation (EP) provides the best results, compared to"}, {"x": 34, "text": "loop w = 1,...,W:"}, {"x": 39, "text": "In our experience, words are skipped only on the first few iterations, before EP has settled into a decent approxima- tion.It can be shown that the safest stepsize for (c) is  = 1/nw, which makes  = .This is the value used in the experiments, though for faster convergence a larger  is often acceptable."}, {"x": 40, "text": "After convergence, the approximate posterior gives the fol- lowing estimate for the likelihood of the document, thus approximating the integral (3): estimates from the previous section and (2) a new approach based on approximative EM.The decision between these two approaches is separate from the decision of using vari- ational inference versus Expectation-Propagation."}, {"x": 42, "text": "Given a set of documents C = <di,i = 1,...,n>, with word counts denoted niw, the learning problem is to max- imize the likelihood as a function of the parameters  = (p( | a), ); the likelihood is given by Notice that each document has its own integral over .It is tempting to use EM for this problem, where we regard  as a hidden variable for each document.However, the E-step requires expectations over the posterior for , p( | d , ), which is an intractable distribution.This section describes two alternative approaches: (1) maximizing the likelihood estimates from the previous section and (2) a new approach based on approximative EM.The decision between these two approaches is separate from the decision of using vari- ational inference versus Expectation-Propagation."}, {"x": 45, "text": "Of course, once the aspect parameters are changed, the op- timal bound parameters q(a | w) also change, so Blei et al.(2001) alternate between optimizing the bound and apply- ing these updates.This can be understood as an EM algo- rithm where both  and the aspect assignments are hidden variables.The aspect parameters at convergence will result in the largest possible variational estimate of the likelihood.The same approach could be taken with EP, where we find the parameters that result in the largest possible EP esti- mate of the likelihood.However, this does not seem to be as simple as in the variational approach.It also seems mis- guided, because an approximation which is close to the true likelihood in an average sense need not have its maximum close to the true maximum."}, {"x": 47, "text": "The second approach is to use an approximative EM algo- rithm, sometimes called variational EM, where we use expectations over an approximate posterior for , call it qi().The inference algorithms in the previous section conveniently give such an approximate posterior.The E- step will compute qi() for each document, and the M- step will maximize the following lower bound to the log- likelihood:"}, {"x": 55, "text": "This section elucidates the difference between variational inference (VB) and Expectation Propagation (EP) using simple, controlled datasets.The algorithms mainly differ in how they approximate (3), thus it is helpful to consider two extremes: (Exact) the exact value of (3) versus (Max) approximating by the maximum over : only serve to restrict the domain of the word probabilities p(w) = a ap(w | a).To maximize likelihood, we would want the domain to be as large as possiblethe as- pects as extreme and distinct as possible.However, when using the exact value of (3), all choices of  contribute, which favors a domain that only includes word probabili- ties matching the frequencies in the documents.In exper- iments, we find that VB behaves like the Max approxima- tion while EP behaves like the exact value."}, {"x": 56, "text": "This update can be used with the s found by either VB or EP.When running EP with the new parameter values, the s can be started from their previous values, so that only a few EP iterations are required."}, {"x": 59, "text": "As shown in Figure 1 (bottom), VB behaves similarly to Max.The solid curve is the exact likelihood, generated by computing the probability of the training documents for all ps on a fine grid.The dashed curve is the VB estimate of the likelihood, scaled up to make its shape visible on the plot, for the same ps.The dot-dashed curve is the EP estimate of the likelihood for the same ps.EP clearly gives a better approximation."}, {"x": 60, "text": "The parameter estimates are indicated by vertical lines.The dashed vertical line corresponds to Blei et al.s algorithm, which as expected converges to the maximum of the VB curve.The solid line is the result of VB combined with the EM update of Section 5.2; as expected, it is closer to the true maximum.The dot-dash vertical line is the result of applying EM using EP and is closest to the true maximum."}, {"x": 61, "text": "To demonstrate the difference between the algorithms in the multidimensional case, 100 documents of length 100 were generated from a simple multinomial model with five words having equal probability.A generative aspect model was fit using three aspects with 1=2=3=1.The EP so- lution correctly chose all aspects to be similar to the gener- ating multinomial; all probabilities were between 0.15 and 0.24.The VB solution is quite different; it chose the ex- treme parameters shown in the following table (rounded to the tenths place):"}, {"x": 62, "text": "Resampling the training documents gives similar results.In terms of convergence rate, learning typically converged after 150 parameter updates with EP, while over 1,000 up- dates were required with VB."}, {"x": 63, "text": "Interestingly, on an independent set of 1000 test docu- ments, the perplexity of the model learned by EP is 5.0 while the VB models is 5.1, a seemingly trivial differ- ence.This is because the perplexity measure, as used by Blei et al., focuses on per-word prediction rather than per- document prediction.As long as there exists a mixture of the aspects which matches the word probabilities in the document, the perplexity will be low.Indeed, if the above VB aspects are evenly mixed, the correct word distribution is produced."}, {"x": 64, "text": "To show that there really is a difference between the mod- els, a synthetic classification problem was constructed.One class had documents sampled from a uniform multinomial over five words.The other class had documents sampled from a multinomial with word probabilities <1 2 3 4 5>/15.There were 50 documents of length 50 in each class.A three-aspect model was trained on each class and test doc- uments (of the same length) were classified according to highest class-conditional probability.The EP models com- mitted 76/2000 errors while the VB models committed 163/2000, which is both statistically and practically signif- icant.As above, EP learned the correct models while VB chose extreme probabilities."}, {"x": 66, "text": "In order to compare variational inference and Expectation- Propagation on more realistic data, a corpus was created by mixing together TREC documents on known topics.From the 1989 AP data on TREC disks 1 and 2, we extracted all of the documents that were judged to be relevant to one of the following six topics:"}, {"x": 67, "text": "Synthetic documents were created by first drawing three topics randomly, with replacement, from the above list of six.A random document from each topic was then se- lected, and the three documents were concatenated together to form a synthetic document containing either one, two or three different aspects.A total of 200 documents were generated in this manner.used to train aspect models using both EP and VB, fixing the number of aspects at six; 75% of the data was used for training, and the remaining 25% was used as test data."}, {"x": 68, "text": "Figure 2 shows the top words for each aspect for the EP- trained model.Because likelihood is used as the objective function, the common, content-free words take up a sig- nificant portion of the probability massa fact that is of- ten not acknowledged in descriptions of aspect models.As seen in this figure, the aspects model variations across doc- uments in the distribution of common words such as SAID, FOR, and WAS.After filtering out the common words from the list, by not displaying words having a unigram proba- bility larger than a threshold of 0.001, the most probable words that remain clearly indicate that the true underlying aspects have been captured, though some more cleanly than others.For example, aspect 1 corresponds to topic 142, and aspect 5 corresponds to topic 59."}, {"x": 69, "text": "The models are compared quantitatively using test set per- plexity, exp(( i log p(di))/  i |di|); lower perplexity is better.The probability function (3) cannot be computed analytically, and we do not want to favor either of the two approximations, so we use importance sampling to com- pute perplexity.In particular, we sample  from the approximate posterior D ( | ) obtained from EP."}, {"x": 70, "text": "Figure 3 shows the test set perplexities for VB and EP; the perplexity for the EP-trained model is consistently lower than the perplexity of the VB-trained model.Based on the results of Section 6.1, we anticipate that for VB the as- pects will be more extreme and specialized.This would make the Dirichlet weights a smaller for the specialized aspects, which are used infrequently, and larger for the as- pects that are used in different topics or that are devoted to the common words.Plots of the Dirichlet parameters (Fig- ure 3, center and right) show that VB results in as that are indeed more spread out towards these extremes, compared with those obtained using EP."}, {"x": 71, "text": "6.3 TREC Interactive Data"}, {"x": 72, "text": "To compare VB and EP on real data having a mixture of aspects, this section considers documents from the TREC interactive collection (Over, 2001).The data used for this track is interesting for studying aspect models because the relevant documents have been hand labeled according to the specific aspects of a topic that they cover.Here we simply evaluate perplexities of the models."}, {"x": 73, "text": "We extracted all of the relevant documents for each of the six topics that the collection has relevance judgements for, resulting in a set of 772 documents.The average docu- ment length is 594 tokens, and the total vocabulary size is 26,319 words.As above, 75% of the data was used for training, and the remaining 25% was used for evaluating perplexities.In these experiments the speed of VB and EP are comparable."}, {"x": 74, "text": "Figure 3 shows the test set perplexity and Dirichlet param- eters a for both EP and VB, trained using A = 10 aspects.As for the controlled TREC data, EP achieves a lower per- plexity, and has aspects that are more balanced compared to those obtained using VB.We suspect that the perplex- ity difference on both the TREC interactive and controlled TREC data is small because the true aspects have little overlap, and thus the posterior of the mixing weights is sharply peaked."}, {"x": 76, "text": "The generative aspect model provides an attractive ap- proach to modeling the variation of word probabilities across documents, making the model well suited to in- formation retrieval and other text processing applications.This paper studied the problem of approximation meth- ods for learning and inference in the generative aspect model, and proposed an algorithm based on Expectation- Propagation as an alternative to the variational method adopted by Blei et al.(2001).Experiments on synthetic data showed that simple variational inference can lead to in- accurate inferences and biased learning, while Expectation- Propagation can lead to more accurate inferences.Exper- iments on TREC data show that Expectation-Propagation achieves lower test set perplexity.We attribute this to the fact that the Jensen bound used by the variational method is inadequate for representing how peaky versus spread out is the posterior on , which happens to be crucial for good parameter estimates.Because there is a separate  for each document, this deficiency is not minimized by ad- ditional documents, but rather compounded."}], "sentence_id_text_dict": {"0": "Abstract", "1": "The generative aspect model is an extension of the multinomial model for text that allows word probabilities to vary stochastically across docu- ments.", "2": "Previous results with aspect models have been promising, but hindered by the computa- tional difficulty of carrying out inference and learning.", "3": "This paper demonstrates that the sim- ple variational methods of Blei et al.", "4": "(2001) can lead to inaccurate inferences and biased learning for the generative aspect model.", "5": "We develop an alternative approach that leads to higher accuracy at comparable cost.", "6": "An extension of Expectation- Propagation is used for inference and then em- bedded in an EM algorithm for learning.", "7": "Exper- imental results are presented for both synthetic and real data sets.", "8": "1 Introduction", "9": "Approximate inference techniques, such as variational methods, are increasingly being used to tackle advanced data models.", "10": "When learning and inference are intractable, approximation can make the difference between a useful model and an impractical model.", "11": "However, if applied in- discriminately, approximation can change the qualitative behavior of a model, leading to unpredictable and unde- sirable results.", "12": "The generative aspect model introduced by Blei et al.", "13": "(2001) is a promising model for discrete data, and provides an interesting example of the need for good approxima- tion strategies.", "14": "When applied to text, the model explic- itly accounts for the intuition that a document may have several subtopics or aspects, making it an attractive tool for several applications in text processing and information retrieval.", "15": "As an example, imagine that rather than sim- ply returning a list of documents that are relevant to a given topic, a search engine could automatically determine the different aspects of the topic that are treated by each document in the list, and reorder the documents to effi- ciently cover these different aspects.", "16": "The TREC interac- tive track (Over, 2001) has been set up to help investigate precisely this task, but from the point of view of users in- teracting with the search engine.", "17": "As an example of the judgements made in this task, for the topic electric au- tomobiles (number 247i), the human assessors identified eleven aspects among the documents that were judged to be relevant, having descriptions such as government fund- ing of electric car development programs, industrial de- velopment of hybrid electric cars, and increased use of aluminum bodies.", "18": "This paper examines computation in the generative as- pect model, proposing new algorithms for approximate in- ference and learning that are based on the Expectation- Propagation framework of Minka (2001b).", "19": "Hofmanns original aspect model involved a large number of param- eters and heuristic procedures to avoid overfitting (Hof- mann, 1999).", "20": "Blei et al.", "21": "(2001) introduced a modified model with a proper generative semantics and used vari- ational methods to carry out inference and learning.", "22": "It is found that the variational methods can lead to inac- curate inferences and biased learning, while Expectation- Propagation gives results that are more true to the model.", "23": "Besides providing a practical new algorithm for a useful model, we hope that this result will shed light on the ques- tion of which approximations are appropriate for which problems.", "24": "The following section presents the generative aspect model, briefly discussing some of the properties that make it at- tractive for modeling documents, and stating the infer- ence and learning problems to be addressed.", "25": "After a brief overview of Expectation-Propagation in Section 3, a new algorithm for approximate inference in the genera- tive aspect model is presented in Section 4.", "26": "Separate from Expectation-Propagation, a new algorithm for approximate learning in the generative aspect model is presented in Section 5.", "27": "Brief descriptions of the corresponding proce- dures with variational methods are included for complete- ness.", "28": "Section 6 describes experiments on synthetic and real data.", "29": "Section 6.1 presents a synthetic data experiment us-ing low dimensional multinomials, which clearly demon- strates how variational methods can result in inaccurate inferences compared to Expectation-Propagation.", "30": "In Sec- tions 6.2 and 6.3 the methods are then compared using doc- ument collections taken from TREC data, where it is seen that Expectation-Propagation attains lower test set perplex- ity.", "31": "Section 7 summarizes the results of the paper.", "32": "2 The Generative Aspect Model", "33": "A simple generative model for documents is the multino- mial model, which assumes that words are drawn one at a time and independently from a fixed word distribution p(w).", "34": "The probability of a document d having word counts nw is thus", "35": "This family is very restrictive, in that a document of length", "36": "n is expected to have np(w) occurrences of word w, with little variation away from this number.", "37": "Even within a ho- mogeneous set of documents, such as machine learning papers, there is typically far more variation in the word counts.", "38": "One way to accommodate this is to allow the word probabilities p to vary across documents, leading to a hier- archical multinomial model.", "39": "This requires us to specify a distribution on p itself, con- sidered as a vector of numbers which sum to one.", "40": "One natural choice is the Dirichlet distribution, which is conju- gate to the multinomial.", "41": "Unfortunately, while the Dirichlet can capture variation in the p(w)s, it cannot capture co- variation, the tendency for some probabilities to move up and down together.", "42": "At the other extreme, we can sample p from a finite set, corresponding to a finite mixture of multi- nomials.", "43": "This model can capture co-variation, but at great expense, since a new mixture component is needed for ev- ery distinct choice of word probabilities.", "44": "In the generative aspect model, it is assumed that there are A underlying aspects, each represented as a multinomial distribution over the words in the vocabulary.", "45": "A document is generated by the following process.", "46": "First,  is sampled from a Dirichlet distribution D ( | ), so that  a a = 1.", "47": "This determines mixing weights for the aspects, yielding a word probability vector:", "48": "The document is then sampled from a multinomial distri- bution with these probabilities.", "49": "Instead of a finite mixture, this distribution might be called a simplicial mixture, since the word probability vector ranges over a simplex with cor- ners p(w|a = 1),...,p(w|a = A).", "50": "The probability of a document is where the parameters  are the Dirichlet parameters a and the multinomial models p( | a);  denotes the (A  1)- dimensional simplex, the sample space of the Dirichlet D ( | ).", "51": "Because  is sampled for each document, differ- ent documents can exhibit the aspects in different propor- tions.", "52": "However, the integral in (3) does not simplify and must be approximated, which is the main complication in using this model.", "53": "The two basic computational tasks for this model are:", "54": "Inference: Evaluate the probability of a document; i.e., the integral in (3).", "55": "Learning: For a set of training documents, find the pa- rameter values  = (p( | a), ) which maximize the likelihood; i.e., maximize the value of the integral in (3).", "56": "3 Expectation-Propagation", "57": "Expectation-Propagation is an algorithm for approximating integrals over functions that factor into simple terms.", "58": "The general form for such integrals in our setting is", "59": "In previous work each count nw was assumed to be 1 (Minka, 2001b).", "60": "Here we present a slight generalization to allow real-valued powers on the terms.", "61": "Expectation- Propagation approximates each term tw() by a simpler term t w(), giving a simpler integral", "62": "The algorithm proceeds by iteratively applying dele- tion/inclusion steps.", "63": "One of the approximate terms is deleted from q(), giving the partial function q\\w() = Then a new approximation for tw () is com- puted so that tw () q () is similar to tw () q (), in the sense of having the same integral and the same set of specified moments.", "64": "The moments used in this paper are the mean and variance.", "65": "The partial function q\\w() thus acts as context for the approximation.", "66": "Unlike variational bounds, this approximation is global, not local, and conse- quently the estimate of the integral is more accurate.", "67": "A fixed point of this algorithm always exists, but we may not always reach one.", "68": "The approximation may oscillate or enter a region where the integral is undefined.", "69": "We uti- lize two techniques to prevent this.", "70": "First, the updates are damped so that t w() cannot oscillate.", "71": "Second, if a deletion-inclusion step leads to an undefined integral, the step is undone and the algorithm continues with the next term.", "72": "4 Inference", "73": "This section describes two algorithms for approximating the integral in (3): variational inference used by Blei et al.", "74": "(2001) and Expectation-Propagation.", "75": "4.1 Variational inference", "76": "To approximate the integral of a function, variational in- ference lower bounds the function and then integrates the lower bound.", "77": "A simple lower bound for (3) comes from Jensens inequality.", "78": "The bound is parameterized by a vec- tor q(a | w): or responsibility of word w to the aspects.", "79": "Given bound parameters q(a | w) for all a and w, the integral is now an- alytic:", "80": "Laplaces method using a softmax transformation, vari- ational inference, and two different Monte Carlo algo- rithms.", "81": "EP gives an integral estimate as well as an approximate posterior for the mixture weights.", "82": "For the generative aspect model, the approximate posterior will be Dirichlet, and the integrand will be factored into terms of the form which resembles a Dirichlet with parameters wa.", "83": "Thus, the approximate posterior is given by", "84": "The best bound parameters are found by maximizing the value of the bound.", "85": "A convenient way to do this is with EM.", "86": "The parameter in the algorithm is q(a | w) and the hidden variable is :", "87": "E-step: a = a+ nwq(a|w) (10) w", "88": "M-step: q(a|w)  p(w|a)exp((a)) (11)", "89": "where  is the digamma function.", "90": "Note that this is the same as the variational algorithm for mixture weights given by Minka (2000).", "91": "The variables a used in this algorithm can be interpreted as defining an approximate Dirichlet poste- rior on : D ( | ).", "92": "4.2 Expectation-Propagation", "93": "As mentioned above, the aspect model integral is the same as marginalizing the weights of a mixture model whose components are known.", "94": "This problem was studied in depth by Minka (2001b) and it was found that Expectation- Propagation (EP) provides the best results, compared to", "95": "loop w = 1,...,W:", "96": "(a) Deletion.", "97": "Remove t w from the posterior to get an old", "98": "(d) Inclusion.", "99": "Incorporate t w back into q() by scaling the change in :", "100": "= old+n( old) (21) a a w wa wa", "101": "This preserves the invariant (16).", "102": "If any a < 0, undo all changes and skip this word.", "103": "In our experience, words are skipped only on the first few iterations, before EP has settled into a decent approxima- tion.", "104": "It can be shown that the safest stepsize for (c) is  = 1/nw, which makes  = .", "105": "This is the value used in the experiments, though for faster convergence a larger  is often acceptable.", "106": "After convergence, the approximate posterior gives the fol- lowing estimate for the likelihood of the document, thus approximating the integral (3): estimates from the previous section and (2) a new approach based on approximative EM.", "107": "The decision between these two approaches is separate from the decision of using vari- ational inference versus Expectation-Propagation.", "108": "5 Learning", "109": "Given a set of documents C = <di,i = 1,...,n>, with word counts denoted niw, the learning problem is to max- imize the likelihood as a function of the parameters  = (p( | a), ); the likelihood is given by Notice that each document has its own integral over .", "110": "It is tempting to use EM for this problem, where we regard  as a hidden variable for each document.", "111": "However, the E-step requires expectations over the posterior for , p( | d , ), which is an intractable distribution.", "112": "This section describes two alternative approaches: (1) maximizing the likelihood estimates from the previous section and (2) a new approach based on approximative EM.", "113": "The decision between these two approaches is separate from the decision of using vari- ational inference versus Expectation-Propagation.", "114": "5.1 Maximizing the estimate", "115": "Given that we can estimate the likelihood function for each document, it seems natural to try to maximize the value of the estimate.", "116": "This is the approach taken by Blei et al.", "117": "(2001).", "118": "For the variational bound (8), the maximum with respect to the parameters is obtained at", "119": "Of course, once the aspect parameters are changed, the op- timal bound parameters q(a | w) also change, so Blei et al.", "120": "(2001) alternate between optimizing the bound and apply- ing these updates.", "121": "This can be understood as an EM algo- rithm where both  and the aspect assignments are hidden variables.", "122": "The aspect parameters at convergence will result in the largest possible variational estimate of the likelihood.", "123": "The same approach could be taken with EP, where we find the parameters that result in the largest possible EP esti- mate of the likelihood.", "124": "However, this does not seem to be as simple as in the variational approach.", "125": "It also seems mis- guided, because an approximation which is close to the true likelihood in an average sense need not have its maximum close to the true maximum.", "126": "5.2 Approximative EM", "127": "The second approach is to use an approximative EM algo- rithm, sometimes called variational EM, where we use expectations over an approximate posterior for , call it qi().", "128": "The inference algorithms in the previous section conveniently give such an approximate posterior.", "129": "The E- step will compute qi() for each document, and the M- step will maximize the following lower bound to the log- likelihood:", "130": "This decouples into separate maximization problems for  and p(w | a).", "131": "Given that qi () is Dirichlet with parameters ia, the optimization problem for  is to maximize", "132": "which is the standard Dirichlet maximum-likelihood prob- lem (Minka, 2001a).", "133": "By zeroing the derivative with respect to p(w | a), we obtain the M-step", "134": "This requires approximating another integral over .", "135": "Up- date (27) is equivalent to assuming that a is constant, at the value exp((a)) (from (11)).", "136": "A more accurate ap- proximation can be obtained by Taylor expansion, as de- scribed in the appendix.", "137": "The resulting update is", "138": "6 Experimental Results", "139": "This section presents the result of experiments carried out on synthetic and real data.", "140": "The first experiments involve a toy data set where the aspects are multinomials over a two word vocabulary.", "141": "Later experiments use documents from two TREC collections.", "142": "6.1 Results on Synthetic Data", "143": "This section elucidates the difference between variational inference (VB) and Expectation Propagation (EP) using simple, controlled datasets.", "144": "The algorithms mainly differ in how they approximate (3), thus it is helpful to consider two extremes: (Exact) the exact value of (3) versus (Max) approximating by the maximum over : only serve to restrict the domain of the word probabilities p(w) = a ap(w | a).", "145": "To maximize likelihood, we would want the domain to be as large as possiblethe as- pects as extreme and distinct as possible.", "146": "However, when using the exact value of (3), all choices of  contribute, which favors a domain that only includes word probabili- ties matching the frequencies in the documents.", "147": "In exper- iments, we find that VB behaves like the Max approxima- tion while EP behaves like the exact value.", "148": "This update can be used with the s found by either VB or EP.", "149": "When running EP with the new parameter values, the s can be started from their previous values, so that only a few EP iterations are required.", "150": "Consider a simple scenario in which there are only two words in the vocabulary, w=1 and w=2.", "151": "This allows each aspect to be represented by one parameter p(w=1 | a), since p(w=2 | a) = 1  p(w=1 | a).", "152": "Let there be two aspects, a=1 and a=2, with 1=2=1, so that D(|) is uni- form.", "153": "This means that the probability of word 1 in the doc- ument collection varies uniformly between p(w=1|a=1) and p(w=1 | a=2).", "154": "Learning the aspects from data amounts to estimating the endpoints of this variation.", "155": "Let p(w=1 | a=2) = 1 so that the only free parameter is p(w=1 | a=1).", "156": "Ten training documents of length 10 are generated from the model with p(w=1 | a=1) = 0.5.", "157": "Figure 1 (top) shows the typical result.", "158": "When we ap- ply the Max approximation, each document i wants to choose p(w=1) (between p(w=1 | a=1) and p(w=1 | a=2)) to match its frequency of word 1: ni1/ni.", "159": "Any choice of (p(w=1|a=1),p(w=1|a=2)) which spans these frequen- cies will maximize likelihood, e.g.", "160": "p(w=1 | a=1) = 0, p(w=1 | a=2) = 1.", "161": "The exact likelihood, by contrast, peaks near the true value of p(w=1 | a=1).", "162": "As the number of train- ing documents increases, the exact likelihood gets sharper around the true value, but the Max approximation gets far- ther away from the truth, because the observed frequencies exhibit more variance.", "163": "As shown in Figure 1 (bottom), VB behaves similarly to Max.", "164": "The solid curve is the exact likelihood, generated by computing the probability of the training documents for all ps on a fine grid.", "165": "The dashed curve is the VB estimate of the likelihood, scaled up to make its shape visible on the plot, for the same ps.", "166": "The dot-dashed curve is the EP estimate of the likelihood for the same ps.", "167": "EP clearly gives a better approximation.", "168": "The parameter estimates are indicated by vertical lines.", "169": "The dashed vertical line corresponds to Blei et al.s algorithm, which as expected converges to the maximum of the VB curve.", "170": "The solid line is the result of VB combined with the EM update of Section 5.2; as expected, it is closer to the true maximum.", "171": "The dot-dash vertical line is the result of applying EM using EP and is closest to the true maximum.", "172": "To demonstrate the difference between the algorithms in the multidimensional case, 100 documents of length 100 were generated from a simple multinomial model with five words having equal probability.", "173": "A generative aspect model was fit using three aspects with 1=2=3=1.", "174": "The EP so- lution correctly chose all aspects to be similar to the gener- ating multinomial; all probabilities were between 0.15 and 0.24.", "175": "The VB solution is quite different; it chose the ex- treme parameters shown in the following table (rounded to the tenths place):", "176": "Resampling the training documents gives similar results.", "177": "In terms of convergence rate, learning typically converged after 150 parameter updates with EP, while over 1,000 up- dates were required with VB.", "178": "Interestingly, on an independent set of 1000 test docu- ments, the perplexity of the model learned by EP is 5.0 while the VB models is 5.1, a seemingly trivial differ- ence.", "179": "This is because the perplexity measure, as used by Blei et al., focuses on per-word prediction rather than per- document prediction.", "180": "As long as there exists a mixture of the aspects which matches the word probabilities in the document, the perplexity will be low.", "181": "Indeed, if the above VB aspects are evenly mixed, the correct word distribution is produced.", "182": "To show that there really is a difference between the mod- els, a synthetic classification problem was constructed.", "183": "One class had documents sampled from a uniform multinomial over five words.", "184": "The other class had documents sampled from a multinomial with word probabilities <1 2 3 4 5>/15.", "185": "There were 50 documents of length 50 in each class.", "186": "A three-aspect model was trained on each class and test doc- uments (of the same length) were classified according to highest class-conditional probability.", "187": "The EP models com- mitted 76/2000 errors while the VB models committed 163/2000, which is both statistically and practically signif- icant.", "188": "As above, EP learned the correct models while VB chose extreme probabilities.", "189": "6.2 Controlled TREC Data", "190": "In order to compare variational inference and Expectation- Propagation on more realistic data, a corpus was created by mixing together TREC documents on known topics.", "191": "From the 1989 AP data on TREC disks 1 and 2, we extracted all of the documents that were judged to be relevant to one of the following six topics:", "192": "Synthetic documents were created by first drawing three topics randomly, with replacement, from the above list of six.", "193": "A random document from each topic was then se- lected, and the three documents were concatenated together to form a synthetic document containing either one, two or three different aspects.", "194": "A total of 200 documents were generated in this manner.", "195": "used to train aspect models using both EP and VB, fixing the number of aspects at six; 75% of the data was used for training, and the remaining 25% was used as test data.", "196": "Figure 2 shows the top words for each aspect for the EP- trained model.", "197": "Because likelihood is used as the objective function, the common, content-free words take up a sig- nificant portion of the probability massa fact that is of- ten not acknowledged in descriptions of aspect models.", "198": "As seen in this figure, the aspects model variations across doc- uments in the distribution of common words such as SAID, FOR, and WAS.", "199": "After filtering out the common words from the list, by not displaying words having a unigram proba- bility larger than a threshold of 0.001, the most probable words that remain clearly indicate that the true underlying aspects have been captured, though some more cleanly than others.", "200": "For example, aspect 1 corresponds to topic 142, and aspect 5 corresponds to topic 59.", "201": "The models are compared quantitatively using test set per- plexity, exp(( i log p(di))/  i |di|); lower perplexity is better.", "202": "The probability function (3) cannot be computed analytically, and we do not want to favor either of the two approximations, so we use importance sampling to com- pute perplexity.", "203": "In particular, we sample  from the approximate posterior D ( | ) obtained from EP.", "204": "Figure 3 shows the test set perplexities for VB and EP; the perplexity for the EP-trained model is consistently lower than the perplexity of the VB-trained model.", "205": "Based on the results of Section 6.1, we anticipate that for VB the as- pects will be more extreme and specialized.", "206": "This would make the Dirichlet weights a smaller for the specialized aspects, which are used infrequently, and larger for the as- pects that are used in different topics or that are devoted to the common words.", "207": "Plots of the Dirichlet parameters (Fig- ure 3, center and right) show that VB results in as that are indeed more spread out towards these extremes, compared with those obtained using EP.", "208": "6.3 TREC Interactive Data", "209": "To compare VB and EP on real data having a mixture of aspects, this section considers documents from the TREC interactive collection (Over, 2001).", "210": "The data used for this track is interesting for studying aspect models because the relevant documents have been hand labeled according to the specific aspects of a topic that they cover.", "211": "Here we simply evaluate perplexities of the models.", "212": "We extracted all of the relevant documents for each of the six topics that the collection has relevance judgements for, resulting in a set of 772 documents.", "213": "The average docu- ment length is 594 tokens, and the total vocabulary size is 26,319 words.", "214": "As above, 75% of the data was used for training, and the remaining 25% was used for evaluating perplexities.", "215": "In these experiments the speed of VB and EP are comparable.", "216": "Figure 3 shows the test set perplexity and Dirichlet param- eters a for both EP and VB, trained using A = 10 aspects.", "217": "As for the controlled TREC data, EP achieves a lower per- plexity, and has aspects that are more balanced compared to those obtained using VB.", "218": "We suspect that the perplex- ity difference on both the TREC interactive and controlled TREC data is small because the true aspects have little overlap, and thus the posterior of the mixing weights is sharply peaked.", "219": "7 Conclusions", "220": "The generative aspect model provides an attractive ap- proach to modeling the variation of word probabilities across documents, making the model well suited to in- formation retrieval and other text processing applications.", "221": "This paper studied the problem of approximation meth- ods for learning and inference in the generative aspect model, and proposed an algorithm based on Expectation- Propagation as an alternative to the variational method adopted by Blei et al.", "222": "(2001).", "223": "Experiments on synthetic data showed that simple variational inference can lead to in- accurate inferences and biased learning, while Expectation- Propagation can lead to more accurate inferences.", "224": "Exper- iments on TREC data show that Expectation-Propagation achieves lower test set perplexity.", "225": "We attribute this to the fact that the Jensen bound used by the variational method is inadequate for representing how peaky versus spread out is the posterior on , which happens to be crucial for good parameter estimates.", "226": "Because there is a separate  for each document, this deficiency is not minimized by ad- ditional documents, but rather compounded.", "227": "This synthetic collection thus simulates a set of retrieved documents to answer a query, which we then wish to ana- lyze in order to extract the aspect structure.", "228": "The data was"}, "scenes": [["Expectation\u2013maximization_algorithm", "Expectation_propagation"], ["Text_Retrieval_Conference"], ["Expectation_propagation"], ["Expectation_propagation"], ["Expectation_propagation"], ["Expectation_propagation"], ["Expectation_propagation"], ["Text_Retrieval_Conference", "Expectation_propagation"], ["Grammatical_aspect"], ["Dirichlet_distribution"], ["Dirichlet_distribution"], ["Dirichlet_distribution"], ["Dirichlet_distribution"], ["Expected_value"], ["Expectation_propagation"], ["Expectation_propagation"], ["Expectation_propagation", "Monte_Carlo_method"], ["Dirichlet_distribution"], ["Expectation\u2013maximization_algorithm"], ["Expectation\u2013maximization_algorithm"], ["Expectation_propagation"], ["Western_Australia"], ["Expectation_propagation"], ["Expectation\u2013maximization_algorithm"], ["Expectation_propagation"], ["Expectation\u2013maximization_algorithm"], ["Expectation\u2013maximization_algorithm"], ["Expectation_propagation"], ["Expectation\u2013maximization_algorithm"], ["Expectation_propagation"], ["Expectation\u2013maximization_algorithm"], ["Expectation_propagation"], ["Expectation_propagation"], ["Expectation_propagation"], ["Expectation_propagation"], ["Expectation_propagation"], ["Expectation\u2013maximization_algorithm"], ["Expectation\u2013maximization_algorithm", "Expectation_propagation"], ["Expectation_propagation"], ["Resampling_(statistics)"], ["Expectation_propagation"], ["Expectation_propagation"], ["Expectation_propagation"], ["Expectation_propagation"], ["Text_Retrieval_Conference", "Expectation_propagation"], ["Text_Retrieval_Conference"], ["Expectation_propagation"], ["Expectation_propagation"], ["Was"], ["Expectation_propagation"], ["Holotype", "Expectation_propagation"], ["Expectation_propagation"], ["Text_Retrieval_Conference"], ["Text_Retrieval_Conference", "Expectation_propagation"], ["Expectation_propagation"], ["Expectation_propagation"], ["Text_Retrieval_Conference", "Expectation_propagation"], ["Text_Retrieval_Conference"], ["Expectation_propagation"], ["Expectation_propagation"], ["Text_Retrieval_Conference", "Expectation_propagation"]], "chapters": [{"text": "Abstract", "sentence_id": "s_0", "sentence_rank": "0", "paragraph_id": "p_0", "paragraph_rank": 0}, {"text": "1 Introduction", "sentence_id": "s_8", "sentence_rank": "8", "paragraph_id": "p_2", "paragraph_rank": 2}, {"text": "2 The Generative Aspect Model", "sentence_id": "s_32", "sentence_rank": "32", "paragraph_id": "p_7", "paragraph_rank": 7}, {"text": "3 Expectation-Propagation", "sentence_id": "s_56", "sentence_rank": "56", "paragraph_id": "p_17", "paragraph_rank": 17}, {"text": "4 Inference", "sentence_id": "s_72", "sentence_rank": "72", "paragraph_id": "p_22", "paragraph_rank": 22}, {"text": "4.1 Variational inference", "sentence_id": "s_75", "sentence_rank": "75", "paragraph_id": "p_24", "paragraph_rank": 24}, {"text": "4.2 Expectation-Propagation", "sentence_id": "s_92", "sentence_rank": "92", "paragraph_id": "p_32", "paragraph_rank": 32}, {"text": "5 Learning", "sentence_id": "s_108", "sentence_rank": "108", "paragraph_id": "p_41", "paragraph_rank": 41}, {"text": "5.1 Maximizing the estimate", "sentence_id": "s_114", "sentence_rank": "114", "paragraph_id": "p_43", "paragraph_rank": 43}, {"text": "5.2 Approximative EM", "sentence_id": "s_126", "sentence_rank": "126", "paragraph_id": "p_46", "paragraph_rank": 46}, {"text": "6 Experimental Results", "sentence_id": "s_138", "sentence_rank": "138", "paragraph_id": "p_52", "paragraph_rank": 52}, {"text": "6.1 Results on Synthetic Data", "sentence_id": "s_142", "sentence_rank": "142", "paragraph_id": "p_54", "paragraph_rank": 54}, {"text": "6.2 Controlled TREC Data", "sentence_id": "s_189", "sentence_rank": "189", "paragraph_id": "p_65", "paragraph_rank": 65}, {"text": "6.3 TREC Interactive Data", "sentence_id": "s_208", "sentence_rank": "208", "paragraph_id": "p_71", "paragraph_rank": 71}, {"text": "7 Conclusions", "sentence_id": "s_219", "sentence_rank": "219", "paragraph_id": "p_75", "paragraph_rank": 75}], "all_paragraphs": [{"paragraph_info": {"end": 8, "start": 0, "text": "Abstract", "rank": 0, "paragraph_comparative_number": 1, "entities": [], "id": "p_0"}, "sentences": [{"end": 8, "text": "Abstract", "rank": 0, "start": 0, "IsComparative": "1", "id": "st_0"}]}, {"paragraph_info": {"end": 745, "start": 8, "text": "The generative aspect model is an extension of the multinomial model for text that allows word probabilities to vary stochastically across docu- ments.Previous results with aspect models have been promising, but hindered by the computa- tional difficulty of carrying out inference and learning.This paper demonstrates that the sim- ple variational methods of Blei et al.(2001) can lead to inaccurate inferences and biased learning for the generative aspect model.We develop an alternative approach that leads to higher accuracy at comparable cost.An extension of Expectation- Propagation is used for inference and then em- bedded in an EM algorithm for learning.Exper- imental results are presented for both synthetic and real data sets.", "rank": 1, "paragraph_comparative_number": 3, "entities": [], "id": "p_1"}, "sentences": [{"end": 159, "text": "The generative aspect model is an extension of the multinomial model for text that allows word probabilities to vary stochastically across docu- ments.", "rank": 1, "start": 8, "IsComparative": "1", "id": "st_1"}, {"end": 302, "text": "Previous results with aspect models have been promising, but hindered by the computa- tional difficulty of carrying out inference and learning.", "rank": 2, "start": 159, "IsComparative": "0", "id": "st_2"}, {"end": 378, "text": "This paper demonstrates that the sim- ple variational methods of Blei et al.", "rank": 3, "start": 302, "IsComparative": "0", "id": "st_3"}, {"end": 471, "text": "(2001) can lead to inaccurate inferences and biased learning for the generative aspect model.", "rank": 4, "start": 378, "IsComparative": "1", "id": "st_4"}, {"end": 555, "text": "We develop an alternative approach that leads to higher accuracy at comparable cost.", "rank": 5, "start": 471, "IsComparative": "1", "id": "st_5"}, {"end": 670, "text": "An extension of Expectation- Propagation is used for inference and then em- bedded in an EM algorithm for learning.", "rank": 6, "start": 555, "IsComparative": "0", "id": "st_6"}, {"end": 745, "text": "Exper- imental results are presented for both synthetic and real data sets.", "rank": 7, "start": 670, "IsComparative": "0", "id": "st_7"}]}, {"paragraph_info": {"end": 759, "start": 745, "text": "1 Introduction", "rank": 2, "paragraph_comparative_number": 0, "entities": [], "id": "p_2"}, "sentences": [{"end": 759, "text": "1 Introduction", "rank": 8, "start": 745, "IsComparative": "0", "id": "st_8"}]}, {"paragraph_info": {"end": 1165, "start": 759, "text": "Approximate inference techniques, such as variational methods, are increasingly being used to tackle advanced data models.When learning and inference are intractable, approximation can make the difference between a useful model and an impractical model.However, if applied in- discriminately, approximation can change the qualitative behavior of a model, leading to unpredictable and unde- sirable results.", "rank": 3, "paragraph_comparative_number": 2, "entities": [], "id": "p_3"}, "sentences": [{"end": 881, "text": "Approximate inference techniques, such as variational methods, are increasingly being used to tackle advanced data models.", "rank": 9, "start": 759, "IsComparative": "1", "id": "st_9"}, {"end": 1012, "text": "When learning and inference are intractable, approximation can make the difference between a useful model and an impractical model.", "rank": 10, "start": 881, "IsComparative": "0", "id": "st_10"}, {"end": 1165, "text": "However, if applied in- discriminately, approximation can change the qualitative behavior of a model, leading to unpredictable and unde- sirable results.", "rank": 11, "start": 1012, "IsComparative": "1", "id": "st_11"}]}, {"paragraph_info": {"end": 2439, "start": 1165, "text": "The generative aspect model introduced by Blei et al.(2001) is a promising model for discrete data, and provides an interesting example of the need for good approxima- tion strategies.When applied to text, the model explic- itly accounts for the intuition that a document may have several subtopics or aspects, making it an attractive tool for several applications in text processing and information retrieval.As an example, imagine that rather than sim- ply returning a list of documents that are relevant to a given topic, a search engine could automatically determine the different aspects of the topic that are treated by each document in the list, and reorder the documents to effi- ciently cover these different aspects.The TREC interac- tive track (Over, 2001) has been set up to help investigate precisely this task, but from the point of view of users in- teracting with the search engine.As an example of the judgements made in this task, for the topic electric au- tomobiles (number 247i), the human assessors identified eleven aspects among the documents that were judged to be relevant, having descriptions such as government fund- ing of electric car development programs, industrial de- velopment of hybrid electric cars, and increased use of aluminum bodies.", "rank": 4, "paragraph_comparative_number": 3, "entities": [], "id": "p_4"}, "sentences": [{"end": 1218, "text": "The generative aspect model introduced by Blei et al.", "rank": 12, "start": 1165, "IsComparative": "0", "id": "st_12"}, {"end": 1349, "text": "(2001) is a promising model for discrete data, and provides an interesting example of the need for good approxima- tion strategies.", "rank": 13, "start": 1218, "IsComparative": "1", "id": "st_13"}, {"end": 1575, "text": "When applied to text, the model explic- itly accounts for the intuition that a document may have several subtopics or aspects, making it an attractive tool for several applications in text processing and information retrieval.", "rank": 14, "start": 1349, "IsComparative": "1", "id": "st_14"}, {"end": 1891, "text": "As an example, imagine that rather than sim- ply returning a list of documents that are relevant to a given topic, a search engine could automatically determine the different aspects of the topic that are treated by each document in the list, and reorder the documents to effi- ciently cover these different aspects.", "rank": 15, "start": 1575, "IsComparative": "0", "id": "st_15"}, {"end": 2063, "text": "The TREC interac- tive track (Over, 2001) has been set up to help investigate precisely this task, but from the point of view of users in- teracting with the search engine.", "rank": 16, "start": 1891, "IsComparative": "1", "id": "st_16"}, {"end": 2439, "text": "As an example of the judgements made in this task, for the topic electric au- tomobiles (number 247i), the human assessors identified eleven aspects among the documents that were judged to be relevant, having descriptions such as government fund- ing of electric car development programs, industrial de- velopment of hybrid electric cars, and increased use of aluminum bodies.", "rank": 17, "start": 2063, "IsComparative": "0", "id": "st_17"}]}, {"paragraph_info": {"end": 3285, "start": 2439, "text": "This paper examines computation in the generative as- pect model, proposing new algorithms for approximate in- ference and learning that are based on the Expectation- Propagation framework of Minka (2001b).Hofmanns original aspect model involved a large number of param- eters and heuristic procedures to avoid overfitting (Hof- mann, 1999).Blei et al.(2001) introduced a modified model with a proper generative semantics and used vari- ational methods to carry out inference and learning.It is found that the variational methods can lead to inac- curate inferences and biased learning, while Expectation- Propagation gives results that are more true to the model.Besides providing a practical new algorithm for a useful model, we hope that this result will shed light on the ques- tion of which approximations are appropriate for which problems.", "rank": 5, "paragraph_comparative_number": 2, "entities": [], "id": "p_5"}, "sentences": [{"end": 2645, "text": "This paper examines computation in the generative as- pect model, proposing new algorithms for approximate in- ference and learning that are based on the Expectation- Propagation framework of Minka (2001b).", "rank": 18, "start": 2439, "IsComparative": "0", "id": "st_18"}, {"end": 2780, "text": "Hofmanns original aspect model involved a large number of param- eters and heuristic procedures to avoid overfitting (Hof- mann, 1999).", "rank": 19, "start": 2645, "IsComparative": "0", "id": "st_19"}, {"end": 2791, "text": "Blei et al.", "rank": 20, "start": 2780, "IsComparative": "0", "id": "st_20"}, {"end": 2928, "text": "(2001) introduced a modified model with a proper generative semantics and used vari- ational methods to carry out inference and learning.", "rank": 21, "start": 2791, "IsComparative": "1", "id": "st_21"}, {"end": 3103, "text": "It is found that the variational methods can lead to inac- curate inferences and biased learning, while Expectation- Propagation gives results that are more true to the model.", "rank": 22, "start": 2928, "IsComparative": "1", "id": "st_22"}, {"end": 3285, "text": "Besides providing a practical new algorithm for a useful model, we hope that this result will shed light on the ques- tion of which approximations are appropriate for which problems.", "rank": 23, "start": 3103, "IsComparative": "0", "id": "st_23"}]}, {"paragraph_info": {"end": 4420, "start": 3285, "text": "The following section presents the generative aspect model, briefly discussing some of the properties that make it at- tractive for modeling documents, and stating the infer- ence and learning problems to be addressed.After a brief overview of Expectation-Propagation in Section 3, a new algorithm for approximate inference in the genera- tive aspect model is presented in Section 4.Separate from Expectation-Propagation, a new algorithm for approximate learning in the generative aspect model is presented in Section 5.Brief descriptions of the corresponding proce- dures with variational methods are included for complete- ness.Section 6 describes experiments on synthetic and real data.Section 6.1 presents a synthetic data experiment us-ing low dimensional multinomials, which clearly demon- strates how variational methods can result in inaccurate inferences compared to Expectation-Propagation.In Sec- tions 6.2 and 6.3 the methods are then compared using doc- ument collections taken from TREC data, where it is seen that Expectation-Propagation attains lower test set perplex- ity.Section 7 summarizes the results of the paper.", "rank": 6, "paragraph_comparative_number": 2, "entities": [], "id": "p_6"}, "sentences": [{"end": 3503, "text": "The following section presents the generative aspect model, briefly discussing some of the properties that make it at- tractive for modeling documents, and stating the infer- ence and learning problems to be addressed.", "rank": 24, "start": 3285, "IsComparative": "1", "id": "st_24"}, {"end": 3668, "text": "After a brief overview of Expectation-Propagation in Section 3, a new algorithm for approximate inference in the genera- tive aspect model is presented in Section 4.", "rank": 25, "start": 3503, "IsComparative": "0", "id": "st_25"}, {"end": 3805, "text": "Separate from Expectation-Propagation, a new algorithm for approximate learning in the generative aspect model is presented in Section 5.", "rank": 26, "start": 3668, "IsComparative": "0", "id": "st_26"}, {"end": 3915, "text": "Brief descriptions of the corresponding proce- dures with variational methods are included for complete- ness.", "rank": 27, "start": 3805, "IsComparative": "0", "id": "st_27"}, {"end": 3974, "text": "Section 6 describes experiments on synthetic and real data.", "rank": 28, "start": 3915, "IsComparative": "0", "id": "st_28"}, {"end": 4185, "text": "Section 6.1 presents a synthetic data experiment us-ing low dimensional multinomials, which clearly demon- strates how variational methods can result in inaccurate inferences compared to Expectation-Propagation.", "rank": 29, "start": 3974, "IsComparative": "1", "id": "st_29"}, {"end": 4374, "text": "In Sec- tions 6.2 and 6.3 the methods are then compared using doc- ument collections taken from TREC data, where it is seen that Expectation-Propagation attains lower test set perplex- ity.", "rank": 30, "start": 4185, "IsComparative": "0", "id": "st_30"}, {"end": 4420, "text": "Section 7 summarizes the results of the paper.", "rank": 31, "start": 4374, "IsComparative": "0", "id": "st_31"}]}, {"paragraph_info": {"end": 4449, "start": 4420, "text": "2 The Generative Aspect Model", "rank": 7, "paragraph_comparative_number": 0, "entities": [], "id": "p_7"}, "sentences": [{"end": 4449, "text": "2 The Generative Aspect Model", "rank": 32, "start": 4420, "IsComparative": "0", "id": "st_32"}]}, {"paragraph_info": {"end": 4681, "start": 4449, "text": "A simple generative model for documents is the multino- mial model, which assumes that words are drawn one at a time and independently from a fixed word distribution p(w).The probability of a document d having word counts nw is thus", "rank": 8, "paragraph_comparative_number": 1, "entities": [], "id": "p_8"}, "sentences": [{"end": 4620, "text": "A simple generative model for documents is the multino- mial model, which assumes that words are drawn one at a time and independently from a fixed word distribution p(w).", "rank": 33, "start": 4449, "IsComparative": "0", "id": "st_33"}, {"end": 4681, "text": "The probability of a document d having word counts nw is thus", "rank": 34, "start": 4620, "IsComparative": "1", "id": "st_34"}]}, {"paragraph_info": {"end": 4742, "start": 4681, "text": "This family is very restrictive, in that a document of length", "rank": 9, "paragraph_comparative_number": 0, "entities": [], "id": "p_9"}, "sentences": [{"end": 4742, "text": "This family is very restrictive, in that a document of length", "rank": 35, "start": 4681, "IsComparative": "0", "id": "st_35"}]}, {"paragraph_info": {"end": 5110, "start": 4742, "text": "n is expected to have np(w) occurrences of word w, with little variation away from this number.Even within a ho- mogeneous set of documents, such as machine learning papers, there is typically far more variation in the word counts.One way to accommodate this is to allow the word probabilities p to vary across documents, leading to a hier- archical multinomial model.", "rank": 10, "paragraph_comparative_number": 0, "entities": [], "id": "p_10"}, "sentences": [{"end": 4837, "text": "n is expected to have np(w) occurrences of word w, with little variation away from this number.", "rank": 36, "start": 4742, "IsComparative": "0", "id": "st_36"}, {"end": 4973, "text": "Even within a ho- mogeneous set of documents, such as machine learning papers, there is typically far more variation in the word counts.", "rank": 37, "start": 4837, "IsComparative": "0", "id": "st_37"}, {"end": 5110, "text": "One way to accommodate this is to allow the word probabilities p to vary across documents, leading to a hier- archical multinomial model.", "rank": 38, "start": 4973, "IsComparative": "0", "id": "st_38"}]}, {"paragraph_info": {"end": 5736, "start": 5110, "text": "This requires us to specify a distribution on p itself, con- sidered as a vector of numbers which sum to one.One natural choice is the Dirichlet distribution, which is conju- gate to the multinomial.Unfortunately, while the Dirichlet can capture variation in the p(w)s, it cannot capture co- variation, the tendency for some probabilities to move up and down together.At the other extreme, we can sample p from a finite set, corresponding to a finite mixture of multi- nomials.This model can capture co-variation, but at great expense, since a new mixture component is needed for ev- ery distinct choice of word probabilities.", "rank": 11, "paragraph_comparative_number": 3, "entities": [], "id": "p_11"}, "sentences": [{"end": 5219, "text": "This requires us to specify a distribution on p itself, con- sidered as a vector of numbers which sum to one.", "rank": 39, "start": 5110, "IsComparative": "1", "id": "st_39"}, {"end": 5309, "text": "One natural choice is the Dirichlet distribution, which is conju- gate to the multinomial.", "rank": 40, "start": 5219, "IsComparative": "0", "id": "st_40"}, {"end": 5478, "text": "Unfortunately, while the Dirichlet can capture variation in the p(w)s, it cannot capture co- variation, the tendency for some probabilities to move up and down together.", "rank": 41, "start": 5309, "IsComparative": "1", "id": "st_41"}, {"end": 5587, "text": "At the other extreme, we can sample p from a finite set, corresponding to a finite mixture of multi- nomials.", "rank": 42, "start": 5478, "IsComparative": "1", "id": "st_42"}, {"end": 5736, "text": "This model can capture co-variation, but at great expense, since a new mixture component is needed for ev- ery distinct choice of word probabilities.", "rank": 43, "start": 5587, "IsComparative": "0", "id": "st_43"}]}, {"paragraph_info": {"end": 6106, "start": 5736, "text": "In the generative aspect model, it is assumed that there are A underlying aspects, each represented as a multinomial distribution over the words in the vocabulary.A document is generated by the following process.First,  is sampled from a Dirichlet distribution D ( | ), so that  a a = 1.This determines mixing weights for the aspects, yielding a word probability vector:", "rank": 12, "paragraph_comparative_number": 0, "entities": [], "id": "p_12"}, "sentences": [{"end": 5899, "text": "In the generative aspect model, it is assumed that there are A underlying aspects, each represented as a multinomial distribution over the words in the vocabulary.", "rank": 44, "start": 5736, "IsComparative": "0", "id": "st_44"}, {"end": 5948, "text": "A document is generated by the following process.", "rank": 45, "start": 5899, "IsComparative": "0", "id": "st_45"}, {"end": 6023, "text": "First,  is sampled from a Dirichlet distribution D ( | ), so that  a a = 1.", "rank": 46, "start": 5948, "IsComparative": "0", "id": "st_46"}, {"end": 6106, "text": "This determines mixing weights for the aspects, yielding a word probability vector:", "rank": 47, "start": 6023, "IsComparative": "0", "id": "st_47"}]}, {"paragraph_info": {"end": 6818, "start": 6106, "text": "The document is then sampled from a multinomial distri- bution with these probabilities.Instead of a finite mixture, this distribution might be called a simplicial mixture, since the word probability vector ranges over a simplex with cor- ners p(w|a = 1),...,p(w|a = A).The probability of a document is where the parameters  are the Dirichlet parameters a and the multinomial models p( | a);  denotes the (A  1)- dimensional simplex, the sample space of the Dirichlet D ( | ).Because  is sampled for each document, differ- ent documents can exhibit the aspects in different propor- tions.However, the integral in (3) does not simplify and must be approximated, which is the main complication in using this model.", "rank": 13, "paragraph_comparative_number": 1, "entities": [], "id": "p_13"}, "sentences": [{"end": 6194, "text": "The document is then sampled from a multinomial distri- bution with these probabilities.", "rank": 48, "start": 6106, "IsComparative": "0", "id": "st_48"}, {"end": 6376, "text": "Instead of a finite mixture, this distribution might be called a simplicial mixture, since the word probability vector ranges over a simplex with cor- ners p(w|a = 1),...,p(w|a = A).", "rank": 49, "start": 6194, "IsComparative": "0", "id": "st_49"}, {"end": 6582, "text": "The probability of a document is where the parameters  are the Dirichlet parameters a and the multinomial models p( | a);  denotes the (A  1)- dimensional simplex, the sample space of the Dirichlet D ( | ).", "rank": 50, "start": 6376, "IsComparative": "1", "id": "st_50"}, {"end": 6694, "text": "Because  is sampled for each document, differ- ent documents can exhibit the aspects in different propor- tions.", "rank": 51, "start": 6582, "IsComparative": "0", "id": "st_51"}, {"end": 6818, "text": "However, the integral in (3) does not simplify and must be approximated, which is the main complication in using this model.", "rank": 52, "start": 6694, "IsComparative": "0", "id": "st_52"}]}, {"paragraph_info": {"end": 6871, "start": 6818, "text": "The two basic computational tasks for this model are:", "rank": 14, "paragraph_comparative_number": 0, "entities": [], "id": "p_14"}, "sentences": [{"end": 6871, "text": "The two basic computational tasks for this model are:", "rank": 53, "start": 6818, "IsComparative": "0", "id": "st_53"}]}, {"paragraph_info": {"end": 6948, "start": 6871, "text": "Inference: Evaluate the probability of a document; i.e., the integral in (3).", "rank": 15, "paragraph_comparative_number": 0, "entities": [], "id": "p_15"}, "sentences": [{"end": 6948, "text": "Inference: Evaluate the probability of a document; i.e., the integral in (3).", "rank": 54, "start": 6871, "IsComparative": "0", "id": "st_54"}]}, {"paragraph_info": {"end": 7113, "start": 6948, "text": "Learning: For a set of training documents, find the pa- rameter values  = (p( | a), ) which maximize the likelihood; i.e., maximize the value of the integral in (3).", "rank": 16, "paragraph_comparative_number": 1, "entities": [], "id": "p_16"}, "sentences": [{"end": 7113, "text": "Learning: For a set of training documents, find the pa- rameter values  = (p( | a), ) which maximize the likelihood; i.e., maximize the value of the integral in (3).", "rank": 55, "start": 6948, "IsComparative": "1", "id": "st_55"}]}, {"paragraph_info": {"end": 7138, "start": 7113, "text": "3 Expectation-Propagation", "rank": 17, "paragraph_comparative_number": 0, "entities": [], "id": "p_17"}, "sentences": [{"end": 7138, "text": "3 Expectation-Propagation", "rank": 56, "start": 7113, "IsComparative": "0", "id": "st_56"}]}, {"paragraph_info": {"end": 7304, "start": 7138, "text": "Expectation-Propagation is an algorithm for approximating integrals over functions that factor into simple terms.The general form for such integrals in our setting is", "rank": 18, "paragraph_comparative_number": 1, "entities": [], "id": "p_18"}, "sentences": [{"end": 7251, "text": "Expectation-Propagation is an algorithm for approximating integrals over functions that factor into simple terms.", "rank": 57, "start": 7138, "IsComparative": "0", "id": "st_57"}, {"end": 7304, "text": "The general form for such integrals in our setting is", "rank": 58, "start": 7251, "IsComparative": "1", "id": "st_58"}]}, {"paragraph_info": {"end": 7554, "start": 7304, "text": "In previous work each count nw was assumed to be 1 (Minka, 2001b).Here we present a slight generalization to allow real-valued powers on the terms.Expectation- Propagation approximates each term tw() by a simpler term t w(), giving a simpler integral", "rank": 19, "paragraph_comparative_number": 0, "entities": [], "id": "p_19"}, "sentences": [{"end": 7370, "text": "In previous work each count nw was assumed to be 1 (Minka, 2001b).", "rank": 59, "start": 7304, "IsComparative": "0", "id": "st_59"}, {"end": 7451, "text": "Here we present a slight generalization to allow real-valued powers on the terms.", "rank": 60, "start": 7370, "IsComparative": "0", "id": "st_60"}, {"end": 7554, "text": "Expectation- Propagation approximates each term tw() by a simpler term t w(), giving a simpler integral", "rank": 61, "start": 7451, "IsComparative": "0", "id": "st_61"}]}, {"paragraph_info": {"end": 8147, "start": 7554, "text": "The algorithm proceeds by iteratively applying dele- tion/inclusion steps.One of the approximate terms is deleted from q(), giving the partial function q\\w() = Then a new approximation for tw () is com- puted so that tw () q () is similar to tw () q (), in the sense of having the same integral and the same set of specified moments.The moments used in this paper are the mean and variance.The partial function q\\w() thus acts as context for the approximation.Unlike variational bounds, this approximation is global, not local, and conse- quently the estimate of the integral is more accurate.", "rank": 20, "paragraph_comparative_number": 3, "entities": [], "id": "p_20"}, "sentences": [{"end": 7628, "text": "The algorithm proceeds by iteratively applying dele- tion/inclusion steps.", "rank": 62, "start": 7554, "IsComparative": "1", "id": "st_62"}, {"end": 7887, "text": "One of the approximate terms is deleted from q(), giving the partial function q\\w() = Then a new approximation for tw () is com- puted so that tw () q () is similar to tw () q (), in the sense of having the same integral and the same set of specified moments.", "rank": 63, "start": 7628, "IsComparative": "1", "id": "st_63"}, {"end": 7944, "text": "The moments used in this paper are the mean and variance.", "rank": 64, "start": 7887, "IsComparative": "0", "id": "st_64"}, {"end": 8014, "text": "The partial function q\\w() thus acts as context for the approximation.", "rank": 65, "start": 7944, "IsComparative": "0", "id": "st_65"}, {"end": 8147, "text": "Unlike variational bounds, this approximation is global, not local, and conse- quently the estimate of the integral is more accurate.", "rank": 66, "start": 8014, "IsComparative": "1", "id": "st_66"}]}, {"paragraph_info": {"end": 8548, "start": 8147, "text": "A fixed point of this algorithm always exists, but we may not always reach one.The approximation may oscillate or enter a region where the integral is undefined.We uti- lize two techniques to prevent this.First, the updates are damped so that t w() cannot oscillate.Second, if a deletion-inclusion step leads to an undefined integral, the step is undone and the algorithm continues with the next term.", "rank": 21, "paragraph_comparative_number": 2, "entities": [], "id": "p_21"}, "sentences": [{"end": 8226, "text": "A fixed point of this algorithm always exists, but we may not always reach one.", "rank": 67, "start": 8147, "IsComparative": "1", "id": "st_67"}, {"end": 8308, "text": "The approximation may oscillate or enter a region where the integral is undefined.", "rank": 68, "start": 8226, "IsComparative": "0", "id": "st_68"}, {"end": 8352, "text": "We uti- lize two techniques to prevent this.", "rank": 69, "start": 8308, "IsComparative": "1", "id": "st_69"}, {"end": 8413, "text": "First, the updates are damped so that t w() cannot oscillate.", "rank": 70, "start": 8352, "IsComparative": "0", "id": "st_70"}, {"end": 8548, "text": "Second, if a deletion-inclusion step leads to an undefined integral, the step is undone and the algorithm continues with the next term.", "rank": 71, "start": 8413, "IsComparative": "0", "id": "st_71"}]}, {"paragraph_info": {"end": 8559, "start": 8548, "text": "4 Inference", "rank": 22, "paragraph_comparative_number": 0, "entities": [], "id": "p_22"}, "sentences": [{"end": 8559, "text": "4 Inference", "rank": 72, "start": 8548, "IsComparative": "0", "id": "st_72"}]}, {"paragraph_info": {"end": 8712, "start": 8559, "text": "This section describes two algorithms for approximating the integral in (3): variational inference used by Blei et al.(2001) and Expectation-Propagation.", "rank": 23, "paragraph_comparative_number": 0, "entities": [], "id": "p_23"}, "sentences": [{"end": 8677, "text": "This section describes two algorithms for approximating the integral in (3): variational inference used by Blei et al.", "rank": 73, "start": 8559, "IsComparative": "0", "id": "st_73"}, {"end": 8712, "text": "(2001) and Expectation-Propagation.", "rank": 74, "start": 8677, "IsComparative": "0", "id": "st_74"}]}, {"paragraph_info": {"end": 8737, "start": 8712, "text": "4.1 Variational inference", "rank": 24, "paragraph_comparative_number": 0, "entities": [], "id": "p_24"}, "sentences": [{"end": 8737, "text": "4.1 Variational inference", "rank": 75, "start": 8712, "IsComparative": "0", "id": "st_75"}]}, {"paragraph_info": {"end": 9099, "start": 8737, "text": "To approximate the integral of a function, variational in- ference lower bounds the function and then integrates the lower bound.A simple lower bound for (3) comes from Jensens inequality.The bound is parameterized by a vec- tor q(a | w): or responsibility of word w to the aspects.Given bound parameters q(a | w) for all a and w, the integral is now an- alytic:", "rank": 25, "paragraph_comparative_number": 0, "entities": [], "id": "p_25"}, "sentences": [{"end": 8866, "text": "To approximate the integral of a function, variational in- ference lower bounds the function and then integrates the lower bound.", "rank": 76, "start": 8737, "IsComparative": "0", "id": "st_76"}, {"end": 8925, "text": "A simple lower bound for (3) comes from Jensens inequality.", "rank": 77, "start": 8866, "IsComparative": "0", "id": "st_77"}, {"end": 9019, "text": "The bound is parameterized by a vec- tor q(a | w): or responsibility of word w to the aspects.", "rank": 78, "start": 8925, "IsComparative": "0", "id": "st_78"}, {"end": 9099, "text": "Given bound parameters q(a | w) for all a and w, the integral is now an- alytic:", "rank": 79, "start": 9019, "IsComparative": "0", "id": "st_79"}]}, {"paragraph_info": {"end": 9215, "start": 9099, "text": "Laplaces method using a softmax transformation, vari- ational inference, and two different Monte Carlo algo- rithms.", "rank": 26, "paragraph_comparative_number": 0, "entities": [], "id": "p_26"}, "sentences": [{"end": 9215, "text": "Laplaces method using a softmax transformation, vari- ational inference, and two different Monte Carlo algo- rithms.", "rank": 80, "start": 9099, "IsComparative": "0", "id": "st_80"}]}, {"paragraph_info": {"end": 9531, "start": 9215, "text": "EP gives an integral estimate as well as an approximate posterior for the mixture weights.For the generative aspect model, the approximate posterior will be Dirichlet, and the integrand will be factored into terms of the form which resembles a Dirichlet with parameters wa.Thus, the approximate posterior is given by", "rank": 27, "paragraph_comparative_number": 1, "entities": [], "id": "p_27"}, "sentences": [{"end": 9305, "text": "EP gives an integral estimate as well as an approximate posterior for the mixture weights.", "rank": 81, "start": 9215, "IsComparative": "1", "id": "st_81"}, {"end": 9488, "text": "For the generative aspect model, the approximate posterior will be Dirichlet, and the integrand will be factored into terms of the form which resembles a Dirichlet with parameters wa.", "rank": 82, "start": 9305, "IsComparative": "0", "id": "st_82"}, {"end": 9531, "text": "Thus, the approximate posterior is given by", "rank": 83, "start": 9488, "IsComparative": "0", "id": "st_83"}]}, {"paragraph_info": {"end": 9714, "start": 9531, "text": "The best bound parameters are found by maximizing the value of the bound.A convenient way to do this is with EM.The parameter in the algorithm is q(a | w) and the hidden variable is :", "rank": 28, "paragraph_comparative_number": 0, "entities": [], "id": "p_28"}, "sentences": [{"end": 9604, "text": "The best bound parameters are found by maximizing the value of the bound.", "rank": 84, "start": 9531, "IsComparative": "0", "id": "st_84"}, {"end": 9643, "text": "A convenient way to do this is with EM.", "rank": 85, "start": 9604, "IsComparative": "0", "id": "st_85"}, {"end": 9714, "text": "The parameter in the algorithm is q(a | w) and the hidden variable is :", "rank": 86, "start": 9643, "IsComparative": "0", "id": "st_86"}]}, {"paragraph_info": {"end": 9744, "start": 9714, "text": "E-step: a = a+ nwq(a|w) (10) w", "rank": 29, "paragraph_comparative_number": 0, "entities": [], "id": "p_29"}, "sentences": [{"end": 9744, "text": "E-step: a = a+ nwq(a|w) (10) w", "rank": 87, "start": 9714, "IsComparative": "0", "id": "st_87"}]}, {"paragraph_info": {"end": 9779, "start": 9744, "text": "M-step: q(a|w)  p(w|a)exp((a)) (11)", "rank": 30, "paragraph_comparative_number": 0, "entities": [], "id": "p_30"}, "sentences": [{"end": 9779, "text": "M-step: q(a|w)  p(w|a)exp((a)) (11)", "rank": 88, "start": 9744, "IsComparative": "0", "id": "st_88"}]}, {"paragraph_info": {"end": 10028, "start": 9779, "text": "where  is the digamma function.Note that this is the same as the variational algorithm for mixture weights given by Minka (2000).The variables a used in this algorithm can be interpreted as defining an approximate Dirichlet poste- rior on : D ( | ).", "rank": 31, "paragraph_comparative_number": 0, "entities": [], "id": "p_31"}, "sentences": [{"end": 9810, "text": "where  is the digamma function.", "rank": 89, "start": 9779, "IsComparative": "0", "id": "st_89"}, {"end": 9908, "text": "Note that this is the same as the variational algorithm for mixture weights given by Minka (2000).", "rank": 90, "start": 9810, "IsComparative": "0", "id": "st_90"}, {"end": 10028, "text": "The variables a used in this algorithm can be interpreted as defining an approximate Dirichlet poste- rior on : D ( | ).", "rank": 91, "start": 9908, "IsComparative": "0", "id": "st_91"}]}, {"paragraph_info": {"end": 10055, "start": 10028, "text": "4.2 Expectation-Propagation", "rank": 32, "paragraph_comparative_number": 0, "entities": [], "id": "p_32"}, "sentences": [{"end": 10055, "text": "4.2 Expectation-Propagation", "rank": 92, "start": 10028, "IsComparative": "0", "id": "st_92"}]}, {"paragraph_info": {"end": 10329, "start": 10055, "text": "As mentioned above, the aspect model integral is the same as marginalizing the weights of a mixture model whose components are known.This problem was studied in depth by Minka (2001b) and it was found that Expectation- Propagation (EP) provides the best results, compared to", "rank": 33, "paragraph_comparative_number": 0, "entities": [], "id": "p_33"}, "sentences": [{"end": 10188, "text": "As mentioned above, the aspect model integral is the same as marginalizing the weights of a mixture model whose components are known.", "rank": 93, "start": 10055, "IsComparative": "0", "id": "st_93"}, {"end": 10329, "text": "This problem was studied in depth by Minka (2001b) and it was found that Expectation- Propagation (EP) provides the best results, compared to", "rank": 94, "start": 10188, "IsComparative": "0", "id": "st_94"}]}, {"paragraph_info": {"end": 10346, "start": 10329, "text": "loop w = 1,...,W:", "rank": 34, "paragraph_comparative_number": 0, "entities": [], "id": "p_34"}, "sentences": [{"end": 10346, "text": "loop w = 1,...,W:", "rank": 95, "start": 10329, "IsComparative": "0", "id": "st_95"}]}, {"paragraph_info": {"end": 10402, "start": 10346, "text": "(a) Deletion.Remove t w from the posterior to get an old", "rank": 35, "paragraph_comparative_number": 0, "entities": [], "id": "p_35"}, "sentences": [{"end": 10359, "text": "(a) Deletion.", "rank": 96, "start": 10346, "IsComparative": "0", "id": "st_96"}, {"end": 10402, "text": "Remove t w from the posterior to get an old", "rank": 97, "start": 10359, "IsComparative": "0", "id": "st_97"}]}, {"paragraph_info": {"end": 10472, "start": 10402, "text": "(d) Inclusion.Incorporate t w back into q() by scaling the change in :", "rank": 36, "paragraph_comparative_number": 0, "entities": [], "id": "p_36"}, "sentences": [{"end": 10416, "text": "(d) Inclusion.", "rank": 98, "start": 10402, "IsComparative": "0", "id": "st_98"}, {"end": 10472, "text": "Incorporate t w back into q() by scaling the change in :", "rank": 99, "start": 10416, "IsComparative": "0", "id": "st_99"}]}, {"paragraph_info": {"end": 10502, "start": 10472, "text": "= old+n( old) (21) a a w wa wa", "rank": 37, "paragraph_comparative_number": 0, "entities": [], "id": "p_37"}, "sentences": [{"end": 10502, "text": "= old+n( old) (21) a a w wa wa", "rank": 100, "start": 10472, "IsComparative": "0", "id": "st_100"}]}, {"paragraph_info": {"end": 10586, "start": 10502, "text": "This preserves the invariant (16).If any a < 0, undo all changes and skip this word.", "rank": 38, "paragraph_comparative_number": 1, "entities": [], "id": "p_38"}, "sentences": [{"end": 10536, "text": "This preserves the invariant (16).", "rank": 101, "start": 10502, "IsComparative": "0", "id": "st_101"}, {"end": 10586, "text": "If any a < 0, undo all changes and skip this word.", "rank": 102, "start": 10536, "IsComparative": "1", "id": "st_102"}]}, {"paragraph_info": {"end": 10889, "start": 10586, "text": "In our experience, words are skipped only on the first few iterations, before EP has settled into a decent approxima- tion.It can be shown that the safest stepsize for (c) is  = 1/nw, which makes  = .This is the value used in the experiments, though for faster convergence a larger  is often acceptable.", "rank": 39, "paragraph_comparative_number": 0, "entities": [], "id": "p_39"}, "sentences": [{"end": 10709, "text": "In our experience, words are skipped only on the first few iterations, before EP has settled into a decent approxima- tion.", "rank": 103, "start": 10586, "IsComparative": "0", "id": "st_103"}, {"end": 10786, "text": "It can be shown that the safest stepsize for (c) is  = 1/nw, which makes  = .", "rank": 104, "start": 10709, "IsComparative": "0", "id": "st_104"}, {"end": 10889, "text": "This is the value used in the experiments, though for faster convergence a larger  is often acceptable.", "rank": 105, "start": 10786, "IsComparative": "0", "id": "st_105"}]}, {"paragraph_info": {"end": 11259, "start": 10889, "text": "After convergence, the approximate posterior gives the fol- lowing estimate for the likelihood of the document, thus approximating the integral (3): estimates from the previous section and (2) a new approach based on approximative EM.The decision between these two approaches is separate from the decision of using vari- ational inference versus Expectation-Propagation.", "rank": 40, "paragraph_comparative_number": 1, "entities": [], "id": "p_40"}, "sentences": [{"end": 11123, "text": "After convergence, the approximate posterior gives the fol- lowing estimate for the likelihood of the document, thus approximating the integral (3): estimates from the previous section and (2) a new approach based on approximative EM.", "rank": 106, "start": 10889, "IsComparative": "0", "id": "st_106"}, {"end": 11259, "text": "The decision between these two approaches is separate from the decision of using vari- ational inference versus Expectation-Propagation.", "rank": 107, "start": 11123, "IsComparative": "1", "id": "st_107"}]}, {"paragraph_info": {"end": 11269, "start": 11259, "text": "5 Learning", "rank": 41, "paragraph_comparative_number": 0, "entities": [], "id": "p_41"}, "sentences": [{"end": 11269, "text": "5 Learning", "rank": 108, "start": 11259, "IsComparative": "0", "id": "st_108"}]}, {"paragraph_info": {"end": 12044, "start": 11269, "text": "Given a set of documents C = <di,i = 1,...,n>, with word counts denoted niw, the learning problem is to max- imize the likelihood as a function of the parameters  = (p( | a), ); the likelihood is given by Notice that each document has its own integral over .It is tempting to use EM for this problem, where we regard  as a hidden variable for each document.However, the E-step requires expectations over the posterior for , p( | d , ), which is an intractable distribution.This section describes two alternative approaches: (1) maximizing the likelihood estimates from the previous section and (2) a new approach based on approximative EM.The decision between these two approaches is separate from the decision of using vari- ational inference versus Expectation-Propagation.", "rank": 42, "paragraph_comparative_number": 4, "entities": [], "id": "p_42"}, "sentences": [{"end": 11527, "text": "Given a set of documents C = <di,i = 1,...,n>, with word counts denoted niw, the learning problem is to max- imize the likelihood as a function of the parameters  = (p( | a), ); the likelihood is given by Notice that each document has its own integral over .", "rank": 109, "start": 11269, "IsComparative": "1", "id": "st_109"}, {"end": 11626, "text": "It is tempting to use EM for this problem, where we regard  as a hidden variable for each document.", "rank": 110, "start": 11527, "IsComparative": "1", "id": "st_110"}, {"end": 11742, "text": "However, the E-step requires expectations over the posterior for , p( | d , ), which is an intractable distribution.", "rank": 111, "start": 11626, "IsComparative": "1", "id": "st_111"}, {"end": 11908, "text": "This section describes two alternative approaches: (1) maximizing the likelihood estimates from the previous section and (2) a new approach based on approximative EM.", "rank": 112, "start": 11742, "IsComparative": "0", "id": "st_112"}, {"end": 12044, "text": "The decision between these two approaches is separate from the decision of using vari- ational inference versus Expectation-Propagation.", "rank": 113, "start": 11908, "IsComparative": "1", "id": "st_113"}]}, {"paragraph_info": {"end": 12071, "start": 12044, "text": "5.1 Maximizing the estimate", "rank": 43, "paragraph_comparative_number": 0, "entities": [], "id": "p_43"}, "sentences": [{"end": 12071, "text": "5.1 Maximizing the estimate", "rank": 114, "start": 12044, "IsComparative": "0", "id": "st_114"}]}, {"paragraph_info": {"end": 12339, "start": 12071, "text": "Given that we can estimate the likelihood function for each document, it seems natural to try to maximize the value of the estimate.This is the approach taken by Blei et al.(2001).For the variational bound (8), the maximum with respect to the parameters is obtained at", "rank": 44, "paragraph_comparative_number": 1, "entities": [], "id": "p_44"}, "sentences": [{"end": 12203, "text": "Given that we can estimate the likelihood function for each document, it seems natural to try to maximize the value of the estimate.", "rank": 115, "start": 12071, "IsComparative": "0", "id": "st_115"}, {"end": 12244, "text": "This is the approach taken by Blei et al.", "rank": 116, "start": 12203, "IsComparative": "0", "id": "st_116"}, {"end": 12251, "text": "(2001).", "rank": 117, "start": 12244, "IsComparative": "1", "id": "st_117"}, {"end": 12339, "text": "For the variational bound (8), the maximum with respect to the parameters is obtained at", "rank": 118, "start": 12251, "IsComparative": "0", "id": "st_118"}]}, {"paragraph_info": {"end": 13124, "start": 12339, "text": "Of course, once the aspect parameters are changed, the op- timal bound parameters q(a | w) also change, so Blei et al.(2001) alternate between optimizing the bound and apply- ing these updates.This can be understood as an EM algo- rithm where both  and the aspect assignments are hidden variables.The aspect parameters at convergence will result in the largest possible variational estimate of the likelihood.The same approach could be taken with EP, where we find the parameters that result in the largest possible EP esti- mate of the likelihood.However, this does not seem to be as simple as in the variational approach.It also seems mis- guided, because an approximation which is close to the true likelihood in an average sense need not have its maximum close to the true maximum.", "rank": 45, "paragraph_comparative_number": 2, "entities": [], "id": "p_45"}, "sentences": [{"end": 12457, "text": "Of course, once the aspect parameters are changed, the op- timal bound parameters q(a | w) also change, so Blei et al.", "rank": 119, "start": 12339, "IsComparative": "0", "id": "st_119"}, {"end": 12532, "text": "(2001) alternate between optimizing the bound and apply- ing these updates.", "rank": 120, "start": 12457, "IsComparative": "1", "id": "st_120"}, {"end": 12636, "text": "This can be understood as an EM algo- rithm where both  and the aspect assignments are hidden variables.", "rank": 121, "start": 12532, "IsComparative": "0", "id": "st_121"}, {"end": 12748, "text": "The aspect parameters at convergence will result in the largest possible variational estimate of the likelihood.", "rank": 122, "start": 12636, "IsComparative": "0", "id": "st_122"}, {"end": 12887, "text": "The same approach could be taken with EP, where we find the parameters that result in the largest possible EP esti- mate of the likelihood.", "rank": 123, "start": 12748, "IsComparative": "0", "id": "st_123"}, {"end": 12962, "text": "However, this does not seem to be as simple as in the variational approach.", "rank": 124, "start": 12887, "IsComparative": "0", "id": "st_124"}, {"end": 13124, "text": "It also seems mis- guided, because an approximation which is close to the true likelihood in an average sense need not have its maximum close to the true maximum.", "rank": 125, "start": 12962, "IsComparative": "1", "id": "st_125"}]}, {"paragraph_info": {"end": 13144, "start": 13124, "text": "5.2 Approximative EM", "rank": 46, "paragraph_comparative_number": 0, "entities": [], "id": "p_46"}, "sentences": [{"end": 13144, "text": "5.2 Approximative EM", "rank": 126, "start": 13124, "IsComparative": "0", "id": "st_126"}]}, {"paragraph_info": {"end": 13540, "start": 13144, "text": "The second approach is to use an approximative EM algo- rithm, sometimes called variational EM, where we use expectations over an approximate posterior for , call it qi().The inference algorithms in the previous section conveniently give such an approximate posterior.The E- step will compute qi() for each document, and the M- step will maximize the following lower bound to the log- likelihood:", "rank": 47, "paragraph_comparative_number": 1, "entities": [], "id": "p_47"}, "sentences": [{"end": 13315, "text": "The second approach is to use an approximative EM algo- rithm, sometimes called variational EM, where we use expectations over an approximate posterior for , call it qi().", "rank": 127, "start": 13144, "IsComparative": "0", "id": "st_127"}, {"end": 13412, "text": "The inference algorithms in the previous section conveniently give such an approximate posterior.", "rank": 128, "start": 13315, "IsComparative": "0", "id": "st_128"}, {"end": 13540, "text": "The E- step will compute qi() for each document, and the M- step will maximize the following lower bound to the log- likelihood:", "rank": 129, "start": 13412, "IsComparative": "1", "id": "st_129"}]}, {"paragraph_info": {"end": 13703, "start": 13540, "text": "This decouples into separate maximization problems for  and p(w | a).Given that qi () is Dirichlet with parameters ia, the optimization problem for  is to maximize", "rank": 48, "paragraph_comparative_number": 1, "entities": [], "id": "p_48"}, "sentences": [{"end": 13609, "text": "This decouples into separate maximization problems for  and p(w | a).", "rank": 130, "start": 13540, "IsComparative": "1", "id": "st_130"}, {"end": 13703, "text": "Given that qi () is Dirichlet with parameters ia, the optimization problem for  is to maximize", "rank": 131, "start": 13609, "IsComparative": "0", "id": "st_131"}]}, {"paragraph_info": {"end": 13779, "start": 13703, "text": "which is the standard Dirichlet maximum-likelihood prob- lem (Minka, 2001a).", "rank": 49, "paragraph_comparative_number": 0, "entities": [], "id": "p_49"}, "sentences": [{"end": 13779, "text": "which is the standard Dirichlet maximum-likelihood prob- lem (Minka, 2001a).", "rank": 132, "start": 13703, "IsComparative": "0", "id": "st_132"}]}, {"paragraph_info": {"end": 13851, "start": 13779, "text": "By zeroing the derivative with respect to p(w | a), we obtain the M-step", "rank": 50, "paragraph_comparative_number": 0, "entities": [], "id": "p_50"}, "sentences": [{"end": 13851, "text": "By zeroing the derivative with respect to p(w | a), we obtain the M-step", "rank": 133, "start": 13779, "IsComparative": "0", "id": "st_133"}]}, {"paragraph_info": {"end": 14119, "start": 13851, "text": "This requires approximating another integral over .Up- date (27) is equivalent to assuming that a is constant, at the value exp((a)) (from (11)).A more accurate ap- proximation can be obtained by Taylor expansion, as de- scribed in the appendix.The resulting update is", "rank": 51, "paragraph_comparative_number": 0, "entities": [], "id": "p_51"}, "sentences": [{"end": 13902, "text": "This requires approximating another integral over .", "rank": 134, "start": 13851, "IsComparative": "0", "id": "st_134"}, {"end": 13996, "text": "Up- date (27) is equivalent to assuming that a is constant, at the value exp((a)) (from (11)).", "rank": 135, "start": 13902, "IsComparative": "0", "id": "st_135"}, {"end": 14096, "text": "A more accurate ap- proximation can be obtained by Taylor expansion, as de- scribed in the appendix.", "rank": 136, "start": 13996, "IsComparative": "0", "id": "st_136"}, {"end": 14119, "text": "The resulting update is", "rank": 137, "start": 14096, "IsComparative": "0", "id": "st_137"}]}, {"paragraph_info": {"end": 14141, "start": 14119, "text": "6 Experimental Results", "rank": 52, "paragraph_comparative_number": 0, "entities": [], "id": "p_52"}, "sentences": [{"end": 14141, "text": "6 Experimental Results", "rank": 138, "start": 14119, "IsComparative": "0", "id": "st_138"}]}, {"paragraph_info": {"end": 14393, "start": 14141, "text": "This section presents the result of experiments carried out on synthetic and real data.The first experiments involve a toy data set where the aspects are multinomials over a two word vocabulary.Later experiments use documents from two TREC collections.", "rank": 53, "paragraph_comparative_number": 0, "entities": [], "id": "p_53"}, "sentences": [{"end": 14228, "text": "This section presents the result of experiments carried out on synthetic and real data.", "rank": 139, "start": 14141, "IsComparative": "0", "id": "st_139"}, {"end": 14335, "text": "The first experiments involve a toy data set where the aspects are multinomials over a two word vocabulary.", "rank": 140, "start": 14228, "IsComparative": "0", "id": "st_140"}, {"end": 14393, "text": "Later experiments use documents from two TREC collections.", "rank": 141, "start": 14335, "IsComparative": "0", "id": "st_141"}]}, {"paragraph_info": {"end": 14422, "start": 14393, "text": "6.1 Results on Synthetic Data", "rank": 54, "paragraph_comparative_number": 1, "entities": [], "id": "p_54"}, "sentences": [{"end": 14422, "text": "6.1 Results on Synthetic Data", "rank": 142, "start": 14393, "IsComparative": "1", "id": "st_142"}]}, {"paragraph_info": {"end": 15235, "start": 14422, "text": "This section elucidates the difference between variational inference (VB) and Expectation Propagation (EP) using simple, controlled datasets.The algorithms mainly differ in how they approximate (3), thus it is helpful to consider two extremes: (Exact) the exact value of (3) versus (Max) approximating by the maximum over : only serve to restrict the domain of the word probabilities p(w) = a ap(w | a).To maximize likelihood, we would want the domain to be as large as possiblethe as- pects as extreme and distinct as possible.However, when using the exact value of (3), all choices of  contribute, which favors a domain that only includes word probabili- ties matching the frequencies in the documents.In exper- iments, we find that VB behaves like the Max approxima- tion while EP behaves like the exact value.", "rank": 55, "paragraph_comparative_number": 3, "entities": [], "id": "p_55"}, "sentences": [{"end": 14563, "text": "This section elucidates the difference between variational inference (VB) and Expectation Propagation (EP) using simple, controlled datasets.", "rank": 143, "start": 14422, "IsComparative": "1", "id": "st_143"}, {"end": 14825, "text": "The algorithms mainly differ in how they approximate (3), thus it is helpful to consider two extremes: (Exact) the exact value of (3) versus (Max) approximating by the maximum over : only serve to restrict the domain of the word probabilities p(w) = a ap(w | a).", "rank": 144, "start": 14563, "IsComparative": "1", "id": "st_144"}, {"end": 14950, "text": "To maximize likelihood, we would want the domain to be as large as possiblethe as- pects as extreme and distinct as possible.", "rank": 145, "start": 14825, "IsComparative": "0", "id": "st_145"}, {"end": 15126, "text": "However, when using the exact value of (3), all choices of  contribute, which favors a domain that only includes word probabili- ties matching the frequencies in the documents.", "rank": 146, "start": 14950, "IsComparative": "1", "id": "st_146"}, {"end": 15235, "text": "In exper- iments, we find that VB behaves like the Max approxima- tion while EP behaves like the exact value.", "rank": 147, "start": 15126, "IsComparative": "0", "id": "st_147"}]}, {"paragraph_info": {"end": 15437, "start": 15235, "text": "This update can be used with the s found by either VB or EP.When running EP with the new parameter values, the s can be started from their previous values, so that only a few EP iterations are required.", "rank": 56, "paragraph_comparative_number": 1, "entities": [], "id": "p_56"}, "sentences": [{"end": 15295, "text": "This update can be used with the s found by either VB or EP.", "rank": 148, "start": 15235, "IsComparative": "0", "id": "st_148"}, {"end": 15437, "text": "When running EP with the new parameter values, the s can be started from their previous values, so that only a few EP iterations are required.", "rank": 149, "start": 15295, "IsComparative": "1", "id": "st_149"}]}, {"paragraph_info": {"end": 15919, "start": 15437, "text": "Consider a simple scenario in which there are only two words in the vocabulary, w=1 and w=2.This allows each aspect to be represented by one parameter p(w=1 | a), since p(w=2 | a) = 1  p(w=1 | a).Let there be two aspects, a=1 and a=2, with 1=2=1, so that D(|) is uni- form.This means that the probability of word 1 in the doc- ument collection varies uniformly between p(w=1|a=1) and p(w=1 | a=2).Learning the aspects from data amounts to estimating the endpoints of this variation.", "rank": 57, "paragraph_comparative_number": 3, "entities": [], "id": "p_57"}, "sentences": [{"end": 15529, "text": "Consider a simple scenario in which there are only two words in the vocabulary, w=1 and w=2.", "rank": 150, "start": 15437, "IsComparative": "1", "id": "st_150"}, {"end": 15633, "text": "This allows each aspect to be represented by one parameter p(w=1 | a), since p(w=2 | a) = 1  p(w=1 | a).", "rank": 151, "start": 15529, "IsComparative": "0", "id": "st_151"}, {"end": 15710, "text": "Let there be two aspects, a=1 and a=2, with 1=2=1, so that D(|) is uni- form.", "rank": 152, "start": 15633, "IsComparative": "1", "id": "st_152"}, {"end": 15834, "text": "This means that the probability of word 1 in the doc- ument collection varies uniformly between p(w=1|a=1) and p(w=1 | a=2).", "rank": 153, "start": 15710, "IsComparative": "1", "id": "st_153"}, {"end": 15919, "text": "Learning the aspects from data amounts to estimating the endpoints of this variation.", "rank": 154, "start": 15834, "IsComparative": "0", "id": "st_154"}]}, {"paragraph_info": {"end": 16711, "start": 15919, "text": "Let p(w=1 | a=2) = 1 so that the only free parameter is p(w=1 | a=1).Ten training documents of length 10 are generated from the model with p(w=1 | a=1) = 0.5.Figure 1 (top) shows the typical result.When we ap- ply the Max approximation, each document i wants to choose p(w=1) (between p(w=1 | a=1) and p(w=1 | a=2)) to match its frequency of word 1: ni1/ni.Any choice of (p(w=1|a=1),p(w=1|a=2)) which spans these frequen- cies will maximize likelihood, e.g.p(w=1 | a=1) = 0, p(w=1 | a=2) = 1.The exact likelihood, by contrast, peaks near the true value of p(w=1 | a=1).As the number of train- ing documents increases, the exact likelihood gets sharper around the true value, but the Max approximation gets far- ther away from the truth, because the observed frequencies exhibit more variance.", "rank": 58, "paragraph_comparative_number": 3, "entities": [], "id": "p_58"}, "sentences": [{"end": 15988, "text": "Let p(w=1 | a=2) = 1 so that the only free parameter is p(w=1 | a=1).", "rank": 155, "start": 15919, "IsComparative": "0", "id": "st_155"}, {"end": 16077, "text": "Ten training documents of length 10 are generated from the model with p(w=1 | a=1) = 0.5.", "rank": 156, "start": 15988, "IsComparative": "0", "id": "st_156"}, {"end": 16117, "text": "Figure 1 (top) shows the typical result.", "rank": 157, "start": 16077, "IsComparative": "0", "id": "st_157"}, {"end": 16276, "text": "When we ap- ply the Max approximation, each document i wants to choose p(w=1) (between p(w=1 | a=1) and p(w=1 | a=2)) to match its frequency of word 1: ni1/ni.", "rank": 158, "start": 16117, "IsComparative": "1", "id": "st_158"}, {"end": 16376, "text": "Any choice of (p(w=1|a=1),p(w=1|a=2)) which spans these frequen- cies will maximize likelihood, e.g.", "rank": 159, "start": 16276, "IsComparative": "1", "id": "st_159"}, {"end": 16411, "text": "p(w=1 | a=1) = 0, p(w=1 | a=2) = 1.", "rank": 160, "start": 16376, "IsComparative": "0", "id": "st_160"}, {"end": 16488, "text": "The exact likelihood, by contrast, peaks near the true value of p(w=1 | a=1).", "rank": 161, "start": 16411, "IsComparative": "0", "id": "st_161"}, {"end": 16711, "text": "As the number of train- ing documents increases, the exact likelihood gets sharper around the true value, but the Max approximation gets far- ther away from the truth, because the observed frequencies exhibit more variance.", "rank": 162, "start": 16488, "IsComparative": "1", "id": "st_162"}]}, {"paragraph_info": {"end": 17136, "start": 16711, "text": "As shown in Figure 1 (bottom), VB behaves similarly to Max.The solid curve is the exact likelihood, generated by computing the probability of the training documents for all ps on a fine grid.The dashed curve is the VB estimate of the likelihood, scaled up to make its shape visible on the plot, for the same ps.The dot-dashed curve is the EP estimate of the likelihood for the same ps.EP clearly gives a better approximation.", "rank": 59, "paragraph_comparative_number": 1, "entities": [], "id": "p_59"}, "sentences": [{"end": 16770, "text": "As shown in Figure 1 (bottom), VB behaves similarly to Max.", "rank": 163, "start": 16711, "IsComparative": "0", "id": "st_163"}, {"end": 16902, "text": "The solid curve is the exact likelihood, generated by computing the probability of the training documents for all ps on a fine grid.", "rank": 164, "start": 16770, "IsComparative": "0", "id": "st_164"}, {"end": 17022, "text": "The dashed curve is the VB estimate of the likelihood, scaled up to make its shape visible on the plot, for the same ps.", "rank": 165, "start": 16902, "IsComparative": "1", "id": "st_165"}, {"end": 17096, "text": "The dot-dashed curve is the EP estimate of the likelihood for the same ps.", "rank": 166, "start": 17022, "IsComparative": "0", "id": "st_166"}, {"end": 17136, "text": "EP clearly gives a better approximation.", "rank": 167, "start": 17096, "IsComparative": "0", "id": "st_167"}]}, {"paragraph_info": {"end": 17540, "start": 17136, "text": "The parameter estimates are indicated by vertical lines.The dashed vertical line corresponds to Blei et al.s algorithm, which as expected converges to the maximum of the VB curve.The solid line is the result of VB combined with the EM update of Section 5.2; as expected, it is closer to the true maximum.The dot-dash vertical line is the result of applying EM using EP and is closest to the true maximum.", "rank": 60, "paragraph_comparative_number": 3, "entities": [], "id": "p_60"}, "sentences": [{"end": 17192, "text": "The parameter estimates are indicated by vertical lines.", "rank": 168, "start": 17136, "IsComparative": "0", "id": "st_168"}, {"end": 17315, "text": "The dashed vertical line corresponds to Blei et al.s algorithm, which as expected converges to the maximum of the VB curve.", "rank": 169, "start": 17192, "IsComparative": "1", "id": "st_169"}, {"end": 17440, "text": "The solid line is the result of VB combined with the EM update of Section 5.2; as expected, it is closer to the true maximum.", "rank": 170, "start": 17315, "IsComparative": "1", "id": "st_170"}, {"end": 17540, "text": "The dot-dash vertical line is the result of applying EM using EP and is closest to the true maximum.", "rank": 171, "start": 17440, "IsComparative": "1", "id": "st_171"}]}, {"paragraph_info": {"end": 18073, "start": 17540, "text": "To demonstrate the difference between the algorithms in the multidimensional case, 100 documents of length 100 were generated from a simple multinomial model with five words having equal probability.A generative aspect model was fit using three aspects with 1=2=3=1.The EP so- lution correctly chose all aspects to be similar to the gener- ating multinomial; all probabilities were between 0.15 and 0.24.The VB solution is quite different; it chose the ex- treme parameters shown in the following table (rounded to the tenths place):", "rank": 61, "paragraph_comparative_number": 3, "entities": [], "id": "p_61"}, "sentences": [{"end": 17739, "text": "To demonstrate the difference between the algorithms in the multidimensional case, 100 documents of length 100 were generated from a simple multinomial model with five words having equal probability.", "rank": 172, "start": 17540, "IsComparative": "1", "id": "st_172"}, {"end": 17806, "text": "A generative aspect model was fit using three aspects with 1=2=3=1.", "rank": 173, "start": 17739, "IsComparative": "1", "id": "st_173"}, {"end": 17944, "text": "The EP so- lution correctly chose all aspects to be similar to the gener- ating multinomial; all probabilities were between 0.15 and 0.24.", "rank": 174, "start": 17806, "IsComparative": "0", "id": "st_174"}, {"end": 18073, "text": "The VB solution is quite different; it chose the ex- treme parameters shown in the following table (rounded to the tenths place):", "rank": 175, "start": 17944, "IsComparative": "1", "id": "st_175"}]}, {"paragraph_info": {"end": 18274, "start": 18073, "text": "Resampling the training documents gives similar results.In terms of convergence rate, learning typically converged after 150 parameter updates with EP, while over 1,000 up- dates were required with VB.", "rank": 62, "paragraph_comparative_number": 1, "entities": [], "id": "p_62"}, "sentences": [{"end": 18129, "text": "Resampling the training documents gives similar results.", "rank": 176, "start": 18073, "IsComparative": "1", "id": "st_176"}, {"end": 18274, "text": "In terms of convergence rate, learning typically converged after 150 parameter updates with EP, while over 1,000 up- dates were required with VB.", "rank": 177, "start": 18129, "IsComparative": "0", "id": "st_177"}]}, {"paragraph_info": {"end": 18801, "start": 18274, "text": "Interestingly, on an independent set of 1000 test docu- ments, the perplexity of the model learned by EP is 5.0 while the VB models is 5.1, a seemingly trivial differ- ence.This is because the perplexity measure, as used by Blei et al., focuses on per-word prediction rather than per- document prediction.As long as there exists a mixture of the aspects which matches the word probabilities in the document, the perplexity will be low.Indeed, if the above VB aspects are evenly mixed, the correct word distribution is produced.", "rank": 63, "paragraph_comparative_number": 0, "entities": [], "id": "p_63"}, "sentences": [{"end": 18447, "text": "Interestingly, on an independent set of 1000 test docu- ments, the perplexity of the model learned by EP is 5.0 while the VB models is 5.1, a seemingly trivial differ- ence.", "rank": 178, "start": 18274, "IsComparative": "0", "id": "st_178"}, {"end": 18579, "text": "This is because the perplexity measure, as used by Blei et al., focuses on per-word prediction rather than per- document prediction.", "rank": 179, "start": 18447, "IsComparative": "0", "id": "st_179"}, {"end": 18709, "text": "As long as there exists a mixture of the aspects which matches the word probabilities in the document, the perplexity will be low.", "rank": 180, "start": 18579, "IsComparative": "0", "id": "st_180"}, {"end": 18801, "text": "Indeed, if the above VB aspects are evenly mixed, the correct word distribution is produced.", "rank": 181, "start": 18709, "IsComparative": "0", "id": "st_181"}]}, {"paragraph_info": {"end": 19510, "start": 18801, "text": "To show that there really is a difference between the mod- els, a synthetic classification problem was constructed.One class had documents sampled from a uniform multinomial over five words.The other class had documents sampled from a multinomial with word probabilities <1 2 3 4 5>/15.There were 50 documents of length 50 in each class.A three-aspect model was trained on each class and test doc- uments (of the same length) were classified according to highest class-conditional probability.The EP models com- mitted 76/2000 errors while the VB models committed 163/2000, which is both statistically and practically signif- icant.As above, EP learned the correct models while VB chose extreme probabilities.", "rank": 64, "paragraph_comparative_number": 4, "entities": [], "id": "p_64"}, "sentences": [{"end": 18916, "text": "To show that there really is a difference between the mod- els, a synthetic classification problem was constructed.", "rank": 182, "start": 18801, "IsComparative": "1", "id": "st_182"}, {"end": 18991, "text": "One class had documents sampled from a uniform multinomial over five words.", "rank": 183, "start": 18916, "IsComparative": "1", "id": "st_183"}, {"end": 19087, "text": "The other class had documents sampled from a multinomial with word probabilities <1 2 3 4 5>/15.", "rank": 184, "start": 18991, "IsComparative": "0", "id": "st_184"}, {"end": 19138, "text": "There were 50 documents of length 50 in each class.", "rank": 185, "start": 19087, "IsComparative": "1", "id": "st_185"}, {"end": 19294, "text": "A three-aspect model was trained on each class and test doc- uments (of the same length) were classified according to highest class-conditional probability.", "rank": 186, "start": 19138, "IsComparative": "0", "id": "st_186"}, {"end": 19433, "text": "The EP models com- mitted 76/2000 errors while the VB models committed 163/2000, which is both statistically and practically signif- icant.", "rank": 187, "start": 19294, "IsComparative": "1", "id": "st_187"}, {"end": 19510, "text": "As above, EP learned the correct models while VB chose extreme probabilities.", "rank": 188, "start": 19433, "IsComparative": "0", "id": "st_188"}]}, {"paragraph_info": {"end": 19534, "start": 19510, "text": "6.2 Controlled TREC Data", "rank": 65, "paragraph_comparative_number": 0, "entities": [], "id": "p_65"}, "sentences": [{"end": 19534, "text": "6.2 Controlled TREC Data", "rank": 189, "start": 19510, "IsComparative": "0", "id": "st_189"}]}, {"paragraph_info": {"end": 19846, "start": 19534, "text": "In order to compare variational inference and Expectation- Propagation on more realistic data, a corpus was created by mixing together TREC documents on known topics.From the 1989 AP data on TREC disks 1 and 2, we extracted all of the documents that were judged to be relevant to one of the following six topics:", "rank": 66, "paragraph_comparative_number": 1, "entities": [], "id": "p_66"}, "sentences": [{"end": 19700, "text": "In order to compare variational inference and Expectation- Propagation on more realistic data, a corpus was created by mixing together TREC documents on known topics.", "rank": 190, "start": 19534, "IsComparative": "0", "id": "st_190"}, {"end": 19846, "text": "From the 1989 AP data on TREC disks 1 and 2, we extracted all of the documents that were judged to be relevant to one of the following six topics:", "rank": 191, "start": 19700, "IsComparative": "1", "id": "st_191"}]}, {"paragraph_info": {"end": 20378, "start": 19846, "text": "Synthetic documents were created by first drawing three topics randomly, with replacement, from the above list of six.A random document from each topic was then se- lected, and the three documents were concatenated together to form a synthetic document containing either one, two or three different aspects.A total of 200 documents were generated in this manner.used to train aspect models using both EP and VB, fixing the number of aspects at six; 75% of the data was used for training, and the remaining 25% was used as test data.", "rank": 67, "paragraph_comparative_number": 4, "entities": [], "id": "p_67"}, "sentences": [{"end": 19964, "text": "Synthetic documents were created by first drawing three topics randomly, with replacement, from the above list of six.", "rank": 192, "start": 19846, "IsComparative": "1", "id": "st_192"}, {"end": 20153, "text": "A random document from each topic was then se- lected, and the three documents were concatenated together to form a synthetic document containing either one, two or three different aspects.", "rank": 193, "start": 19964, "IsComparative": "1", "id": "st_193"}, {"end": 20208, "text": "A total of 200 documents were generated in this manner.", "rank": 194, "start": 20153, "IsComparative": "1", "id": "st_194"}, {"end": 20378, "text": "used to train aspect models using both EP and VB, fixing the number of aspects at six; 75% of the data was used for training, and the remaining 25% was used as test data.", "rank": 195, "start": 20208, "IsComparative": "1", "id": "st_195"}]}, {"paragraph_info": {"end": 21166, "start": 20378, "text": "Figure 2 shows the top words for each aspect for the EP- trained model.Because likelihood is used as the objective function, the common, content-free words take up a sig- nificant portion of the probability massa fact that is of- ten not acknowledged in descriptions of aspect models.As seen in this figure, the aspects model variations across doc- uments in the distribution of common words such as SAID, FOR, and WAS.After filtering out the common words from the list, by not displaying words having a unigram proba- bility larger than a threshold of 0.001, the most probable words that remain clearly indicate that the true underlying aspects have been captured, though some more cleanly than others.For example, aspect 1 corresponds to topic 142, and aspect 5 corresponds to topic 59.", "rank": 68, "paragraph_comparative_number": 1, "entities": [], "id": "p_68"}, "sentences": [{"end": 20449, "text": "Figure 2 shows the top words for each aspect for the EP- trained model.", "rank": 196, "start": 20378, "IsComparative": "0", "id": "st_196"}, {"end": 20662, "text": "Because likelihood is used as the objective function, the common, content-free words take up a sig- nificant portion of the probability massa fact that is of- ten not acknowledged in descriptions of aspect models.", "rank": 197, "start": 20449, "IsComparative": "0", "id": "st_197"}, {"end": 20797, "text": "As seen in this figure, the aspects model variations across doc- uments in the distribution of common words such as SAID, FOR, and WAS.", "rank": 198, "start": 20662, "IsComparative": "0", "id": "st_198"}, {"end": 21081, "text": "After filtering out the common words from the list, by not displaying words having a unigram proba- bility larger than a threshold of 0.001, the most probable words that remain clearly indicate that the true underlying aspects have been captured, though some more cleanly than others.", "rank": 199, "start": 20797, "IsComparative": "0", "id": "st_199"}, {"end": 21166, "text": "For example, aspect 1 corresponds to topic 142, and aspect 5 corresponds to topic 59.", "rank": 200, "start": 21081, "IsComparative": "1", "id": "st_200"}]}, {"paragraph_info": {"end": 21551, "start": 21166, "text": "The models are compared quantitatively using test set per- plexity, exp(( i log p(di))/  i |di|); lower perplexity is better.The probability function (3) cannot be computed analytically, and we do not want to favor either of the two approximations, so we use importance sampling to com- pute perplexity.In particular, we sample  from the approximate posterior D ( | ) obtained from EP.", "rank": 69, "paragraph_comparative_number": 1, "entities": [], "id": "p_69"}, "sentences": [{"end": 21291, "text": "The models are compared quantitatively using test set per- plexity, exp(( i log p(di))/  i |di|); lower perplexity is better.", "rank": 201, "start": 21166, "IsComparative": "0", "id": "st_201"}, {"end": 21469, "text": "The probability function (3) cannot be computed analytically, and we do not want to favor either of the two approximations, so we use importance sampling to com- pute perplexity.", "rank": 202, "start": 21291, "IsComparative": "1", "id": "st_202"}, {"end": 21551, "text": "In particular, we sample  from the approximate posterior D ( | ) obtained from EP.", "rank": 203, "start": 21469, "IsComparative": "0", "id": "st_203"}]}, {"paragraph_info": {"end": 22222, "start": 21551, "text": "Figure 3 shows the test set perplexities for VB and EP; the perplexity for the EP-trained model is consistently lower than the perplexity of the VB-trained model.Based on the results of Section 6.1, we anticipate that for VB the as- pects will be more extreme and specialized.This would make the Dirichlet weights a smaller for the specialized aspects, which are used infrequently, and larger for the as- pects that are used in different topics or that are devoted to the common words.Plots of the Dirichlet parameters (Fig- ure 3, center and right) show that VB results in as that are indeed more spread out towards these extremes, compared with those obtained using EP.", "rank": 70, "paragraph_comparative_number": 0, "entities": [], "id": "p_70"}, "sentences": [{"end": 21713, "text": "Figure 3 shows the test set perplexities for VB and EP; the perplexity for the EP-trained model is consistently lower than the perplexity of the VB-trained model.", "rank": 204, "start": 21551, "IsComparative": "0", "id": "st_204"}, {"end": 21827, "text": "Based on the results of Section 6.1, we anticipate that for VB the as- pects will be more extreme and specialized.", "rank": 205, "start": 21713, "IsComparative": "0", "id": "st_205"}, {"end": 22036, "text": "This would make the Dirichlet weights a smaller for the specialized aspects, which are used infrequently, and larger for the as- pects that are used in different topics or that are devoted to the common words.", "rank": 206, "start": 21827, "IsComparative": "0", "id": "st_206"}, {"end": 22222, "text": "Plots of the Dirichlet parameters (Fig- ure 3, center and right) show that VB results in as that are indeed more spread out towards these extremes, compared with those obtained using EP.", "rank": 207, "start": 22036, "IsComparative": "0", "id": "st_207"}]}, {"paragraph_info": {"end": 22247, "start": 22222, "text": "6.3 TREC Interactive Data", "rank": 71, "paragraph_comparative_number": 0, "entities": [], "id": "p_71"}, "sentences": [{"end": 22247, "text": "6.3 TREC Interactive Data", "rank": 208, "start": 22222, "IsComparative": "0", "id": "st_208"}]}, {"paragraph_info": {"end": 22630, "start": 22247, "text": "To compare VB and EP on real data having a mixture of aspects, this section considers documents from the TREC interactive collection (Over, 2001).The data used for this track is interesting for studying aspect models because the relevant documents have been hand labeled according to the specific aspects of a topic that they cover.Here we simply evaluate perplexities of the models.", "rank": 72, "paragraph_comparative_number": 1, "entities": [], "id": "p_72"}, "sentences": [{"end": 22393, "text": "To compare VB and EP on real data having a mixture of aspects, this section considers documents from the TREC interactive collection (Over, 2001).", "rank": 209, "start": 22247, "IsComparative": "1", "id": "st_209"}, {"end": 22579, "text": "The data used for this track is interesting for studying aspect models because the relevant documents have been hand labeled according to the specific aspects of a topic that they cover.", "rank": 210, "start": 22393, "IsComparative": "0", "id": "st_210"}, {"end": 22630, "text": "Here we simply evaluate perplexities of the models.", "rank": 211, "start": 22579, "IsComparative": "0", "id": "st_211"}]}, {"paragraph_info": {"end": 23044, "start": 22630, "text": "We extracted all of the relevant documents for each of the six topics that the collection has relevance judgements for, resulting in a set of 772 documents.The average docu- ment length is 594 tokens, and the total vocabulary size is 26,319 words.As above, 75% of the data was used for training, and the remaining 25% was used for evaluating perplexities.In these experiments the speed of VB and EP are comparable.", "rank": 73, "paragraph_comparative_number": 3, "entities": [], "id": "p_73"}, "sentences": [{"end": 22786, "text": "We extracted all of the relevant documents for each of the six topics that the collection has relevance judgements for, resulting in a set of 772 documents.", "rank": 212, "start": 22630, "IsComparative": "1", "id": "st_212"}, {"end": 22877, "text": "The average docu- ment length is 594 tokens, and the total vocabulary size is 26,319 words.", "rank": 213, "start": 22786, "IsComparative": "1", "id": "st_213"}, {"end": 22985, "text": "As above, 75% of the data was used for training, and the remaining 25% was used for evaluating perplexities.", "rank": 214, "start": 22877, "IsComparative": "0", "id": "st_214"}, {"end": 23044, "text": "In these experiments the speed of VB and EP are comparable.", "rank": 215, "start": 22985, "IsComparative": "1", "id": "st_215"}]}, {"paragraph_info": {"end": 23519, "start": 23044, "text": "Figure 3 shows the test set perplexity and Dirichlet param- eters a for both EP and VB, trained using A = 10 aspects.As for the controlled TREC data, EP achieves a lower per- plexity, and has aspects that are more balanced compared to those obtained using VB.We suspect that the perplex- ity difference on both the TREC interactive and controlled TREC data is small because the true aspects have little overlap, and thus the posterior of the mixing weights is sharply peaked.", "rank": 74, "paragraph_comparative_number": 3, "entities": [], "id": "p_74"}, "sentences": [{"end": 23161, "text": "Figure 3 shows the test set perplexity and Dirichlet param- eters a for both EP and VB, trained using A = 10 aspects.", "rank": 216, "start": 23044, "IsComparative": "1", "id": "st_216"}, {"end": 23303, "text": "As for the controlled TREC data, EP achieves a lower per- plexity, and has aspects that are more balanced compared to those obtained using VB.", "rank": 217, "start": 23161, "IsComparative": "1", "id": "st_217"}, {"end": 23519, "text": "We suspect that the perplex- ity difference on both the TREC interactive and controlled TREC data is small because the true aspects have little overlap, and thus the posterior of the mixing weights is sharply peaked.", "rank": 218, "start": 23303, "IsComparative": "1", "id": "st_218"}]}, {"paragraph_info": {"end": 23532, "start": 23519, "text": "7 Conclusions", "rank": 75, "paragraph_comparative_number": 0, "entities": [], "id": "p_75"}, "sentences": [{"end": 23532, "text": "7 Conclusions", "rank": 219, "start": 23519, "IsComparative": "0", "id": "st_219"}]}, {"paragraph_info": {"end": 24648, "start": 23532, "text": "The generative aspect model provides an attractive ap- proach to modeling the variation of word probabilities across documents, making the model well suited to in- formation retrieval and other text processing applications.This paper studied the problem of approximation meth- ods for learning and inference in the generative aspect model, and proposed an algorithm based on Expectation- Propagation as an alternative to the variational method adopted by Blei et al.(2001).Experiments on synthetic data showed that simple variational inference can lead to in- accurate inferences and biased learning, while Expectation- Propagation can lead to more accurate inferences.Exper- iments on TREC data show that Expectation-Propagation achieves lower test set perplexity.We attribute this to the fact that the Jensen bound used by the variational method is inadequate for representing how peaky versus spread out is the posterior on , which happens to be crucial for good parameter estimates.Because there is a separate  for each document, this deficiency is not minimized by ad- ditional documents, but rather compounded.", "rank": 76, "paragraph_comparative_number": 3, "entities": [], "id": "p_76"}, "sentences": [{"end": 23755, "text": "The generative aspect model provides an attractive ap- proach to modeling the variation of word probabilities across documents, making the model well suited to in- formation retrieval and other text processing applications.", "rank": 220, "start": 23532, "IsComparative": "0", "id": "st_220"}, {"end": 23998, "text": "This paper studied the problem of approximation meth- ods for learning and inference in the generative aspect model, and proposed an algorithm based on Expectation- Propagation as an alternative to the variational method adopted by Blei et al.", "rank": 221, "start": 23755, "IsComparative": "0", "id": "st_221"}, {"end": 24005, "text": "(2001).", "rank": 222, "start": 23998, "IsComparative": "1", "id": "st_222"}, {"end": 24201, "text": "Experiments on synthetic data showed that simple variational inference can lead to in- accurate inferences and biased learning, while Expectation- Propagation can lead to more accurate inferences.", "rank": 223, "start": 24005, "IsComparative": "1", "id": "st_223"}, {"end": 24297, "text": "Exper- iments on TREC data show that Expectation-Propagation achieves lower test set perplexity.", "rank": 224, "start": 24201, "IsComparative": "0", "id": "st_224"}, {"end": 24518, "text": "We attribute this to the fact that the Jensen bound used by the variational method is inadequate for representing how peaky versus spread out is the posterior on , which happens to be crucial for good parameter estimates.", "rank": 225, "start": 24297, "IsComparative": "1", "id": "st_225"}, {"end": 24648, "text": "Because there is a separate  for each document, this deficiency is not minimized by ad- ditional documents, but rather compounded.", "rank": 226, "start": 24518, "IsComparative": "0", "id": "st_226"}]}, {"paragraph_info": {"end": 24822, "start": 24648, "text": "This synthetic collection thus simulates a set of retrieved documents to answer a query, which we then wish to ana- lyze in order to extract the aspect structure.The data was", "rank": 77, "paragraph_comparative_number": 1, "entities": [], "id": "p_77"}, "sentences": [{"end": 24810, "text": "This synthetic collection thus simulates a set of retrieved documents to answer a query, which we then wish to ana- lyze in order to extract the aspect structure.", "rank": 227, "start": 24648, "IsComparative": "1", "id": "st_227"}, {"end": 24822, "text": "The data was", "rank": 228, "start": 24810, "IsComparative": "0", "id": "st_228"}]}], "sentence_scenes_info": [{"x": 6, "text": "An extension of Expectation- Propagation is used for inference and then em- bedded in an EM algorithm for learning."}, {"x": 16, "text": "The TREC interac- tive track (Over, 2001) has been set up to help investigate precisely this task, but from the point of view of users in- teracting with the search engine."}, {"x": 18, "text": "This paper examines computation in the generative as- pect model, proposing new algorithms for approximate in- ference and learning that are based on the Expectation- Propagation framework of Minka (2001b)."}, {"x": 22, "text": "It is found that the variational methods can lead to inac- curate inferences and biased learning, while Expectation- Propagation gives results that are more true to the model."}, {"x": 25, "text": "After a brief overview of Expectation-Propagation in Section 3, a new algorithm for approximate inference in the genera- tive aspect model is presented in Section 4."}, {"x": 26, "text": "Separate from Expectation-Propagation, a new algorithm for approximate learning in the generative aspect model is presented in Section 5."}, {"x": 29, "text": "Section 6.1 presents a synthetic data experiment us-ing low dimensional multinomials, which clearly demon- strates how variational methods can result in inaccurate inferences compared to Expectation-Propagation."}, {"x": 30, "text": "In Sec- tions 6.2 and 6.3 the methods are then compared using doc- ument collections taken from TREC data, where it is seen that Expectation-Propagation attains lower test set perplex- ity."}, {"x": 32, "text": "2 The Generative Aspect Model"}, {"x": 40, "text": "One natural choice is the Dirichlet distribution, which is conju- gate to the multinomial."}, {"x": 41, "text": "Unfortunately, while the Dirichlet can capture variation in the p(w)s, it cannot capture co- variation, the tendency for some probabilities to move up and down together."}, {"x": 46, "text": "First,  is sampled from a Dirichlet distribution D ( | ), so that  a a = 1."}, {"x": 50, "text": "The probability of a document is where the parameters  are the Dirichlet parameters a and the multinomial models p( | a);  denotes the (A  1)- dimensional simplex, the sample space of the Dirichlet D ( | )."}, {"x": 56, "text": "3 Expectation-Propagation"}, {"x": 60, "text": "Here we present a slight generalization to allow real-valued powers on the terms."}, {"x": 74, "text": "(2001) and Expectation-Propagation."}, {"x": 80, "text": "Laplaces method using a softmax transformation, vari- ational inference, and two different Monte Carlo algo- rithms."}, {"x": 82, "text": "For the generative aspect model, the approximate posterior will be Dirichlet, and the integrand will be factored into terms of the form which resembles a Dirichlet with parameters wa."}, {"x": 85, "text": "A convenient way to do this is with EM."}, {"x": 92, "text": "4.2 Expectation-Propagation"}, {"x": 94, "text": "This problem was studied in depth by Minka (2001b) and it was found that Expectation- Propagation (EP) provides the best results, compared to"}, {"x": 95, "text": "loop w = 1,...,W:"}, {"x": 103, "text": "In our experience, words are skipped only on the first few iterations, before EP has settled into a decent approxima- tion."}, {"x": 106, "text": "After convergence, the approximate posterior gives the fol- lowing estimate for the likelihood of the document, thus approximating the integral (3): estimates from the previous section and (2) a new approach based on approximative EM."}, {"x": 107, "text": "The decision between these two approaches is separate from the decision of using vari- ational inference versus Expectation-Propagation."}, {"x": 110, "text": "It is tempting to use EM for this problem, where we regard  as a hidden variable for each document."}, {"x": 112, "text": "This section describes two alternative approaches: (1) maximizing the likelihood estimates from the previous section and (2) a new approach based on approximative EM."}, {"x": 113, "text": "The decision between these two approaches is separate from the decision of using vari- ational inference versus Expectation-Propagation."}, {"x": 121, "text": "This can be understood as an EM algo- rithm where both  and the aspect assignments are hidden variables."}, {"x": 123, "text": "The same approach could be taken with EP, where we find the parameters that result in the largest possible EP esti- mate of the likelihood."}, {"x": 127, "text": "The second approach is to use an approximative EM algo- rithm, sometimes called variational EM, where we use expectations over an approximate posterior for , call it qi()."}, {"x": 143, "text": "This section elucidates the difference between variational inference (VB) and Expectation Propagation (EP) using simple, controlled datasets."}, {"x": 147, "text": "In exper- iments, we find that VB behaves like the Max approxima- tion while EP behaves like the exact value."}, {"x": 148, "text": "This update can be used with the s found by either VB or EP."}, {"x": 149, "text": "When running EP with the new parameter values, the s can be started from their previous values, so that only a few EP iterations are required."}, {"x": 166, "text": "The dot-dashed curve is the EP estimate of the likelihood for the same ps."}, {"x": 170, "text": "The solid line is the result of VB combined with the EM update of Section 5.2; as expected, it is closer to the true maximum."}, {"x": 171, "text": "The dot-dash vertical line is the result of applying EM using EP and is closest to the true maximum."}, {"x": 174, "text": "The EP so- lution correctly chose all aspects to be similar to the gener- ating multinomial; all probabilities were between 0.15 and 0.24."}, {"x": 175, "text": "The VB solution is quite different; it chose the ex- treme parameters shown in the following table (rounded to the tenths place):"}, {"x": 177, "text": "In terms of convergence rate, learning typically converged after 150 parameter updates with EP, while over 1,000 up- dates were required with VB."}, {"x": 178, "text": "Interestingly, on an independent set of 1000 test docu- ments, the perplexity of the model learned by EP is 5.0 while the VB models is 5.1, a seemingly trivial differ- ence."}, {"x": 187, "text": "The EP models com- mitted 76/2000 errors while the VB models committed 163/2000, which is both statistically and practically signif- icant."}, {"x": 188, "text": "As above, EP learned the correct models while VB chose extreme probabilities."}, {"x": 190, "text": "In order to compare variational inference and Expectation- Propagation on more realistic data, a corpus was created by mixing together TREC documents on known topics."}, {"x": 191, "text": "From the 1989 AP data on TREC disks 1 and 2, we extracted all of the documents that were judged to be relevant to one of the following six topics:"}, {"x": 195, "text": "used to train aspect models using both EP and VB, fixing the number of aspects at six; 75% of the data was used for training, and the remaining 25% was used as test data."}, {"x": 196, "text": "Figure 2 shows the top words for each aspect for the EP- trained model."}, {"x": 198, "text": "As seen in this figure, the aspects model variations across doc- uments in the distribution of common words such as SAID, FOR, and WAS."}, {"x": 203, "text": "In particular, we sample  from the approximate posterior D ( | ) obtained from EP."}, {"x": 204, "text": "Figure 3 shows the test set perplexities for VB and EP; the perplexity for the EP-trained model is consistently lower than the perplexity of the VB-trained model."}, {"x": 207, "text": "Plots of the Dirichlet parameters (Fig- ure 3, center and right) show that VB results in as that are indeed more spread out towards these extremes, compared with those obtained using EP."}, {"x": 208, "text": "6.3 TREC Interactive Data"}, {"x": 209, "text": "To compare VB and EP on real data having a mixture of aspects, this section considers documents from the TREC interactive collection (Over, 2001)."}, {"x": 215, "text": "In these experiments the speed of VB and EP are comparable."}, {"x": 216, "text": "Figure 3 shows the test set perplexity and Dirichlet param- eters a for both EP and VB, trained using A = 10 aspects."}, {"x": 217, "text": "As for the controlled TREC data, EP achieves a lower per- plexity, and has aspects that are more balanced compared to those obtained using VB."}, {"x": 218, "text": "We suspect that the perplex- ity difference on both the TREC interactive and controlled TREC data is small because the true aspects have little overlap, and thus the posterior of the mixing weights is sharply peaked."}, {"x": 221, "text": "This paper studied the problem of approximation meth- ods for learning and inference in the generative aspect model, and proposed an algorithm based on Expectation- Propagation as an alternative to the variational method adopted by Blei et al."}, {"x": 223, "text": "Experiments on synthetic data showed that simple variational inference can lead to in- accurate inferences and biased learning, while Expectation- Propagation can lead to more accurate inferences."}, {"x": 224, "text": "Exper- iments on TREC data show that Expectation-Propagation achieves lower test set perplexity."}], "characters": [{"name": "Grammatical aspect", "offsets": [4437], "paragraph_occurrences": [7], "sentence_occurrences": [32], "affiliation": "light", "frequency": 1, "id": "Grammatical_aspect"}, {"name": "Expected value", "offsets": [7115], "paragraph_occurrences": [17], "sentence_occurrences": [56], "affiliation": "light", "frequency": 1, "id": "Expected_value"}, {"name": "Dirichlet distribution", "offsets": [5245, 5974, 5334, 6439, 6564, 9372, 9459], "paragraph_occurrences": [11, 12, 11, 13, 13, 27, 27], "sentence_occurrences": [40, 46, 41, 50, 50, 82, 82], "affiliation": "light", "frequency": 7, "id": "Dirichlet_distribution"}, {"name": "Western Australia", "offsets": [10344], "paragraph_occurrences": [34], "sentence_occurrences": [95], "affiliation": "light", "frequency": 1, "id": "Western_Australia"}, {"name": "Text Retrieval Conference", "offsets": [1895, 4281, 19669, 19725, 22226, 22352, 23183, 23359, 23391, 24218], "paragraph_occurrences": [4, 6, 66, 66, 71, 72, 74, 74, 74, 76], "sentence_occurrences": [16, 30, 190, 191, 208, 209, 217, 218, 218, 224], "affiliation": "light", "frequency": 10, "id": "Text_Retrieval_Conference"}, {"name": "Expectation\u2013maximization algorithm", "offsets": [644, 9640, 10032, 11120, 11549, 11905, 12561, 13191, 13236, 17368, 17493], "paragraph_occurrences": [1, 28, 32, 40, 42, 42, 45, 47, 47, 60, 60], "sentence_occurrences": [6, 85, 92, 106, 110, 112, 121, 127, 127, 170, 171], "affiliation": "light", "frequency": 11, "id": "Expectation\u2013maximization_algorithm"}, {"name": "Expectation propagation", "offsets": [571, 2593, 3032, 7451, 3529, 3682, 4161, 4314, 8688, 9215, 10261, 10287, 10664, 12786, 12855, 14525, 15203, 15292, 15308, 15410, 17050, 17096, 17502, 17810, 18221, 18376, 19298, 19443, 11235, 12020, 14500, 19580, 23907, 24139, 20247, 20431, 21548, 21603, 21630, 22219, 22265, 23026, 23121, 23194, 24238], "paragraph_occurrences": [1, 5, 5, 19, 6, 6, 6, 6, 23, 26, 33, 33, 39, 45, 45, 55, 55, 56, 56, 56, 59, 59, 60, 61, 62, 63, 64, 64, 40, 42, 55, 66, 76, 76, 67, 68, 69, 70, 70, 70, 72, 73, 74, 74, 76], "sentence_occurrences": [6, 18, 22, 60, 25, 26, 29, 30, 74, 80, 94, 94, 103, 123, 123, 143, 147, 148, 149, 149, 166, 166, 171, 174, 177, 178, 187, 188, 107, 113, 143, 190, 221, 223, 195, 196, 203, 204, 204, 207, 209, 215, 216, 217, 224], "affiliation": "light", "frequency": 45, "id": "Expectation_propagation"}, {"name": "Holotype", "offsets": [21713], "paragraph_occurrences": [70], "sentence_occurrences": [204], "affiliation": "light", "frequency": 1, "id": "Holotype"}, {"name": "Was", "offsets": [20793], "paragraph_occurrences": [68], "sentence_occurrences": [198], "affiliation": "light", "frequency": 1, "id": "Was"}, {"name": "Resampling", "offsets": [18073], "paragraph_occurrences": [61], "sentence_occurrences": [175], "affiliation": "light", "frequency": 1, "id": "Resampling_(statistics)"}, {"name": "Monte Carlo method", "offsets": [9190], "paragraph_occurrences": [26], "sentence_occurrences": [80], "affiliation": "light", "frequency": 1, "id": "Monte_Carlo_method"}]}