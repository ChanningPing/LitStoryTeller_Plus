{"paragraph_scenes_info": [{"x": 1, "text": "We present a method that learns word embedding for Twitter sentiment classification in this paper.Most existing algorithms for learning continuous word representations typically only model the syntactic context of words but ignore the sentiment of text.This is problematic for sentiment analysis as they usually map words with similar syntactic context but opposite sentiment polarity, such as good and bad, to neighboring word vectors.We address this issue by learning sentimentspecific word embedding (SSWE), which encodes sentiment information in the continuous representation of words.Specifically, we develop three neural networks to effectively incorporate the supervision from sentiment polarity of text (e.g.sentences or tweets) in their loss functions.To obtain large scale training corpora, we learn the sentiment-specific word embedding from massive distant-supervised tweets collected by positive and negative emoticons.Experiments on applying SSWE to a benchmark Twitter sentiment classification dataset in SemEval 2013 show that (1) the SSWE feature performs comparably with hand-crafted features in the top-performed system; (2) the performance is further improved by concatenating SSWE with existing feature set."}, {"x": 3, "text": "Twitter sentiment classification has attracted increasing research interest in recent years (Jiang et al., 2011; Hu et al., 2013).The objective is to classify the sentiment polarity of a tweet as positivenegative or neutral.The majority of existing approaches follow Pang et al.(2002) and employ machine learning algorithms to build classifiers from tweets with manually annotated sentiment polarity.Under this direction, most studies focus on designing effective features to obtain better classification performance.For example, Mohammad et al.(2013) build the top-performed system in the Twitter sentiment classification track of SemEval 2013 (Nakov et al., 2013), using diverse sentiment lexicons and a variety of hand-crafted features."}, {"x": 4, "text": "Feature engineering is important but laborintensive.It is therefore desirable to discover explanatory factors from the data and make the learning algorithms less dependent on extensive feature engineering (Bengio, 2013).For the task of sentiment classification, an effective feature learning method is to compose the representation of a sentence (or document) from the representations of the words or phrases it contains (Socher et al., 2013b; Yessenalina and Cardie, 2011).Accordingly, it is a crucial step to learn the word representation (or word embedding), which is a dense, low-dimensional and real-valued vector for a word.Although existing word embedding learning algorithms (Collobert et al., 2011; Mikolov et al., 2013) are intuitive choices, they are not effective enough if directly used for sentiment classi- fication.The most serious problem is that traditional methods typically model the syntactic context of words but ignore the sentiment information of text.As a result, words with opposite polarity, such as good and bad, are mapped into close vectors.It is meaningful for some tasks such as pos-tagging (Zheng et al., 2013) as the two words have similar usages and grammatical roles, but it becomes a disaster for sentiment analysis as they have the opposite sentiment polarity."}, {"x": 5, "text": "In this paper, we propose learning sentimentspecific word embedding (SSWE) for sentiment analysis.We encode the sentiment information in-to the continuous representation of words, so that it is able to separate good and bad to opposite ends of the spectrum.To this end, we extend the existing word embedding learning algorithm (Collobert et al., 2011) and develop three neural networks to effectively incorporate the supervision from sentiment polarity of text (e.g.sentences or tweets) in their loss functions.We learn the sentiment-specific word embedding from tweets, leveraging massive tweets with emoticons as distant-supervised corpora without any manual annotations.These automatically collected tweets contain noises so they cannot be directly used as gold training data to build sentiment classifiers, but they are effective enough to provide weakly supervised signals for training the sentimentspecific word embedding."}, {"x": 6, "text": "We apply SSWE as features in a supervised learning framework for Twitter sentiment classi- fication, and evaluate it on the benchmark dataset in SemEval 2013.In the task of predicting positive/negative polarity of tweets, our method yields 84.89% in macro-F1 by only using SSWE as feature, which is comparable to the top-performed system based on hand-crafted features (84.70%).After concatenating the SSWE feature with existing feature set, we push the state-of-the-art to 86.58% in macro-F1.The quality of SSWE is also directly evaluated by measuring the word similarity in the embedding space for sentiment lexicons.In the accuracy of polarity consistency between each sentiment word and its top N closest words, SSWE outperforms existing word embedding learning algorithms."}, {"x": 8, "text": "We develop three neural networks to learn sentiment-specific word embedding (SSWE) from massive distant-supervised tweets without any manual annotations;"}, {"x": 9, "text": "To our knowledge, this is the first work that exploits word embedding for Twitter sentiment classification.We report the results that the SSWE feature performs comparably with hand-crafted features in the top-performed system in SemEval 2013;"}, {"x": 10, "text": "We release the sentiment-specific word embedding learned from 10 million tweets, which can be adopted off-the-shell in other sentiment analysis tasks."}, {"x": 12, "text": "In this section, we present a brief review of the related work from two perspectives, Twitter sentiment classification and learning continuous representations for sentiment classification."}, {"x": 13, "text": "2.1 Twitter Sentiment Classification"}, {"x": 14, "text": "Twitter sentiment classification, which identifies the sentiment polarity of short, informal tweets, has attracted increasing research interest (Jiang et al., 2011; Hu et al., 2013) in recent years.Generally, the methods employed in Twitter sentiment classification follow traditional sentiment classifi- cation approaches.The lexicon-based approaches (Turney, 2002; Ding et al., 2008; Taboada et al., 2011; Thelwall et al., 2012) mostly use a dictionary of sentiment words with their associated sentiment polarity, and incorporate negation and intensification to compute the sentiment polarity for each sentence (or document)."}, {"x": 15, "text": "The learning based methods for Twitter sentiment classification follow Pang et al.(2002)s work, which treat sentiment classification of texts as a special case of text categorization issue.Many studies on Twitter sentiment classification (Pak and Paroubek, 2010; Davidov et al., 2010; Barbosa and Feng, 2010; Kouloumpis et al., 2011; Zhao et al., 2012) leverage massive noisy-labeled tweets selected by positive and negative emoticons as training set and build sentiment classifiers directly, which is called distant supervision (Go et al., 2009).Instead of directly using the distantsupervised data as training set, Liu et al.(2012) adopt the tweets with emoticons to smooth the language model and Hu et al.(2013) incorporate the emotional signals into an unsupervised learning framework for Twitter sentiment classification."}, {"x": 16, "text": "Many existing learning based methods on Twitter sentiment classification focus on feature engineering.The reason is that the performance of sentiment classifier being heavily dependent on the choice of feature representation of tweets.The most representative system is introduced by Mohammad et al.(2013), which is the state-of-theart system (the top-performed system in SemEval 2013 Twitter Sentiment Classification Track) by implementing a number of hand-crafted features.Unlike the previous studies, we focus on learning discriminative features automatically from massive distant-supervised tweets."}, {"x": 19, "text": "With the revival of interest in deep learning (Bengio et al., 2013), incorporating the continuous representation of a word as features has been proving effective in a variety of NLP tasks, such as parsing (Socher et al., 2013a), language modeling (Bengio et al., 2003; Mnih and Hinton, 2009) and NER (Turian et al., 2010).In the field of sentiment analysis, Bespalov et al.(2011; 2012) initialize the word embedding by Latent Semantic Analysis and further represent each document as the linear weighted of ngram vectors for sentiment classification.Yessenalina and Cardie (2011) model each word as a matrix and combine words using iterated matrix multiplication.Glorot et al.(2011) explore Stacked Denoising Autoencoders for domain adaptation in sentiment classification.Socher et al.propose Recursive Neural Network (RNN) (2011b), matrixvector RNN (2012) and Recursive Neural Tensor Network (RNTN) (2013b) to learn the compositionality of phrases of any length based on the representation of each pair of children recursively.Hermann et al.(2013) present Combinatory Categorial Autoencoders to learn the compositionality of sentence, which marries the Combinatory Categorial Grammar with Recursive Autoencoder."}, {"x": 20, "text": "The representation of words heavily relies on the applications or tasks in which it is used (Labutov and Lipson, 2013).This paper focuses on learning sentiment-specific word embedding, which is tailored for sentiment analysis.Unlike Maas et al.(2011) that follow the probabilistic document model (Blei et al., 2003) and give an sentiment predictor function to each word, we develop neural networks and map each ngram to the sentiment polarity of sentence.Unlike Socher et al.(2011c) that utilize manually labeled texts to learn the meaning of phrase (or sentence) through compositionality, we focus on learning the meaning of word, namely word embedding, from massive distant-supervised tweets.Unlike Labutov and Lipson (2013) that produce task-specific embedding from an existing word embedding, we learn sentiment-specific word embedding from scratch."}, {"x": 21, "text": "3 Sentiment-Specific Word Embedding"}, {"x": 22, "text": "for Twitter Sentiment Classification In this section, we present the details of learning sentiment-specific word embedding (SSWE) for Twitter sentiment classification.We propose incorporating the sentiment information of sentences to learn continuous representations for words and phrases.We extend the existing word embedding learning algorithm (Collobert et al., 2011) and develop three neural networks to learn SSWE.In the following sections, we introduce the traditional method before presenting the details of SSWE learning algorithms.We then describe the use of SSWE in a supervised learning framework for Twitter sentiment classification."}, {"x": 23, "text": "3.1 C&W Model"}, {"x": 24, "text": "Collobert et al.(2011) introduce C&W model to learn word embedding based on the syntactic contexts of words.Given an ngram cat chills on a mat, C&W replaces the center word with a random word w r and derives a corrupted ngram cat chills w r a mat.The training objective is that the original ngram is expected to obtain a higher language model score than the corrupted ngram by a margin of 1.The ranking objective function can be optimized by a hinge loss, losscw(t, tr ) = max(0, 1  f cw(t) + f cw(t r )) (1) where t is the original ngram, t r is the corrupted ngram, f cw() is a one-dimensional scalar representing the language model score of the input ngram.Figure 1(a) illustrates the neural architecture of C&W, which consists of four layers, namely lookup  linear  hT anh  linear (from bottom to top).The original and corrupted ngrams are treated as inputs of the feed-forward neural network, respectively.The output f cw is the language model score of the input, which is calculated as given in Equation 2, where L is the lookup table of word embedding, w1, w2, b1, b2 are the parameters of linear layers.f cw(t) = w2(a) + b2 a = hT anh(w1Lt + b1) (3) hT anh(x) =    1 if x < 1 x if  1  x  1 1 if x > 1 (4)"}, {"x": 25, "text": "3.2 Sentiment-Specific Word Embedding"}, {"x": 26, "text": "Following the traditional C&W model (Collobert et al., 2011), we incorporate the sentiment information into the neural network to learn sentimentspecific word embedding.We develop three neural networks with different strategies to integrate the sentiment information of tweets."}, {"x": 27, "text": "Basic Model 1 (SSWEh).As an unsupervised approach, C&W model does not explicitly capture the sentiment information of texts.An intuitive solution to integrate the sentiment information is predicting the sentiment distribution of text based on input ngram.We do not utilize the entire sentence as input because the length of different sentences might be variant.We therefore slide the window of ngram across a sentence, and then predict the sentiment polarity based on each ngram with a shared neural network.In the neural network, the distributed representation of higher layer are interpreted as features describing the input.Thus, we utilize the continuous vector of top layer to predict the sentiment distribution of text."}, {"x": 28, "text": "Assuming there are K labels, we modify the dimension of top layer in C&W model as K and add a sof tmax layer upon the top layer.The neural network (SSWEh) is given in Figure 1(b).Sof tmax layer is suitable for this scenario because its outputs are interpreted as conditional probabilities.Unlike C&W, SSWEh does not generate any corrupted ngram.Let f g (t), where K denotes the number of sentiment polarity labels, be the gold K-dimensional multinomial distribution of input t and P k f g k (t) = 1.For positive/negative classification, the distribution is of the form <1,0> for positive and <0,1> for negative.The cross-entropy error of the sof tmax layer is : lossh(t) =  X k=<0,1> f g k (t)  log(f h k (t)) (5) where f g (t) is the gold sentiment distribution and f h (t) is the predicted sentiment distribution."}, {"x": 29, "text": "Basic Model 2 (SSWEr).SSWEh is trained by predicting the positive ngram as <1,0> and the negative ngram as <0,1>.However, the constraint of SSWEh is too strict.The distribution of <0.7,0.3> can also be interpreted as a positive label because the positive score is larger than the negative score.Similarly, the distribution of <0.2,0.8> indicates negative polarity.Based on the above observation, the hard constraints in SSWEh should be relaxed.If the sentiment polarity of a tweet is positive, the predicted positive score is expected to be larger than the predicted negative score, and the exact reverse if the tweet has negative polarity."}, {"x": 30, "text": "We model the relaxed constraint with a ranking objective function and borrow the bottom four layers from SSWEh, namely lookup  linear  hT anh  linear in Figure 1(b), to build the relaxed neural network (SSWEr).Compared with SSWEh, the sof tmax layer is removed because SSWEr does not require probabilistic interpretation.The hinge loss of SSWEr is modeled as de- scribed below.lossr(t) = max(0, 1  s(t)f r 0 (t) + s(t)f r 1 (t) ) (6) where f r 0 is the predicted positive score, f r 1 is the predicted negative score, s(t) is an indicator function reflecting the sentiment polarity of a sentence, s(t) = ( 1 if f g (t) = <1, 0> 1 if f g (t) = <0, 1> (7) Similar with SSWEh, SSWEr also does not generate the corrupted ngram."}, {"x": 31, "text": "Unified Model (SSWEu).The C&W model learns word embedding by modeling syntactic contexts of words but ignoring sentiment information.By contrast, SSWEh and SSWEr learn sentiment-specific word embedding by integrating the sentiment polarity of sentences but leaving out the syntactic contexts of words.We develop a uni- fied model (SSWEu) in this part, which captures the sentiment information of sentences as well as the syntactic contexts of words.SSWEu is illustrated in Figure 1(c)."}, {"x": 32, "text": "Given an original (or corrupted) ngram and the sentiment polarity of a sentence as the input, SSWEu predicts a two-dimensional vector for each input ngram.The two scalars (f u 0 , f u 1 ) stand for language model score and sentiment score of the input ngram, respectively.The training objectives of SSWEu are that (1) the original ngram should obtain a higher language model score f u 0 (t)than the corrupted ngram f u 0 (t r ), and (2) the sentiment score of original ngram f u 1 (t) should be more consistent with the gold polarity annotation of sentence than corrupted ngram f u 1 (t r ).The loss function of SSWEu is the linear combination of two hinge losses, lossu(t, tr ) =   losscw(t, tr )+ (1  )  lossus(t, tr ) (8) where losscw(t, tr ) is the syntactic loss as given in Equation 1, lossus(t, tr ) is the sentiment loss as described in Equation 9.The hyper-parameter  weighs the two parts.lossus(t, tr ) = max(0, 1  s(t)f u 1 (t) + s(t)f u 1 (t r ) ) (9)"}, {"x": 33, "text": "Model Training.We train sentiment-specific word embedding from massive distant-supervised tweets collected with positive and negative emoticons1 .We crawl tweets from April 1st, 2013 to April 30th, 2013 with TwitterAPI.We tokenize each tweet with TwitterNLP (Gimpel et al., 2011), remove the @user and URLs of each tweet, and filter the tweets that are too short (< 7 words).Finally, we collect 10M tweets, selected by 5M tweets with positive emoticons and 5M tweets with negative emoticons."}, {"x": 34, "text": "We train SSWEh, SSWEr and SSWEu by taking the derivative of the loss through backpropagation with respect to the whole set of parameters (Collobert et al., 2011), and use AdaGrad (Duchi et al., 2011) to update the parameters.We empirically set the window size as 3, the embedding length as 50, the length of hidden layer as 20 and the learning rate of AdaGrad as 0.1 for all baseline and our models.We learn embedding for unigrams, bigrams and trigrams separately with same neural network and same parameter setting.The contexts of unigram (bigram/trigram) are the surrounding unigrams (bigrams/trigrams), respectively."}, {"x": 35, "text": "3.3 Twitter Sentiment Classification"}, {"x": 36, "text": "We apply sentiment-specific word embedding for Twitter sentiment classification under a supervised learning framework as in previous work (Pang et al., 2002).Instead of hand-crafting features, we incorporate the continuous representation of words and phrases as the feature of a tweet.The sentiment classifier is built from tweets with manually annotated sentiment polarity."}, {"x": 39, "text": "We conduct experiments to evaluate SSWE by incorporating it into a supervised learning framework for Twitter sentiment classification.We also directly evaluate the effectiveness of the SSWE by measuring the word similarity in the embedding space for sentiment lexicons."}, {"x": 40, "text": "4.1 Twitter Sentiment Classification"}, {"x": 41, "text": "Experiment Setup and Datasets.We conduct experiments on the latest Twitter sentiment classification benchmark dataset in SemEval 2013 (Nakov et al., 2013).The training and development sets were completely in full to task participants.However, we were unable to download all the training and development sets because some tweets were deleted or not available due to modified authorization status.The test set is directly provided to the participants.The distribution of our dataset is given in Table 1.We train sentiment classifier with LibLinear (Fan et al., 2008) on the training set, tune parameter c on the dev set and evaluate on the test set.Evaluation metric is the Macro-F1 of positive and negative categories 2 .Positive Negative Neutral Total Train 2,642 994 3,436 7,072 Dev 408 219 493 1,120 Test 1,570 601 1,639 3,810 Table 1: Statistics of the SemEval 2013 Twitter sentiment classification dataset.2We investigate 2-class Twitter sentiment classification (positive/negative) instead of 3-class Twitter sentiment classification (positive/negative/neutral) in SemEval2013."}, {"x": 42, "text": "Baseline Methods.We compare our method with the following sentiment classification algorithms:"}, {"x": 43, "text": "(1) DistSuper: We use the 10 million tweets selected by positive and negative emoticons as training data, and build sentiment classifier with LibLinear and ngram features (Go et al., 2009)."}, {"x": 44, "text": "(2) SVM: The ngram features and Support Vector Machine are widely used baseline methods to build sentiment classifiers (Pang et al., 2002).LibLinear is used to train the SVM classifier."}, {"x": 45, "text": "(3) NBSVM: NBSVM (Wang and Manning, 2012) is a state-of-the-art performer on many sentiment classification datasets, which trades-off between Naive Bayes and NB-enhanced SVM."}, {"x": 46, "text": "(4) RAE: Recursive Autoencoder (Socher et al., 2011c) has been proven effective in many sentiment analysis tasks by learning compositionality automatically.We run RAE with randomly initialized word embedding."}, {"x": 47, "text": "(5) NRC: NRC builds the top-performed system in SemEval 2013 Twitter sentiment classification track which incorporates diverse sentiment lexicons and many manually designed features.We re-implement this system because the codes are not publicly available 3 .NRC-ngram refers to the feature set of NRC leaving out ngram features."}, {"x": 48, "text": "Except for DistSuper, other baseline methods are conducted in a supervised manner.We do not compare with RNTN (Socher et al., 2013b) because we cannot efficiently train the RNTN model.The reason lies in that the tweets in our dataset do not have accurately parsed results or fine grained sentiment labels for phrases.Another reason is that the RNTN model trained on movie reviews cannot be directly applied on tweets due to the differences between domains (Blitzer et al., 2007)."}, {"x": 49, "text": "Results and Analysis.Table 2 shows the macroF1 of the baseline systems as well as the SSWEbased methods on positive/negative sentiment classification of tweets.Distant supervision is relatively weak because the noisy-labeled tweets are treated as the gold standard, which affects the performance of classifier.The results of bagof-ngram (uni/bi/tri-gram) features are not satis- fied because the one-hot word representation cannot capture the latent connections between words.NBSVM and RAE perform comparably and have 3 For 3-class sentiment classification in SemEval 2013, our re-implementation of NRC achieved 68.3%, 0.7% lower than NRC (69%) due to less training data.1560 Method Macro-F1 DistSuper + unigram 61.74 DistSuper + uni/bi/tri-gram 63.84 SVM + unigram 74.50 SVM + uni/bi/tri-gram 75.06 NBSVM 75.28 RAE 75.12 NRC (Top System in SemEval) 84.73 NRC - ngram 84.17 SSWEu 84.98 SSWEu+NRC 86.58 SSWEu+NRC-ngram 86.48 Table 2: Macro-F1 on positive/negative classification of tweets.a big gap in comparison with the NRC and SSWEbased methods.The reason is that RAE and NBSVM learn the representation of tweets from the small-scale manually annotated training set, which cannot well capture the comprehensive linguistic phenomenons of words."}, {"x": 50, "text": "NRC implements a variety of features and reaches 84.73% in macro-F1, verifying the importance of a better feature representation for Twitter sentiment classification.We achieve 84.98% by using only SSWEu as features without borrowing any sentiment lexicons or hand-crafted rules.The results indicate that SSWEu automatically learns discriminative features from massive tweets and performs comparable with the state-of-the-art manually designed features.After concatenating SSWEu with the feature set of NRC, the performance is further improved to 86.58%.We also compare SSWEu with the ngram feature by integrating SSWE into NRC-ngram.The concatenated features SSWEu+NRC-ngram (86.48%) outperform the original feature set of NRC (84.73%)."}, {"x": 51, "text": "As a reference, we apply SSWEu on subjective classification of tweets, and obtain 72.17% in macro-F1 by using only SSWEu as feature.After combining SSWEu with the feature set of NRC, we improve NRC from 74.86% to 75.39% for subjective classification."}, {"x": 52, "text": "Comparision between Different Word Embedding.We compare sentiment-specific word embedding (SSWEh, SSWEr, SSWEu) with baseline embedding learning algorithms by only using word embedding as features for Twitter sentiment classification.We use the embedding of unigrams, bigrams and trigrams in the experiment.The embeddings of C&W (Collobert et al., 2011), word2vec4 , WVSA (Maas et al., 2011) and our models are trained with the same dataset and same parameter setting.We compare with C&W and word2vec as they have been proved effective in many NLP tasks.The trade-off parameter of ReEmb (Labutov and Lipson, 2013) is tuned on the development set of SemEval 2013."}, {"x": 53, "text": "Table 3 shows the performance on the positive/negative classification of tweets5 .ReEmb(C&W) and ReEmb(w2v) stand for the use of embeddings learned from 10 million distantsupervised tweets with C&W and word2vec, respectively.Each row of Table 3 represents a word embedding learning algorithm.Each column stands for a type of embedding used to compose features of tweets.The column uni+bi denotes the use of unigram and bigram embedding, and the column uni+bi+tri indicates the use of unigram, bigram and trigram embedding.Embedding unigram uni+bi uni+bi+tri C&W 74.89 75.24 75.89 Word2vec 73.21 75.07 76.31 ReEmb(C&W) 75.87   ReEmb(w2v) 75.21   WVSA 77.04   SSWEh 81.33 83.16 83.37 SSWEr 80.45 81.52 82.60 SSWEu 83.70 84.70 84.98 Table 3: Macro-F1 on positive/negative classification of tweets with different word embeddings."}, {"x": 54, "text": "From the first column of Table 3, we can see that the performance of C&W and word2vec are obviously lower than sentiment-specific word embeddings by only using unigram embedding as features.The reason is that C&W and word2vec do not explicitly exploit the sentiment information of the text, resulting in that the words with opposite polarity such as good and bad are mapped to close word vectors.When such word embeddings are fed as features to a Twitter sentiment classifier, the discriminative ability of sentiment words are weakened thus the classification performance is affected.Sentiment-specific word em- 4Available at https://code.google.com/p/word2vec/.We utilize the Skip-gram model because it performs better than CBOW in our experiments.5MVSA and ReEmb are not suitable for learning bigram and trigram embedding because their sentiment predictor functions only utilize the unigram embedding.1561 beddings (SSWEh, SSWEr, SSWEu) effectively distinguish words with opposite sentiment polarity and perform best in three settings.SSWE outperforms MVSA by exploiting more contextual information in the sentiment predictor function.SSWE outperforms ReEmb by leveraging more sentiment information from massive distant-supervised tweets.Among three sentiment-specific word embeddings, SSWEu captures more context information and yields best performance.SSWEh and SSWEr obtain comparative results."}, {"x": 55, "text": "From each row of Table 3, we can see that the bigram and trigram embeddings consistently improve the performance of Twitter sentiment classi- fication.The underlying reason is that a phrase, which cannot be accurately represented by unigram embedding, is directly encoded into the ngram embedding as an idiomatic unit.A typical case in sentiment analysis is that the composed phrase and multiword expression may have a different sentiment polarity than the individual words it contains, such as not <bad> and <great> deal of (the word in the bracket has different sentiment polarity with the ngram).A very recent study by Mikolov et al.(2013) also verified the effectiveness of phrase embedding for analogically reasoning phrases."}, {"x": 56, "text": "Effect of  in SSWEu We tune the hyperparameter  of SSWEu on the development set by using unigram embedding as features.As given in Equation 8,  is the weighting score of syntactic loss of SSWEu and trades-off the syntactic and sentiment losses.SSWEu is trained from 10 million distant-supervised tweets.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0.77 0.78 0.79 0.8 0.81 0.82 0.83 0.84  MacroF1 SSWEu Figure 2: Macro-F1 of SSWEu on the development set of SemEval 2013 with different .Figure 2 shows the macro-F1 of SSWEu on positive/negative classification of tweets with different  on our development set.We can see that SSWEu performs better when  is in the range of <0.5, 0.6>, which balances the syntactic context and sentiment information.The model with =1 stands for C&W model, which only encodes the syntactic contexts of words.The sharp decline at =1 reflects the importance of sentiment information in learning word embedding for Twitter sentiment classification."}, {"x": 57, "text": "Effect of Distant-supervised Data in SSWEu We investigate how the size of the distantsupervised data affects the performance of SSWEu feature for Twitter sentiment classification.We vary the number of distant-supervised tweets from 1 million to 12 million, increased by 1 million.We set the  of SSWEu as 0.5, according to the experiments shown in Figure 2.Results of positive/negative classification of tweets on our development set are given in Figure 3.1 2 3 4 5 6 7 8 9 10 11 12 x 106 0.77 0.78 0.79 0.8 0.81 0.82 0.83 0.84 # of distantsupervised tweets MacroF1 SSWEu Figure 3: Macro-F1 of SSWEu with different size of distant-supervised data on our development set."}, {"x": 58, "text": "We can see that when more distant-supervised tweets are added, the accuracy of SSWEu consistently improves.The underlying reason is that when more tweets are incorporated, the word embedding is better estimated as the vocabulary size is larger and the context and sentiment information are richer.When we have 10 million distantsupervised tweets, the SSWEu feature increases the macro-F1 of positive/negative classification of tweets to 82.94% on our development set.When we have more than 10 million tweets, the performance remains stable as the contexts of words have been mostly covered."}, {"x": 59, "text": "4.2 Word Similarity of Sentiment Lexicons"}, {"x": 60, "text": "The quality of SSWE has been implicitly evaluated when applied in Twitter sentiment classification in the previous subsection.We explicitly evaluate it in this section through word similarity in the em- 1562 bedding space for sentiment lexicons.The evaluation metric is the accuracy of polarity consistency between each sentiment word and its top N closest words in the sentiment lexicon, Accuracy = P#Lex i=1 PN j=1 (wi , cij ) #Lex  N (10) where #Lex is the number of words in the sentiment lexicon, wi is the i-th word in the lexicon, cij is the j-th closest word to wi in the lexicon with cosine similarity, (wi , cij ) is an indicator function that is equal to 1 if wi and cij have the same sentiment polarity and 0 for the opposite case.The higher accuracy refers to a better polarity consistency of words in the sentiment lexicon.We set N as 100 in our experiment."}, {"x": 61, "text": "Experiment Setup and Datasets We utilize the widely-used sentiment lexicons, namely MPQA (Wilson et al., 2005) and HL (Hu and Liu, 2004), to evaluate the quality of word embedding.For each lexicon, we remove the words that do not appear in the lookup table of word embedding.We only use unigram embedding in this section because these sentiment lexicons do not contain phrases.The distribution of the lexicons used in this paper is listed in Table 4.Lexicon Positive Negative Total HL 1,331 2,647 3,978 MPQA 1,932 2,817 4,749 Joint 1,051 2,024 3,075 Table 4: Statistics of the sentiment lexicons.Joint stands for the words that occur in both HL and MPQA with the same sentiment polarity."}, {"x": 62, "text": "Results.Table 5 shows our results compared to other word embedding learning algorithms.The accuracy of random result is 50% as positive and negative words are randomly occurred in the nearest neighbors of each word.Sentiment-specific word embeddings (SSWEh, SSWEr, SSWEu) outperform existing neural models (C&W, word2vec) by large margins.SSWEu performs best in three lexicons.SSWEh and SSWEr have comparable performances.Experimental results further demonstrate that sentiment-specific word embeddings are able to capture the sentiment information of texts and distinguish words with opposite sentiment polarity, which are not well solved in traditional neural Embedding HL MPQA Joint Random 50.00 50.00 50.00 C&W 63.10 58.13 62.58 Word2vec 66.22 60.72 65.59 ReEmb(C&W) 64.81 59.76 64.09 ReEmb(w2v) 67.16 61.81 66.39 WVSA 68.14 64.07 67.12 SSWEh 74.17 68.36 74.03 SSWEr 73.65 68.02 73.14 SSWEu 77.30 71.74 77.33 Table 5: Accuracy of the polarity consistency of words in different sentiment lexicons.models like C&W and word2vec.SSWE outperforms MVSA and ReEmb by exploiting more context information of words and sentiment information of sentences, respectively."}, {"x": 64, "text": "In this paper, we propose learning continuous word representations as features for Twitter sentiment classification under a supervised learning framework.We show that the word embedding learned by traditional neural networks are not effective enough for Twitter sentiment classification.These methods typically only model the context information of words so that they cannot distinguish words with similar context but opposite sentiment polarity (e.g.good and bad).We learn sentiment-specific word embedding (SSWE) by integrating the sentiment information into the loss functions of three neural networks.We train SSWE with massive distant-supervised tweets selected by positive and negative emoticons.The effectiveness of SSWE has been implicitly evaluated by using it as features in sentiment classification on the benchmark dataset in SemEval 2013, and explicitly verified by measuring word similarity in the embedding space for sentiment lexicons.Our unified model combining syntactic context of words and sentiment information of sentences yields the best performance in both experiments."}], "chapters": [{"text": "Abstract", "sentence_id": "s_0", "sentence_rank": "0", "paragraph_id": "p_0", "paragraph_rank": 0}, {"text": "1 Introduction", "sentence_id": "s_9", "sentence_rank": "9", "paragraph_id": "p_2", "paragraph_rank": 2}, {"text": "2 Related Work", "sentence_id": "s_41", "sentence_rank": "41", "paragraph_id": "p_11", "paragraph_rank": 11}, {"text": "2.1 Twitter Sentiment Classification", "sentence_id": "s_43", "sentence_rank": "43", "paragraph_id": "p_13", "paragraph_rank": 13}, {"text": "2.2 Learning Continuous Representations for Sentiment Classification", "sentence_id": "s_58", "sentence_rank": "58", "paragraph_id": "p_17", "paragraph_rank": 17}, {"text": "3 Sentiment-Specific Word Embedding", "sentence_id": "s_81", "sentence_rank": "81", "paragraph_id": "p_21", "paragraph_rank": 21}, {"text": "3.1 C&W Model", "sentence_id": "s_87", "sentence_rank": "87", "paragraph_id": "p_23", "paragraph_rank": 23}, {"text": "3.2 Sentiment-Specific Word Embedding", "sentence_id": "s_97", "sentence_rank": "97", "paragraph_id": "p_25", "paragraph_rank": 25}, {"text": "3.3 Twitter Sentiment Classification", "sentence_id": "s_145", "sentence_rank": "145", "paragraph_id": "p_35", "paragraph_rank": 35}, {"text": "4 Experiment", "sentence_id": "s_156", "sentence_rank": "156", "paragraph_id": "p_38", "paragraph_rank": 38}, {"text": "4.1 Twitter Sentiment Classification", "sentence_id": "s_159", "sentence_rank": "159", "paragraph_id": "p_40", "paragraph_rank": 40}, {"text": "4.2 Word Similarity of Sentiment Lexicons", "sentence_id": "s_246", "sentence_rank": "246", "paragraph_id": "p_59", "paragraph_rank": 59}, {"text": "5 Conclusion", "sentence_id": "s_267", "sentence_rank": "267", "paragraph_id": "p_63", "paragraph_rank": 63}], "scenes": [["Twitter", "SemEval", "SSWE", "sentiment"], ["Twitter", "SemEval", "sentiment"], ["sentiment"], ["SSWE", "sentiment"], ["Twitter", "SemEval", "SSWE", "sentiment"], ["SSWE", "sentiment"], ["Twitter", "SemEval", "SSWE", "sentiment"], ["sentiment"], ["Twitter", "sentiment"], ["Twitter"], ["Twitter", "sentiment"], ["Twitter", "Phoenix_Suns", "sentiment"], ["Twitter", "SemEval", "sentiment"], ["Latent_semantic_analysis", "Autoencoder", "Tensor", "sentiment", "Combinatory_categorial_grammar", "Natural_language_processing", "Noise_reduction", "Recursive_neural_network"], ["sentiment"], ["Word"], ["Twitter", "SSWE", "sentiment"], ["Country_music"], ["Country_music"], ["Word"], ["sentiment"], ["Mathematical_model", "SSWE", "Country_music", "sentiment"], ["SSWE", "Country_music", "Dimension", "sentiment"], ["Mathematical_model", "SSWE", "sentiment"], ["Unified_Model", "SSWE", "sentiment"], ["SSWE", "Country_music", "sentiment"], ["Mathematical_model", "SSWE", "sentiment"], ["Uniform_Resource_Locator", "sentiment"], ["SSWE"], ["Twitter"], ["Twitter", "sentiment"], ["Twitter", "SSWE", "sentiment"], ["Twitter"], ["Formula_One", "sentiment", "Twitter", "LIBSVM", "SemEval", "Rugby_union"], ["sentiment"], ["LIBSVM", "sentiment"], ["Support_vector_machine", "LIBSVM", "sentiment"], ["Naive_Bayes_classifier", "Support_vector_machine", "sentiment"], ["sentiment"], ["Twitter", "SemEval", "sentiment"], ["sentiment"], ["Support_vector_machine", "SSWE", "Formula_One", "SemEval", "sentiment"], ["Twitter", "SSWE", "sentiment"], ["SSWE"], ["Word2vec", "sentiment", "Twitter", "Natural_language_processing", "SemEval", "SSWE", "Country_music"], ["Formula_One", "SSWE", "Country_music", "Word2vec"], ["Twitter", "Country_music", "Word2vec", "SSWE", "sentiment"], ["Twitter", "sentiment"], ["Formula_One", "sentiment", "Twitter", "Macro_(computer_science)", "SemEval", "SSWE", "Country_music"], ["Macro_(computer_science)", "Formula_One", "Twitter", "sentiment", "SSWE"], ["SSWE", "sentiment"], ["Word"], ["Twitter", "SSWE", "sentiment"], ["Lexicon", "sentiment"], ["Randomness", "Word2vec", "sentiment", "Embedding", "SSWE", "Country_music"], ["Twitter", "SemEval", "SSWE", "sentiment"]], "characters": [{"name": "Latent semantic analysis", "offsets": [8891], "paragraph_occurrences": [19], "sentence_occurrences": [66], "affiliation": "light", "frequency": 1, "id": "Latent_semantic_analysis"}, {"name": "Tensor", "offsets": [9349], "paragraph_occurrences": [19], "sentence_occurrences": [71], "affiliation": "light", "frequency": 1, "id": "Tensor"}, {"name": "Combinatory categorial grammar", "offsets": [9625], "paragraph_occurrences": [19], "sentence_occurrences": [73], "affiliation": "light", "frequency": 1, "id": "Combinatory_categorial_grammar"}, {"name": "Mathematical model", "offsets": [12761, 14301, 17106], "paragraph_occurrences": [27, 29, 32], "sentence_occurrences": [100, 114, 135], "affiliation": "light", "frequency": 3, "id": "Mathematical_model"}, {"name": "Macro", "offsets": [29012, 30154], "paragraph_occurrences": [56, 57], "sentence_occurrences": [232, 241], "affiliation": "light", "frequency": 2, "id": "Macro_(computer_science)"}, {"name": "Naive Bayes classifier", "offsets": [21710, 21726], "paragraph_occurrences": [45, 45], "sentence_occurrences": [175, 175], "affiliation": "light", "frequency": 2, "id": "Naive_Bayes_classifier"}, {"name": "Word2vec", "offsets": [25481, 25853, 26553, 26231, 32743, 33451, 33164], "paragraph_occurrences": [52, 53, 54, 53, 62, 62, 62], "sentence_occurrences": [205, 208, 213, 212, 261, 265, 264], "affiliation": "light", "frequency": 7, "id": "Word2vec"}, {"name": "Natural language processing", "offsets": [8650, 25533], "paragraph_occurrences": [19, 52], "sentence_occurrences": [64, 205], "affiliation": "light", "frequency": 2, "id": "Natural_language_processing"}, {"name": "Support vector machine", "offsets": [21387, 21553, 21738, 23509, 23529, 21415], "paragraph_occurrences": [44, 44, 45, 49, 49, 44], "sentence_occurrences": [173, 174, 175, 190, 190, 173], "affiliation": "light", "frequency": 6, "id": "Support_vector_machine"}, {"name": "Randomness", "offsets": [33117], "paragraph_occurrences": [62], "sentence_occurrences": [264], "affiliation": "light", "frequency": 1, "id": "Randomness"}, {"name": "Word", "offsets": [10557, 12464, 30836], "paragraph_occurrences": [21, 25, 59], "sentence_occurrences": [81, 97, 246], "affiliation": "light", "frequency": 3, "id": "Word"}, {"name": "LIBSVM", "offsets": [20554, 21336, 21522], "paragraph_occurrences": [41, 43, 44], "sentence_occurrences": [166, 172, 173], "affiliation": "light", "frequency": 3, "id": "LIBSVM"}, {"name": "SemEval", "offsets": [1028, 1882, 4360, 5449, 7674, 20139, 20874, 21998, 23317, 23598, 25638, 29056, 34443], "paragraph_occurrences": [1, 3, 6, 9, 16, 41, 41, 47, 49, 49, 52, 56, 64], "sentence_occurrences": [8, 16, 31, 39, 56, 161, 168, 178, 189, 190, 206, 232, 274], "affiliation": "light", "frequency": 13, "id": "SemEval"}, {"name": "Embedding", "offsets": [33093], "paragraph_occurrences": [62], "sentence_occurrences": [264], "affiliation": "light", "frequency": 1, "id": "Embedding"}, {"name": "Formula One", "offsets": [20696, 23446, 23696, 26396, 29018, 30160], "paragraph_occurrences": [41, 49, 49, 53, 56, 57], "sentence_occurrences": [167, 190, 190, 212, 232, 241], "affiliation": "light", "frequency": 6, "id": "Formula_One"}, {"name": "Country", "offsets": [11220, 11262, 11373, 11940, 12806, 13549, 13776, 15684, 25314, 25473, 25739, 25845, 26264, 26545, 26685, 29374, 32738, 33142, 33197, 33443], "paragraph_occurrences": [23, 24, 24, 24, 27, 28, 28, 31, 52, 52, 53, 53, 53, 54, 54, 56, 62, 62, 62, 62], "sentence_occurrences": [87, 89, 90, 93, 101, 107, 110, 126, 204, 205, 208, 208, 212, 213, 214, 235, 261, 264, 264, 265], "affiliation": "light", "frequency": 20, "id": "Country_music"}, {"name": "Uniform resource locator", "offsets": [17408], "paragraph_occurrences": [33], "sentence_occurrences": [139], "affiliation": "light", "frequency": 1, "id": "Uniform_Resource_Locator"}, {"name": "Dimension", "offsets": [13907], "paragraph_occurrences": [28], "sentence_occurrences": [111], "affiliation": "light", "frequency": 1, "id": "Dimension"}, {"name": "Unified Model", "offsets": [15658], "paragraph_occurrences": [30], "sentence_occurrences": [124], "affiliation": "light", "frequency": 1, "id": "Unified_Model"}, {"name": "Lexicon", "offsets": [32194], "paragraph_occurrences": [61], "sentence_occurrences": [255], "affiliation": "light", "frequency": 1, "id": "Lexicon"}, {"name": "Autoencoder", "offsets": [9671], "paragraph_occurrences": [19], "sentence_occurrences": [73], "affiliation": "light", "frequency": 1, "id": "Autoencoder"}, {"name": "Twitter Inc.", "offsets": [59, 984, 1840, 4280, 5294, 5712, 5818, 6083, 6508, 6682, 7270, 7343, 7687, 10575, 10705, 11183, 18220, 18299, 19814, 19986, 20085, 20887, 20952, 21024, 22011, 24135, 25190, 26923, 27991, 29540, 29719, 30939, 33688, 33859], "paragraph_occurrences": [1, 1, 3, 6, 9, 12, 13, 14, 15, 15, 15, 16, 16, 22, 22, 22, 35, 36, 39, 40, 41, 41, 41, 41, 47, 50, 52, 54, 55, 56, 57, 60, 64, 64], "sentence_occurrences": [1, 8, 16, 31, 38, 42, 43, 45, 47, 49, 52, 53, 56, 82, 82, 86, 145, 146, 157, 159, 161, 168, 169, 169, 178, 193, 202, 215, 224, 236, 237, 247, 268, 269], "affiliation": "light", "frequency": 34, "id": "Twitter"}, {"name": "Recursive neural network", "offsets": [9264, 9290, 9317], "paragraph_occurrences": [19, 19, 19], "sentence_occurrences": [71, 71, 71], "affiliation": "light", "frequency": 3, "id": "Recursive_neural_network"}, {"name": "Phoenix Suns", "offsets": [7007], "paragraph_occurrences": [15], "sentence_occurrences": [49], "affiliation": "light", "frequency": 1, "id": "Phoenix_Suns"}, {"name": "Noise reduction", "offsets": [9170], "paragraph_occurrences": [19], "sentence_occurrences": [69], "affiliation": "light", "frequency": 1, "id": "Noise_reduction"}, {"name": "Rugby union", "offsets": [20820], "paragraph_occurrences": [41], "sentence_occurrences": [168], "affiliation": "light", "frequency": 1, "id": "Rugby_union"}, {"name": "SSWE", "offsets": [], "paragraph_occurrences": [1, 5, 6, 8, 9, 22, 27, 28, 29, 30, 31, 32, 34, 39, 49, 50, 51, 52, 53, 54, 56, 57, 58, 60, 62, 64], "sentence_occurrences": [4, 8, 25, 31, 32, 33, 34, 35, 37, 39, 82, 84, 85, 86, 100, 108, 110, 114, 115, 116, 119, 121, 122, 123, 124, 125, 127, 128, 129, 130, 132, 133, 141, 157, 158, 186, 190, 191, 194, 195, 196, 197, 198, 199, 200, 202, 212, 219, 220, 221, 222, 223, 229, 230, 231, 232, 233, 234, 237, 239, 241, 242, 244, 247, 261, 262, 263, 264, 266, 272, 273, 274], "affiliation": "light", "frequency": 100, "id": "SSWE"}, {"name": "sentiment", "offsets": [], "paragraph_occurrences": [1, 3, 4, 5, 6, 8, 9, 10, 12, 14, 15, 16, 19, 20, 22, 26, 27, 28, 29, 30, 31, 32, 33, 36, 39, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 52, 54, 55, 56, 57, 58, 60, 61, 62, 64], "sentence_occurrences": [1, 2, 3, 4, 5, 7, 8, 10, 11, 13, 16, 19, 21, 22, 24, 25, 26, 27, 29, 30, 31, 34, 35, 37, 38, 40, 42, 44, 45, 46, 47, 48, 49, 52, 53, 54, 65, 66, 69, 75, 77, 80, 82, 83, 86, 98, 99, 101, 102, 104, 106, 111, 113, 120, 124, 126, 127, 128, 130, 131, 132, 133, 137, 146, 148, 157, 158, 161, 166, 168, 169, 171, 172, 173, 175, 176, 178, 183, 186, 189, 193, 194, 202, 213, 214, 215, 218, 219, 220, 221, 222, 224, 226, 230, 234, 236, 237, 243, 247, 248, 249, 250, 252, 254, 256, 257, 264, 266, 268, 269, 270, 272, 274, 275], "affiliation": "light", "frequency": 151, "id": "sentiment"}], "all_paragraphs": [{"paragraph_info": {"end": 8, "start": 0, "text": "Abstract", "rank": 0, "paragraph_comparative_number": 0, "entities": [], "id": "p_0"}, "sentences": [{"end": 8, "text": "Abstract", "rank": 0, "start": 0, "IsComparative": "0", "id": "st_0"}]}, {"paragraph_info": {"end": 1236, "start": 8, "text": "We present a method that learns word embedding for Twitter sentiment classification in this paper.Most existing algorithms for learning continuous word representations typically only model the syntactic context of words but ignore the sentiment of text.This is problematic for sentiment analysis as they usually map words with similar syntactic context but opposite sentiment polarity, such as good and bad, to neighboring word vectors.We address this issue by learning sentimentspecific word embedding (SSWE), which encodes sentiment information in the continuous representation of words.Specifically, we develop three neural networks to effectively incorporate the supervision from sentiment polarity of text (e.g.sentences or tweets) in their loss functions.To obtain large scale training corpora, we learn the sentiment-specific word embedding from massive distant-supervised tweets collected by positive and negative emoticons.Experiments on applying SSWE to a benchmark Twitter sentiment classification dataset in SemEval 2013 show that (1) the SSWE feature performs comparably with hand-crafted features in the top-performed system; (2) the performance is further improved by concatenating SSWE with existing feature set.", "rank": 1, "paragraph_comparative_number": 3, "entities": [], "id": "p_1"}, "sentences": [{"end": 106, "text": "We present a method that learns word embedding for Twitter sentiment classification in this paper.", "rank": 1, "start": 8, "IsComparative": "0", "id": "st_1"}, {"end": 261, "text": "Most existing algorithms for learning continuous word representations typically only model the syntactic context of words but ignore the sentiment of text.", "rank": 2, "start": 106, "IsComparative": "1", "id": "st_2"}, {"end": 444, "text": "This is problematic for sentiment analysis as they usually map words with similar syntactic context but opposite sentiment polarity, such as good and bad, to neighboring word vectors.", "rank": 3, "start": 261, "IsComparative": "1", "id": "st_3"}, {"end": 597, "text": "We address this issue by learning sentimentspecific word embedding (SSWE), which encodes sentiment information in the continuous representation of words.", "rank": 4, "start": 444, "IsComparative": "0", "id": "st_4"}, {"end": 724, "text": "Specifically, we develop three neural networks to effectively incorporate the supervision from sentiment polarity of text (e.g.", "rank": 5, "start": 597, "IsComparative": "0", "id": "st_5"}, {"end": 769, "text": "sentences or tweets) in their loss functions.", "rank": 6, "start": 724, "IsComparative": "0", "id": "st_6"}, {"end": 940, "text": "To obtain large scale training corpora, we learn the sentiment-specific word embedding from massive distant-supervised tweets collected by positive and negative emoticons.", "rank": 7, "start": 769, "IsComparative": "1", "id": "st_7"}, {"end": 1236, "text": "Experiments on applying SSWE to a benchmark Twitter sentiment classification dataset in SemEval 2013 show that (1) the SSWE feature performs comparably with hand-crafted features in the top-performed system; (2) the performance is further improved by concatenating SSWE with existing feature set.", "rank": 8, "start": 940, "IsComparative": "0", "id": "st_8"}]}, {"paragraph_info": {"end": 1250, "start": 1236, "text": "1 Introduction", "rank": 2, "paragraph_comparative_number": 1, "entities": [], "id": "p_2"}, "sentences": [{"end": 1250, "text": "1 Introduction", "rank": 9, "start": 1236, "IsComparative": "1", "id": "st_9"}]}, {"paragraph_info": {"end": 1989, "start": 1250, "text": "Twitter sentiment classification has attracted increasing research interest in recent years (Jiang et al., 2011; Hu et al., 2013).The objective is to classify the sentiment polarity of a tweet as positivenegative or neutral.The majority of existing approaches follow Pang et al.(2002) and employ machine learning algorithms to build classifiers from tweets with manually annotated sentiment polarity.Under this direction, most studies focus on designing effective features to obtain better classification performance.For example, Mohammad et al.(2013) build the top-performed system in the Twitter sentiment classification track of SemEval 2013 (Nakov et al., 2013), using diverse sentiment lexicons and a variety of hand-crafted features.", "rank": 3, "paragraph_comparative_number": 2, "entities": [], "id": "p_3"}, "sentences": [{"end": 1380, "text": "Twitter sentiment classification has attracted increasing research interest in recent years (Jiang et al., 2011; Hu et al., 2013).", "rank": 10, "start": 1250, "IsComparative": "0", "id": "st_10"}, {"end": 1474, "text": "The objective is to classify the sentiment polarity of a tweet as positivenegative or neutral.", "rank": 11, "start": 1380, "IsComparative": "0", "id": "st_11"}, {"end": 1528, "text": "The majority of existing approaches follow Pang et al.", "rank": 12, "start": 1474, "IsComparative": "1", "id": "st_12"}, {"end": 1650, "text": "(2002) and employ machine learning algorithms to build classifiers from tweets with manually annotated sentiment polarity.", "rank": 13, "start": 1528, "IsComparative": "0", "id": "st_13"}, {"end": 1767, "text": "Under this direction, most studies focus on designing effective features to obtain better classification performance.", "rank": 14, "start": 1650, "IsComparative": "1", "id": "st_14"}, {"end": 1795, "text": "For example, Mohammad et al.", "rank": 15, "start": 1767, "IsComparative": "0", "id": "st_15"}, {"end": 1989, "text": "(2013) build the top-performed system in the Twitter sentiment classification track of SemEval 2013 (Nakov et al., 2013), using diverse sentiment lexicons and a variety of hand-crafted features.", "rank": 16, "start": 1795, "IsComparative": "0", "id": "st_16"}]}, {"paragraph_info": {"end": 3287, "start": 1989, "text": "Feature engineering is important but laborintensive.It is therefore desirable to discover explanatory factors from the data and make the learning algorithms less dependent on extensive feature engineering (Bengio, 2013).For the task of sentiment classification, an effective feature learning method is to compose the representation of a sentence (or document) from the representations of the words or phrases it contains (Socher et al., 2013b; Yessenalina and Cardie, 2011).Accordingly, it is a crucial step to learn the word representation (or word embedding), which is a dense, low-dimensional and real-valued vector for a word.Although existing word embedding learning algorithms (Collobert et al., 2011; Mikolov et al., 2013) are intuitive choices, they are not effective enough if directly used for sentiment classi- fication.The most serious problem is that traditional methods typically model the syntactic context of words but ignore the sentiment information of text.As a result, words with opposite polarity, such as good and bad, are mapped into close vectors.It is meaningful for some tasks such as pos-tagging (Zheng et al., 2013) as the two words have similar usages and grammatical roles, but it becomes a disaster for sentiment analysis as they have the opposite sentiment polarity.", "rank": 4, "paragraph_comparative_number": 0, "entities": [], "id": "p_4"}, "sentences": [{"end": 2041, "text": "Feature engineering is important but laborintensive.", "rank": 17, "start": 1989, "IsComparative": "0", "id": "st_17"}, {"end": 2209, "text": "It is therefore desirable to discover explanatory factors from the data and make the learning algorithms less dependent on extensive feature engineering (Bengio, 2013).", "rank": 18, "start": 2041, "IsComparative": "0", "id": "st_18"}, {"end": 2463, "text": "For the task of sentiment classification, an effective feature learning method is to compose the representation of a sentence (or document) from the representations of the words or phrases it contains (Socher et al., 2013b; Yessenalina and Cardie, 2011).", "rank": 19, "start": 2209, "IsComparative": "0", "id": "st_19"}, {"end": 2619, "text": "Accordingly, it is a crucial step to learn the word representation (or word embedding), which is a dense, low-dimensional and real-valued vector for a word.", "rank": 20, "start": 2463, "IsComparative": "0", "id": "st_20"}, {"end": 2820, "text": "Although existing word embedding learning algorithms (Collobert et al., 2011; Mikolov et al., 2013) are intuitive choices, they are not effective enough if directly used for sentiment classi- fication.", "rank": 21, "start": 2619, "IsComparative": "0", "id": "st_21"}, {"end": 2965, "text": "The most serious problem is that traditional methods typically model the syntactic context of words but ignore the sentiment information of text.", "rank": 22, "start": 2820, "IsComparative": "0", "id": "st_22"}, {"end": 3060, "text": "As a result, words with opposite polarity, such as good and bad, are mapped into close vectors.", "rank": 23, "start": 2965, "IsComparative": "0", "id": "st_23"}, {"end": 3287, "text": "It is meaningful for some tasks such as pos-tagging (Zheng et al., 2013) as the two words have similar usages and grammatical roles, but it becomes a disaster for sentiment analysis as they have the opposite sentiment polarity.", "rank": 24, "start": 3060, "IsComparative": "0", "id": "st_24"}]}, {"paragraph_info": {"end": 4215, "start": 3287, "text": "In this paper, we propose learning sentimentspecific word embedding (SSWE) for sentiment analysis.We encode the sentiment information in-to the continuous representation of words, so that it is able to separate good and bad to opposite ends of the spectrum.To this end, we extend the existing word embedding learning algorithm (Collobert et al., 2011) and develop three neural networks to effectively incorporate the supervision from sentiment polarity of text (e.g.sentences or tweets) in their loss functions.We learn the sentiment-specific word embedding from tweets, leveraging massive tweets with emoticons as distant-supervised corpora without any manual annotations.These automatically collected tweets contain noises so they cannot be directly used as gold training data to build sentiment classifiers, but they are effective enough to provide weakly supervised signals for training the sentimentspecific word embedding.", "rank": 5, "paragraph_comparative_number": 0, "entities": [], "id": "p_5"}, "sentences": [{"end": 3385, "text": "In this paper, we propose learning sentimentspecific word embedding (SSWE) for sentiment analysis.", "rank": 25, "start": 3287, "IsComparative": "0", "id": "st_25"}, {"end": 3544, "text": "We encode the sentiment information in-to the continuous representation of words, so that it is able to separate good and bad to opposite ends of the spectrum.", "rank": 26, "start": 3385, "IsComparative": "0", "id": "st_26"}, {"end": 3753, "text": "To this end, we extend the existing word embedding learning algorithm (Collobert et al., 2011) and develop three neural networks to effectively incorporate the supervision from sentiment polarity of text (e.g.", "rank": 27, "start": 3544, "IsComparative": "0", "id": "st_27"}, {"end": 3798, "text": "sentences or tweets) in their loss functions.", "rank": 28, "start": 3753, "IsComparative": "0", "id": "st_28"}, {"end": 3960, "text": "We learn the sentiment-specific word embedding from tweets, leveraging massive tweets with emoticons as distant-supervised corpora without any manual annotations.", "rank": 29, "start": 3798, "IsComparative": "0", "id": "st_29"}, {"end": 4215, "text": "These automatically collected tweets contain noises so they cannot be directly used as gold training data to build sentiment classifiers, but they are effective enough to provide weakly supervised signals for training the sentimentspecific word embedding.", "rank": 30, "start": 3960, "IsComparative": "0", "id": "st_30"}]}, {"paragraph_info": {"end": 4992, "start": 4215, "text": "We apply SSWE as features in a supervised learning framework for Twitter sentiment classi- fication, and evaluate it on the benchmark dataset in SemEval 2013.In the task of predicting positive/negative polarity of tweets, our method yields 84.89% in macro-F1 by only using SSWE as feature, which is comparable to the top-performed system based on hand-crafted features (84.70%).After concatenating the SSWE feature with existing feature set, we push the state-of-the-art to 86.58% in macro-F1.The quality of SSWE is also directly evaluated by measuring the word similarity in the embedding space for sentiment lexicons.In the accuracy of polarity consistency between each sentiment word and its top N closest words, SSWE outperforms existing word embedding learning algorithms.", "rank": 6, "paragraph_comparative_number": 0, "entities": [], "id": "p_6"}, "sentences": [{"end": 4373, "text": "We apply SSWE as features in a supervised learning framework for Twitter sentiment classi- fication, and evaluate it on the benchmark dataset in SemEval 2013.", "rank": 31, "start": 4215, "IsComparative": "0", "id": "st_31"}, {"end": 4593, "text": "In the task of predicting positive/negative polarity of tweets, our method yields 84.89% in macro-F1 by only using SSWE as feature, which is comparable to the top-performed system based on hand-crafted features (84.70%).", "rank": 32, "start": 4373, "IsComparative": "0", "id": "st_32"}, {"end": 4708, "text": "After concatenating the SSWE feature with existing feature set, we push the state-of-the-art to 86.58% in macro-F1.", "rank": 33, "start": 4593, "IsComparative": "0", "id": "st_33"}, {"end": 4834, "text": "The quality of SSWE is also directly evaluated by measuring the word similarity in the embedding space for sentiment lexicons.", "rank": 34, "start": 4708, "IsComparative": "0", "id": "st_34"}, {"end": 4992, "text": "In the accuracy of polarity consistency between each sentiment word and its top N closest words, SSWE outperforms existing word embedding learning algorithms.", "rank": 35, "start": 4834, "IsComparative": "0", "id": "st_35"}]}, {"paragraph_info": {"end": 5067, "start": 4992, "text": "The major contributions of the work presented in this paper are as follows.", "rank": 7, "paragraph_comparative_number": 0, "entities": [], "id": "p_7"}, "sentences": [{"end": 5067, "text": "The major contributions of the work presented in this paper are as follows.", "rank": 36, "start": 4992, "IsComparative": "0", "id": "st_36"}]}, {"paragraph_info": {"end": 5220, "start": 5067, "text": "We develop three neural networks to learn sentiment-specific word embedding (SSWE) from massive distant-supervised tweets without any manual annotations;", "rank": 8, "paragraph_comparative_number": 0, "entities": [], "id": "p_8"}, "sentences": [{"end": 5220, "text": "We develop three neural networks to learn sentiment-specific word embedding (SSWE) from massive distant-supervised tweets without any manual annotations;", "rank": 37, "start": 5067, "IsComparative": "0", "id": "st_37"}]}, {"paragraph_info": {"end": 5462, "start": 5220, "text": "To our knowledge, this is the first work that exploits word embedding for Twitter sentiment classification.We report the results that the SSWE feature performs comparably with hand-crafted features in the top-performed system in SemEval 2013;", "rank": 9, "paragraph_comparative_number": 0, "entities": [], "id": "p_9"}, "sentences": [{"end": 5327, "text": "To our knowledge, this is the first work that exploits word embedding for Twitter sentiment classification.", "rank": 38, "start": 5220, "IsComparative": "0", "id": "st_38"}, {"end": 5462, "text": "We report the results that the SSWE feature performs comparably with hand-crafted features in the top-performed system in SemEval 2013;", "rank": 39, "start": 5327, "IsComparative": "0", "id": "st_39"}]}, {"paragraph_info": {"end": 5612, "start": 5462, "text": "We release the sentiment-specific word embedding learned from 10 million tweets, which can be adopted off-the-shell in other sentiment analysis tasks.", "rank": 10, "paragraph_comparative_number": 0, "entities": [], "id": "p_10"}, "sentences": [{"end": 5612, "text": "We release the sentiment-specific word embedding learned from 10 million tweets, which can be adopted off-the-shell in other sentiment analysis tasks.", "rank": 40, "start": 5462, "IsComparative": "0", "id": "st_40"}]}, {"paragraph_info": {"end": 5626, "start": 5612, "text": "2 Related Work", "rank": 11, "paragraph_comparative_number": 0, "entities": [], "id": "p_11"}, "sentences": [{"end": 5626, "text": "2 Related Work", "rank": 41, "start": 5612, "IsComparative": "0", "id": "st_41"}]}, {"paragraph_info": {"end": 5814, "start": 5626, "text": "In this section, we present a brief review of the related work from two perspectives, Twitter sentiment classification and learning continuous representations for sentiment classification.", "rank": 12, "paragraph_comparative_number": 0, "entities": [], "id": "p_12"}, "sentences": [{"end": 5814, "text": "In this section, we present a brief review of the related work from two perspectives, Twitter sentiment classification and learning continuous representations for sentiment classification.", "rank": 42, "start": 5626, "IsComparative": "0", "id": "st_42"}]}, {"paragraph_info": {"end": 5850, "start": 5814, "text": "2.1 Twitter Sentiment Classification", "rank": 13, "paragraph_comparative_number": 0, "entities": [], "id": "p_13"}, "sentences": [{"end": 5850, "text": "2.1 Twitter Sentiment Classification", "rank": 43, "start": 5814, "IsComparative": "0", "id": "st_43"}]}, {"paragraph_info": {"end": 6477, "start": 5850, "text": "Twitter sentiment classification, which identifies the sentiment polarity of short, informal tweets, has attracted increasing research interest (Jiang et al., 2011; Hu et al., 2013) in recent years.Generally, the methods employed in Twitter sentiment classification follow traditional sentiment classifi- cation approaches.The lexicon-based approaches (Turney, 2002; Ding et al., 2008; Taboada et al., 2011; Thelwall et al., 2012) mostly use a dictionary of sentiment words with their associated sentiment polarity, and incorporate negation and intensification to compute the sentiment polarity for each sentence (or document).", "rank": 14, "paragraph_comparative_number": 0, "entities": [], "id": "p_14"}, "sentences": [{"end": 6048, "text": "Twitter sentiment classification, which identifies the sentiment polarity of short, informal tweets, has attracted increasing research interest (Jiang et al., 2011; Hu et al., 2013) in recent years.", "rank": 44, "start": 5850, "IsComparative": "0", "id": "st_44"}, {"end": 6173, "text": "Generally, the methods employed in Twitter sentiment classification follow traditional sentiment classifi- cation approaches.", "rank": 45, "start": 6048, "IsComparative": "0", "id": "st_45"}, {"end": 6477, "text": "The lexicon-based approaches (Turney, 2002; Ding et al., 2008; Taboada et al., 2011; Thelwall et al., 2012) mostly use a dictionary of sentiment words with their associated sentiment polarity, and incorporate negation and intensification to compute the sentiment polarity for each sentence (or document).", "rank": 46, "start": 6173, "IsComparative": "0", "id": "st_46"}]}, {"paragraph_info": {"end": 7303, "start": 6477, "text": "The learning based methods for Twitter sentiment classification follow Pang et al.(2002)s work, which treat sentiment classification of texts as a special case of text categorization issue.Many studies on Twitter sentiment classification (Pak and Paroubek, 2010; Davidov et al., 2010; Barbosa and Feng, 2010; Kouloumpis et al., 2011; Zhao et al., 2012) leverage massive noisy-labeled tweets selected by positive and negative emoticons as training set and build sentiment classifiers directly, which is called distant supervision (Go et al., 2009).Instead of directly using the distantsupervised data as training set, Liu et al.(2012) adopt the tweets with emoticons to smooth the language model and Hu et al.(2013) incorporate the emotional signals into an unsupervised learning framework for Twitter sentiment classification.", "rank": 15, "paragraph_comparative_number": 0, "entities": [], "id": "p_15"}, "sentences": [{"end": 6559, "text": "The learning based methods for Twitter sentiment classification follow Pang et al.", "rank": 47, "start": 6477, "IsComparative": "0", "id": "st_47"}, {"end": 6666, "text": "(2002)s work, which treat sentiment classification of texts as a special case of text categorization issue.", "rank": 48, "start": 6559, "IsComparative": "0", "id": "st_48"}, {"end": 7024, "text": "Many studies on Twitter sentiment classification (Pak and Paroubek, 2010; Davidov et al., 2010; Barbosa and Feng, 2010; Kouloumpis et al., 2011; Zhao et al., 2012) leverage massive noisy-labeled tweets selected by positive and negative emoticons as training set and build sentiment classifiers directly, which is called distant supervision (Go et al., 2009).", "rank": 49, "start": 6666, "IsComparative": "0", "id": "st_49"}, {"end": 7104, "text": "Instead of directly using the distantsupervised data as training set, Liu et al.", "rank": 50, "start": 7024, "IsComparative": "0", "id": "st_50"}, {"end": 7185, "text": "(2012) adopt the tweets with emoticons to smooth the language model and Hu et al.", "rank": 51, "start": 7104, "IsComparative": "0", "id": "st_51"}, {"end": 7303, "text": "(2013) incorporate the emotional signals into an unsupervised learning framework for Twitter sentiment classification.", "rank": 52, "start": 7185, "IsComparative": "0", "id": "st_52"}]}, {"paragraph_info": {"end": 7904, "start": 7303, "text": "Many existing learning based methods on Twitter sentiment classification focus on feature engineering.The reason is that the performance of sentiment classifier being heavily dependent on the choice of feature representation of tweets.The most representative system is introduced by Mohammad et al.(2013), which is the state-of-theart system (the top-performed system in SemEval 2013 Twitter Sentiment Classification Track) by implementing a number of hand-crafted features.Unlike the previous studies, we focus on learning discriminative features automatically from massive distant-supervised tweets.", "rank": 16, "paragraph_comparative_number": 0, "entities": [], "id": "p_16"}, "sentences": [{"end": 7405, "text": "Many existing learning based methods on Twitter sentiment classification focus on feature engineering.", "rank": 53, "start": 7303, "IsComparative": "0", "id": "st_53"}, {"end": 7538, "text": "The reason is that the performance of sentiment classifier being heavily dependent on the choice of feature representation of tweets.", "rank": 54, "start": 7405, "IsComparative": "0", "id": "st_54"}, {"end": 7601, "text": "The most representative system is introduced by Mohammad et al.", "rank": 55, "start": 7538, "IsComparative": "0", "id": "st_55"}, {"end": 7777, "text": "(2013), which is the state-of-theart system (the top-performed system in SemEval 2013 Twitter Sentiment Classification Track) by implementing a number of hand-crafted features.", "rank": 56, "start": 7601, "IsComparative": "0", "id": "st_56"}, {"end": 7904, "text": "Unlike the previous studies, we focus on learning discriminative features automatically from massive distant-supervised tweets.", "rank": 57, "start": 7777, "IsComparative": "0", "id": "st_57"}]}, {"paragraph_info": {"end": 7972, "start": 7904, "text": "2.2 Learning Continuous Representations for Sentiment Classification", "rank": 17, "paragraph_comparative_number": 0, "entities": [], "id": "p_17"}, "sentences": [{"end": 7972, "text": "2.2 Learning Continuous Representations for Sentiment Classification", "rank": 58, "start": 7904, "IsComparative": "0", "id": "st_58"}]}, {"paragraph_info": {"end": 8472, "start": 7972, "text": "Pang et al.(2002) pioneer this field by using bagof-word representation, representing each word as a one-hot vector.It has the same length as the size of the vocabulary, and only one dimension is 1, with all others being 0.Under this assumption, many feature learning algorithms are proposed to obtain better classification performance (Pang and Lee, 2008; Liu, 2012; Feldman, 2013).However, the one-hot word representation cannot sufficiently capture the complex linguistic characteristics of words.", "rank": 18, "paragraph_comparative_number": 2, "entities": [], "id": "p_18"}, "sentences": [{"end": 7983, "text": "Pang et al.", "rank": 59, "start": 7972, "IsComparative": "0", "id": "st_59"}, {"end": 8088, "text": "(2002) pioneer this field by using bagof-word representation, representing each word as a one-hot vector.", "rank": 60, "start": 7983, "IsComparative": "0", "id": "st_60"}, {"end": 8195, "text": "It has the same length as the size of the vocabulary, and only one dimension is 1, with all others being 0.", "rank": 61, "start": 8088, "IsComparative": "0", "id": "st_61"}, {"end": 8355, "text": "Under this assumption, many feature learning algorithms are proposed to obtain better classification performance (Pang and Lee, 2008; Liu, 2012; Feldman, 2013).", "rank": 62, "start": 8195, "IsComparative": "1", "id": "st_62"}, {"end": 8472, "text": "However, the one-hot word representation cannot sufficiently capture the complex linguistic characteristics of words.", "rank": 63, "start": 8355, "IsComparative": "1", "id": "st_63"}]}, {"paragraph_info": {"end": 9683, "start": 8472, "text": "With the revival of interest in deep learning (Bengio et al., 2013), incorporating the continuous representation of a word as features has been proving effective in a variety of NLP tasks, such as parsing (Socher et al., 2013a), language modeling (Bengio et al., 2003; Mnih and Hinton, 2009) and NER (Turian et al., 2010).In the field of sentiment analysis, Bespalov et al.(2011; 2012) initialize the word embedding by Latent Semantic Analysis and further represent each document as the linear weighted of ngram vectors for sentiment classification.Yessenalina and Cardie (2011) model each word as a matrix and combine words using iterated matrix multiplication.Glorot et al.(2011) explore Stacked Denoising Autoencoders for domain adaptation in sentiment classification.Socher et al.propose Recursive Neural Network (RNN) (2011b), matrixvector RNN (2012) and Recursive Neural Tensor Network (RNTN) (2013b) to learn the compositionality of phrases of any length based on the representation of each pair of children recursively.Hermann et al.(2013) present Combinatory Categorial Autoencoders to learn the compositionality of sentence, which marries the Combinatory Categorial Grammar with Recursive Autoencoder.", "rank": 19, "paragraph_comparative_number": 0, "entities": [], "id": "p_19"}, "sentences": [{"end": 8794, "text": "With the revival of interest in deep learning (Bengio et al., 2013), incorporating the continuous representation of a word as features has been proving effective in a variety of NLP tasks, such as parsing (Socher et al., 2013a), language modeling (Bengio et al., 2003; Mnih and Hinton, 2009) and NER (Turian et al., 2010).", "rank": 64, "start": 8472, "IsComparative": "0", "id": "st_64"}, {"end": 8845, "text": "In the field of sentiment analysis, Bespalov et al.", "rank": 65, "start": 8794, "IsComparative": "0", "id": "st_65"}, {"end": 9021, "text": "(2011; 2012) initialize the word embedding by Latent Semantic Analysis and further represent each document as the linear weighted of ngram vectors for sentiment classification.", "rank": 66, "start": 8845, "IsComparative": "0", "id": "st_66"}, {"end": 9134, "text": "Yessenalina and Cardie (2011) model each word as a matrix and combine words using iterated matrix multiplication.", "rank": 67, "start": 9021, "IsComparative": "0", "id": "st_67"}, {"end": 9147, "text": "Glorot et al.", "rank": 68, "start": 9134, "IsComparative": "0", "id": "st_68"}, {"end": 9243, "text": "(2011) explore Stacked Denoising Autoencoders for domain adaptation in sentiment classification.", "rank": 69, "start": 9147, "IsComparative": "0", "id": "st_69"}, {"end": 9256, "text": "Socher et al.", "rank": 70, "start": 9243, "IsComparative": "0", "id": "st_70"}, {"end": 9499, "text": "propose Recursive Neural Network (RNN) (2011b), matrixvector RNN (2012) and Recursive Neural Tensor Network (RNTN) (2013b) to learn the compositionality of phrases of any length based on the representation of each pair of children recursively.", "rank": 71, "start": 9256, "IsComparative": "0", "id": "st_71"}, {"end": 9513, "text": "Hermann et al.", "rank": 72, "start": 9499, "IsComparative": "0", "id": "st_72"}, {"end": 9683, "text": "(2013) present Combinatory Categorial Autoencoders to learn the compositionality of sentence, which marries the Combinatory Categorial Grammar with Recursive Autoencoder.", "rank": 73, "start": 9513, "IsComparative": "0", "id": "st_73"}]}, {"paragraph_info": {"end": 10536, "start": 9683, "text": "The representation of words heavily relies on the applications or tasks in which it is used (Labutov and Lipson, 2013).This paper focuses on learning sentiment-specific word embedding, which is tailored for sentiment analysis.Unlike Maas et al.(2011) that follow the probabilistic document model (Blei et al., 2003) and give an sentiment predictor function to each word, we develop neural networks and map each ngram to the sentiment polarity of sentence.Unlike Socher et al.(2011c) that utilize manually labeled texts to learn the meaning of phrase (or sentence) through compositionality, we focus on learning the meaning of word, namely word embedding, from massive distant-supervised tweets.Unlike Labutov and Lipson (2013) that produce task-specific embedding from an existing word embedding, we learn sentiment-specific word embedding from scratch.", "rank": 20, "paragraph_comparative_number": 0, "entities": [], "id": "p_20"}, "sentences": [{"end": 9802, "text": "The representation of words heavily relies on the applications or tasks in which it is used (Labutov and Lipson, 2013).", "rank": 74, "start": 9683, "IsComparative": "0", "id": "st_74"}, {"end": 9909, "text": "This paper focuses on learning sentiment-specific word embedding, which is tailored for sentiment analysis.", "rank": 75, "start": 9802, "IsComparative": "0", "id": "st_75"}, {"end": 9927, "text": "Unlike Maas et al.", "rank": 76, "start": 9909, "IsComparative": "0", "id": "st_76"}, {"end": 10138, "text": "(2011) that follow the probabilistic document model (Blei et al., 2003) and give an sentiment predictor function to each word, we develop neural networks and map each ngram to the sentiment polarity of sentence.", "rank": 77, "start": 9927, "IsComparative": "0", "id": "st_77"}, {"end": 10158, "text": "Unlike Socher et al.", "rank": 78, "start": 10138, "IsComparative": "0", "id": "st_78"}, {"end": 10377, "text": "(2011c) that utilize manually labeled texts to learn the meaning of phrase (or sentence) through compositionality, we focus on learning the meaning of word, namely word embedding, from massive distant-supervised tweets.", "rank": 79, "start": 10158, "IsComparative": "0", "id": "st_79"}, {"end": 10536, "text": "Unlike Labutov and Lipson (2013) that produce task-specific embedding from an existing word embedding, we learn sentiment-specific word embedding from scratch.", "rank": 80, "start": 10377, "IsComparative": "0", "id": "st_80"}]}, {"paragraph_info": {"end": 10571, "start": 10536, "text": "3 Sentiment-Specific Word Embedding", "rank": 21, "paragraph_comparative_number": 0, "entities": [], "id": "p_21"}, "sentences": [{"end": 10571, "text": "3 Sentiment-Specific Word Embedding", "rank": 81, "start": 10536, "IsComparative": "0", "id": "st_81"}]}, {"paragraph_info": {"end": 11216, "start": 10571, "text": "for Twitter Sentiment Classification In this section, we present the details of learning sentiment-specific word embedding (SSWE) for Twitter sentiment classification.We propose incorporating the sentiment information of sentences to learn continuous representations for words and phrases.We extend the existing word embedding learning algorithm (Collobert et al., 2011) and develop three neural networks to learn SSWE.In the following sections, we introduce the traditional method before presenting the details of SSWE learning algorithms.We then describe the use of SSWE in a supervised learning framework for Twitter sentiment classification.", "rank": 22, "paragraph_comparative_number": 0, "entities": [], "id": "p_22"}, "sentences": [{"end": 10738, "text": "for Twitter Sentiment Classification In this section, we present the details of learning sentiment-specific word embedding (SSWE) for Twitter sentiment classification.", "rank": 82, "start": 10571, "IsComparative": "0", "id": "st_82"}, {"end": 10860, "text": "We propose incorporating the sentiment information of sentences to learn continuous representations for words and phrases.", "rank": 83, "start": 10738, "IsComparative": "0", "id": "st_83"}, {"end": 10990, "text": "We extend the existing word embedding learning algorithm (Collobert et al., 2011) and develop three neural networks to learn SSWE.", "rank": 84, "start": 10860, "IsComparative": "0", "id": "st_84"}, {"end": 11111, "text": "In the following sections, we introduce the traditional method before presenting the details of SSWE learning algorithms.", "rank": 85, "start": 10990, "IsComparative": "0", "id": "st_85"}, {"end": 11216, "text": "We then describe the use of SSWE in a supervised learning framework for Twitter sentiment classification.", "rank": 86, "start": 11111, "IsComparative": "0", "id": "st_86"}]}, {"paragraph_info": {"end": 11229, "start": 11216, "text": "3.1 C&W Model", "rank": 23, "paragraph_comparative_number": 0, "entities": [], "id": "p_23"}, "sentences": [{"end": 11229, "text": "3.1 C&W Model", "rank": 87, "start": 11216, "IsComparative": "0", "id": "st_87"}]}, {"paragraph_info": {"end": 12441, "start": 11229, "text": "Collobert et al.(2011) introduce C&W model to learn word embedding based on the syntactic contexts of words.Given an ngram cat chills on a mat, C&W replaces the center word with a random word w r and derives a corrupted ngram cat chills w r a mat.The training objective is that the original ngram is expected to obtain a higher language model score than the corrupted ngram by a margin of 1.The ranking objective function can be optimized by a hinge loss, losscw(t, tr ) = max(0, 1  f cw(t) + f cw(t r )) (1) where t is the original ngram, t r is the corrupted ngram, f cw() is a one-dimensional scalar representing the language model score of the input ngram.Figure 1(a) illustrates the neural architecture of C&W, which consists of four layers, namely lookup  linear  hT anh  linear (from bottom to top).The original and corrupted ngrams are treated as inputs of the feed-forward neural network, respectively.The output f cw is the language model score of the input, which is calculated as given in Equation 2, where L is the lookup table of word embedding, w1, w2, b1, b2 are the parameters of linear layers.f cw(t) = w2(a) + b2 a = hT anh(w1Lt + b1) (3) hT anh(x) =    1 if x < 1 x if  1  x  1 1 if x > 1 (4)", "rank": 24, "paragraph_comparative_number": 0, "entities": [], "id": "p_24"}, "sentences": [{"end": 11245, "text": "Collobert et al.", "rank": 88, "start": 11229, "IsComparative": "0", "id": "st_88"}, {"end": 11337, "text": "(2011) introduce C&W model to learn word embedding based on the syntactic contexts of words.", "rank": 89, "start": 11245, "IsComparative": "0", "id": "st_89"}, {"end": 11476, "text": "Given an ngram cat chills on a mat, C&W replaces the center word with a random word w r and derives a corrupted ngram cat chills w r a mat.", "rank": 90, "start": 11337, "IsComparative": "0", "id": "st_90"}, {"end": 11620, "text": "The training objective is that the original ngram is expected to obtain a higher language model score than the corrupted ngram by a margin of 1.", "rank": 91, "start": 11476, "IsComparative": "0", "id": "st_91"}, {"end": 11889, "text": "The ranking objective function can be optimized by a hinge loss, losscw(t, tr ) = max(0, 1  f cw(t) + f cw(t r )) (1) where t is the original ngram, t r is the corrupted ngram, f cw() is a one-dimensional scalar representing the language model score of the input ngram.", "rank": 92, "start": 11620, "IsComparative": "0", "id": "st_92"}, {"end": 12035, "text": "Figure 1(a) illustrates the neural architecture of C&W, which consists of four layers, namely lookup  linear  hT anh  linear (from bottom to top).", "rank": 93, "start": 11889, "IsComparative": "0", "id": "st_93"}, {"end": 12140, "text": "The original and corrupted ngrams are treated as inputs of the feed-forward neural network, respectively.", "rank": 94, "start": 12035, "IsComparative": "0", "id": "st_94"}, {"end": 12340, "text": "The output f cw is the language model score of the input, which is calculated as given in Equation 2, where L is the lookup table of word embedding, w1, w2, b1, b2 are the parameters of linear layers.", "rank": 95, "start": 12140, "IsComparative": "0", "id": "st_95"}, {"end": 12441, "text": "f cw(t) = w2(a) + b2 a = hT anh(w1Lt + b1) (3) hT anh(x) =    1 if x < 1 x if  1  x  1 1 if x > 1 (4)", "rank": 96, "start": 12340, "IsComparative": "0", "id": "st_96"}]}, {"paragraph_info": {"end": 12478, "start": 12441, "text": "3.2 Sentiment-Specific Word Embedding", "rank": 25, "paragraph_comparative_number": 0, "entities": [], "id": "p_25"}, "sentences": [{"end": 12478, "text": "3.2 Sentiment-Specific Word Embedding", "rank": 97, "start": 12441, "IsComparative": "0", "id": "st_97"}]}, {"paragraph_info": {"end": 12755, "start": 12478, "text": "Following the traditional C&W model (Collobert et al., 2011), we incorporate the sentiment information into the neural network to learn sentimentspecific word embedding.We develop three neural networks with different strategies to integrate the sentiment information of tweets.", "rank": 26, "paragraph_comparative_number": 0, "entities": [], "id": "p_26"}, "sentences": [{"end": 12647, "text": "Following the traditional C&W model (Collobert et al., 2011), we incorporate the sentiment information into the neural network to learn sentimentspecific word embedding.", "rank": 98, "start": 12478, "IsComparative": "0", "id": "st_98"}, {"end": 12755, "text": "We develop three neural networks with different strategies to integrate the sentiment information of tweets.", "rank": 99, "start": 12647, "IsComparative": "0", "id": "st_99"}]}, {"paragraph_info": {"end": 13480, "start": 12755, "text": "Basic Model 1 (SSWEh).As an unsupervised approach, C&W model does not explicitly capture the sentiment information of texts.An intuitive solution to integrate the sentiment information is predicting the sentiment distribution of text based on input ngram.We do not utilize the entire sentence as input because the length of different sentences might be variant.We therefore slide the window of ngram across a sentence, and then predict the sentiment polarity based on each ngram with a shared neural network.In the neural network, the distributed representation of higher layer are interpreted as features describing the input.Thus, we utilize the continuous vector of top layer to predict the sentiment distribution of text.", "rank": 27, "paragraph_comparative_number": 0, "entities": [], "id": "p_27"}, "sentences": [{"end": 12777, "text": "Basic Model 1 (SSWEh).", "rank": 100, "start": 12755, "IsComparative": "0", "id": "st_100"}, {"end": 12879, "text": "As an unsupervised approach, C&W model does not explicitly capture the sentiment information of texts.", "rank": 101, "start": 12777, "IsComparative": "0", "id": "st_101"}, {"end": 13010, "text": "An intuitive solution to integrate the sentiment information is predicting the sentiment distribution of text based on input ngram.", "rank": 102, "start": 12879, "IsComparative": "0", "id": "st_102"}, {"end": 13116, "text": "We do not utilize the entire sentence as input because the length of different sentences might be variant.", "rank": 103, "start": 13010, "IsComparative": "0", "id": "st_103"}, {"end": 13263, "text": "We therefore slide the window of ngram across a sentence, and then predict the sentiment polarity based on each ngram with a shared neural network.", "rank": 104, "start": 13116, "IsComparative": "0", "id": "st_104"}, {"end": 13382, "text": "In the neural network, the distributed representation of higher layer are interpreted as features describing the input.", "rank": 105, "start": 13263, "IsComparative": "0", "id": "st_105"}, {"end": 13480, "text": "Thus, we utilize the continuous vector of top layer to predict the sentiment distribution of text.", "rank": 106, "start": 13382, "IsComparative": "0", "id": "st_106"}]}, {"paragraph_info": {"end": 14295, "start": 13480, "text": "Assuming there are K labels, we modify the dimension of top layer in C&W model as K and add a sof tmax layer upon the top layer.The neural network (SSWEh) is given in Figure 1(b).Sof tmax layer is suitable for this scenario because its outputs are interpreted as conditional probabilities.Unlike C&W, SSWEh does not generate any corrupted ngram.Let f g (t), where K denotes the number of sentiment polarity labels, be the gold K-dimensional multinomial distribution of input t and P k f g k (t) = 1.For positive/negative classification, the distribution is of the form <1,0> for positive and <0,1> for negative.The cross-entropy error of the sof tmax layer is : lossh(t) =  X k=<0,1> f g k (t)  log(f h k (t)) (5) where f g (t) is the gold sentiment distribution and f h (t) is the predicted sentiment distribution.", "rank": 28, "paragraph_comparative_number": 0, "entities": [], "id": "p_28"}, "sentences": [{"end": 13608, "text": "Assuming there are K labels, we modify the dimension of top layer in C&W model as K and add a sof tmax layer upon the top layer.", "rank": 107, "start": 13480, "IsComparative": "0", "id": "st_107"}, {"end": 13659, "text": "The neural network (SSWEh) is given in Figure 1(b).", "rank": 108, "start": 13608, "IsComparative": "0", "id": "st_108"}, {"end": 13769, "text": "Sof tmax layer is suitable for this scenario because its outputs are interpreted as conditional probabilities.", "rank": 109, "start": 13659, "IsComparative": "0", "id": "st_109"}, {"end": 13825, "text": "Unlike C&W, SSWEh does not generate any corrupted ngram.", "rank": 110, "start": 13769, "IsComparative": "0", "id": "st_110"}, {"end": 13979, "text": "Let f g (t), where K denotes the number of sentiment polarity labels, be the gold K-dimensional multinomial distribution of input t and P k f g k (t) = 1.", "rank": 111, "start": 13825, "IsComparative": "0", "id": "st_111"}, {"end": 14091, "text": "For positive/negative classification, the distribution is of the form <1,0> for positive and <0,1> for negative.", "rank": 112, "start": 13979, "IsComparative": "0", "id": "st_112"}, {"end": 14295, "text": "The cross-entropy error of the sof tmax layer is : lossh(t) =  X k=<0,1> f g k (t)  log(f h k (t)) (5) where f g (t) is the gold sentiment distribution and f h (t) is the predicted sentiment distribution.", "rank": 113, "start": 14091, "IsComparative": "0", "id": "st_113"}]}, {"paragraph_info": {"end": 14935, "start": 14295, "text": "Basic Model 2 (SSWEr).SSWEh is trained by predicting the positive ngram as <1,0> and the negative ngram as <0,1>.However, the constraint of SSWEh is too strict.The distribution of <0.7,0.3> can also be interpreted as a positive label because the positive score is larger than the negative score.Similarly, the distribution of <0.2,0.8> indicates negative polarity.Based on the above observation, the hard constraints in SSWEh should be relaxed.If the sentiment polarity of a tweet is positive, the predicted positive score is expected to be larger than the predicted negative score, and the exact reverse if the tweet has negative polarity.", "rank": 29, "paragraph_comparative_number": 0, "entities": [], "id": "p_29"}, "sentences": [{"end": 14317, "text": "Basic Model 2 (SSWEr).", "rank": 114, "start": 14295, "IsComparative": "0", "id": "st_114"}, {"end": 14408, "text": "SSWEh is trained by predicting the positive ngram as <1,0> and the negative ngram as <0,1>.", "rank": 115, "start": 14317, "IsComparative": "0", "id": "st_115"}, {"end": 14455, "text": "However, the constraint of SSWEh is too strict.", "rank": 116, "start": 14408, "IsComparative": "0", "id": "st_116"}, {"end": 14590, "text": "The distribution of <0.7,0.3> can also be interpreted as a positive label because the positive score is larger than the negative score.", "rank": 117, "start": 14455, "IsComparative": "0", "id": "st_117"}, {"end": 14659, "text": "Similarly, the distribution of <0.2,0.8> indicates negative polarity.", "rank": 118, "start": 14590, "IsComparative": "0", "id": "st_118"}, {"end": 14739, "text": "Based on the above observation, the hard constraints in SSWEh should be relaxed.", "rank": 119, "start": 14659, "IsComparative": "0", "id": "st_119"}, {"end": 14935, "text": "If the sentiment polarity of a tweet is positive, the predicted positive score is expected to be larger than the predicted negative score, and the exact reverse if the tweet has negative polarity.", "rank": 120, "start": 14739, "IsComparative": "0", "id": "st_120"}]}, {"paragraph_info": {"end": 15658, "start": 14935, "text": "We model the relaxed constraint with a ranking objective function and borrow the bottom four layers from SSWEh, namely lookup  linear  hT anh  linear in Figure 1(b), to build the relaxed neural network (SSWEr).Compared with SSWEh, the sof tmax layer is removed because SSWEr does not require probabilistic interpretation.The hinge loss of SSWEr is modeled as de- scribed below.lossr(t) = max(0, 1  s(t)f r 0 (t) + s(t)f r 1 (t) ) (6) where f r 0 is the predicted positive score, f r 1 is the predicted negative score, s(t) is an indicator function reflecting the sentiment polarity of a sentence, s(t) = ( 1 if f g (t) = <1, 0> 1 if f g (t) = <0, 1> (7) Similar with SSWEh, SSWEr also does not generate the corrupted ngram.", "rank": 30, "paragraph_comparative_number": 0, "entities": [], "id": "p_30"}, "sentences": [{"end": 15145, "text": "We model the relaxed constraint with a ranking objective function and borrow the bottom four layers from SSWEh, namely lookup  linear  hT anh  linear in Figure 1(b), to build the relaxed neural network (SSWEr).", "rank": 121, "start": 14935, "IsComparative": "0", "id": "st_121"}, {"end": 15256, "text": "Compared with SSWEh, the sof tmax layer is removed because SSWEr does not require probabilistic interpretation.", "rank": 122, "start": 15145, "IsComparative": "0", "id": "st_122"}, {"end": 15312, "text": "The hinge loss of SSWEr is modeled as de- scribed below.", "rank": 123, "start": 15256, "IsComparative": "0", "id": "st_123"}, {"end": 15658, "text": "lossr(t) = max(0, 1  s(t)f r 0 (t) + s(t)f r 1 (t) ) (6) where f r 0 is the predicted positive score, f r 1 is the predicted negative score, s(t) is an indicator function reflecting the sentiment polarity of a sentence, s(t) = ( 1 if f g (t) = <1, 0> 1 if f g (t) = <0, 1> (7) Similar with SSWEh, SSWEr also does not generate the corrupted ngram.", "rank": 124, "start": 15312, "IsComparative": "0", "id": "st_124"}]}, {"paragraph_info": {"end": 16143, "start": 15658, "text": "Unified Model (SSWEu).The C&W model learns word embedding by modeling syntactic contexts of words but ignoring sentiment information.By contrast, SSWEh and SSWEr learn sentiment-specific word embedding by integrating the sentiment polarity of sentences but leaving out the syntactic contexts of words.We develop a uni- fied model (SSWEu) in this part, which captures the sentiment information of sentences as well as the syntactic contexts of words.SSWEu is illustrated in Figure 1(c).", "rank": 31, "paragraph_comparative_number": 1, "entities": [], "id": "p_31"}, "sentences": [{"end": 15680, "text": "Unified Model (SSWEu).", "rank": 125, "start": 15658, "IsComparative": "0", "id": "st_125"}, {"end": 15791, "text": "The C&W model learns word embedding by modeling syntactic contexts of words but ignoring sentiment information.", "rank": 126, "start": 15680, "IsComparative": "0", "id": "st_126"}, {"end": 15959, "text": "By contrast, SSWEh and SSWEr learn sentiment-specific word embedding by integrating the sentiment polarity of sentences but leaving out the syntactic contexts of words.", "rank": 127, "start": 15791, "IsComparative": "0", "id": "st_127"}, {"end": 16107, "text": "We develop a uni- fied model (SSWEu) in this part, which captures the sentiment information of sentences as well as the syntactic contexts of words.", "rank": 128, "start": 15959, "IsComparative": "1", "id": "st_128"}, {"end": 16143, "text": "SSWEu is illustrated in Figure 1(c).", "rank": 129, "start": 16107, "IsComparative": "0", "id": "st_129"}]}, {"paragraph_info": {"end": 17106, "start": 16143, "text": "Given an original (or corrupted) ngram and the sentiment polarity of a sentence as the input, SSWEu predicts a two-dimensional vector for each input ngram.The two scalars (f u 0 , f u 1 ) stand for language model score and sentiment score of the input ngram, respectively.The training objectives of SSWEu are that (1) the original ngram should obtain a higher language model score f u 0 (t)than the corrupted ngram f u 0 (t r ), and (2) the sentiment score of original ngram f u 1 (t) should be more consistent with the gold polarity annotation of sentence than corrupted ngram f u 1 (t r ).The loss function of SSWEu is the linear combination of two hinge losses, lossu(t, tr ) =   losscw(t, tr )+ (1  )  lossus(t, tr ) (8) where losscw(t, tr ) is the syntactic loss as given in Equation 1, lossus(t, tr ) is the sentiment loss as described in Equation 9.The hyper-parameter  weighs the two parts.lossus(t, tr ) = max(0, 1  s(t)f u 1 (t) + s(t)f u 1 (t r ) ) (9)", "rank": 32, "paragraph_comparative_number": 0, "entities": [], "id": "p_32"}, "sentences": [{"end": 16298, "text": "Given an original (or corrupted) ngram and the sentiment polarity of a sentence as the input, SSWEu predicts a two-dimensional vector for each input ngram.", "rank": 130, "start": 16143, "IsComparative": "0", "id": "st_130"}, {"end": 16415, "text": "The two scalars (f u 0 , f u 1 ) stand for language model score and sentiment score of the input ngram, respectively.", "rank": 131, "start": 16298, "IsComparative": "0", "id": "st_131"}, {"end": 16734, "text": "The training objectives of SSWEu are that (1) the original ngram should obtain a higher language model score f u 0 (t)than the corrupted ngram f u 0 (t r ), and (2) the sentiment score of original ngram f u 1 (t) should be more consistent with the gold polarity annotation of sentence than corrupted ngram f u 1 (t r ).", "rank": 132, "start": 16415, "IsComparative": "0", "id": "st_132"}, {"end": 16999, "text": "The loss function of SSWEu is the linear combination of two hinge losses, lossu(t, tr ) =   losscw(t, tr )+ (1  )  lossus(t, tr ) (8) where losscw(t, tr ) is the syntactic loss as given in Equation 1, lossus(t, tr ) is the sentiment loss as described in Equation 9.", "rank": 133, "start": 16734, "IsComparative": "0", "id": "st_133"}, {"end": 17041, "text": "The hyper-parameter  weighs the two parts.", "rank": 134, "start": 16999, "IsComparative": "0", "id": "st_134"}, {"end": 17106, "text": "lossus(t, tr ) = max(0, 1  s(t)f u 1 (t) + s(t)f u 1 (t r ) ) (9)", "rank": 135, "start": 17041, "IsComparative": "0", "id": "st_135"}]}, {"paragraph_info": {"end": 17597, "start": 17106, "text": "Model Training.We train sentiment-specific word embedding from massive distant-supervised tweets collected with positive and negative emoticons1 .We crawl tweets from April 1st, 2013 to April 30th, 2013 with TwitterAPI.We tokenize each tweet with TwitterNLP (Gimpel et al., 2011), remove the @user and URLs of each tweet, and filter the tweets that are too short (< 7 words).Finally, we collect 10M tweets, selected by 5M tweets with positive emoticons and 5M tweets with negative emoticons.", "rank": 33, "paragraph_comparative_number": 1, "entities": [], "id": "p_33"}, "sentences": [{"end": 17121, "text": "Model Training.", "rank": 136, "start": 17106, "IsComparative": "0", "id": "st_136"}, {"end": 17252, "text": "We train sentiment-specific word embedding from massive distant-supervised tweets collected with positive and negative emoticons1 .", "rank": 137, "start": 17121, "IsComparative": "1", "id": "st_137"}, {"end": 17325, "text": "We crawl tweets from April 1st, 2013 to April 30th, 2013 with TwitterAPI.", "rank": 138, "start": 17252, "IsComparative": "0", "id": "st_138"}, {"end": 17481, "text": "We tokenize each tweet with TwitterNLP (Gimpel et al., 2011), remove the @user and URLs of each tweet, and filter the tweets that are too short (< 7 words).", "rank": 139, "start": 17325, "IsComparative": "0", "id": "st_139"}, {"end": 17597, "text": "Finally, we collect 10M tweets, selected by 5M tweets with positive emoticons and 5M tweets with negative emoticons.", "rank": 140, "start": 17481, "IsComparative": "0", "id": "st_140"}]}, {"paragraph_info": {"end": 18216, "start": 17597, "text": "We train SSWEh, SSWEr and SSWEu by taking the derivative of the loss through backpropagation with respect to the whole set of parameters (Collobert et al., 2011), and use AdaGrad (Duchi et al., 2011) to update the parameters.We empirically set the window size as 3, the embedding length as 50, the length of hidden layer as 20 and the learning rate of AdaGrad as 0.1 for all baseline and our models.We learn embedding for unigrams, bigrams and trigrams separately with same neural network and same parameter setting.The contexts of unigram (bigram/trigram) are the surrounding unigrams (bigrams/trigrams), respectively.", "rank": 34, "paragraph_comparative_number": 0, "entities": [], "id": "p_34"}, "sentences": [{"end": 17822, "text": "We train SSWEh, SSWEr and SSWEu by taking the derivative of the loss through backpropagation with respect to the whole set of parameters (Collobert et al., 2011), and use AdaGrad (Duchi et al., 2011) to update the parameters.", "rank": 141, "start": 17597, "IsComparative": "0", "id": "st_141"}, {"end": 17996, "text": "We empirically set the window size as 3, the embedding length as 50, the length of hidden layer as 20 and the learning rate of AdaGrad as 0.1 for all baseline and our models.", "rank": 142, "start": 17822, "IsComparative": "0", "id": "st_142"}, {"end": 18113, "text": "We learn embedding for unigrams, bigrams and trigrams separately with same neural network and same parameter setting.", "rank": 143, "start": 17996, "IsComparative": "0", "id": "st_143"}, {"end": 18216, "text": "The contexts of unigram (bigram/trigram) are the surrounding unigrams (bigrams/trigrams), respectively.", "rank": 144, "start": 18113, "IsComparative": "0", "id": "st_144"}]}, {"paragraph_info": {"end": 18252, "start": 18216, "text": "3.3 Twitter Sentiment Classification", "rank": 35, "paragraph_comparative_number": 0, "entities": [], "id": "p_35"}, "sentences": [{"end": 18252, "text": "3.3 Twitter Sentiment Classification", "rank": 145, "start": 18216, "IsComparative": "0", "id": "st_145"}]}, {"paragraph_info": {"end": 18626, "start": 18252, "text": "We apply sentiment-specific word embedding for Twitter sentiment classification under a supervised learning framework as in previous work (Pang et al., 2002).Instead of hand-crafting features, we incorporate the continuous representation of words and phrases as the feature of a tweet.The sentiment classifier is built from tweets with manually annotated sentiment polarity.", "rank": 36, "paragraph_comparative_number": 0, "entities": [], "id": "p_36"}, "sentences": [{"end": 18410, "text": "We apply sentiment-specific word embedding for Twitter sentiment classification under a supervised learning framework as in previous work (Pang et al., 2002).", "rank": 146, "start": 18252, "IsComparative": "0", "id": "st_146"}, {"end": 18537, "text": "Instead of hand-crafting features, we incorporate the continuous representation of words and phrases as the feature of a tweet.", "rank": 147, "start": 18410, "IsComparative": "0", "id": "st_147"}, {"end": 18626, "text": "The sentiment classifier is built from tweets with manually annotated sentiment polarity.", "rank": 148, "start": 18537, "IsComparative": "0", "id": "st_148"}]}, {"paragraph_info": {"end": 19701, "start": 18626, "text": "We explore min, average and max convolutional layers (Collobert et al., 2011; Socher et al., 2011a), which have been used as simple and effective methods for compositionality learning in vector-based semantics (Mitchell and Lapata, 2010), to obtain the tweet representation.The result is the concatenation of vectors derived from different convolutional layers.z(tw) = <zmax(tw), zmin(tw), zaverage(tw)> where z(tw) is the representation of tweet tw and zx(tw) is the results of the convolutional layer x  <min, max, average>.Each convolutional layer zx employs the embedding of unigrams, bigrams and trigrams separately and conducts the matrixvector operation of x on the sequence represented by columns in each lookup table.The output of zx is the concatenation of results obtained from different lookup tables.zx(tw) = <wxhLunii tw, wxhLbii tw, wxhLtrii tw> where wx is the convolutional function of zx, hLi tw is the concatenated column vectors of the words in the tweet.Luni, Lbi and Ltri are the lookup tables of the unigram, bigram and trigram embedding, respectively.", "rank": 37, "paragraph_comparative_number": 2, "entities": [], "id": "p_37"}, "sentences": [{"end": 18900, "text": "We explore min, average and max convolutional layers (Collobert et al., 2011; Socher et al., 2011a), which have been used as simple and effective methods for compositionality learning in vector-based semantics (Mitchell and Lapata, 2010), to obtain the tweet representation.", "rank": 149, "start": 18626, "IsComparative": "0", "id": "st_149"}, {"end": 18987, "text": "The result is the concatenation of vectors derived from different convolutional layers.", "rank": 150, "start": 18900, "IsComparative": "0", "id": "st_150"}, {"end": 19152, "text": "z(tw) = <zmax(tw), zmin(tw), zaverage(tw)> where z(tw) is the representation of tweet tw and zx(tw) is the results of the convolutional layer x  <min, max, average>.", "rank": 151, "start": 18987, "IsComparative": "0", "id": "st_151"}, {"end": 19352, "text": "Each convolutional layer zx employs the embedding of unigrams, bigrams and trigrams separately and conducts the matrixvector operation of x on the sequence represented by columns in each lookup table.", "rank": 152, "start": 19152, "IsComparative": "0", "id": "st_152"}, {"end": 19439, "text": "The output of zx is the concatenation of results obtained from different lookup tables.", "rank": 153, "start": 19352, "IsComparative": "1", "id": "st_153"}, {"end": 19601, "text": "zx(tw) = <wxhLunii tw, wxhLbii tw, wxhLtrii tw> where wx is the convolutional function of zx, hLi tw is the concatenated column vectors of the words in the tweet.", "rank": 154, "start": 19439, "IsComparative": "1", "id": "st_154"}, {"end": 19701, "text": "Luni, Lbi and Ltri are the lookup tables of the unigram, bigram and trigram embedding, respectively.", "rank": 155, "start": 19601, "IsComparative": "0", "id": "st_155"}]}, {"paragraph_info": {"end": 19713, "start": 19701, "text": "4 Experiment", "rank": 38, "paragraph_comparative_number": 1, "entities": [], "id": "p_38"}, "sentences": [{"end": 19713, "text": "4 Experiment", "rank": 156, "start": 19701, "IsComparative": "1", "id": "st_156"}]}, {"paragraph_info": {"end": 19982, "start": 19713, "text": "We conduct experiments to evaluate SSWE by incorporating it into a supervised learning framework for Twitter sentiment classification.We also directly evaluate the effectiveness of the SSWE by measuring the word similarity in the embedding space for sentiment lexicons.", "rank": 39, "paragraph_comparative_number": 0, "entities": [], "id": "p_39"}, "sentences": [{"end": 19847, "text": "We conduct experiments to evaluate SSWE by incorporating it into a supervised learning framework for Twitter sentiment classification.", "rank": 157, "start": 19713, "IsComparative": "0", "id": "st_157"}, {"end": 19982, "text": "We also directly evaluate the effectiveness of the SSWE by measuring the word similarity in the embedding space for sentiment lexicons.", "rank": 158, "start": 19847, "IsComparative": "0", "id": "st_158"}]}, {"paragraph_info": {"end": 20018, "start": 19982, "text": "4.1 Twitter Sentiment Classification", "rank": 40, "paragraph_comparative_number": 0, "entities": [], "id": "p_40"}, "sentences": [{"end": 20018, "text": "4.1 Twitter Sentiment Classification", "rank": 159, "start": 19982, "IsComparative": "0", "id": "st_159"}]}, {"paragraph_info": {"end": 21100, "start": 20018, "text": "Experiment Setup and Datasets.We conduct experiments on the latest Twitter sentiment classification benchmark dataset in SemEval 2013 (Nakov et al., 2013).The training and development sets were completely in full to task participants.However, we were unable to download all the training and development sets because some tweets were deleted or not available due to modified authorization status.The test set is directly provided to the participants.The distribution of our dataset is given in Table 1.We train sentiment classifier with LibLinear (Fan et al., 2008) on the training set, tune parameter c on the dev set and evaluate on the test set.Evaluation metric is the Macro-F1 of positive and negative categories 2 .Positive Negative Neutral Total Train 2,642 994 3,436 7,072 Dev 408 219 493 1,120 Test 1,570 601 1,639 3,810 Table 1: Statistics of the SemEval 2013 Twitter sentiment classification dataset.2We investigate 2-class Twitter sentiment classification (positive/negative) instead of 3-class Twitter sentiment classification (positive/negative/neutral) in SemEval2013.", "rank": 41, "paragraph_comparative_number": 1, "entities": [], "id": "p_41"}, "sentences": [{"end": 20048, "text": "Experiment Setup and Datasets.", "rank": 160, "start": 20018, "IsComparative": "0", "id": "st_160"}, {"end": 20173, "text": "We conduct experiments on the latest Twitter sentiment classification benchmark dataset in SemEval 2013 (Nakov et al., 2013).", "rank": 161, "start": 20048, "IsComparative": "0", "id": "st_161"}, {"end": 20252, "text": "The training and development sets were completely in full to task participants.", "rank": 162, "start": 20173, "IsComparative": "1", "id": "st_162"}, {"end": 20413, "text": "However, we were unable to download all the training and development sets because some tweets were deleted or not available due to modified authorization status.", "rank": 163, "start": 20252, "IsComparative": "0", "id": "st_163"}, {"end": 20467, "text": "The test set is directly provided to the participants.", "rank": 164, "start": 20413, "IsComparative": "0", "id": "st_164"}, {"end": 20519, "text": "The distribution of our dataset is given in Table 1.", "rank": 165, "start": 20467, "IsComparative": "0", "id": "st_165"}, {"end": 20665, "text": "We train sentiment classifier with LibLinear (Fan et al., 2008) on the training set, tune parameter c on the dev set and evaluate on the test set.", "rank": 166, "start": 20519, "IsComparative": "0", "id": "st_166"}, {"end": 20738, "text": "Evaluation metric is the Macro-F1 of positive and negative categories 2 .", "rank": 167, "start": 20665, "IsComparative": "0", "id": "st_167"}, {"end": 20928, "text": "Positive Negative Neutral Total Train 2,642 994 3,436 7,072 Dev 408 219 493 1,120 Test 1,570 601 1,639 3,810 Table 1: Statistics of the SemEval 2013 Twitter sentiment classification dataset.", "rank": 168, "start": 20738, "IsComparative": "0", "id": "st_168"}, {"end": 21100, "text": "2We investigate 2-class Twitter sentiment classification (positive/negative) instead of 3-class Twitter sentiment classification (positive/negative/neutral) in SemEval2013.", "rank": 169, "start": 20928, "IsComparative": "0", "id": "st_169"}]}, {"paragraph_info": {"end": 21194, "start": 21100, "text": "Baseline Methods.We compare our method with the following sentiment classification algorithms:", "rank": 42, "paragraph_comparative_number": 0, "entities": [], "id": "p_42"}, "sentences": [{"end": 21117, "text": "Baseline Methods.", "rank": 170, "start": 21100, "IsComparative": "0", "id": "st_170"}, {"end": 21194, "text": "We compare our method with the following sentiment classification algorithms:", "rank": 171, "start": 21117, "IsComparative": "0", "id": "st_171"}]}, {"paragraph_info": {"end": 21383, "start": 21194, "text": "(1) DistSuper: We use the 10 million tweets selected by positive and negative emoticons as training data, and build sentiment classifier with LibLinear and ngram features (Go et al., 2009).", "rank": 43, "paragraph_comparative_number": 0, "entities": [], "id": "p_43"}, "sentences": [{"end": 21383, "text": "(1) DistSuper: We use the 10 million tweets selected by positive and negative emoticons as training data, and build sentiment classifier with LibLinear and ngram features (Go et al., 2009).", "rank": 172, "start": 21194, "IsComparative": "0", "id": "st_172"}]}, {"paragraph_info": {"end": 21568, "start": 21383, "text": "(2) SVM: The ngram features and Support Vector Machine are widely used baseline methods to build sentiment classifiers (Pang et al., 2002).LibLinear is used to train the SVM classifier.", "rank": 44, "paragraph_comparative_number": 0, "entities": [], "id": "p_44"}, "sentences": [{"end": 21522, "text": "(2) SVM: The ngram features and Support Vector Machine are widely used baseline methods to build sentiment classifiers (Pang et al., 2002).", "rank": 173, "start": 21383, "IsComparative": "0", "id": "st_173"}, {"end": 21568, "text": "LibLinear is used to train the SVM classifier.", "rank": 174, "start": 21522, "IsComparative": "0", "id": "st_174"}]}, {"paragraph_info": {"end": 21742, "start": 21568, "text": "(3) NBSVM: NBSVM (Wang and Manning, 2012) is a state-of-the-art performer on many sentiment classification datasets, which trades-off between Naive Bayes and NB-enhanced SVM.", "rank": 45, "paragraph_comparative_number": 0, "entities": [], "id": "p_45"}, "sentences": [{"end": 21742, "text": "(3) NBSVM: NBSVM (Wang and Manning, 2012) is a state-of-the-art performer on many sentiment classification datasets, which trades-off between Naive Bayes and NB-enhanced SVM.", "rank": 175, "start": 21568, "IsComparative": "0", "id": "st_175"}]}, {"paragraph_info": {"end": 21950, "start": 21742, "text": "(4) RAE: Recursive Autoencoder (Socher et al., 2011c) has been proven effective in many sentiment analysis tasks by learning compositionality automatically.We run RAE with randomly initialized word embedding.", "rank": 46, "paragraph_comparative_number": 0, "entities": [], "id": "p_46"}, "sentences": [{"end": 21898, "text": "(4) RAE: Recursive Autoencoder (Socher et al., 2011c) has been proven effective in many sentiment analysis tasks by learning compositionality automatically.", "rank": 176, "start": 21742, "IsComparative": "0", "id": "st_176"}, {"end": 21950, "text": "We run RAE with randomly initialized word embedding.", "rank": 177, "start": 21898, "IsComparative": "0", "id": "st_177"}]}, {"paragraph_info": {"end": 22278, "start": 21950, "text": "(5) NRC: NRC builds the top-performed system in SemEval 2013 Twitter sentiment classification track which incorporates diverse sentiment lexicons and many manually designed features.We re-implement this system because the codes are not publicly available 3 .NRC-ngram refers to the feature set of NRC leaving out ngram features.", "rank": 47, "paragraph_comparative_number": 0, "entities": [], "id": "p_47"}, "sentences": [{"end": 22132, "text": "(5) NRC: NRC builds the top-performed system in SemEval 2013 Twitter sentiment classification track which incorporates diverse sentiment lexicons and many manually designed features.", "rank": 178, "start": 21950, "IsComparative": "0", "id": "st_178"}, {"end": 22208, "text": "We re-implement this system because the codes are not publicly available 3 .", "rank": 179, "start": 22132, "IsComparative": "0", "id": "st_179"}, {"end": 22278, "text": "NRC-ngram refers to the feature set of NRC leaving out ngram features.", "rank": 180, "start": 22208, "IsComparative": "0", "id": "st_180"}]}, {"paragraph_info": {"end": 22757, "start": 22278, "text": "Except for DistSuper, other baseline methods are conducted in a supervised manner.We do not compare with RNTN (Socher et al., 2013b) because we cannot efficiently train the RNTN model.The reason lies in that the tweets in our dataset do not have accurately parsed results or fine grained sentiment labels for phrases.Another reason is that the RNTN model trained on movie reviews cannot be directly applied on tweets due to the differences between domains (Blitzer et al., 2007).", "rank": 48, "paragraph_comparative_number": 0, "entities": [], "id": "p_48"}, "sentences": [{"end": 22360, "text": "Except for DistSuper, other baseline methods are conducted in a supervised manner.", "rank": 181, "start": 22278, "IsComparative": "0", "id": "st_181"}, {"end": 22462, "text": "We do not compare with RNTN (Socher et al., 2013b) because we cannot efficiently train the RNTN model.", "rank": 182, "start": 22360, "IsComparative": "0", "id": "st_182"}, {"end": 22595, "text": "The reason lies in that the tweets in our dataset do not have accurately parsed results or fine grained sentiment labels for phrases.", "rank": 183, "start": 22462, "IsComparative": "0", "id": "st_183"}, {"end": 22757, "text": "Another reason is that the RNTN model trained on movie reviews cannot be directly applied on tweets due to the differences between domains (Blitzer et al., 2007).", "rank": 184, "start": 22595, "IsComparative": "0", "id": "st_184"}]}, {"paragraph_info": {"end": 24002, "start": 22757, "text": "Results and Analysis.Table 2 shows the macroF1 of the baseline systems as well as the SSWEbased methods on positive/negative sentiment classification of tweets.Distant supervision is relatively weak because the noisy-labeled tweets are treated as the gold standard, which affects the performance of classifier.The results of bagof-ngram (uni/bi/tri-gram) features are not satis- fied because the one-hot word representation cannot capture the latent connections between words.NBSVM and RAE perform comparably and have 3 For 3-class sentiment classification in SemEval 2013, our re-implementation of NRC achieved 68.3%, 0.7% lower than NRC (69%) due to less training data.1560 Method Macro-F1 DistSuper + unigram 61.74 DistSuper + uni/bi/tri-gram 63.84 SVM + unigram 74.50 SVM + uni/bi/tri-gram 75.06 NBSVM 75.28 RAE 75.12 NRC (Top System in SemEval) 84.73 NRC - ngram 84.17 SSWEu 84.98 SSWEu+NRC 86.58 SSWEu+NRC-ngram 86.48 Table 2: Macro-F1 on positive/negative classification of tweets.a big gap in comparison with the NRC and SSWEbased methods.The reason is that RAE and NBSVM learn the representation of tweets from the small-scale manually annotated training set, which cannot well capture the comprehensive linguistic phenomenons of words.", "rank": 49, "paragraph_comparative_number": 1, "entities": [], "id": "p_49"}, "sentences": [{"end": 22778, "text": "Results and Analysis.", "rank": 185, "start": 22757, "IsComparative": "0", "id": "st_185"}, {"end": 22917, "text": "Table 2 shows the macroF1 of the baseline systems as well as the SSWEbased methods on positive/negative sentiment classification of tweets.", "rank": 186, "start": 22778, "IsComparative": "1", "id": "st_186"}, {"end": 23067, "text": "Distant supervision is relatively weak because the noisy-labeled tweets are treated as the gold standard, which affects the performance of classifier.", "rank": 187, "start": 22917, "IsComparative": "0", "id": "st_187"}, {"end": 23233, "text": "The results of bagof-ngram (uni/bi/tri-gram) features are not satis- fied because the one-hot word representation cannot capture the latent connections between words.", "rank": 188, "start": 23067, "IsComparative": "0", "id": "st_188"}, {"end": 23428, "text": "NBSVM and RAE perform comparably and have 3 For 3-class sentiment classification in SemEval 2013, our re-implementation of NRC achieved 68.3%, 0.7% lower than NRC (69%) due to less training data.", "rank": 189, "start": 23233, "IsComparative": "0", "id": "st_189"}, {"end": 23745, "text": "1560 Method Macro-F1 DistSuper + unigram 61.74 DistSuper + uni/bi/tri-gram 63.84 SVM + unigram 74.50 SVM + uni/bi/tri-gram 75.06 NBSVM 75.28 RAE 75.12 NRC (Top System in SemEval) 84.73 NRC - ngram 84.17 SSWEu 84.98 SSWEu+NRC 86.58 SSWEu+NRC-ngram 86.48 Table 2: Macro-F1 on positive/negative classification of tweets.", "rank": 190, "start": 23428, "IsComparative": "0", "id": "st_190"}, {"end": 23804, "text": "a big gap in comparison with the NRC and SSWEbased methods.", "rank": 191, "start": 23745, "IsComparative": "0", "id": "st_191"}, {"end": 24002, "text": "The reason is that RAE and NBSVM learn the representation of tweets from the small-scale manually annotated training set, which cannot well capture the comprehensive linguistic phenomenons of words.", "rank": 192, "start": 23804, "IsComparative": "0", "id": "st_192"}]}, {"paragraph_info": {"end": 24739, "start": 24002, "text": "NRC implements a variety of features and reaches 84.73% in macro-F1, verifying the importance of a better feature representation for Twitter sentiment classification.We achieve 84.98% by using only SSWEu as features without borrowing any sentiment lexicons or hand-crafted rules.The results indicate that SSWEu automatically learns discriminative features from massive tweets and performs comparable with the state-of-the-art manually designed features.After concatenating SSWEu with the feature set of NRC, the performance is further improved to 86.58%.We also compare SSWEu with the ngram feature by integrating SSWE into NRC-ngram.The concatenated features SSWEu+NRC-ngram (86.48%) outperform the original feature set of NRC (84.73%).", "rank": 50, "paragraph_comparative_number": 0, "entities": [], "id": "p_50"}, "sentences": [{"end": 24168, "text": "NRC implements a variety of features and reaches 84.73% in macro-F1, verifying the importance of a better feature representation for Twitter sentiment classification.", "rank": 193, "start": 24002, "IsComparative": "0", "id": "st_193"}, {"end": 24281, "text": "We achieve 84.98% by using only SSWEu as features without borrowing any sentiment lexicons or hand-crafted rules.", "rank": 194, "start": 24168, "IsComparative": "0", "id": "st_194"}, {"end": 24455, "text": "The results indicate that SSWEu automatically learns discriminative features from massive tweets and performs comparable with the state-of-the-art manually designed features.", "rank": 195, "start": 24281, "IsComparative": "0", "id": "st_195"}, {"end": 24556, "text": "After concatenating SSWEu with the feature set of NRC, the performance is further improved to 86.58%.", "rank": 196, "start": 24455, "IsComparative": "0", "id": "st_196"}, {"end": 24636, "text": "We also compare SSWEu with the ngram feature by integrating SSWE into NRC-ngram.", "rank": 197, "start": 24556, "IsComparative": "0", "id": "st_197"}, {"end": 24739, "text": "The concatenated features SSWEu+NRC-ngram (86.48%) outperform the original feature set of NRC (84.73%).", "rank": 198, "start": 24636, "IsComparative": "0", "id": "st_198"}]}, {"paragraph_info": {"end": 24989, "start": 24739, "text": "As a reference, we apply SSWEu on subjective classification of tweets, and obtain 72.17% in macro-F1 by using only SSWEu as feature.After combining SSWEu with the feature set of NRC, we improve NRC from 74.86% to 75.39% for subjective classification.", "rank": 51, "paragraph_comparative_number": 0, "entities": [], "id": "p_51"}, "sentences": [{"end": 24871, "text": "As a reference, we apply SSWEu on subjective classification of tweets, and obtain 72.17% in macro-F1 by using only SSWEu as feature.", "rank": 199, "start": 24739, "IsComparative": "0", "id": "st_199"}, {"end": 24989, "text": "After combining SSWEu with the feature set of NRC, we improve NRC from 74.86% to 75.39% for subjective classification.", "rank": 200, "start": 24871, "IsComparative": "0", "id": "st_200"}]}, {"paragraph_info": {"end": 25651, "start": 24989, "text": "Comparision between Different Word Embedding.We compare sentiment-specific word embedding (SSWEh, SSWEr, SSWEu) with baseline embedding learning algorithms by only using word embedding as features for Twitter sentiment classification.We use the embedding of unigrams, bigrams and trigrams in the experiment.The embeddings of C&W (Collobert et al., 2011), word2vec4 , WVSA (Maas et al., 2011) and our models are trained with the same dataset and same parameter setting.We compare with C&W and word2vec as they have been proved effective in many NLP tasks.The trade-off parameter of ReEmb (Labutov and Lipson, 2013) is tuned on the development set of SemEval 2013.", "rank": 52, "paragraph_comparative_number": 1, "entities": [], "id": "p_52"}, "sentences": [{"end": 25034, "text": "Comparision between Different Word Embedding.", "rank": 201, "start": 24989, "IsComparative": "0", "id": "st_201"}, {"end": 25223, "text": "We compare sentiment-specific word embedding (SSWEh, SSWEr, SSWEu) with baseline embedding learning algorithms by only using word embedding as features for Twitter sentiment classification.", "rank": 202, "start": 25034, "IsComparative": "1", "id": "st_202"}, {"end": 25296, "text": "We use the embedding of unigrams, bigrams and trigrams in the experiment.", "rank": 203, "start": 25223, "IsComparative": "0", "id": "st_203"}, {"end": 25457, "text": "The embeddings of C&W (Collobert et al., 2011), word2vec4 , WVSA (Maas et al., 2011) and our models are trained with the same dataset and same parameter setting.", "rank": 204, "start": 25296, "IsComparative": "0", "id": "st_204"}, {"end": 25543, "text": "We compare with C&W and word2vec as they have been proved effective in many NLP tasks.", "rank": 205, "start": 25457, "IsComparative": "0", "id": "st_205"}, {"end": 25651, "text": "The trade-off parameter of ReEmb (Labutov and Lipson, 2013) is tuned on the development set of SemEval 2013.", "rank": 206, "start": 25543, "IsComparative": "0", "id": "st_206"}]}, {"paragraph_info": {"end": 26476, "start": 25651, "text": "Table 3 shows the performance on the positive/negative classification of tweets5 .ReEmb(C&W) and ReEmb(w2v) stand for the use of embeddings learned from 10 million distantsupervised tweets with C&W and word2vec, respectively.Each row of Table 3 represents a word embedding learning algorithm.Each column stands for a type of embedding used to compose features of tweets.The column uni+bi denotes the use of unigram and bigram embedding, and the column uni+bi+tri indicates the use of unigram, bigram and trigram embedding.Embedding unigram uni+bi uni+bi+tri C&W 74.89 75.24 75.89 Word2vec 73.21 75.07 76.31 ReEmb(C&W) 75.87   ReEmb(w2v) 75.21   WVSA 77.04   SSWEh 81.33 83.16 83.37 SSWEr 80.45 81.52 82.60 SSWEu 83.70 84.70 84.98 Table 3: Macro-F1 on positive/negative classification of tweets with different word embeddings.", "rank": 53, "paragraph_comparative_number": 0, "entities": [], "id": "p_53"}, "sentences": [{"end": 25733, "text": "Table 3 shows the performance on the positive/negative classification of tweets5 .", "rank": 207, "start": 25651, "IsComparative": "0", "id": "st_207"}, {"end": 25876, "text": "ReEmb(C&W) and ReEmb(w2v) stand for the use of embeddings learned from 10 million distantsupervised tweets with C&W and word2vec, respectively.", "rank": 208, "start": 25733, "IsComparative": "0", "id": "st_208"}, {"end": 25943, "text": "Each row of Table 3 represents a word embedding learning algorithm.", "rank": 209, "start": 25876, "IsComparative": "0", "id": "st_209"}, {"end": 26021, "text": "Each column stands for a type of embedding used to compose features of tweets.", "rank": 210, "start": 25943, "IsComparative": "0", "id": "st_210"}, {"end": 26173, "text": "The column uni+bi denotes the use of unigram and bigram embedding, and the column uni+bi+tri indicates the use of unigram, bigram and trigram embedding.", "rank": 211, "start": 26021, "IsComparative": "0", "id": "st_211"}, {"end": 26476, "text": "Embedding unigram uni+bi uni+bi+tri C&W 74.89 75.24 75.89 Word2vec 73.21 75.07 76.31 ReEmb(C&W) 75.87   ReEmb(w2v) 75.21   WVSA 77.04   SSWEh 81.33 83.16 83.37 SSWEr 80.45 81.52 82.60 SSWEu 83.70 84.70 84.98 Table 3: Macro-F1 on positive/negative classification of tweets with different word embeddings.", "rank": 212, "start": 26173, "IsComparative": "0", "id": "st_212"}]}, {"paragraph_info": {"end": 27875, "start": 26476, "text": "From the first column of Table 3, we can see that the performance of C&W and word2vec are obviously lower than sentiment-specific word embeddings by only using unigram embedding as features.The reason is that C&W and word2vec do not explicitly exploit the sentiment information of the text, resulting in that the words with opposite polarity such as good and bad are mapped to close word vectors.When such word embeddings are fed as features to a Twitter sentiment classifier, the discriminative ability of sentiment words are weakened thus the classification performance is affected.Sentiment-specific word em- 4Available at https://code.google.com/p/word2vec/.We utilize the Skip-gram model because it performs better than CBOW in our experiments.5MVSA and ReEmb are not suitable for learning bigram and trigram embedding because their sentiment predictor functions only utilize the unigram embedding.1561 beddings (SSWEh, SSWEr, SSWEu) effectively distinguish words with opposite sentiment polarity and perform best in three settings.SSWE outperforms MVSA by exploiting more contextual information in the sentiment predictor function.SSWE outperforms ReEmb by leveraging more sentiment information from massive distant-supervised tweets.Among three sentiment-specific word embeddings, SSWEu captures more context information and yields best performance.SSWEh and SSWEr obtain comparative results.", "rank": 54, "paragraph_comparative_number": 3, "entities": [], "id": "p_54"}, "sentences": [{"end": 26666, "text": "From the first column of Table 3, we can see that the performance of C&W and word2vec are obviously lower than sentiment-specific word embeddings by only using unigram embedding as features.", "rank": 213, "start": 26476, "IsComparative": "0", "id": "st_213"}, {"end": 26872, "text": "The reason is that C&W and word2vec do not explicitly exploit the sentiment information of the text, resulting in that the words with opposite polarity such as good and bad are mapped to close word vectors.", "rank": 214, "start": 26666, "IsComparative": "0", "id": "st_214"}, {"end": 27060, "text": "When such word embeddings are fed as features to a Twitter sentiment classifier, the discriminative ability of sentiment words are weakened thus the classification performance is affected.", "rank": 215, "start": 26872, "IsComparative": "0", "id": "st_215"}, {"end": 27138, "text": "Sentiment-specific word em- 4Available at https://code.google.com/p/word2vec/.", "rank": 216, "start": 27060, "IsComparative": "0", "id": "st_216"}, {"end": 27225, "text": "We utilize the Skip-gram model because it performs better than CBOW in our experiments.", "rank": 217, "start": 27138, "IsComparative": "0", "id": "st_217"}, {"end": 27379, "text": "5MVSA and ReEmb are not suitable for learning bigram and trigram embedding because their sentiment predictor functions only utilize the unigram embedding.", "rank": 218, "start": 27225, "IsComparative": "0", "id": "st_218"}, {"end": 27513, "text": "1561 beddings (SSWEh, SSWEr, SSWEu) effectively distinguish words with opposite sentiment polarity and perform best in three settings.", "rank": 219, "start": 27379, "IsComparative": "0", "id": "st_219"}, {"end": 27613, "text": "SSWE outperforms MVSA by exploiting more contextual information in the sentiment predictor function.", "rank": 220, "start": 27513, "IsComparative": "0", "id": "st_220"}, {"end": 27716, "text": "SSWE outperforms ReEmb by leveraging more sentiment information from massive distant-supervised tweets.", "rank": 221, "start": 27613, "IsComparative": "1", "id": "st_221"}, {"end": 27832, "text": "Among three sentiment-specific word embeddings, SSWEu captures more context information and yields best performance.", "rank": 222, "start": 27716, "IsComparative": "1", "id": "st_222"}, {"end": 27875, "text": "SSWEh and SSWEr obtain comparative results.", "rank": 223, "start": 27832, "IsComparative": "1", "id": "st_223"}]}, {"paragraph_info": {"end": 28605, "start": 27875, "text": "From each row of Table 3, we can see that the bigram and trigram embeddings consistently improve the performance of Twitter sentiment classi- fication.The underlying reason is that a phrase, which cannot be accurately represented by unigram embedding, is directly encoded into the ngram embedding as an idiomatic unit.A typical case in sentiment analysis is that the composed phrase and multiword expression may have a different sentiment polarity than the individual words it contains, such as not <bad> and <great> deal of (the word in the bracket has different sentiment polarity with the ngram).A very recent study by Mikolov et al.(2013) also verified the effectiveness of phrase embedding for analogically reasoning phrases.", "rank": 55, "paragraph_comparative_number": 1, "entities": [], "id": "p_55"}, "sentences": [{"end": 28026, "text": "From each row of Table 3, we can see that the bigram and trigram embeddings consistently improve the performance of Twitter sentiment classi- fication.", "rank": 224, "start": 27875, "IsComparative": "0", "id": "st_224"}, {"end": 28193, "text": "The underlying reason is that a phrase, which cannot be accurately represented by unigram embedding, is directly encoded into the ngram embedding as an idiomatic unit.", "rank": 225, "start": 28026, "IsComparative": "0", "id": "st_225"}, {"end": 28474, "text": "A typical case in sentiment analysis is that the composed phrase and multiword expression may have a different sentiment polarity than the individual words it contains, such as not <bad> and <great> deal of (the word in the bracket has different sentiment polarity with the ngram).", "rank": 226, "start": 28193, "IsComparative": "0", "id": "st_226"}, {"end": 28511, "text": "A very recent study by Mikolov et al.", "rank": 227, "start": 28474, "IsComparative": "0", "id": "st_227"}, {"end": 28605, "text": "(2013) also verified the effectiveness of phrase embedding for analogically reasoning phrases.", "rank": 228, "start": 28511, "IsComparative": "1", "id": "st_228"}]}, {"paragraph_info": {"end": 29573, "start": 28605, "text": "Effect of  in SSWEu We tune the hyperparameter  of SSWEu on the development set by using unigram embedding as features.As given in Equation 8,  is the weighting score of syntactic loss of SSWEu and trades-off the syntactic and sentiment losses.SSWEu is trained from 10 million distant-supervised tweets.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0.77 0.78 0.79 0.8 0.81 0.82 0.83 0.84  MacroF1 SSWEu Figure 2: Macro-F1 of SSWEu on the development set of SemEval 2013 with different .Figure 2 shows the macro-F1 of SSWEu on positive/negative classification of tweets with different  on our development set.We can see that SSWEu performs better when  is in the range of <0.5, 0.6>, which balances the syntactic context and sentiment information.The model with =1 stands for C&W model, which only encodes the syntactic contexts of words.The sharp decline at =1 reflects the importance of sentiment information in learning word embedding for Twitter sentiment classification.", "rank": 56, "paragraph_comparative_number": 1, "entities": [], "id": "p_56"}, "sentences": [{"end": 28724, "text": "Effect of  in SSWEu We tune the hyperparameter  of SSWEu on the development set by using unigram embedding as features.", "rank": 229, "start": 28605, "IsComparative": "0", "id": "st_229"}, {"end": 28849, "text": "As given in Equation 8,  is the weighting score of syntactic loss of SSWEu and trades-off the syntactic and sentiment losses.", "rank": 230, "start": 28724, "IsComparative": "0", "id": "st_230"}, {"end": 28908, "text": "SSWEu is trained from 10 million distant-supervised tweets.", "rank": 231, "start": 28849, "IsComparative": "0", "id": "st_231"}, {"end": 29085, "text": "0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0.77 0.78 0.79 0.8 0.81 0.82 0.83 0.84  MacroF1 SSWEu Figure 2: Macro-F1 of SSWEu on the development set of SemEval 2013 with different .", "rank": 232, "start": 28908, "IsComparative": "0", "id": "st_232"}, {"end": 29207, "text": "Figure 2 shows the macro-F1 of SSWEu on positive/negative classification of tweets with different  on our development set.", "rank": 233, "start": 29085, "IsComparative": "0", "id": "st_233"}, {"end": 29345, "text": "We can see that SSWEu performs better when  is in the range of <0.5, 0.6>, which balances the syntactic context and sentiment information.", "rank": 234, "start": 29207, "IsComparative": "0", "id": "st_234"}, {"end": 29436, "text": "The model with =1 stands for C&W model, which only encodes the syntactic contexts of words.", "rank": 235, "start": 29345, "IsComparative": "0", "id": "st_235"}, {"end": 29573, "text": "The sharp decline at =1 reflects the importance of sentiment information in learning word embedding for Twitter sentiment classification.", "rank": 236, "start": 29436, "IsComparative": "1", "id": "st_236"}]}, {"paragraph_info": {"end": 30242, "start": 29573, "text": "Effect of Distant-supervised Data in SSWEu We investigate how the size of the distantsupervised data affects the performance of SSWEu feature for Twitter sentiment classification.We vary the number of distant-supervised tweets from 1 million to 12 million, increased by 1 million.We set the  of SSWEu as 0.5, according to the experiments shown in Figure 2.Results of positive/negative classification of tweets on our development set are given in Figure 3.1 2 3 4 5 6 7 8 9 10 11 12 x 106 0.77 0.78 0.79 0.8 0.81 0.82 0.83 0.84 # of distantsupervised tweets MacroF1 SSWEu Figure 3: Macro-F1 of SSWEu with different size of distant-supervised data on our development set.", "rank": 57, "paragraph_comparative_number": 1, "entities": [], "id": "p_57"}, "sentences": [{"end": 29752, "text": "Effect of Distant-supervised Data in SSWEu We investigate how the size of the distantsupervised data affects the performance of SSWEu feature for Twitter sentiment classification.", "rank": 237, "start": 29573, "IsComparative": "0", "id": "st_237"}, {"end": 29853, "text": "We vary the number of distant-supervised tweets from 1 million to 12 million, increased by 1 million.", "rank": 238, "start": 29752, "IsComparative": "0", "id": "st_238"}, {"end": 29929, "text": "We set the  of SSWEu as 0.5, according to the experiments shown in Figure 2.", "rank": 239, "start": 29853, "IsComparative": "0", "id": "st_239"}, {"end": 30028, "text": "Results of positive/negative classification of tweets on our development set are given in Figure 3.", "rank": 240, "start": 29929, "IsComparative": "0", "id": "st_240"}, {"end": 30242, "text": "1 2 3 4 5 6 7 8 9 10 11 12 x 106 0.77 0.78 0.79 0.8 0.81 0.82 0.83 0.84 # of distantsupervised tweets MacroF1 SSWEu Figure 3: Macro-F1 of SSWEu with different size of distant-supervised data on our development set.", "rank": 241, "start": 30028, "IsComparative": "1", "id": "st_241"}]}, {"paragraph_info": {"end": 30832, "start": 30242, "text": "We can see that when more distant-supervised tweets are added, the accuracy of SSWEu consistently improves.The underlying reason is that when more tweets are incorporated, the word embedding is better estimated as the vocabulary size is larger and the context and sentiment information are richer.When we have 10 million distantsupervised tweets, the SSWEu feature increases the macro-F1 of positive/negative classification of tweets to 82.94% on our development set.When we have more than 10 million tweets, the performance remains stable as the contexts of words have been mostly covered.", "rank": 58, "paragraph_comparative_number": 0, "entities": [], "id": "p_58"}, "sentences": [{"end": 30349, "text": "We can see that when more distant-supervised tweets are added, the accuracy of SSWEu consistently improves.", "rank": 242, "start": 30242, "IsComparative": "0", "id": "st_242"}, {"end": 30539, "text": "The underlying reason is that when more tweets are incorporated, the word embedding is better estimated as the vocabulary size is larger and the context and sentiment information are richer.", "rank": 243, "start": 30349, "IsComparative": "0", "id": "st_243"}, {"end": 30709, "text": "When we have 10 million distantsupervised tweets, the SSWEu feature increases the macro-F1 of positive/negative classification of tweets to 82.94% on our development set.", "rank": 244, "start": 30539, "IsComparative": "0", "id": "st_244"}, {"end": 30832, "text": "When we have more than 10 million tweets, the performance remains stable as the contexts of words have been mostly covered.", "rank": 245, "start": 30709, "IsComparative": "0", "id": "st_245"}]}, {"paragraph_info": {"end": 30873, "start": 30832, "text": "4.2 Word Similarity of Sentiment Lexicons", "rank": 59, "paragraph_comparative_number": 0, "entities": [], "id": "p_59"}, "sentences": [{"end": 30873, "text": "4.2 Word Similarity of Sentiment Lexicons", "rank": 246, "start": 30832, "IsComparative": "0", "id": "st_246"}]}, {"paragraph_info": {"end": 31744, "start": 30873, "text": "The quality of SSWE has been implicitly evaluated when applied in Twitter sentiment classification in the previous subsection.We explicitly evaluate it in this section through word similarity in the em- 1562 bedding space for sentiment lexicons.The evaluation metric is the accuracy of polarity consistency between each sentiment word and its top N closest words in the sentiment lexicon, Accuracy = P#Lex i=1 PN j=1 (wi , cij ) #Lex  N (10) where #Lex is the number of words in the sentiment lexicon, wi is the i-th word in the lexicon, cij is the j-th closest word to wi in the lexicon with cosine similarity, (wi , cij ) is an indicator function that is equal to 1 if wi and cij have the same sentiment polarity and 0 for the opposite case.The higher accuracy refers to a better polarity consistency of words in the sentiment lexicon.We set N as 100 in our experiment.", "rank": 60, "paragraph_comparative_number": 0, "entities": [], "id": "p_60"}, "sentences": [{"end": 30999, "text": "The quality of SSWE has been implicitly evaluated when applied in Twitter sentiment classification in the previous subsection.", "rank": 247, "start": 30873, "IsComparative": "0", "id": "st_247"}, {"end": 31118, "text": "We explicitly evaluate it in this section through word similarity in the em- 1562 bedding space for sentiment lexicons.", "rank": 248, "start": 30999, "IsComparative": "0", "id": "st_248"}, {"end": 31616, "text": "The evaluation metric is the accuracy of polarity consistency between each sentiment word and its top N closest words in the sentiment lexicon, Accuracy = P#Lex i=1 PN j=1 (wi , cij ) #Lex  N (10) where #Lex is the number of words in the sentiment lexicon, wi is the i-th word in the lexicon, cij is the j-th closest word to wi in the lexicon with cosine similarity, (wi , cij ) is an indicator function that is equal to 1 if wi and cij have the same sentiment polarity and 0 for the opposite case.", "rank": 249, "start": 31118, "IsComparative": "0", "id": "st_249"}, {"end": 31710, "text": "The higher accuracy refers to a better polarity consistency of words in the sentiment lexicon.", "rank": 250, "start": 31616, "IsComparative": "0", "id": "st_250"}, {"end": 31744, "text": "We set N as 100 in our experiment.", "rank": 251, "start": 31710, "IsComparative": "0", "id": "st_251"}]}, {"paragraph_info": {"end": 32431, "start": 31744, "text": "Experiment Setup and Datasets We utilize the widely-used sentiment lexicons, namely MPQA (Wilson et al., 2005) and HL (Hu and Liu, 2004), to evaluate the quality of word embedding.For each lexicon, we remove the words that do not appear in the lookup table of word embedding.We only use unigram embedding in this section because these sentiment lexicons do not contain phrases.The distribution of the lexicons used in this paper is listed in Table 4.Lexicon Positive Negative Total HL 1,331 2,647 3,978 MPQA 1,932 2,817 4,749 Joint 1,051 2,024 3,075 Table 4: Statistics of the sentiment lexicons.Joint stands for the words that occur in both HL and MPQA with the same sentiment polarity.", "rank": 61, "paragraph_comparative_number": 0, "entities": [], "id": "p_61"}, "sentences": [{"end": 31924, "text": "Experiment Setup and Datasets We utilize the widely-used sentiment lexicons, namely MPQA (Wilson et al., 2005) and HL (Hu and Liu, 2004), to evaluate the quality of word embedding.", "rank": 252, "start": 31744, "IsComparative": "0", "id": "st_252"}, {"end": 32019, "text": "For each lexicon, we remove the words that do not appear in the lookup table of word embedding.", "rank": 253, "start": 31924, "IsComparative": "0", "id": "st_253"}, {"end": 32121, "text": "We only use unigram embedding in this section because these sentiment lexicons do not contain phrases.", "rank": 254, "start": 32019, "IsComparative": "0", "id": "st_254"}, {"end": 32194, "text": "The distribution of the lexicons used in this paper is listed in Table 4.", "rank": 255, "start": 32121, "IsComparative": "0", "id": "st_255"}, {"end": 32340, "text": "Lexicon Positive Negative Total HL 1,331 2,647 3,978 MPQA 1,932 2,817 4,749 Joint 1,051 2,024 3,075 Table 4: Statistics of the sentiment lexicons.", "rank": 256, "start": 32194, "IsComparative": "0", "id": "st_256"}, {"end": 32431, "text": "Joint stands for the words that occur in both HL and MPQA with the same sentiment polarity.", "rank": 257, "start": 32340, "IsComparative": "0", "id": "st_257"}]}, {"paragraph_info": {"end": 33593, "start": 32431, "text": "Results.Table 5 shows our results compared to other word embedding learning algorithms.The accuracy of random result is 50% as positive and negative words are randomly occurred in the nearest neighbors of each word.Sentiment-specific word embeddings (SSWEh, SSWEr, SSWEu) outperform existing neural models (C&W, word2vec) by large margins.SSWEu performs best in three lexicons.SSWEh and SSWEr have comparable performances.Experimental results further demonstrate that sentiment-specific word embeddings are able to capture the sentiment information of texts and distinguish words with opposite sentiment polarity, which are not well solved in traditional neural Embedding HL MPQA Joint Random 50.00 50.00 50.00 C&W 63.10 58.13 62.58 Word2vec 66.22 60.72 65.59 ReEmb(C&W) 64.81 59.76 64.09 ReEmb(w2v) 67.16 61.81 66.39 WVSA 68.14 64.07 67.12 SSWEh 74.17 68.36 74.03 SSWEr 73.65 68.02 73.14 SSWEu 77.30 71.74 77.33 Table 5: Accuracy of the polarity consistency of words in different sentiment lexicons.models like C&W and word2vec.SSWE outperforms MVSA and ReEmb by exploiting more context information of words and sentiment information of sentences, respectively.", "rank": 62, "paragraph_comparative_number": 2, "entities": [], "id": "p_62"}, "sentences": [{"end": 32439, "text": "Results.", "rank": 258, "start": 32431, "IsComparative": "0", "id": "st_258"}, {"end": 32518, "text": "Table 5 shows our results compared to other word embedding learning algorithms.", "rank": 259, "start": 32439, "IsComparative": "1", "id": "st_259"}, {"end": 32646, "text": "The accuracy of random result is 50% as positive and negative words are randomly occurred in the nearest neighbors of each word.", "rank": 260, "start": 32518, "IsComparative": "0", "id": "st_260"}, {"end": 32770, "text": "Sentiment-specific word embeddings (SSWEh, SSWEr, SSWEu) outperform existing neural models (C&W, word2vec) by large margins.", "rank": 261, "start": 32646, "IsComparative": "1", "id": "st_261"}, {"end": 32808, "text": "SSWEu performs best in three lexicons.", "rank": 262, "start": 32770, "IsComparative": "0", "id": "st_262"}, {"end": 32853, "text": "SSWEh and SSWEr have comparable performances.", "rank": 263, "start": 32808, "IsComparative": "0", "id": "st_263"}, {"end": 33431, "text": "Experimental results further demonstrate that sentiment-specific word embeddings are able to capture the sentiment information of texts and distinguish words with opposite sentiment polarity, which are not well solved in traditional neural Embedding HL MPQA Joint Random 50.00 50.00 50.00 C&W 63.10 58.13 62.58 Word2vec 66.22 60.72 65.59 ReEmb(C&W) 64.81 59.76 64.09 ReEmb(w2v) 67.16 61.81 66.39 WVSA 68.14 64.07 67.12 SSWEh 74.17 68.36 74.03 SSWEr 73.65 68.02 73.14 SSWEu 77.30 71.74 77.33 Table 5: Accuracy of the polarity consistency of words in different sentiment lexicons.", "rank": 264, "start": 32853, "IsComparative": "0", "id": "st_264"}, {"end": 33460, "text": "models like C&W and word2vec.", "rank": 265, "start": 33431, "IsComparative": "0", "id": "st_265"}, {"end": 33593, "text": "SSWE outperforms MVSA and ReEmb by exploiting more context information of words and sentiment information of sentences, respectively.", "rank": 266, "start": 33460, "IsComparative": "0", "id": "st_266"}]}, {"paragraph_info": {"end": 33605, "start": 33593, "text": "5 Conclusion", "rank": 63, "paragraph_comparative_number": 1, "entities": [], "id": "p_63"}, "sentences": [{"end": 33605, "text": "5 Conclusion", "rank": 267, "start": 33593, "IsComparative": "1", "id": "st_267"}]}, {"paragraph_info": {"end": 34698, "start": 33605, "text": "In this paper, we propose learning continuous word representations as features for Twitter sentiment classification under a supervised learning framework.We show that the word embedding learned by traditional neural networks are not effective enough for Twitter sentiment classification.These methods typically only model the context information of words so that they cannot distinguish words with similar context but opposite sentiment polarity (e.g.good and bad).We learn sentiment-specific word embedding (SSWE) by integrating the sentiment information into the loss functions of three neural networks.We train SSWE with massive distant-supervised tweets selected by positive and negative emoticons.The effectiveness of SSWE has been implicitly evaluated by using it as features in sentiment classification on the benchmark dataset in SemEval 2013, and explicitly verified by measuring word similarity in the embedding space for sentiment lexicons.Our unified model combining syntactic context of words and sentiment information of sentences yields the best performance in both experiments.", "rank": 64, "paragraph_comparative_number": 2, "entities": [], "id": "p_64"}, "sentences": [{"end": 33759, "text": "In this paper, we propose learning continuous word representations as features for Twitter sentiment classification under a supervised learning framework.", "rank": 268, "start": 33605, "IsComparative": "0", "id": "st_268"}, {"end": 33892, "text": "We show that the word embedding learned by traditional neural networks are not effective enough for Twitter sentiment classification.", "rank": 269, "start": 33759, "IsComparative": "0", "id": "st_269"}, {"end": 34056, "text": "These methods typically only model the context information of words so that they cannot distinguish words with similar context but opposite sentiment polarity (e.g.", "rank": 270, "start": 33892, "IsComparative": "1", "id": "st_270"}, {"end": 34070, "text": "good and bad).", "rank": 271, "start": 34056, "IsComparative": "1", "id": "st_271"}, {"end": 34210, "text": "We learn sentiment-specific word embedding (SSWE) by integrating the sentiment information into the loss functions of three neural networks.", "rank": 272, "start": 34070, "IsComparative": "0", "id": "st_272"}, {"end": 34307, "text": "We train SSWE with massive distant-supervised tweets selected by positive and negative emoticons.", "rank": 273, "start": 34210, "IsComparative": "0", "id": "st_273"}, {"end": 34556, "text": "The effectiveness of SSWE has been implicitly evaluated by using it as features in sentiment classification on the benchmark dataset in SemEval 2013, and explicitly verified by measuring word similarity in the embedding space for sentiment lexicons.", "rank": 274, "start": 34307, "IsComparative": "0", "id": "st_274"}, {"end": 34698, "text": "Our unified model combining syntactic context of words and sentiment information of sentences yields the best performance in both experiments.", "rank": 275, "start": 34556, "IsComparative": "0", "id": "st_275"}]}]}