{"paragraph_scenes_info": [{"x": 3, "text": "The contributions of the thesis include (1) the two-stage dimension reduction framework that better handles significant information loss in visualization of high- dimensional data, (2) efficient parametric updating of computational methods for fast and smooth user interactions, and (3) an iteration-wise integration framework of computational methods in real-time visual analytics.The latter parts of the thesis focus on the development of visual analytics systems involving the presented compu- tational methods, such as (1) Testbed: an interactive visual testbed system for various dimension reduction and clustering methods, (2) iVisClassifier: an interactive visual classification system using supervised dimension reduction, and (3) VisIRR: an inter- active visual information retrieval and recommender system for large-scale document data."}, {"x": 6, "text": "In these days, an increasing amount of data is being generated in various forms such as documents, images, etc.To analyze these data, the raw data are encoded as high-dimensional vectors and then computational methods are typically applied in the context of statistical machine learning and data mining.For instance, in order to perform the facial recognition given a set of facial image data, the image data are first encoded as a bag-of-feature-points scheme <91>, and then a certain classification technique, such as support vector machines <118>, is applied.In many cases, however, these computational methods are done in a fully automated manner, and such approaches bear many limitations as follows: data rithms exist, and users may not know which one to apply.Furthermore, each method imposes its own assumptions on the data and involves a set of parame- ters to be carefully determined.However, these issues are by no means straight- forward to solve.For instance, the characteristics of the data do not meet the underlying assumption in the algorithms.Recent manifold learning algorithms <125, 111, 17> assume the low-dimensional curvi-linear manifold structure, but there is no guarantee that the data at hand have such a structure.As an- other example, even though the recent nonlinear kernel-based methods sound appealing, how to determine the optimal kernel parameters, e.g., a bandwidth parameter in Gaussian kernels, is also dependent on the data."}, {"x": 11, "text": "3.Ambiguity in task formulation.Large-scale data make it hard to explore and understand them, and sometimes they even obscure what to solve and what to be able to do with our data.In this situation, people may seek for some insight about the data as to which data items may behave differently from the rest.In addition, even if people have a clear goal in mind, it is often the case that the mathematical formulation required to apply computational methods is not straightforward.For instance, suppose one wants to analyze social network data to identify which person or group has caused a certain movement.This task may not be simply interpreted as a mathematical objective function or fit to the existing formulation of computational methods."}, {"x": 12, "text": "In contrast to the fully automated computational approaches that lack deep under- standing and careful treatment of the data, the area of visual analytics <127, 78>, which is defined as the science of analytical reasoning facilitated by interactive visual inter- faces, has gained increasing interest.Visual analytics has fascinating characteristics that leverage humans ability of quick visual insight in the data analysis and decision processes, and compared with the well-established literature of information visual- ization, visual analytics typically focuses on reasoning and decision-making processes rather than just understanding the data visually.Unfortunately, however, most of the state-of-the-art visual analytics techniques or systems do not properly accommodate large-scale data.One of the reasons is that although humans are good at quick visual insight, such an ability deteriorates when the number of visualized objects, either data items or features, is large.Furthermore, the limited screen space tends to create visual clutter when visualizing large-scale high-dimensional data.For instance, paral- lel coordinates, a widely-used visualization technique for multi-dimensional data, do not scale when the dimension exceeds several tens or hundreds."}, {"x": 14, "text": "Such appealing capabilities of computational methods motivated people towards the tight integration of them with visual analytics for large-scale data.For example, Seo et al.<117> have provided an interactive visualization system to explore the clustering results obtained by the widely-used hierarchical clustering method.A recently proposed method, latent Dirichlet allocation, has been utilized in visual analytics tools for text documents <134, 50, 38, 37>.Even though various efforts have been made for utilizing computational methods in visual analytics, there is still significant room to improve such utility.That is, even though numerous advanced computational methods are currently being proposed and some of them claim that they can be easily adopted in visualization applications, practical visual analytics systems do not seem to currently take full advantage of these advanced methods.As a result, people still tend to only use a few of the basic computational methods, e.g., principal component analysis (PCA) <74> for dimension reduction and k-means<19> for clustering in many real-world analysis tasks.In addition, the above-mentioned intelligent information could be useful in interactive visualization approaches, but its usage in this direction is still limited.In this thesis, I address several hurdles in achieving an appropriate integration as follows:"}, {"x": 16, "text": "This thesis aims to overcome these hurdles and achieve the tight integration between computational methods and visual analytics in modern data analysis scenarios.I claim that to this end, the computational methods have to be customized and even be re-invented for use in visual analytics, and at the same time, the visual analytics systems have to expose them to users out of a black box via interactive capabilities of choosing the right methods and the best parameters.Based on this claim, the thesis provides (1) several novel approaches for customizing computational methods and (2) the visual analytics systems integrating such customization in various application domains."}, {"x": 19, "text": "The theoretical and algorithmic customization of computational methods will enable their appropriate integration with visual analytics for analyses of complex large-scale high-dimensional data.Visual analytics systems equipped with interactive capa- bilities with various computational methods will help analysts better understand the data at hand and solve complicated anal- ysis tasks."}, {"x": 20, "text": "Under this statement, the thesis aims at achieving the true visual analytics where the computational analyses and the user-driven interactive visual exploration are tightly integrated.More specifically, the thesis mainly focuses on two key categories of computational methods: dimension reduction and clustering.Dimension reduction and clustering play an essential role in dealing with large-scale high-dimensional data in visual analytics by reducing the data dimension and the number of data items.In other words, dimension reduction can reveal meaningful dimensions or features out of numerous original dimensions as well as provide a means of visualizing high- dimensional data in visual 2D/3D spaces so that analysts can obtain the insight about data relationships with respect to geometric locations of data."}, {"x": 23, "text": "1.Which characteristics in terms of data, algorithms, and humans, should be exploited in order to make computational methods better support visual ana- lytics?Based on these characteristics, how can the computational methods be re-designed in visual analytics?"}, {"x": 35, "text": "The rest of the thesis presents each of the above-listed contributions in more detail.Chapter 2 presents a novel framework of two-stage dimension reduction for visu- alization of high-dimensional data.It is inevitable that significant information will be lost when reducing the original high dimension of data into 2D/3D in visualization.By using the formulations in terms of cluster-wise measures, the two-stage dimension reduction framework, which separates the criteria of the original dimension reduction methods and the further information loss, are presented.The thesis presents the de- tailed criteria using widely-used dimension reduction methods and their visualization"}, {"x": 38, "text": "Chapter 4 presents another novel approach called PIVE (Per-Iteration Visualization Environment for supporting real-time interactive visualization with computational methods) to make computational methods significantly efficient in visual analytics.In this chapter, the presented approach exploits the fact that most computational methods are built upon iterative algorithms.Rather than waiting for the entire it- eration to finish, the iteration-wise framework visualizes the intermediate results of computational methods and enables users to interact with them in real time during it- erations.The details of the proposed framework and its applications using well-known visual analytics systems are presented."}, {"x": 39, "text": "Chapter 5 describes the fundamental visual analytics system called the Testbed system, which makes various traditional and advanced dimension reduction and clus- tering algorithms readily available in visual analytics scenarios.In addition to the basic but crucial capabilities of selecting different methods and their parameters, ex- ploring raw data, and brushing-and-linking, the system features a novel capability of alignment between multiple visualization results for easy comparisons.The system details and several usage scenarios are discussed."}, {"x": 40, "text": "Chapter 6 presents iVisClassifier, another visual analytics system developed based on the Testbed system.iVisClassifier is a customized system for classification appli- cations, which mainly utilizes a specific dimension reduction among those available in the Testbed system.iVisClassifier supports ample features in order to help users un- derstand data, which are, for example, what classes are confusing against each other, which data items are easy/difficult to classify, as well as enables users to intervene in the classification processes.The system details and the usage scenarios in the facial recognition context are described."}, {"x": 41, "text": "Chapter 6 presents VisIRR, an interactive visual information retrieval and recom- mender system based on the Testbed system.This system directly tackles the large scale of data by starting with more than 400,000 data items.Besides the basic capa- bilities of clustering and visualizing the retrieved documents based on users queries, the system has the capability to provide recommendations based on users prefer- ence information.The details of the computational methods used and visualization processes are presented along with several usage scenarios."}, {"x": 44, "text": "TWO-STAGE FRAMEWORK FOR VISUALIZATION OF CLUSTERED HIGH-DIMENSIONAL DATA"}, {"x": 45, "text": "In this chapter, we will discuss dimension reduction methods for 2D visualization of high dimensional clustered data.We propose a two-stage framework for visualizing such data based on dimension reduction methods.In the first stage, we obtain the reduced dimensional data by applying a supervised dimension reduction method such as linear discriminant analysis which preserves the original cluster structure in terms of its criteria.The resulting optimal reduced dimension depends on the optimization criteria and is often larger than two.In the second stage, the dimension is further reduced to two for visualization purposes by another dimension reduction method such as principal component analysis.The role of the second stage is to minimize the loss of information due to reducing the dimension all the way to two.Using this framework, we propose several two-stage methods, and present their theoretical characteristics as well as experimental comparisons on both artificial and real-world text data sets."}, {"x": 47, "text": "Within the visual analytics community, various types of information content are rep- resented using high dimensional signatures.To make these signatures useful they often need to be transformed into a lower dimension (i.e., 2D or 3D) for a variety of visual representations such as scatter plots.Many researchers in this community have used a wide assortment of dimension reduction techniques, e.g., self-organizing map (SOM) <83>, principal component analysis (PCA) <75>, multidimensional scaling (MDS) <45>, etc.However, it is not always clear why a certain technique has been chosen over another, especially to the end user.Typically, the goal of dimension reduction techniques can be viewed in terms of two aspects: efficiency and accuracy.Efficiency as defined here is the time to compute the reduction, but accuracy may not be as simple to quantify.Many would amiably agree to quantify accuracy as a measure of the relationship preservation in the high dimensional space to the reduced dimensional space.Note that most techniques either directly or indirectly work on this principle."}, {"x": 48, "text": "There are other properties that are important to those interpreting the semantics of the reduced space.Specifically, we note that while local neighbor preservation is important it depends upon the analysis task.No single reduction technique will provide the complete view as various properties of the space are obscured or lost.We have mentioned that typically the primary objective is relationship preservation.However, there are at least two others: outlier and macro structure visualization.Outliers are conceptually easy (i.e., a variance beyond some threshold), but more difficult to quantify, as we do not necessarily know which set of outliers are important to accentuate to the user.Certain techniques (e.g., PCA) tend to show outliers more readily, however tend to compress the reduced space at the expense of showcasing the outliers.Other techniques (e.g., SOM) maximize space usage well, but do so at the expense of masking or even hiding those outliers.Likewise, macro structures of the high dimensional space may be masked or massively distorted during the reduction.Macro structures are those larger order groupings (e.g., clusters) that exist in the original dimensional space.We recognize they are important in dimension reduction research and to those in the visual analytics community.However, few of them focus on data representation especially for visualization of the clustered data <147, 84, 49>."}, {"x": 51, "text": "The focus of this chapter is the fundamental characteristics of dimension reduction techniques for visualizing high dimensional data in the form of a 2D scatter plot when the data has cluster structure.The role of dimension reduction here is to give a 2-dimensional representation of data while preserving cluster structure as much as possible.To this end, supervised dimension reduction methods that incorporate cluster information such as linear discriminant analysis (LDA) <60> or orthogonal centroid method (OCM) <71> can be naturally considered."}, {"x": 52, "text": "However, one of the issues is that with many dimension reduction methods de- signed to preserve the cluster structure in the data, the theoretically optimal reduced dimension, which is the smallest dimension that is acceptable with respect to the optimization criteria of the dimension reduction method, is usually larger than 2.For example, in LDA, the minimum reduced dimension that preserves the cluster struc- ture quality measure defined as a trace maximization problem is one less than the number of clusters in the data in general <68, 67>."}, {"x": 53, "text": "In this case, one may simply choose the two dimensions that contribute most to such a measure.However, with only two dimensions, such a measure may become significantly smaller than the original quantity after dimension reduction.This results in loss of information that hinders visualization in properly reflecting the true cluster relationship of the data.A similar situation may occur when using PCA for visualizing the data not having a cluster structure.Even though PCA finds the principal axes that maximally capture the variance of the data, when the resulting 2-dimensional representation of the data maintains only a small fraction of the total variance, the relationships of the data in 2 dimension are likely to be highly inconsistent with those in the original dimension."}, {"x": 55, "text": "In this chapter, we present both theoretical and empirical answers to these is- sues.Specifically, we propose several two-stage methods utilizing linear dimension reduction methods such as LDA, orthogonal centroid method (OCM), and principal component analysis (PCA), and we present their theoretical justifications by mod- eling the optimization criteria for which each method provides the optimal solution.Also, we illustrate and compare the effectiveness of the proposed methods by show- ing empirical visualization on synthetic and real-world data sets.Although nonlinear dimension reduction methods such as MDS or other manifold learning methods such as isometric feature mapping <125> and locally linear embedding <111> may also be utilized for the effective 2D visualization of high dimensional data, our focus in this chapter is on linear methods.The linear methods are computationally more efficient in general, and unlike most of the manifold learning methods, they also provide di- mension reducing transformations that can be applied to map and visualize unseen data points in the same space where the existing data are visualized."}, {"x": 57, "text": "The rest of this chapter is organized as follows.In Section 2.3, LDA, OCM, and PCA are described based on a unified framework of the scatter matrices and their trace optimization problems.In Section 2.4, we formulate two-stage dimension reduction methods, and in Section 2.5, several two-stage methods for visualization are proposed and compared along with their criteria.Experimental comparisons are given using artificial and real-world data sets in Section 2.6, and conclusions are drawn in Section 2.7."}, {"x": 58, "text": "2.3 Dimension Reduction as Trace Optimization Problem"}, {"x": 60, "text": "Suppose a dimension reducing linear transformation GT  Rlm maps an m- dimensional data vector x to a vector z in an l-dimensional space (m > l):"}, {"x": 61, "text": "Let Ni denote the set of column indices that belong to cluster i, and ni the size of Ni.The i-th cluster centroid c(i) and the global centroid c are defined, respectively, as we obtain values that can be used to measure the cluster quality.Note that from Eqs.(8) and (9), trace(Sb) can be viewed as the squared sum of the pairwise distances between cluster centroids as well as that of the distances between each centroid and the global centroid."}, {"x": 62, "text": "The cluster structure quality can be defined by analyzing how well each clus- ter can be discriminated from each other.High quality clusters usually have small trace(Sw) and large trace(Sb), relating to the small variance within each cluster and the large distances between clusters.Subsequently, dimension reduction methods may be intended to maximize trace(GT SbG) and minimize trace(GT SwG) in the reduced dimensional space.This simultaneous optimization can be approximated to a single criterion as"}, {"x": 63, "text": "On the other hand, regardless of cluster dependent terms, Sw and Sb, the trace of the total scatter matrix St can be maximized as"}, {"x": 64, "text": "Jt(G) = max trace(GT StG), (13) GT G=I"}, {"x": 65, "text": "which turns out to be the criterion of PCA.In Eqs.(12) and (13), without the constraint, GT G = I, Jb(G) and Jt(G) can become arbitrarily large."}, {"x": 66, "text": "In what follows, LDA, OCM, and PCA are discussed based on such maximization criteria, and their properties relevant to visualization are identified."}, {"x": 67, "text": "2.3.1 Linear Discriminant Analysis (LDA)"}, {"x": 68, "text": "Conceptually, in LDA, we are looking for a dimension reducing transformation that keeps the between-cluster relationship as remote as possible by maximizing trace(GT SbG) while keeping the within cluster relationship as compact as possible by minimizing trace(GTSwG).As shown in Eq.(11), the criterion of LDA can be written as"}, {"x": 69, "text": "Jb/w(G) = max trace((GT SwG)1(GT SbG)).(14) ItcanbeshownthatforanyGRml wherem>l,"}, {"x": 70, "text": "trace((GT S G)1(GT S G))  trace(S1S ), (15) wbwb"}, {"x": 72, "text": "creased after dimension reduction <60>.By setting the derivative of Eq.(14) with respect to G to zero, which gives the first order optimality condition, it can be shown that the solution of LDA, where we denote it as GLDA, has the columns which are the leading generalized eigenvectors u of the generalized eigenvalue problem,"}, {"x": 73, "text": "Sbu = Swu.(16) Since the rank of Sb is at most k1, LDA achieves the upper bound of trace((GT SwG)1"}, {"x": 76, "text": "2.3.2 Orthogonal Centroid Method (OCM)"}, {"x": 77, "text": "Orthogonal centroid method (OCM) <71> focuses only on maximizing trace(GT SbG) under the constraint of GT G = I. The criterion of OCM is shown as"}, {"x": 78, "text": "Jb(G) = max trace(GT SbG).(18) GT G=I"}, {"x": 80, "text": "trace(GT SbG)  trace(Sb), (19)"}, {"x": 81, "text": "which means the cluster structure quality measured by trace(Sb) cannot be increased after dimension reduction.The solution of Eq.(18) can be obtained by setting the columns of G as the leading eigenvectors of Sb.Since Sb has at most k  1 nonzero eigenvalues, the upper bound of trace(GT SbG) in Eq.(19) can be achieved for any l such that l  k  1, i.e.,"}, {"x": 82, "text": "trace(GT SbG) = trace(Sb) for l  k  1.(20) 18"}, {"x": 83, "text": "Eq.(20) indicates trace(Sb) is preserved between the original and the reduced dimen- sional spaces."}, {"x": 84, "text": "An advantage of OCM is that it achieves an upper bound of trace(GT SbG) more efficiently by using QR decomposition, avoiding the eigendecomposition.The algo- rithm of OCM is as follows.First the centroid matrix C is formed so that each column of C is composed of each clusters centroid vector, i.e., C =   c1 c2    ck  .Then the reduced QR decomposition <61> of C is computed for C = QkR where Qk  Rmk with QTk Qk = I and R  Rkk is upper triangular.The solution of OCM, GOCM, is found as"}, {"x": 87, "text": "spanned by the centroids, and l = k in this case.Finally, OCM achieves trace(GTOCM SbGOCM ) = trace(Sb), where l = k."}, {"x": 88, "text": "By using the equivalence between Eqs.(3) and (4), one can prove that each pair- wise distance between cluster centroids is also preserved in the reduced dimensional space obtained by OCM."}, {"x": 89, "text": "Another important property of OCM is that by projecting data into the subspace spanned by the centroids, the order of similarities between any particular point and centroids are preserved in terms of Euclidean norm and cosine similarity measure <71, 67>.In other words, for any vector q  Rm1 and cluster centroids c(i) and c(j), we have"}, {"x": 90, "text": "2.3.3 Principal Component Analysis (PCA)"}, {"x": 91, "text": "PCA is a well-known dimension reduction method that captures the maximal variance in the data.The criterion of PCA can be written as"}, {"x": 92, "text": "Jt(G) = max trace(GT StG).(21) GT G=I"}, {"x": 94, "text": "trace(GT StG)  trace(St), (22)"}, {"x": 95, "text": "which means trace(St) cannot be increased after dimension reduction.The solution of Eq.(21), where we denote it as GPCA, can be obtained by setting the columns of G as the leading eigenvectors of St.Since the rank of St is at most min(m, n), PCA achieves the upper bound of trace(GTStG) in Eq.(22) for any l such that l  min(m, n), i.e., trace(GTP CAStGP CA) = trace(St) for l  min(m, n)."}, {"x": 96, "text": "In many applications of PCA, however, l is usually chosen as a fixed value less than the ranke of St for the purpose of dimension reduction or noise reduction.This noisy subspace corresponds to the smallest eigenvectors of St, and they are removed by PCA for better representation of the data."}, {"x": 97, "text": "Although St is related to Sb and Sw as in Eq.(6), St as it is does not contain any information on cluster labels.That is, unlike LDA and OCM, PCA ignores the cluster structure represented by Sb and/or Sw, which is why PCA is considered as an unsupervised dimension reduction method."}, {"x": 98, "text": "Usually, PCA assumes that the global centroid is zero by subtracting the empirical mean of the data from each data vector.The centered data can be represented as A  ceT , where e is n-dimensional vector whose components are all 1s."}, {"x": 101, "text": "GPCA =arg min GGT(AceT)(AceT), G, GT G=Il"}, {"x": 102, "text": "where the matrix norm    is either a Frobenius norm or a Euclidean norm.The three discussed methods are summarized in Table 1."}, {"x": 106, "text": "Further assume that it is composed of two stages of dimension reductions as follows.In the first stage, a dimension reducing linear transformation GT  Rlm maps an m-dimensional data vector x to a vector y in the l-dimensional space (l  m):"}, {"x": 110, "text": "HT :yRl1 z=HTyR21.(25) Such consecutive dimension reductions performed by GT followed by HT can be"}, {"x": 113, "text": "In the next section, discussion will be focused on various ways for choosing the first stage dimension reducing transformation G and the second stage dimension transfor- mation H with a purpose to construct combined dimension reducing transformation V T = HT GT for 2-dimensional visualization according to various optimization crite- ria."}, {"x": 115, "text": "All the proposed two-stage methods start from one of the supervised dimension re-duction methods such as LDA or OCM that are designed for clustered data.In the first stage (by GT  Rlm in Eq.(24)), the dimension is reduced by LDA or OCM to the smallest dimension that satisfies Eq.(17) or (20), respectively.Therefore in the first stage, the cluster structure quality measured either by trace(S1S ) or trace(S ) wbb is preserved.Then we perform the second-stage dimension reduction (by HT  R2l in Eq.(25)) that minimizes the loss of information either by applying the same cri- terion used in the first stage or by using Jt in Eq.(21), i.e., that of PCA.As seen in Section 3.3, Eq.(21) gives the best approximation of the first-stage results that minimize the difference in terms of Frobenius/Euclidean norm."}, {"x": 118, "text": "In this method, LDA is applied in the first stage, and trace(S1S ) is preserved in the wb"}, {"x": 122, "text": "2.5.2 LDA followed by PCA"}, {"x": 123, "text": "In this method, LDA is applied in the first stage, and trace(S1Sb) is preserved in w the l-dimensional space where l = k  1.In the second stage, PCA is applied in order to obtain the best approximation of the l-dimensional first-stage results in terms of Frobenius/Euclidean norm."}, {"x": 125, "text": "2.5.3 OCM followed by PCA"}, {"x": 126, "text": "In this method, OCM is applied in the first stage, and trace(Sb) is preserved in the l-dimensional space where l = k.In the second stage, PCA is applied in order to obtain the best approximation of the l-dimensional first-stage results in terms of Frobenius/Euclidean norm."}, {"x": 130, "text": "2.5.4 Rank-2 PCA on Sb"}, {"x": 134, "text": "Eq.(35) holds since the eigenvectors of Sb, u1 and u2, are in the range space of GOCM .The criterion of Eq.(36) has been used in one of the successful visual analytic systems, IN-SPIRE, for 2D representation of document data <138>."}, {"x": 138, "text": "2.6.1 Regularization on LDA for undersampled data"}, {"x": 139, "text": "In undersampled cases, the LDA criterion shown in Eq.(14) cannot be applied di- rectly because Sw is singular.In order to overcome this singularity problem, Howland et al.proposed a universal algorithmic framework of LDA using the generalized sin- gular value decomposition (LDA/GSVD) <68, 67>.Specifically, for the case when m  n  k, which is usual for most undersampled problems, LDA/GSVD gives the solution for G such that GT SwG = 0 while maintaining the maximum value of trace(GT SbG).This solution makes sense since LDA criterion is formulated to min- imize trace(GTSwG).However, it means that all of the data points belonging to a specific cluster are represented as a single point in the reduced dimensional space, which lessens the generalization ability for classification as well as for visualizing the individual data relationship within each cluster."}, {"x": 140, "text": "On the contrary, the fact that LDA makes GTSwG = 0 can be viewed as an advantage for visualization purposes since LDA has the capability to fully minimize trace(GT Sw G).By means of regularization on Sw one can control trace(GT Sw G), which determines the scatter of the data points within each cluster.In regularized LDA which was originally proposed to avoid the singularity of Sw in classification context, Sw is replaced by a nonsingular matrix Sw + I where I is an identity matrix, and  is a control parameter.In general, as  is increased, the within- cluster distance, trace(GTSwG), also becomes larger with data points being more scattered around their corresponding centroids.As  is decreased, the within-cluster distance becomes smaller, and the data points gather closer around their centroids.Such manipulation of  can be exploited in a visualization context because one can choose an appropriate value of  so that the second-stage method such as PCA, which maximizes trace(GT StG) = trace(GT SbG + GT SwG), does not focus too much on trace(GT SwG).The results that follow are based on such choices of ."}, {"x": 142, "text": "The data sets tested are composed of one artificially-generated Gaussian-mixture dataset (GAUSSIAN) and three real-world text data sets (MEDLINE, NEWSGROUPS, and REUTERS) that are clustered based on their topics.All the text documents are encoded as term-document matrices where each dimension corresponds to a particular word, and the value of a certain dimension represents the frequency of the correspond- ing word shown in the document.Each data set is set to have an equal number of data per cluster, and have a mean of zero which is attained by subtracting the global mean.(See Section 6.3.)"}, {"x": 144, "text": "The GAUSSIAN data set is a randomly generated Gaussian mixture with 10 clusters.Each cluster is made up of 100 data vectors, which add up to 1000 in total, and the dimension is set to 1100, which is slightly more than the number of the data items.In its visualization shown in Fig.(1), the clusters are labeled using letters as"}, {"x": 145, "text": "The MEDLINE data set is a document corpus related to medical science from the"}, {"x": 148, "text": "where the letters in parentheses are used in the visualization shown in Fig.(2)."}, {"x": 149, "text": "The NEWSGROUPS data set <11> is a collection of newsgroup documents, and originally composed of 20 topics.However, we have chosen 11 topics for visualization, and each cluster is set to have 70 documents.The original dimension is 16702, and"}, {"x": 152, "text": "where the letters in parentheses are used in the visualization shown in Fig.(3)."}, {"x": 153, "text": "The REUTERS data set <11> is the document collection that appeared in the Reuters newswire in 1987, and originally composed of hundreds of topics.Among them, 10 topics related to economic subjects are chosen for visualization, and each cluster has 80 documents.The original dimension is 3907, and the cluster labels are"}, {"x": 156, "text": "where the letters in parentheses are used in the visualization shown in Fig.(4)."}, {"x": 158, "text": "Fig.5 is the example of applying OCM+PCA to the MEDLINE data set with and without data centering.Once the MEDLINE data set is encoded as a term-document matrix, every component has a non-negative value, which results in the global centroid that is significantly far from the origin.Then performing PCA without data centering might give the first principal axis as the one reflecting the global centroid rather than that discriminating clusters.If we consider projecting the data onto each of the horizontal and the vertical axes in Fig.5, the former, which corresponds to the first principal axis, does not help in showing the cluster structure clearly, and only the vertical axis, which corresponds to the second principal axis from PCA, discriminates clusters.We have found that such undesirable behavior is common in many cases without data centering, which is why we assume that data is centered throughout this chapter.Accordingly, all the results shown in Figs 1-4 are obtained after data centering."}, {"x": 160, "text": "The results of four two-stage methods for the tested data sets are shown in Figs.1-42.In all cases, LDA-based methods show cluster structures more clearly than OCM- based methods.This proves the effectiveness of LDA that considers both within- and between-cluster measures while OCM only takes into account the latter.Due to this difference, OCM generally produces a widely-scattered data representation within each cluster.As a result, in the NEWSGROUPS dataset, such a wide within-cluster variance significantly deteriorates the cluster structure visualization even if OCM still"}, {"x": 162, "text": "In the MEDLINE and the REUTERS data sets, all of the four methods produce"}, {"x": 163, "text": "relatively similar results.However, we have controlled the within-cluster variance in LDA-based methods using the regularization term I. In addition, the fact that rank- 2 LDA and LDA+PCA produce almost identical results indicates that GTLDAStGLDA is dominated by GTLDASbGLDA after LDA is applied in the first stage as we expected."}, {"x": 165, "text": "Rank-2 LDA represents each cluster most compactly by minimizing the within- cluster radii both in the first and the second stage.However, it may reduce the between-cluster distances as well because Jb/w maximizes the conceptual ratio of two scatter measures.As can be seen in the two LDA-based methods applied to the NEWGROUPS data set, while rank-2 LDA minimizes the within-cluster radii, it also places the centroids closer to each other as compared to those in LDA+PCA.Due to this effect, which one is preferable between rank-2 LDA and LDA+PCA depends on the data set to be visualized."}, {"x": 166, "text": "Overall, OCM+PCA and Rank-2 PCA on Sb show similar results.It means GT SbG  GT StG in that the difference between two methods lies in whether PCA is applied to GT SbG or GT StG in the second stage.Since performing PCA on GT SbG is computationally more efficient than PCA on GT StG, Rank-2 PCA on Sb can be a good alternative to OCM+PCA in case efficient computation is important."}, {"x": 167, "text": "Finally, these visualization results reveal the interesting cluster relationships un- derlying in the data.In Fig.(2), the clusters for colon cancer (c) and oral cancer (o)"}, {"x": 168, "text": "are shown close to each other.In Fig.(3), the clusters of soc.religion."}, {"x": 170, "text": "(a), and those of sci.crypt (y) and sci.med (d) are closely located respectively in LDA-based methods.In addition, the two clusters, misc.forsale (f) and rec.sport.baseball (b), are shown to be the most distinctive, which makes sense because those topics"}, {"x": 171, "text": "are quite irrelevant to the others.In Fig.(4), the clusters of grain (g), wheat (w), and corn (c) as well as those of money-fx (m) and interest (i) are visualized very close."}, {"x": 173, "text": "According to our results, LDA-based methods are shown to be superior to OCM-based methods since both within- and between-cluster relationships are taken into account religion.misc (r), those of comp.sys.ibm.in LDA.Especially, combined with PCA in the second stage, LDA+PCA achieves a clear discrimination between clusters as well as the best approximation of the results of LDA when the distance between data is measured in terms of Frobenius/Euclidean norm."}, {"x": 175, "text": "CHAPTER III EFFICIENT UPDATING OF COMPUTATIONAL METHODS DUE TO PARAMETER CHANGES"}, {"x": 176, "text": "One of the most widely-used nonlinear data embedding methods is ISOMAP.Based on a manifold learning framework, ISOMAP has a parameter k or o that controls how many edges a neighborhood graph has.However, a suitable parameter value is often difficult to determine because of a time-consuming optimization process based on certain criteria, which may not be clearly justified.When ISOMAP is used to visualize data, users might want to test different parameter values in order to gain various insights about data, but such interaction between humans and such visual- izations requires reasonably efficient updating, even for large-scale data.To tackle these problems, we propose an efficient updating algorithm for ISOMAP with pa- rameter changes, called p-ISOMAP.We present not only a complexity analysis but also an empirical running time comparison, which show the advantage of p-ISOMAP.We also show interesting visualization applications of p-ISOMAP and demonstrate how to discover various characteristics of data through visualizations using different parameter values."}, {"x": 178, "text": "One of the most widely-used data mining techniques that reduce noise in data and im- prove efficiency in terms of computation time and memory usage is dimension reduc- tion.Recently, nonlinear dimension reduction techniques, which have been actively investigated, revealed the underlying nonlinear structure in data.Such nonlinearity is often considered as a curvilinear manifold with a much lower dimension than that in the original high-dimensional space.Among the most recent nonlinear dimension reduction methods, isometric feature mapping (ISOMAP) has shown its effectiveness in capturing the underlying manifold structure in the reduced dimensional space by being successfully applied to synthetic data such as Swiss roll data and real-world data such as facial image data <125>."}, {"x": 179, "text": "ISOMAP shares the basic idea with a traditional technique, classical multidimen- sional scaling (MDS).Classical MDS first constructs the pairwise similarity matrix, which is usually measured by the Euclidean distance, and computes the reduced dimensional mapping that maximally preserves such a similarity matrix in a given reduced dimension.ISOMAP differs from classical MDS in that it constructs the pairwise similarity matrix based on the geodesic distance estimated by the short- est path in the neighborhood graph of data.The neighborhood graph is formed by having vertices as data points and setting each edge weight between the nodes as their Euclidean distance only if at least one node is one of the k-nearest neighbors (k-NN) of the other node (k-ISOMAP) or if their Euclidean distance is smaller than o (o-ISOMAP).Hence, ISOMAP has an either parameter k or o to construct the neighborhood graph."}, {"x": 180, "text": "This chapter focuses on the algorithm and applications of the dynamic updating of ISOMAP when the value of k or o varies.It is generally known that in ISOMAP, if k or o is too small, the graph becomes sparse, resulting in infinite geodesic distances between some pairs of data points, and if k or o is too large, it is prone to short circuit the true geometry of the manifold.However, it is not always easy to figure out which value of k or o is appropriate for the data at hand.One way of optimizing these parameters is using certain quantitative measures such as residual variance <12, 125, 112> and finding the elbow point at which the residual variance curve stops decreasing significantly as the parameter value changes.However, running ISOMAP repeatedly using different parameter values for k or o may be time-consuming since it involves computationally intensive processes such as the all-pairs shortest path computation and the eigendecomposition, whose complexity is usually O(n3) in which n is the number of data points.1"}, {"x": 181, "text": "In practice, there is also often no guarantee of the existence of the underlying well-defined manifold structure in data, and thus, one may not be sure if manifold learning methods such as ISOMAP are suitable for the data at hand.Even so, one may still want to try ISOMAP or another manifold learning method in order to see if it serves ones purpose.In this case, however, it may not be a good idea to rely on a particular value of k or o to achieve a reasonable dimension reduction since the optimal value tends to be indistinct in terms of a certain measure.When it comes to the visualization of high-dimensional data in two- or three-dimensional space, we can acquire different insights on the data by using various dimension reduction techniques <35>.This statement also holds true even when we use just a single dimension reduc- tion method, e.g., ISOMAP, while we test its various parameter values.In short, visualizations using ISOMAP with different parameter values for k or o can provide us with various aspects of our data.In instances of the Swiss roll and toroidal helix data sets shown in Fig.6, one may want to visualize them based on the unfolded version of its manifold, as shown in Figs.6(b) and 6(f), but sometimes one may also want to see how the underlying manifold is curved in the original space, i.e., the curvature of the manifold itself, as shown in Figs.6(d) and 6(h).2 It is also possible that visualizations of the transition between these two cases, shown in Figs.6(c) and 6(g), imply different insight about data.In this sense, it is worthwhile for users to test different parameter values in ISOMAP to visualize data in various ways."}, {"x": 182, "text": "1The complexity of the (all-pairs) shortest path computation depends on the algorithm.Floyd- Warshall algorithm requires O(n3) computations while Dijkstras algorithm does O(|e|nlogn) com- putations <13> in which |e| is the number of edges."}, {"x": 184, "text": "Such visualizations, however, should provide users with smooth and prompt in- teraction that requires fast and efficient computations of the results.In other words, when users change the parameter value, if they have to wait for a significant amount of time, then such interaction would not be practical.Motivated by the above men- tioned cases, we propose p-ISOMAP, an efficient dynamic updating algorithm for ISOMAP when the parameter value changes.Given the ISOMAP result from a par- ticular parameter value, our proposed algorithm updates the previous result to obtain another ISOMAP result of the same data with a new parameter value instead of re- computing ISOMAP for different parameter values from scratch.We present the complexity analysis of our algorithms as well as the experimental comparison of their computation times.In addition, we demonstrate several visualization examples by varying the parameters in ISOMAP, which not only show the interesting aspects of the tested data but also help us thoroughly understand the behavior of ISOMAP in terms of parameter values."}, {"x": 185, "text": "The rest of this chapter is organized as follows.Section 3.2 briefly introduces ISOMAP and its algorithm, and Section 3.3 discusses previous work related to p- ISOMAP.Section 3.4 describes the algorithmic details and the complexity analysis of p-ISOMAP, and Section 3.5 presents not only the experimental results that compare the computation times of ISOMAP and p-ISOMAP but also interesting visualization examples of real-world data using p-ISOMAP.Finally, Section 3.6 concludes our work."}, {"x": 187, "text": "Given a set of data points represented as M-dimensional vectors xi  RM for i = 1, ..., n, ISOMAP assumes a lower dimensional manifold structure in which the data are embedded.It yields the m-dimensional representation of xi as yi  Rm (m  M) such that the Euclidean distance between yi and yj approximates their geodesic distance along the underlying manifold as much as possible.Such an approximation builds on the classical MDS framework, but unlike MDS, ISOMAP has the capability of handling nonlinearity existing in the original space since a geodesic distance reflects an arbitrary curvilinear shape of the manifold.On input, ISOMAP takes a data matrix X =   x1 x2  xn    RMn, a reduced dimension m, and a parameter k or o.The algorithm is composed of three steps:"}, {"x": 188, "text": "1.Neighborhood graph construction.ISOMAP first computes the pairwise Eu- clidean distance matrix, DX  Rnn, in which DX (i, j) is the Euclidean dis- tance between xi and xj.Then it determines the set of neighbors for each point either by k-nearest neighbors or by those within a fixed radius o. Be- tween a point xi and each of its neighbors xj, an edge e(i, j) is assigned with a weight equivalent to their Euclidean distance, and in this way, ISOMAP forms a weighted undirected neighborhood graph G = (V, E), where the vertices in V correspond to the data points xis."}, {"x": 189, "text": "2.Geodesic distance estimation.In the second step, ISOMAP estimates the pair- wise geodesic distance based on the shortest path length for every vertex pair along the neighborhood graph G, which is represented as a matrix DG  Rnn in which DG(i, j) is the shortest path length between xi and xj in G."}, {"x": 190, "text": "3.Lower dimensional embedding.The final step performs classical MDS on DG, producing m-dimensional data embedding, Y =   y1 y2  yn    Rmn.First, the pairwise geodesic distance matrix DG is converted to an inner product matrix BG as avoid ambiguity about which changes of neighbors in a directed graph cause actual edge changes in an undirected one in which we have to actually compute the shortest paths.The detailed procedure of the neighborhood graph update are described in Algorithm 1.As an output, it produces the set of effectively inserted/removed edges, which is, in turn, used in the shortest path update stage."}, {"x": 191, "text": "3.4.1.1 Time Complexity"}, {"x": 192, "text": "In ISOMAP, the time complexity in constructing a neighborhood graph is as fol- lows.It starts with a sort operation for a given data set whose time complexity is O(n2 log n).Then obtaining G and G requires O(nq), in which q is the maximum degree of vertices in the graph G.In p-ISOMAP, the time complexity required in the neighborhood graph update is bounded by O(n  maxi |ei|), in which |ei| is the number of inserted/removed edges associated with xi."}, {"x": 193, "text": "3.4.2 Shortest Path Update"}, {"x": 194, "text": "The shortest path update stage, which is one of the most computationally inten- sive steps in p-ISOMAP, takes the input as either A or D and updates the shortest path length matrix DG.In order to facilitate this process, p-ISOMAP maintains and updates the information about the shortest path itself with a minimal memory requirement in the form of a predecessor matrix P  Rnn, in which P(i, j) stores the node index immediately preceding xj in the shortest path from xi to xj.3 For instance, if the shortest path from x1 to x2 is composed of x1  x4  x3  x2, then we set P (1, 2) = 3.For the shortest path update, p-ISOMAP performs two steps:"}, {"x": 197, "text": "summarizes the shortest path update process based on F.3.4.2.3 Time Complexity"}, {"x": 198, "text": "When a parameter increases, Algorithm 2 requires the time complexity of O(|A|nq"}, {"x": 199, "text": "maxi,a |T (i; a)|) in which maxi,a |T (i; a)| is the maximum number of nodes in sub-"}, {"x": 202, "text": "inserted edges in A.For a decreasing parameter, the time complexity of Algorithm 3"}, {"x": 203, "text": "requires O(n2) computations since it visits every vertex pair exactly once.Now, let"}, {"x": 205, "text": "<xj|(xi, xj)  F> for a certain xi.Then, the complexity of Algorithm 4 is represented"}, {"x": 206, "text": "as O(n  maxi(|E | log |Vd(i)| + (|E |)) in which E = <e(xa, xb)  G|xa, xb  Vd(i)> iii"}, {"x": 210, "text": "Let us denote the updated DG after the shortest path update described in Section 4.2 as Dnew.In this step, Dnew is first converted into the pairwise inner product matrix GG"}, {"x": 213, "text": "Bnew are not much different in any sense.The original ISOMAP uses the Lanczos G algorithm <61>, which is an iterative method that is appropriate for solving the first few leading eigenvalue/vector pairs.The Lanczos algorithm iteratively refines the solution in the Krylov subspace that grows from an initial vector by multiplying itTable 6: Computation time in seconds required to determine the optimal k value by minimizing residual variances."}, {"x": 214, "text": "with the matrix, i.e., span(b, Bnewb, (Bnew)2b, ...).The performance of the Lanczos GG"}, {"x": 215, "text": "algorithm largely depends on how quickly such a Krylov subspace covers that spanned"}, {"x": 216, "text": "by the eigenvectors.Another characteristic of the Lanczos algorithm is that the"}, {"x": 218, "text": "In other words, when the Krylov subspace becomes k dimensions, the first leading"}, {"x": 222, "text": "recovers (new, vnew).As a result, we can expect the Lanczos algorithm to terminate mm"}, {"x": 225, "text": "In this section, we present an empirical comparison between the computation times of ISOMAP and those of p-ISOMAP using both synthetic and real-world data sets.In addition, we show visualization applications of p-ISOMAP for real-world data sets.In our experiments, we used the code of ISOMAP provided by the original author.4 However, the original code does not take advantage of sparse graphs, so we compared p-ISOMAP with an improved version of ISOMAP that runs Dijkstras algorithm in C++ with a sparse representation of the graph.p-ISOMAP was implemented mainly in MATLAB except for the shortest path update part, which runs in C++.In both ISOMAP and p-ISOMAP, the eigendecomposition was done by MATLAB built- in function eigs, which performs the Lanczos algorithm by using Fortran library"}, {"x": 226, "text": "ARPACK <89>.Throughout all experiments, we used the ISOMAP parameter as k, where the neighborhood graph is constructed by k-NN, since we can easily bound |A| or |D| by O(nk) in which k = knew  k. All the experiments were done using MATLAB 7.7.0 on Windows Vista 64bit with 3.0GHz CPU with a 4.0GB memory."}, {"x": 228, "text": "To compare the computation times between ISOMAP and p-ISOMAP, we tested two synthetic data sets (Rand and Swiss roll) and two real-world data sets (Pendigits and Medline).Rand data set was made by sampling a uniform distribution in a 5,000- dimensional hypercube, <0, 1>5000, where the number of data is 3,500.Swiss roll data set has 4,000 data points in three-dimensional space.Pendigits data set5 contains 10,992 handwritten digit data in a form of pen traces in 16-dimensional space <11>, but we selected 3,000 data with an equal number of data per cluster because of memory constraints.Finally, Medline data set6 is a document corpus related to medical science from the National Institutes of Health, and it has 2,500 documents encoded in 22,095- dimensional space.Table 5 compares computation times of ISOMAP with those of p-ISOMAP for each data set.In most cases, p-ISOMAP runs significantly faster than ISOMAP.However, as the number of vertex pairs whose shortest paths need to be updated increases, the computational advantage of p-ISOMAP over ISOMAP gradually vanishes.Nonetheless, except for Swiss roll data set, which involves a large number of the shortest path update even with a slight parameter change, most data sets require only about 10-40% the shortest path update for a reasonable parameter change, e.g., within 5.Fig.7 shows the behaviors of p-ISOMAP depending on the number of data, k, and an initial k value.We selected Rand data since it was the most suitable one to clearly observe its behaviors.Fig.7(a) shows the computation"}, {"x": 231, "text": "time in terms of the number of data.As we can see, p-ISOMAP scales well in terms of the number of data compared to ISOMAP.In Fig.7(b), as the parameter change k gets bigger, the running time of p-ISOMAP increases linearly, which tells that |A| or |D|, which is proportional to k, has a dominant influence on the performance of p-ISOMAP.Finally, Fig.7(c) shows an increasing performance gap between two methods as an initial k value grows.This is mainly because the original Dijkstras algorithm used in ISOMAP needs more computations as the graph gets denser while p-ISOMAP depends only on |A|, |D|, or correspondingly |F|, which probably does not increase over different initial k values.Finally, for each data set, we measured the computation times to take to determine the optimal k value that minimizes residual variances <12>.As shown in Table 6, we could significantly reduce the computation times by utilizing the dynamic update of p-ISOMAP."}, {"x": 232, "text": "3.5.2 Knowledge Discovery via Visualization using p-ISOMAP"}, {"x": 233, "text": "In this section, we present interesting visualization examples of real-world data sets using p-ISOMAP.To be specific, we show how ISOMAP with different parameters can discover various knowledge about data and how the information acquired through visualization can facilitate traditional data mining problems such as a classification task.p-ISOMAP was used to efficiently update ISOMAP results throughout all the visualization experiments.To begin with, we have chosen three real-world data sets (Weizmann, Medline, and Pendigits) that have cluster structures in order to make it easy to analyze their visualization.Weizmann data set is a facial image data set7 that has 28 persons images with various angles, illuminations, and facial expressions.To obtain an understandable visualization, we have chosen three particular persons images with three different viewing angles as shown in Fig.8(a), in which each com- bination of a particular person and a viewing angle contains multiple images that"}, {"x": 235, "text": "vary based on other factors such as illuminations and facial expressions.In their visualizations shown in Figs.8(b)-(d), each of these images is represented as a letter that corresponds to its cluster from Fig.8(a).Medline data set, which is a document collection, has 5 topic clusters, heart attack (h), colon cancer (c), diabetes (d), oral cancer (o), and tooth decay (t), in which the letters in parentheses are used in its visualization in Fig.9.Pendigits data set, which is described in Section 5.1, has 10 clusters in terms of which digit each data item corresponds to, i.e., 0, 1, ..., 9.Several interesting visualization examples of these data based on p-ISOMAP are shown in Figs.8-108 where cluster centroids and neighborhood connections are also shown in the form of letters in rectangles and grey lines in the background, re- spectively.Among visualization examples of Weizmann data set, Fig.8(c), which well resembles the layout of clusters in Fig.8(a), successfully straightens its intrinsic manifold defined by the two factors, a person and an angle.This is mainly because of the neighborhood graph constructed by a proper k value that forms its edges either within a particular person or within a particular angle, which is why we mostly see horizonal and vertical neighborhood connections as well as gaps between grid-shaped cluster centroids in Fig.8(c).Regarding a comparison between Figs.8(b) and 8(d), fewer neighbors in Fig.8(b) bring connections only within images with the same angle, which in turn results in a clustered form of visualization based on angles.This indi- cates that even if we prefer the similarity in terms of a person to that in terms of an angle, the actual distances in the vector space into which the images are transformed are dominated by an angle.On the other hand, Fig.8(d) connects almost all the data points between each other, which would reflect the Euclidean distances in the original space just like MDS does.In addition, we can consider the layout of cluster structure shown in Fig.8(d) as a curved version of manifold as it appears in the original space,"}, {"x": 236, "text": "8These figures can be arbitrarily magnified without losing the resolution in the electronic version of this thesis.which is analogous to what we discussed in Fig.6.Medline data shown in Fig.9 is not visualized in a well-clustered form by ISOMAP because it is usually difficult to find a well-defined manifold structure with few meaningful dimensions for document data.However, by manipulating k values, we can at least obtain various visualization results that possibly reveal different aspects of the data.For example, when k = 30 in Fig.8(b), the topic cluster, tooth decay (t), is shown distinct from the other clusters while so does the cluster, diabetes (d), in the other cases.In this situation, if one wants to focus on a certain cluster separately from the others, it would be necessary to change k values for a suitable visualization result.Visualizations of Pendigits data set shown in Fig.10 give numerous interesting characteristics.First of all, as the parameter k increases, the overall transition from Fig.10(a) to 10(f) is shown similar to that of Swiss roll data set from Fig.6(c) to 6(d).In other words, a larger k value places more data in a curved shape, which reflects the underlying curvature in the original space, while a smaller k value does more data in a linear shape, which corresponds to a straightened manifold.To be specific, starting from Fig.10(b), the cluster 8 gradually gets scattered and curved with an increasing k. Similarly, the cluster 0 maintains a linear shape before k = 50, and finally it becomes scattered in Fig.10(f).In short, ISOMAP with a small parameter value tends to unroll the curved manifold due to geodesic paths, but that with a large parameter better shows its curvature itself.In view of clustering, Fig.10(a) well separates the clusters 2 and 7 whereas the other visualizations gradually overlap them with increasing k values.In addition, the clusters 3 and 6 appears to overlap for a certain range of k between 9 and 11 as shown in Figs.10(b)-(d).Now let us discuss about subcluster/outlier discovery through various visualization examples.In most examples in Fig.10, the cluster 5 is shown to have two subclusters, one of which is near the cluster 8, and the other between the clusters 3 and 9.Based on this observation, we examined some sample data from each cluster and found out such subclusters are due to the different way to write 5.9 From the examples in these two subclusters shown in Figs.11(a)-(b), we can see that some people write 5 starting from the hat, which is the top horizontal line in 5, while others write the hat after finishing the bottom part.Similarly, the cluster 7 has a majority of data near the cluster 2, but it also has two minor groups of data near the cluster 1 and the cluster 6, respectively.(See, for example, the coordinates around (100, 50) and (50, 150) in Fig.10(c).)After looking at the actual data samples from these groups, we found that most people write 7 in a way shown in Fig.11(c).However, some people first write an additional small vertical line in the top-left part but by omitting the small horizontal line in the middle part as shown in Fig.11(d), which corresponds to the minor data near the cluster 1, but some others just reverse the direction to write the small horizontal line in the middle part of 7 as shown in Fig.11(e), which corresponds to those near the cluster 6.In addition, their different traces and shapes impose similarities to those of the clusters 1 and 6, respectively.Finally, in Fig.10(d), some data in the cluster 0 seems to deviate from its major line-shaped data in Figs.10(a)-(c).Figs.11(f) and 11(g) represent the latter and the former data, respectively.We can see that such deviated ones shown in Fig.11(g) start from the top-right corner rather than from the top-middle part when writing 0, which causes their connections to the clus- ter 5 that also starts from the top-right corner.Finally, we have incorporated the above findings in a handwritten digit recognition, which is a classification problem, using Pendigits data set.Based on the information that the cluster 5 has two clear subclusters, we modified the training data labels in the cluster 5 into two different labels and classified the test data that are assigned either label to the cluster 5.As a classification method, we have chosen the linear discriminant analysis combined with k-nearest neighbor classification, which is a common setting in classification."}, {"x": 240, "text": "In this chapter, we proposed p-ISOMAP, an efficient algorithmic framework to dynam- ically update ISOMAP embedding for varying parameter values.The experiments using both synthetic and real-world data with various settings validate its efficiency.This advantage of p-ISOMAP can not only speed up the parameter optimization pro- cesses but also enable users to interact with visual analytics systems more smoothly.Such interaction provides us with deep understanding about data, which can improve even the computational data mining problems such as classification."}, {"x": 242, "text": "ITERATION-WISE INTEGRATION FRAMEWORK OF COMPUTATIONAL METHODS"}, {"x": 243, "text": "Visual analytics has been gaining increasing interest due to its fascinating charac- teristic that leverages both humans visual perception and the power of computing.Although various computational methods are being proposed, they do not properly support visual analytics.One of the biggest obstacles towards their real-time vi- sual analytic integration is their high computational complexity.As a way to tackle this problem, this chapter presents PIVE, a Per-Iteration Visualization Environment for supporting real-time interactive visualization with computational methods.The main idea behind PIVE is that most advanced computational methods work by re- fining the solution iteratively.By visually delivering the result from each iteration to users, the proposed framework enables users to quickly acquire the information that the computational method provides as well as the ability to perform continuous interactions with them in real time.We show the effectiveness of PIVE in terms of real-time visualization and interaction capabilities by customizing various dimension reduction methods such as principal component analysis, multidimensional scaling, and t-distributed stochastic neighborhood embedding, and clustering methods such as k-means and latent Dirichlet allocation."}, {"x": 245, "text": "The innate ability of humans to quickly perceive insight through visual analysis and decision processes has been a key factor in the growth of visual analytic research <78, 127>.One of the most significant efforts made by visual analytics researchers is the integration of various computational methods from data mining and machine learning areas with visual analytics so that users can benefit from intelligent mean- ingful information generated by these techniques.For example, dimension reduction and clustering methods have been commonly used in high-dimensional data visual analytics <23, 117>.More recently, latent Dirichlet allocation (LDA) <20>, a popular method for document topic modeling, has been adopted in a wide variety of visual analytics systems for document analysis <134, 50, 88>."}, {"x": 246, "text": "However, a critical hurdle in the integration of computational methods into visual analytics is the significant amount of computational time required by these meth- ods.As computational methods become more advanced and capable, they usually run much slower, making it almost impossible to visualize and interact with them smoothly in real-time visual analytics.Due to this significant running time, even though numerous computational methods are currently being developed and some methods such as t-distributed stochastic neighbor embedding (t-SNE) <130> even claim their suitability directly in visualization applications, the state-of-the-art in visual analytics does not seem to fully utilize the advancements in computational methods.Consequently, in many domain areas, people still resort to only a few basic computational methods such as principal component analysis (PCA) <74> and multi- dimensional scaling (MDS) <45> for dimension reduction, hierarchical clustering and k-means <19> for clustering, etc."}, {"x": 250, "text": "However, apart from well-principled convergence criteria studied in most com- putational methods, it is not straightforward to determine when to terminate the iteration at which the result is reasonably accurate from the perspective of humans perceptual precision.Instead, we propose an alternative approach called PIVE (Per- Iteration Visualization Environment for supporting real-time interactive visualization with computational methods), which visualizes the intermediate result per iteration as soon as they become available.Unlike the previous approaches, which typically treat a particular computational method as a black box, the main novelty of PIVE lies in the idea to break computational methods down to the iteration level and tightly integrate it with the interactive visualization so that users can check the result of computational methods without any delays and interact with them in real-time."}, {"x": 255, "text": "Realizations of PIVE with various well-known computational methods (PCA,"}, {"x": 256, "text": "MDS, t-SNE, k-means, and LDA) in established visual analytics systems"}, {"x": 263, "text": "4.2.1 Efficient Interactive Visualization"}, {"x": 264, "text": "Not surprisingly, numerous studies have focus on the visualization applications of large-scale data.Among various approaches, one of the straightforward but reason- able approaches is by using a subset of data by sampling.For example, Fisher et al.<57> has proposed an efficient way of dealing with large-scale data visualization by initially using only a small portion of data and then perform an incremental update on the visualization.Ellis et al.<54> has also taken a random sampling-based visual- ization approach mainly for avoiding the visualization clutter due to a large number of visualized objects while considering the efficiency issues during visualization."}, {"x": 267, "text": "Furthermore, efficient interactive visualization has been a main concern in the context of dynamic/streaming data.When visualizing dynamic/streaming data, the overall theme found in various approaches is to update the visualization efficiently given incremental changes in a data set.In this context, Cottam et al.<44> has recently discussed about a taxonomy for dynamic data visualization.Although the detailed approaches may differ, several prior studies <139, 140> have started from a relatively similar idea that the visualization update is carried out only when significant changes/events have been detected.Additionally, Alsakran et al.<5> has visualized the streaming documents using a GPU-accelerated force-directed layout technique."}, {"x": 269, "text": "4.2.2 User Interaction with Computational Methods"}, {"x": 272, "text": "4.3 Per-Iteration Visualization Environment (PIVE)"}, {"x": 275, "text": "The final output of the computational module is then passed to the visualization module, which encodes it in a visual space and finally delivers its visualization to users.For example, the output of a dimension reduction method, e.g., PCA, can map data items onto the coordinates of the screen space, and the output of clustering can be used to color-code each group of data clusters."}, {"x": 285, "text": "Modern computers are usually equipped with at least two or more cores on the CPU.These two threads can be executed virtually in parallel, which hardly slow down the computational methods compared to the standard approach.Although not included in this chapter, for the computational methods we customized, we compared the total computing time between PIVE and the standard frameworks, but with multi-threading implemented, there were essentially no differences in their running times."}, {"x": 287, "text": "Finally, the other overhead comes from copying results from each iteration to the message queue, which results in a memory write operation.In the standard approach, these results for each iteration are usually written to the same memory space over iterations since the results from previous iterations do not need to be maintained.However, memory write operations are generally very fast.Furthermore, the out- puts from each iteration of computational methods take up a much smaller memory compared to input data.For example, even if the data is a very high-dimensional, say, in the hundreds of thousands of dimensions, such as is the case in text data, the dimension reduction outputs would only be two-dimensional representations assum- ing they are visualized in a 2D space.Since the amount of additional computational time and memory that is required by our approach is minimal, we do not see memory overheads being a critical issue."}, {"x": 297, "text": "4.4.1 Principal Component Analysis (PCA)"}, {"x": 298, "text": "PCA <75> is a well-known dimension reduction method that captures the maximal variance in the data via a linear projection.PCA is mainly based on the method called eigendecomposition, the algorithms of which are categorized into two different methods, the QR algorithm and the Lanczos algorithm <61>."}, {"x": 299, "text": "Basically, the Lanczos algorithm approximates a given data matrix by a much smaller one in the Krylov subspace <61>, the dimension of which iteratively expands, and efficiently solves the eigendecomposition on the latter matrix.Due to the nature that this matrix well-approximates the largest eigenvectors of the original one, the Lanczos algorithm performs much faster than the QR algorithm in visual analytics in which only a few dimensions are needed."}, {"x": 300, "text": "We customize the Lanczos-based PCA implementation of FodavaTestbed so that the results for each iteration are dynamically visualized."}, {"x": 301, "text": "4.4.2 Multidimensional Scaling (MDS)"}, {"x": 302, "text": "MDS <45> is a traditional dimension reduction method that attempts to preserve given distances/relationships of data items in a lower-dimensional space.Given the ideal distance ij between xi and xj, MDS solves"}, {"x": 303, "text": "where dij is the distance between the reduced dimensional vectors xi and xj.A Euclidean distance xi  xj2 is usually used for dij.Solving Eq.(39) iteratively refines xis based on various optimization techniques <46>.We customize MDS in FodavaTestbed by extracting the xis at each iteration from the MDS implementation."}, {"x": 304, "text": "4.4.2.1 User Interaction Capabilities"}, {"x": 305, "text": "Additionally, while the results for each iteration of MDS are visualized in a scatter plot, we support the interaction capability that enables users to move the data points by mouse via drag-and-drop, similar to the Prefuse force-directed layout.Then, during the MDS iterations, their new positions in the screen space are translated back to the MDS output coordinates, xis.The changes in xis at a particular iteration then affect the following iterations by generating different dijs.In terms of how MDS behaves due to these changes, we provide two different capabilities: soft vs.hard placement.The soft placement continues iterations without any changes in MDS behaviors.It is equivalent to restarting MDS with the intermediate result at the particular iteration as the initial values for xis."}, {"x": 306, "text": "The hard placement capability fixes the values of xis for points moved by the user.This can be easily achieved by skipping the update step of these xis in the following iterations.Note that, however, even though their values do not change, other data points are still influenced by these fixed points, and in this sense, our approach is a semi-supervised MDS that reflects user interventions."}, {"x": 307, "text": "When using the semi-supervised MDS, an important advantage of the proposed framework is that users can immediately check the effects of these interactions via the iteration-wise visualization.Our modifications in FodavaTestbed support both types of interactions."}, {"x": 308, "text": "4.4.3 t-Distributed Stochastic Neighbor Embedding (t-SNE)"}, {"x": 310, "text": "Although we skip the detailed formulations because of the scope of the visual an- alytics community, the algorithm works iteratively by refining the lower-dimensional coordinates based on a gradient descent-based framework.In practice, however, t- SNE does not provide a clear stopping criterion, and thus it typically iterates several hundred times by default for any data set, which usually takes a significant amount of time.We customize the t-SNE in FodavaTestbed in a similar manner to the way we altered MDS."}, {"x": 311, "text": "4.4.3.1 User Interaction Capabilities"}, {"x": 312, "text": "Likewise, we provide both the soft and hard placement interactions for t-SNE, as discussed in MDS.Although the algorithm details are different, the overall iterative procedure turns out to be quite similar to MDS.Thus, for the soft placement, we restart t-SNE with the intermediate results immediately during iterations.For the hard one, we skip the update step for data items moved by the user in the following iterations while they still influence other points in the t-SNE iterations.Therefore, our altered method can be viewed as a semi-supervised t-SNE."}, {"x": 316, "text": "(a) PCA criteria values and out- put changes"}, {"x": 318, "text": "Figure 13: The behavior for each iteration of PCA and its visualization snapshots.In (a), the red lines represent the PCA criteria value, the lower-dimensional variance in PCA.The blue lines are the Euclidean distances of the lower-dimensional outputs between the current and the previous iterations, and the black lines are the Euclidean distances of the lower-dimensional outputs between the current and the final itera- tions.In (a), the black and the blue lines almost coincide.1,420 facial image data representing pixel values in 2,048 dimensions have been used."}, {"x": 320, "text": "4.4.4.1 User Interaction Capabilities"}, {"x": 321, "text": "Additionally, we add several interaction capabilities in the proposed framework.One is to split/merge clusters during iterations.On a split/merge interaction, similar to the soft placement in MDS and t-SNE, k-means restarts with the intermediate cluster memberships that reflect split/merged clusters, involving dynamic changes in a k-means parameter which represents the number of clusters."}, {"x": 323, "text": "4.4.5 Latent Dirichlet Allocation (LDA)"}, {"x": 324, "text": "LDA <20> is a popular topic modeling method for documents based on a generative probabilistic model.Given a number of topics, it gives two outputs: the term-wise distribution of each topic and the topic-wise distribution of each document.The iterations of LDA basically update these two outputs alternately.From a clustering viewpoint, the former corresponds to a centroid vector of each topic cluster, and the latter to a soft-clustering coefficient.By taking the topic index that has the maximum"}, {"x": 328, "text": "Finally, LDA, which is a sampling-based approach, shows a significantly different"}, {"x": 334, "text": "Figure 19: The results of the PIVE integration of k-means in Jigsaw.At the sixth iteration, the interaction of fixing the yellow-colored clusters is made (b).The final result with and without this interaction is shown in (c) and (d), respectively.The NSF-awarded abstract data have been used.The detailed keyword summary is shown in Table 7"}, {"x": 336, "text": "Fig.16 shows an interesting interaction which involves moving a data point in t- SNE.Given some overlapping clusters in a particular visualization generated by t- SNE (Fig.16(a)), users place several points from different clusters far apart (Fig.16(b)), and then t-SNE reflects these changes in the following iterations, resulting in tje separation of most points in two clusters from each other (Figs.16(c)(d)).This simple, yet powerful example clearly illustrates the advantage of providing users with"}, {"x": 352, "text": "The ability to filter noisy documents has been an appealing interaction for LDA in iVisClustering.To be specific, given parallel coordinate representations of topic-wise distributions of documents, users can interactively filter out documents that are not strongly related to a single topic, i.e., documents that have a very small maximum value in the topic-wise distribution.By removing them and re-running LDA, iVis- Cluster generally obtains significantly clearer topics.In PIVE, we performed this interaction near the 300th iteration (Fig.21(b)), which is an early iteration when compared to the total number of iterations performed by LDA.However, such an interaction successfully generates clearer topics (Fig.21(c)) over the standard ap- proach where users have to wait for the algorithm to finish its full iterations in order to perform the same interaction."}, {"x": 354, "text": "We have presented PIVE (Per-Iteration Visualization Environment for supporting real-time interactive visualization with computational methods).One of its apparent advantages is its ability to present users with the intermediate results during the inter- actions, which could reveal a significant amount of information immediately in visual analytics.Another important advantage is that it indeed opens up the possibility of performing small multiple interactions, which in the past have been considered to be too inefficient, and allows the real-time control over computational methods in visual analytics.In fact, the interactions we proposed in this chapter are relatively simple, which do not involve any major algorithmic modifications, but after a sequence of interactions, the results reflects the intention of users sufficiently well in real-time.In this sense, PIVE makes them significantly useful by enabling users to perform these interactions easily and efficiently."}, {"x": 355, "text": "However, the advantage of our framework can be limited when the changes be- tween iterations remain nontrivial, resulting in inconsistent visualizations.We have seen this kinds of limitations when using LDA under PIVE due to the random nature of the used LDA algorithm.As a future work, we plan to tackle this problem more actively by, for example, post-processing the results or even imposing additional con- straints in computational methods so that the results from the following iterations do not change much from the current ones.Finally, another interesting research direction we will pursue is to extend PIVE to various parallelized computational algorithms for the large-scale data visual analytics."}, {"x": 357, "text": "TESTBED: AN INTERACTIVE VISUAL TESTBED SYSTEM FOR VARIOUS DIMENSION REDUCTION AND CLUSTERING METHODS"}, {"x": 358, "text": "Many of the modern data sets such as text and image data can be represented in high- dimensional vector spaces and have benefited from computational methods that uti- lize advanced computational methods.Visual analytics approaches have contributed greatly to data understanding and analysis due to their capability of leveraging hu- mans ability for quick visual perception.However, visual analytics targeting large- scale data such as text and image data has been challenging due to the limited screen space in terms of both the numbers of data points and features to represent.Among various computational methods supporting visual analytics, dimension reduction and clustering have played essential roles by reducing these numbers in an intelligent way to visually manageable sizes.Given numerous dimension reduction and clustering methods available, however, the decision on the choice of algorithms and their pa- rameters becomes difficult.In this chapter, we present an interactive visual testbed system for dimension reduction and clustering in a large-scale high-dimensional data analysis.The Testbed system enables users to apply various dimension reduction and clustering methods with different settings, visually compare the results from different algorithmic methods to obtain rich knowledge for the data and tasks at hand, and eventually choose the most appropriate path for a collection of algorithms and param- eters.Using various data sets such as documents, images, and others that are already encoded in vectors, we demonstrate how the Testbed system can support these tasks."}, {"x": 361, "text": "Given high-dimensional data, understanding and analyzing these data become more challenging.Visual analytics <78, 127> has gained increasing interest due to its capability of leveraging humans ability of quick visual insight in data analyses and decision processes.However, many state-of-the-art visual analytics techniques or systems are not equipped for high-dimensional large-scale data.One of the reasons is that although humans are good at visually grasping an overall structure, when the number of visualized objects becomes large, it is often difficult to extract meaningful information from visualization.Another factor is the limited dimension of a screen space where high-dimensional data have to be visualized.For instance, parallel co- ordinates, a widely-used visualization technique for multi-dimensional data, do not scale well even when the dimension reaches several tens."}, {"x": 362, "text": "To improve this scalability issue, computational methods can support visual ana- lytics by transforming the original data into a more compact and meaningful represen- tation.Among various methods, two main ones, dimension reduction and clustering, play an essential role in visual analytics of large-scale high-dimensional data owing to their nature to reduce the numbers of features and data items into manageable sizes, respectively.Dimension reduction methods can reveal meaningful information by al- lowing the visual representation of high-dimensional data in a much lower-dimensional space.In addition, it allows visualization of high-dimensional data in the form of a 2D/3D scatter plot in which one can obtain insight about data relationships with respect to the geometric locations of data.On the other hand, clustering provides an overview of large-scale data in terms of a small number of groups based on their semantic coherences.Such cluster information can then guide us to a proper data group of interest on which we can further focus our analysis."}, {"x": 365, "text": "The main contributions of the proposed Testbed system are as follows.First of all, given various types of input data such as text documents, images, and vector-encoded data, the testbed system provides extensive capabilities to interactively select data pre-processing options and choose a wide variety of clustering and dimension reduc- tion methods along with their parameters.The output of these processes are then visualized in several forms, e.g., parallel coordinates and a scatter plot, equipped with various interaction capabilities, e.g., accessing the original data items and brushing and linking between multiple views.Additionally, the Testbed system facilitates easy comparisons between different dimension reduction and clustering results by com- putationally aligning them.Finally, the Testbed system is implemented in a highly modular way so that new data types and dimension reduction/clustering methods can be easily integrated to the current system."}, {"x": 366, "text": "Note that even though the Testbed system can be used by anyone who wants to apply various methods to their own data, some background knowledge about machine learning and data mining would be of great help in fully utilizing the Testbed system via understanding the data and the applied methods simultaneously.For example, machine learning researchers/developers, who wish to easily plug in and visually evaluate their own methods in practical data analysis scenarios, would be able to receive significant benefit from the Testbed system."}, {"x": 367, "text": "The rest of this chapter is organized as follows.In Section 5.2, we review the relevant literature in terms of dimension reduction and clustering methods as well as the visual analytics systems adopting them.Section 5.3 describes the details of the Testbed system as well as several main computational methods used in the system.Section 5.4 shows various usage scenarios of the Testbed system, and finally Section 5.5 presents conclusions along with future work."}, {"x": 368, "text": "Figure 22: 2D Scatter plots obtained by two dimension reduction methods, MDS (left) and LDA (right), for a facial image data set.A different color corresponds to a different cluster."}, {"x": 371, "text": "5.2.1 Dimension Reduction and Clustering for Visualization"}, {"x": 372, "text": "Dimension reduction has long been one of the main research topics in data mining and statistical machine learning areas.Numerous dimension reduction methods have been proposed, among which the most commonly used dimension reduction methods include principal component analysis (PCA) <75>, multidimensional scaling (MDS) <45>, and linear discriminant analysis (LDA) <60, 68>."}, {"x": 373, "text": "In addition to traditional data analysis problems, they have also been widely utilized in visualization due to their capability of representing high-dimensional data as a form of scatter plots in 2D/3D space.In a scatter plot, each data item is represented as a point and its 2D/3D coordinate is determined from the dimension- reduced representation.In general, the relative locations among data points reflect the pairwise relationships or proximities among data items."}, {"x": 374, "text": "Each dimension reduction method has its own optimization criteria and behaviors, which result in different visualizations.For instance, the recently proposed man- ifold learning algorithms, e.g., isometric feature mapping (ISOMAP) <125>, locally linear embedding (LLE) <111>, and Laplacian Eigenmaps (LE) <17>, try to preserve the relationships between the local neighborhood rather than global relationships.These methods have been successfully applied to the data that originally have a low- dimensional manifold structure, and often they demonstrated their capability to reveal such a manifold structure in 2D/3D visualizations.However, most of these methods present just several visualization snapshots of limited data sets with no interaction abilities."}, {"x": 375, "text": "Another aspect to consider when applying dimension reduction in visualization applications is the cluster structure of data.A majority of dimension reduction methods take into account only the pairwise relationships between data items.In practice, however, it is not easy to obtain much insight from the 2D/3D scatter plot generated by them for a large number of data items.The left figure in Fig.22 is a visualization example of such a dimension reduction method, MDS, for a facial image data set.Let us, for now, ignore the colors, which indicate the cluster labels.This visualization shows most of the data as a single chunk with a few outliers placed outside.Although these points may give some interesting insight about why they appear to be outliers, one cannot get much more information from this visualization."}, {"x": 376, "text": "Another type of dimension reduction methods incorporates additional information about the cluster structure of data in addition to individual data items.Since these dimension reduction methods require the assigned cluster label associated with each data item as an input, they are called supervised dimension reduction methods while the previous methods are called unsupervised dimension reduction methods.Some representative supervised methods include LDA <60> and orthogonal centroid method (OCM) <102>.The right figure in Fig.22 is an example of LDA visualization.This figure visualizes the data as groups of items computed by LDA based on the given cluster labels, and one can obtain better insight about the overall data structure at the cluster level over the individual data level."}, {"x": 377, "text": "Representing the cluster structure has been one of the main concerns in many studies on dimension reduction and 2D/3D scatter plot visualizations even when unsupervised dimension reduction methods are used.Many methods have been eval- uated regarding their ability to visualize cluster structures, which are hidden at the time of computing dimension reduction.For instance, a recently proposed dimension reduction method, t-distributed stochastic neighborhood embedding (t-SNE) <130>, shows its capability of grouping data and revealing the true cluster structure in 2D scatter plot visualizations."}, {"x": 379, "text": "Clustering, along with dimension reduction, has also been one of the well-studied research topics in data mining and machine learning areas.Widely-used methods include k-means clustering, spectral clustering <96>, and Gaussian mixture models.Recently, more advanced methods such as non-negative matrix factorization (NMF) <79> and latent Dirichlet allocation <20> have shown their successful applications in image segmentation and document topic modeling, etc.These methods are usually evaluated using the data set whose cluster label information is already known and by comparing between the true cluster labels with those obtained by the computational method.However, given the data set that may not have a clear cluster structure, clustering is typically a very challenging task, and thus it is often the case that the resulting cluster quality is unsatisfactory.From a visual analytics perspective, unsatisfactory clustering makes it difficult to understand the coherent meaning of each cluster and how one cluster contrasts with another.For instance, in recent applications of latent Dirichlet allocation for document topic modeling, while several coherent topic clusters have been successfully revealed for the document data, many other topics often seem unclear to understand."}, {"x": 381, "text": "5.2.2 Visual Analytic Systems using Dimension Reduction and Cluster- ing"}, {"x": 382, "text": "In information visualization and visual analytics communities, various visual analyt- ics systems incorporating computational methods such as dimension reduction and clustering have been proposed to deal with large-scale high-dimensional data.In this section, several systems such as IN-SPIRE <138>, Jigsaw <121>, GGobi <43>, iPCA <72>, and WEKA <65> are discussed."}, {"x": 383, "text": "IN-SPIRE <138> is one of the well-known visual analytics systems for document data in which dimension reduction and clustering play main roles.Given a set of documents, IN-SPIRE first encodes them as high-dimensional vectors using a bag- of-words model.Then it applies k-means clustering with a pre-defined number of clusters.PCA is computed on cluster centroids and applied to the entire data, which gives 2D coordinates of document data.Based on these 2D coordinates, a galaxy view similar to a scatter plot is shown to users with a keyword summary for each cluster placed at the cluster centroid.Owing to the simple algorithms adopted such as PCA and k-means, IN-SPIRE can deal with a fairly large amount of data, but it provides only a limited number of interaction capabilities to change the algorithms and their settings."}, {"x": 384, "text": "Jigsaw <121> is another well-known visual analytics system for document analysis.The main information that Jigsaw utilizes for visualization is named entities such as person name and location and their co-occurrences between documents.Automatic named-entity extraction is one of the key computational components in the analysis in Jigsaw.The named-entity extraction can be viewed as dimension reduction that reduces the number of keywords out of the entire vocabulary.Users can modify the list of named-entities by manually adding/removing them.Jigsaw also provides a cluster view by using the k-means algorithm and visualizes the resulting clusters as groups of documents as well as their keyword summary.Jigsaw also supports basic interactions with clustering such as changing the number of clusters and providing seed documents."}, {"x": 385, "text": "GGobi <43> is an interactive visualization system for high-dimensional data that are already encoded.It mainly uses a 2D scatter plot, where the two dimensions are generated by grand tour <10>.The difference of grand tour from other dimension reduction methods is that it provides an interaction to explore the high-dimensional space by continuously changing the basis vectors that data items are projected into.However, the grand tour method is applicable only when the data dimension is not significantly high, and thus its application is limited when dealing with hundreds or thousands of dimensions, which is often the case in many data types such as text documents, images, and bio data."}, {"x": 386, "text": "Another system, iPCA <72>, which also takes high-dimensional data as an input, utilizes PCA as the main visualization technique.One of the main advantages of iPCA is that beyond 2D/3D scatter plots, it visualizes the reduced-dimensional data in a higher dimension than 2D or 3D via parallel coordinates.In general, dimension reduction from the original high-dimensional space to 2D/3D space introduces signif- icant information loss.In iPCA, it follows a useful idea to reduce the data dimension to an intermediate one that can be visualized without much clutter via parallel coor- dinates and then to interact with these intermediate dimensions to obtain particular scatter plots.Another aspect of iPCA is that it visualizes the PCA basis vectors in addition to the data items.In doing so, users can understand the role of the reduced dimensions in their visualizations, which leads to a better understanding about the data set as well.Even with these advantages, however, iPCA cannot handle very high-dimensional data since iPCA visualizes each of the original dimensions."}, {"x": 387, "text": "Finally, WEKA <65> is mainly a library of various machine learning algorithms for high-dimensional data with several interaction capabilities.Various algorithms can be applied to data, and their performances can be evaluated based on various measures.In addition, WEKA provides simple types of visualizations such as histograms, scatter plots, etc.Although WEKA is similar to our Testbed system in that it provides flexible algorithm choices and settings, most of its visualizations and interactions are focused on the used methods rather than data exploration.For example, WEKA does not support any interactions from its visualizations such as filtering operations and raw data access."}, {"x": 388, "text": "As discussed above, most of the current visual analytics systems do not fully utilize a wide variety of computational methods.They adopt generic traditional methods for a broad applicability to various data sets and/or treat computational methods as a black box with little options to control them, which would hamper the interactive visual analysis.In this respect, the Testbed system provides the unique capability of bringing a variety of algorithms along with full control to practical visual analytics scenarios."}, {"x": 389, "text": "5.3 Testbed System"}, {"x": 390, "text": "In this Section, we describe the Testbed system1 in detail.First, in Section 5.3.1, we introduce the modules in the system and explain how the overall system works.Next, we describe the details of each module from both the computational and the interactive visualization points of view in Sections 5.3.2 and 5.3.3, respectively.Fi- nally, in Section 5.3.4, we discuss implementation details of the system and how the current system can be extended to adopt new data types and clustering/dimension reduction methods."}, {"x": 392, "text": "As shown in Fig.23, the Testbed system mainly has two parts: the computational and the interactive visualization parts.At the computational part, the Testbed system is composed of 1. vector encoding, 2. pre-processing, 3. clustering, and 4. dimension reduction.At the interactive visualization part, the Testbed provides the following interactive visualization modules: 1. parallel coordinates, 2. the scatter plot, 3. the cluster label view, and 4. the original data viewer."}, {"x": 393, "text": "The basic workflow of the Testbed system is as follows.Once a data set is loaded, data items are represented as high-dimensional vectors via a default encoding scheme.Then, users can interactively change the options for pre-processing, clustering, and dimension reduction methods.Each specification of these three components instan- tiates a particular visualization set composed of the parallel coordinates view, the scatter plot view, and the cluster label view.To generate these views, the output"}, {"x": 395, "text": "The Testbed system can generate as many visualization sets as needed depending on different specifications of dimension reduction and clustering, and users can ex- plore a certain visualization set and compare between different visualization sets.To facilitate an easy comparison between different visualization results, the Testbed sys- tem offers the capability of aligning the different clustering and dimension reduction outputs.In addition, users can highlight and/or filter out certain clusters/data items and look into the details of the selected data items in the original data viewer.Users can also apply another set of clustering and/or dimension reduction to the selected data items to create new visualization sets."}, {"x": 398, "text": "The Testbed system can take various types of data such as text documents, images, and pre-encoded vectors in a comma-separated-values (CSV) file format.For docu- ment and image data, the Testbed system provides built-in vector encoding modules.For instance, the Testbed system supports bag-of-words encoding for document data in a sparse matrix form with stop word removal and stemming.Image data are converted into vectors of rasterized gray-scale pixel values.The high-dimensional vectors obtained in this stage act as initial default vectors on which the following pre-processing is performed."}, {"x": 401, "text": "In addition, for text documents, we provide options of 1. removing the terms appearing in less than a user-specified number and 2. applying the term-frequency- inverse-document-frequency (TF-IDF) weighting scheme.For images, available are the following options: 1. reducing image sizes to a user-specified ratio to enhance the computational efficiency and 2. applying contrast limited adaptive histogram equalization <107>."}, {"x": 402, "text": "The Testbed system maintains multiple instances of different pre-processed vector sets, and users can interactively generate and/or choose one of them and proceed to perform its clustering and dimension reduction."}, {"x": 404, "text": "Given the default or pre-processed set of high-dimensional vectors, the clustering module performs a user-selected clustering method with specified options (Fig.23B), which assigns each data item a cluster label.The Testbed system currently provides the following clustering methods: 1. k-means, 2. agglomerative hierarchical clustering <66>, 3.Gaussian mixture models, and 4.NMF.Once a specific method is selected in the system, user interfaces to specify the number of clusters as well as method-specific parameters are dynamically shown with their suggested default values (Fig.23B)."}, {"x": 406, "text": "5.3.2.4 Dimension Reduction"}, {"x": 407, "text": "Given the high-dimensional vector representations of data items, the dimension re- duction module reduces the data dimension from possibly hundreds of thousands to a visually manageable size, which makes it possible to visualize the data in forms of parallel coordinates and/or a scatter plot.The Testbed system provides both super- vised and unsupervised dimension reduction methods, as discussed in Section 5.2.1.In cases of supervised methods, the cluster label, which is an additional required input to run dimension reduction, is taken from the output of the clustering module."}, {"x": 408, "text": "The currently available dimension reduction methods in the system include su- pervised ones such as 1.LDA, 2.OCM, 3. centroid method (CM) <102>, 4. two-stage methods (TSTG) <35>, 5. discriminative neighborhood metric learning (DNML) <133>, and 6. kernel LDA <101>, and unsupervised ones such as 7.PCA, 8. metric and non- metric MDS, 9.Sammon mapping <113>, 10.ISOMAP, 11.LLE, 12. local tangent space alignment (LTSA) <148>, 13. maximum variance unfolding (MVU) <135>, 14.LE, 15. diffusion maps (DM) <42>, 16. t-SNE, and 17.Kernel PCA <115>.Similar to the clustering module, once a specific method is selected in the system, user interfaces to specify the number of reduced dimensions as well as method-specific parameters are dynamically shown along with their suggested default values (Fig.23C)."}, {"x": 410, "text": "5.3.3.1 Parallel Coordinates View"}, {"x": 411, "text": "Given an output from the computational part, i.e., lower-dimensional representations of data items and their cluster labels, the Testbed system takes a natural way to visualize the lower-dimensional data in parallel coordinates with a color coding based on the cluster labels (Fig.23E).In this view, the Testbed system supports zoom- in/out via mouse wheel scroll and data selection via mouse drag-and-drop."}, {"x": 412, "text": "5.3.3.2 Scatter Plot View"}, {"x": 413, "text": "Although parallel coordinates can fully visualize an output from the computational part, this view is often ineffective for humans to perceive the relationships between data items, and it does not scale well in terms of the number of data items and dimensions since each line representing a single data item occupies numerous pixels in a screen space.Due to these limitations, the Testbed system visualizes data in a 2D scatter plot (Fig.23F) by selecting two of the parallel coordinates dimensions with the same color encoding as in the parallel coordinates view."}, {"x": 414, "text": "In the scatter plot view, users can interactively change these dimensions corre- sponding to horizontal and vertical axes via combo boxes shown in the lower left part of the view.In addition, the Testbed system shows cluster centroids and ellipses, which summarize how the data within each class are distributed, and these features can be turned on/off via check boxes shown in the upper left part of the view.Similar to the parallel coordinates view, supported are zoom-in/out via mouse wheel scroll and data selection via mouse drag-and-drop.Once a subset of data is selected, users can apply another clustering/dimension reduction only on the selected data items."}, {"x": 415, "text": "5.3.3.3 Cluster Label View"}, {"x": 418, "text": "Both in the parallel coordinates and the scatter plot view, users can access the original form of data for user-selected data items/clusters in the original data viewer.Cur- rently, the Testbed system provides three different original data viewers depending on the data type, e.g., text documents, images, and pre-encoded vectors (Fig.23H)."}, {"x": 421, "text": "Once the visualize button is clicked (Fig.23C) after specifying computational meth- ods, i.e., pre-processing, clustering, and dimension reduction, a new set of the parallel coordinates, the scatter plot, and the cluster label views are instantiated.Each of these three views is created as an individual tab in its corresponding location, and multiple views are maintained flexibly in the Testbed system, as shown in Fig.23(a).For example, any view can be popped out as an independent window and/or split horizontally/vertically in order to make it easy to compare between different sets of views due to different computational methods.When a certain view is activated by a mouse click, all the options of pre-processing, clustering, and dimension reduction used to generate the view are shown in the left in Fig.23(a)."}, {"x": 422, "text": "Between different views with such a flexible layout, the Testbed system supports a brushing-and-linking capability.In the current Testbed system, if certain data items/custers are selected in one view, the corresponding data items in all the other views are highlighted as well.We use different colors for highlighting depending on whether the highlighted data items are due to the same view or a different view, which helps identifying the source view in which the data selection was made."}, {"x": 424, "text": "In addition to the above-described multi-view management and brushing-and-linking capability, the Testbed system provides a more active means to facilitate easy com- parison between visualization sets composed of different clustering and dimension reduction results.To be specific, for a user-selected pair of visualization sets, users can align the clustering and/or dimension reduction outputs (Fig.23D), which are then reflected to visualization sets."}, {"x": 425, "text": "To align the two different clustering results, the Testbed system performs the Hungarian algorithm <85>.Given two different cluster assignments of the same data items, the Hungarian algorithm finds the best pairwise matchings between their clus- ter indices so that the number of common data items within matching cluster pairs is maximized.Once the Hungarian algorithm finishes, the Testbed system changes the cluster indices and colors of the second visualization set according to the matching clusters of the first visualization set.As a result, users can maintain the consistent cluster indices/colors when comparing the two given visualization sets."}, {"x": 426, "text": "On the other hand, the Testbed system handles the alignment of dimension re- duction results via Procrustes analysis <69, 53>.Although there exist many advanced methods to align the two sets of vectors <30>, we chose Procrustes analysis due to its computational efficiency.Procrustes analysis transforms the second set of vectors via a rigid transformation, which allows only translation, rotation, and reflection, so that their Euclidean distances to the corresponding data vectors in the first set are mini- mized.Currently, instead of aligning the entire dimensions, the Testbed aligns only the two dimensions selected in the scatter plot view so that the alignment between the two scatter plot views are maximized.These alignment functionalities help users understand how differently the corresponding data items/clusters are placed between the two scatter plot views."}, {"x": 428, "text": "The current Testbed system is mainly implemented in JAVA to achieve various GUI and interaction capabilities.In order to support flexible window management, Net- Beans Rich Client Platform and IDE2 are used."}, {"x": 429, "text": "Most of the internal computational methods are, however, written in MATLAB.There are several reasons of using MATLAB codes instead of porting them to JAVA.First of all, in many cases, the source codes of advanced computational methods are readily available in MATLAB due to its simplicity for matrix computations.In this respect, it would be burdensome to re-implement each of these methods in a different programming language in order to make them visual and interactive, and it will eventually become difficult to keep up with the pace of new technologies."}, {"x": 430, "text": "Furthermore, MATLAB provides highly optimized matrix computations.For in- stance, MATLAB, by default, auto-identifies the parallelizable subroutines in the code and runs full CPU cores even in a single PC.There also exist many efficient ma- ture core computational methods.For example, the k-means function in MATLAB provides various options for a distance metric to be used (Euclidean, city block, co- sine, and correlation), and a seed initialization (random, uniform, pilot-clustered, and user-selected seeds).Due to these reasons, the current Testbed system interface with the computational methods via a custom JAVA library file created by MATLAB.3"}, {"x": 432, "text": "In terms of the extensibility of the Testbed system, we designed it in a completely modular way so that it can easily accept new data types and clustering/dimension reduction methods.For instance, if one wants to use the Testbed system for a speech data type, one needs to implement only the encoding module, the possible pre-processing options specific to the speech data type, and the original data viewer that can play audio data.Otherwise, by performing vector encoding separately and putting the encoded vectors as an input to the system, one can easily utilize the full capability of the Testbed."}, {"x": 433, "text": "Adding new dimension reduction/clustering methods is also a simple process.Cur- rently, the implementation of each computational method is composed of two source code files.One file performs the computation by taking an input and generating an output as a primitive two-dimensional double array type, and the other is for user interfaces to change the method-specific options.Therefore, whether the implemen- tation of a new method is written in MATLAB or JAVA, as long as it deals with two-dimensional double array type as an input and an output, it can be easily inte- grated into the current Testbed system without having to modify the entire system."}, {"x": 436, "text": "To show how the Testbed system can be utilized in various visual analytics scenarios, we use three different data sets: 1.Pendigits (pre-encoded vectors), 2.Weizmann (images), and 3.InfoVisVAST (text documents)."}, {"x": 438, "text": "4 http://archive.ics.uci.edu/ml/datasets/Pen-Based+Recognition+of+Handwritten+Digits cluster, resulting in 500 items in total.The Weizmann data set5 contains 28 persons facial images with various angles, illuminations, and facial expressions.Excluding unclear images, we have chosen 52 images from each of 15 persons, resulting in 780 data items of 15 clusters.The size of each facial image is 88128, resulting in a 11,264 dimensional vector.The InfoVisVAST data set6 is a document corpus of paper abstracts in IEEE Infovis (1995-2010) and VAST (2006-2010) conferences.It includes 515 documents encoded in 4,185 dimensions via a bag-of-words encoding after stemming and stop word removal."}, {"x": 439, "text": "5.4.2 Parallel Coordinates: Guiding beyond Two Leading Dimensions"}, {"x": 440, "text": "When using dimension reduction in high-dimensional data visualization, the leading two dimensions of a dimension reduction method have been usually used to gener- ate a single scatter plot while ignoring the other dimensions.Unlike these previous approaches, the Testbed system first visualizes the reduced-dimensional data in the parallel coordinates view, and then two of these dimensions are interactively selected for the scatter plot view."}, {"x": 442, "text": "As another example, as shown in Fig.24, given the 10-dimensional results of PCA, the scatter plot view of (1, 2)-dimensions mixes up all the clusters together.However, hinted by the parallel coordinates view showing the peaks of the cluster 12"}, {"x": 443, "text": "5http://www.wisdom.weizmann.ac.il/ vision/FaceBase 6 http://www.cc.gatech.edu/gvu/ii/jigsaw/datafiles.html and 14 at dimensions 3 and 4, respectively, the scatter plot view of these dimensions turns out to give a well-clustered view.Such an observation is surprising because PCA is an unsupervised method, which does not take into account label information.This indicates that the leading two dimensions may not give enough information for high-dimensional data in visual analytics."}, {"x": 445, "text": "Trying various methods/settings on a given data set and comparing between different visualizations is in the heart of the Testbed system, and the alignment functionality of the system supports this process.Fig.25 shows the effects of the alignment for clustering and dimension reduction."}, {"x": 447, "text": "On the other hand, Fig.25(b) shows the example of aligning dimension reduction.In this example, the cluster labels are unchanged for all data items in the three fig- ures, but two different dimension reduction methods, TSTG and ISOMAP, are used.When comparing between the first and the third figures, which show the different coordinates generated by these two methods, it is difficult to recognize the correspon- dences between data items/clusters.Between the first and the second figures, whose dimension reduction results are aligned, one can perceive the correspondences in a much easier way.For example, the cluster 4 is shown to be close to the cluster 8 in TSTG, which is not the case in ISOMAP.Any data items in the cluster 6 are not located close to the cluster 7 in ISOMAP, but some data items between the two clusters overlap in TSTG.Such analyses cannot be easily made without the alignment."}, {"x": 448, "text": "5.4.4 Dimension Reduction: Supporting Multiple Perspectives"}, {"x": 449, "text": "Different dimension reduction methods can reveal different aspects of data.To show an example, we now look into the first two figures in Fig.25(b) from the perspective of supervised vs.unsupervised methods.Given a certain assignment of cluster labels, a supervised method, TSTG, gives a clear overview in terms of cluster relationships since most of the clusters are shown relatively compact, as shown in the first figure.On the contrary, an unsupervised method, ISOMAP, may reveal different aspects of data.For example, the second figure indicates that the cluster 6 is composed of two distinct subclusters shown at the top left and the bottom right.However, when the data do not have a clear cluster structure, e.g., most of the text document corpora, unsupervised dimension reduction methods give the results similar to the second figure in Fig.24, which significantly reduces the utility of the scatter plot.In this case, supervised dimension reduction would be the only choice to start with in visual analytics."}, {"x": 450, "text": "Even with a single dimension reduction method with different parameter values, different aspects of data can be obtained.In Fig.26(a), one can see that the cluster 5 (the digit 4) of the Pendigits data set moves from the top left near the cluster 10 (the digit 9) towards the cluster 7 (the digit 6) as the ISOMAP parameter k increases.In general, ISOMAP with a smaller k value focuses more on preserving the local neighborhood relationships by making non-neighborhood distances longer.Based on the sample data of each digit shown in 26(b), it can be inferred that the digit 4 is represented much closer to the digit 9, which forms their neighborhood relationships with small k values, than to the digit 6.As the k value increases, the neighborhood relationship between the digits 4 and 6 starts to be formed, which is why they become closer at a bigger k value.In this way, varying the parameter values with the same method can further reveal different interesting insight about data."}, {"x": 451, "text": "5.4.5 Clustering: Combining Knowledge from Different Clustering"}, {"x": 452, "text": "Clustering is a challenging task, and any single clustering method tends not to give fully satisfactory results.The Testbed system can remedy this problem by enabling users to perform different clustering methods and obtain more meaningful clusters by comparing between them.Fig.27 shows the scatter plot views of TSTG with the cluster labels obtained by two different clustering methods, k-means and NMF.The InfoVisVAST data set are used, and the keyword summaries of clusters for each method are as follows: k-means"}, {"x": 469, "text": "Among these clusters, the cluster 1 of the k-means clustering has a clear meaning of graph-related visualization, e.g., graph drawing, graph layout, and graph clustering.This cluster is also shown to be clearly separated from the other clusters in the left figure.As we perform brushing-and-linking on this cluster, it turns out that this cluster mainly corresponds partially to the clusters 1, 6, and 7 of the NMF clustering.Considering that these clusters contain the keywords, graph and layout and their separations from the other clusters in the right figure are not as clear as that of the cluster 1 in the left figure, one can regard the cluster 1 of k-means as a cluster with better quality."}, {"x": 470, "text": "On the other hand, in the NMF clustering, the cluster 4 seems to be clearly re- lated to multi-variate/multi-dimensional data visualization.By brushing-and-linking on this cluster, we found it corresponds mostly to the clusters 4 and 7 of the k- means clustering, which makes sense based on their keyword summaries although they are relatively more ambiguous than the cluster 4 of the NMF clustering.This observation is also supported by their cluster separations in Fig.27 , which indicates a clearer separation of the cluster 4 in the right figure than that of the clusters 4 and 7 in the left figure."}, {"x": 471, "text": "As shown in these cases, one can apply different clustering methods and take full advantage of them by visually analyzing them in the Testbed system."}, {"x": 473, "text": "In this chapter, we have presented the visual testbed system for dimension reduction and clustering in high-dimensional data visual analytics.The main contribution of our system is to bring a wide variety of traditional and state-of-the-art dimension reduction and clustering methods to visual analytics.The Testbed system provides full control of these methods with interactive visual access to their results.In addition, our system offers a flexible extensibility for new data types and methods."}, {"x": 475, "text": "In addition, we plan to enhance the alignment capability by incorporating other advanced algorithms and user interfaces.To be specific, the currently used algorithms do not change anything in the reference view, and the Procrustes analysis does not change internal relationships within each visualization at all.This may limit the performance of alignment for easy comparison between visualizations when they are significantly different.To deal with this problem, we plan to utilize other advanced methods such as graph-embedding-based methods <30>."}, {"x": 479, "text": "Figure 24: The 10-dimensional results of PCA for the Weizmann facial image data set.The pre-given person ID was used as a color label.The first figure is the parallel coordinates of the entire 10-dimensional representations, and the second and the third are the scatter plots of (1, 2)- and (3, 4)-dimensions, respectively."}, {"x": 480, "text": "(a) The alignment of clustering.For the Pendigits data set, the first figure uses the original cluster labels, and the other two uses the same cluster labels generated by k-means.In all three figures, ISOMAP is used with the same parameter values."}, {"x": 481, "text": "(b) The alignment of dimension reduction.For the Pendigits data set, the first figure uses TSTG and the other two use ISOMAP with the same parameter values.In all three figures, the original cluster labels are used."}, {"x": 483, "text": "(a) The scatter plots for the Pendigits data set generated by ISOMAP with different parameter values, k =12, 20, 30, and 50, respectively.The cluster labels represent the digits of data items, as shown on the left.The three figures on the right are aligned plots with respect to the first.As the parameter increases, the cluster 5 (the digit 4), moves from the top left near the cluster 10 (the digit 9) towards the cluster 7 (the digit 6)."}, {"x": 485, "text": "Figure 26: The effects of a parameter change in ISOMAP."}, {"x": 486, "text": "Figure 27: The scatter plot view of two different clustering, k-means and NMF, using TSTG for the InfoVisVAST data set.The right figure is aligned with respect to the left one for both clustering and dimension reduction."}, {"x": 487, "text": "CHAPTER VI IVISCLASSIFIER: AN INTERACTIVE VISUAL CLASSIFICATION SYSTEM USING SUPERVISED DIMENSION REDUCTION"}, {"x": 488, "text": "We present an interactive visual analytics system for classification, iVisClassifier, based on a supervised dimension reduction method, linear discriminant analysis (LDA).Given high-dimensional data and associated cluster labels, LDA gives their reduced dimensional representation, which provides a good overview about the cluster struc- ture.Instead of a single two- or three-dimensional scatter plot, iVisClassifier fully interacts with all the reduced dimensions obtained by LDA through parallel coor- dinates and a scatter plot.Furthermore, it significantly improves the interactivity and interpretability of LDA.LDA enables users to understand each of the reduced dimensions and how they influence the data by reconstructing the basis vector into the original data domain.By using heat maps, iVisClassifier gives an overview about the cluster relationship in terms of pairwise distances between cluster centroids both in the original space and in the reduced dimensional space.Equipped with these functionalities, iVisClassifier supports users classification tasks in an efficient way.Using several facial image data, we show how the above analysis is performed."}, {"x": 491, "text": "Figure 28: 2D Scatter plots obtained by two dimension reduction methods, LDA and PCA, for artificial Gaussian mixture data with 7 clusters and 1000 original dimen- sions.A different color corresponds to a different cluster.facial recognition, document categorization, spam filtering, and disease detection.Numerous classification algorithms such as an artificial neural network, decision trees, and support vector machines have been developed so far, and each method has advantages and disadvantages making it more suitable in certain domains.Even with its broad applicability, however, most of the classification algorithms are often per- formed in a fully automated manner that prevents users from not only understanding how the algorithm works on their data but also reflecting their domain knowledge into the classification process.Ironically, as classification algorithms become more sophisticated and advanced, they tend to be less interpretable to users due to their complicated internal procedure.These limitations may cause unsatisfactory classifi- cation results in real-world applications such as biometrics in which the reliability of the system is critical <137>.In some cases, there may be no option other than using the manual classification process without being supported by automated techniques.This chapter addresses how visual analytics systems support automated classi- fication for a real-world problem.As in other analytical tasks, the first step is to understand the data.From a classification perspective, users need to gain insight in terms of clusters such as how much the data within each cluster varies, which clusters are close to or distinct from each other, and which data are the most representative ones or outliers for each cluster.The next step is to understand both the charac- teristics of the chosen classifier itself and how they work on the data at hand.For instance, decision trees give a set of rules for classification, which are simple to inter- pret, and users can see which features in the data play an important role.In addition, analysis of misclassified data provides a better understanding of which types of clus- ters and/or data are difficult to classify.Such insight can then be fed back to the classification process in both the training and the test phases.In the training phase, users can refine the training data or modify the automated classification process for better performance in the long run.In the test phase, users can actively participate in determining the label of a new data by verifying each result that the automated process suggests and by performing further classification based on the interaction with a visual analytic system.The latter case ensures nearly perfect classification accuracy while maintaining much better efficiency than in the case of purely manual classification."}, {"x": 492, "text": "Not all classification algorithms are suitable for interactive visualization of how they work.Moreover, when the data is high dimensional such as image, text, and gene expression data, the problem becomes more challenging.To resolve this issue, we choose the classification method based on linear discriminant analysis (LDA) <60>, one of the supervised dimension reduction methods.Unlike other unsupervised methods such as multidimensional scaling (MDS) and principal component analysis (PCA), which only use data, supervised ones also involve additional information such as cluster labels associated in the data.In case of LDA, it maximally discriminates different clusters while keeping the relationship among data within each cluster tight in the reduced dimensional space.This behavior of LDA has two advantages for interactive classification systems.The first advantage is that LDA is able to visualize the data so that their cluster structure can be well exposed.For example, as seen in Fig.28, LDA reveals the cluster structure better than PCA, and through LDA, users can easily find the cluster relationship and explore the data based on it.The other advantage is that the reduced dimensional representation of the data by LDA does not require a sophisticated classification algorithm in general since the data is already transformed to a well-clustered form, and such a transformation would map an unseen data item to a nearby area of its true cluster.Thus, after applying LDA, a simple classification algorithm such as k-nearest neighbors <51> can be performed, which has been successfully applied to many areas <16, 123>.Owing to this simplicity, users can get an idea about how the new data would be classified by looking at a nearby region based on visualization through LDA."}, {"x": 493, "text": "Inspired by the above ideas, we have developed a system called iVisClassifier, in which users can visually explore and classify data based on LDA.The first contribu- tion of iVisClassifier lies in its emphasis on interpretation of and interaction with LDA for data understanding.Then, iVisClassifier features the ability to let users cooperate with the LDA visualization for the classification process.To show the usefulness of iVisClassifier, we present facial recognition examples, where LDA-based classification works well."}, {"x": 494, "text": "The rest of this chapter is organized as follows.Section 6.2 discusses previous work related to interactive data mining systems and dimension reduction methods.Section 6.3 briefly introduces LDA and its use of the regularization in visualization, and Section 6.4 describes the details of iVisClassifier.Section 6.5 shows case studies, and Section 6.6 concludes our work."}, {"x": 496, "text": "Supporting data mining tasks with interactive systems is an active area of study.As for clustering, an interactive system for hierarchical clustering was presented in <117>, and a visualization-based clustering framework was proposed in <29>, where users can analyze the clustering results and impose their domain knowledge into the next-stage clustering.In addition, various research has been conducted to make the dimension reduction process interactive.Yang et al.<143, 142> proposed a visual hierarchical dimension reduction method, which groups dimensions and visualizes data by using the subset of dimensions obtained from each group.Novel user-defined quality metrics was introduced for effective visualization of high-dimensional data in <73>.A user-driven visualization approach using MDS was proposed in <136>."}, {"x": 499, "text": "Even with such technical advances, people still prefer traditional methods such as PCA, MDS, and self-organizing maps (SOM) because the state-of-the-art methods tend not to work universally for various types of data and they often lack inter- pretability.Motivated by this, a recently proposed system called iPCA <72> enables users to interact with PCA and its visualization results in the form of scatter plots and parallel coordinates.Our system shares a lot in common with iPCA in that users can play with LDA via scatter plots and parallel coordinates.Other than data understanding, our system aims further to support classification tasks utilizing the supervised dimension reduction."}, {"x": 500, "text": "6.3 Linear Discriminant Analysis"}, {"x": 501, "text": "In this section, we briefly introduce LDA and skip rigorous mathematical derivations due to a page limit.For more technical details about LDA and its use in visualization, refer to our previous work <35>."}, {"x": 503, "text": "LDA is a linear dimension reduction method that represents each of the reduced dimensions as a linear combination of the original dimensions.By projecting the data onto such a linear subspace, LDA puts cluster centroids as remote to each other as possible (by maximizing the weighted sum, B, of squared distances between cluster centroids, as shown in Fig.29(a)), while keeping each cluster as compact as possible (by minimizing the squared sum, W, of the disances between each data item in the cluster and its cluster centroid, as shown in Fig.29(b)), in the reduced dimensional space.Due to this characteristic, LDA can highlight the cluster relationship as shown in Fig.28(a), as opposed to other dimension reduction methods such as PCA.In LDA, this simultaneous optimization is formulated as a generalized eigenvalue problem that maximizes B while keeping its minimum value of W. Theoretically, the objective function value of LDA cannot exceed that in the original space, and such an upper bound is achieved as long as at least k  1 dimensions are allowed in LDA, where k is the number of clusters.Due to this characteristic, LDA usually reduces the data (a) Maximization of distances be-(b) Minimization of approximate tween cluster centroids cluster radii"}, {"x": 506, "text": "Although LDA can reduce the data dimension down to k  1 dimensions without"}, {"x": 507, "text": "compromising its maximum objective function value, it is often not enough to use for 2D or 3D visualization purposes.In this case, users can either select a few of the most significant dimensions or perform an additional dimension reduction step to further reduce the dimension to two or three <35>.In iVisClassifier, we adopt the former strategy so that we can easily interpret the dimension reduction step while interacting with all the LDA reduced dimensions."}, {"x": 508, "text": "6.3.2 Regularization to Control the Cluster Radius"}, {"x": 509, "text": "In regularized LDA, a scalar multiple of an identity matrix I is added to the within- scatter matrix Sw, the trace of which represents W.1 It was applied to LDA <58> in order to circumvent a singularity problem when the data matrix has more dimensions than the number of data items, i.e., an undersampled case.In addition, regularization also has an advantage against overfitting in the classification context."}, {"x": 510, "text": "On the other hand, a unified algorithmic framework of LDA using the generalized"}, {"x": 511, "text": "1Instead of W, the LDA formulation uses Sw, which is then replaced with Sw + I by regular- ization.For more details, refer to <35>."}, {"x": 512, "text": "Figure 30: Effects of a regularization parameter  in Sw + I. It can control how scattered each cluster is in the visualization.The data is one of the facial image data called SCface, and we chose the first six persons images.singular value decomposition (LDA/GSVD) was proposed <68>, which broadens the applicability of LDA regardless of the singularity.For undersampled data, e.g., text and image data, LDA/GSVD can fully minimize the cluster radii, making them all equal to zero.However, making the cluster radii zero results in representing all the data points in each cluster as a single point.Although it makes sense in terms of the LDA criteria, it does not keep any information to visualize at an individual data level.Thus, we utilize regularization to control the radius or scatteredness of clusters in the visualization to either focus on the data relationship or the cluster relationship, as shown in Fig.30.In an extreme case, when we sufficiently increase the regularization parameter , Sw is almost ignored in the minimization term, i.e., Sw + I  I, so that LDA focuses only on maximizing B without minimizing W. Mathematically, this case is equivalent to applying PCA on the cluster centroids <35>."}, {"x": 514, "text": "To ensure real-time interactions, it is important to design an efficient algorithm for LDA.Therefore, we reduce the data matrix size by applying either QR decomposition for undersampled cases or Cholesky decomposition for the other cases before running LDA.The main idea here is to transform a rectangular data matrix of size m  n into a square matrix of size min(m, n)  min(m, n) without losing any information.Then, the GSVD-based LDA algorithm is performed on this reduced matrix much efficiently.For more details, refer to <103>."}, {"x": 520, "text": "Once the data matrix whose columns represent data items is obtained, LDA is per- formed on this matrix with its associated labels.Users can recompute LDA with different regularization parameter values  through a horizontal slide bar interface until the data within each cluster are adequately scattered.As described in Section 3, LDA reduces the data dimension to k1 where k is the number of clusters.Just as the reduced dimensions in PCA are in an order to preserve the most variance, those in LDA are also in an order of preserving the most value of the LDA criterion.That is, the first reduced dimension represents each cluster most compactly while keeping different clusters most distinctly.With this in mind, we visualize LDA results in four different ways: parallel coordinates (Fig.31A), the basis view (Fig.31B), heat maps (Fig.31C), and 2D scatter plots (Fig.31F)."}, {"x": 522, "text": "Parallel coordinates is a common way to visualize multi-dimensional data.In parallel coordinates, the dimension axes are placed side by side as a set of parallel lines, and the data item is represented as a polyline whose vertices on these axes indicate the values in the corresponding dimensions.The main problem of parallel coordinates is that it does not scale well in terms of both the number of data items and the number of dimensions.However, LDA can deal with both problems effectively in the following ways.First, with a manageable number of clusters, k, LDA reduces the number of dimensions to k  1, without losing any information on the cluster structure based on the LDA criterion.In addition, in terms of the number of data items, LDA plays the role of data reduction for undersampled cases since it can represent all the data items within each cluster as a single point by setting  = 0, which in turn visualizes the entire data as k items.The dimension-reduced data by LDA may suffer the same scalability problem when the number of clusters and/or the regularization parameter  increases.Nonetheless, in most cases, LDA significantly alleviates the clutter in parallel coordinates in that dealing with a large number of clusters is not practical and that users can always start their analysis with  = 0."}, {"x": 525, "text": "When data go through any kind of computational algorithms, it is crucial to have a better understanding of what happens in the process.For instance, even though the dimension reduction result is given by LDA, users may need to know the meaning behind each dimension and the reasons why those dimensions maximize the LDA cri- terion.Without such information, users cannot readily understand why certain data points look like outliers or certain clusters are prominent in the LDA result.Follow- ing this motivation, we provide users with the meaning of each reduced dimension of LDA in the following way."}, {"x": 526, "text": "First of all, LDA is a linear method where each reduced dimension is represented as a linear combination of those in the original space.Thus, we have a linear combination coefficient for each reduced dimension, which we call a basis vector, and the dimension of this basis vector is the same as the original dimension.For image data in which the original dimension is the number of pixels in the image, each coefficient value in this basis vector corresponds to each of the pixels.Based on this idea, we reconstruct the LDA basis in the original data domain, e.g., an image in our case.However, it is not always straightforward to convert the basis back to the original data domain.For example, pixel values in an image have a certain specification that they have to be all integers between 0 and 255 while the LDA basis is real-valued with positive and negative signs mixed.In the past, several heuristics to handle this issue were used in the context of PCA by mapping basis vectors to grayscale images <129, 131> by taking either its absolute value or adding the minimum value.However, these heuristic methods lose or distort the information contained in the basis vectors.Therefore, we map positive and negative numbers in the basis vector into two color channels, red and blue, respectively.In this way, we obtain the reconstructed images of LDA basis vectors as shown in Fig.31B."}, {"x": 527, "text": "6.4.2.3 Heat maps"}, {"x": 531, "text": "The scatter plot visualizes data points in the two user-selected reduced dimensions of LDA with a zoom-in/out functionality.In this view, a data item is represented as a point with an initial letter and a different color of its corresponding cluster label.Additionally, the first and the second order statistics per cluster, which are the mean and the covariance ellipse, give the effective information about clusters."}, {"x": 532, "text": "Our scatter plot view given by LDA allows users to interactively explore the data in view of the overall cluster structure in the following senses: 1. which data points are outliers or representative points in their corresponding clusters, 2. which data points are outliers or representative points in their corresponding clusters, 3. how widely the data points within a cluster are distributed and accordingly, which clusters have potential subclusters, and 4. which data points overlap between different clusters."}, {"x": 535, "text": "After obtaining insight from exploring the data with known cluster labels, users can now interactively perform classification on the new data whose labels are to be de- termined.This process works as follows.First, a new data item is mapped onto the reduced dimensional space formed by the previous data.It is then visualized in parallel coordinates and in the scatter plot view.Such visualization significantly increases the efficiency of users classification tasks by visually reducing the search space.Within this reduced visual search space, users can easily compare the new data item with the existing data or clusters nearby.When the new data point falls into a cluttered region where many different clusters overlap, users can select or filter out some data or clusters and recompute LDA with this subset of data including the new point, which we call a computational zoom-in process.In other words, LDA takes into account the selected clusters and/or those corresponding to the selected data, which requires a much smaller number of dimensions than k  1 for LDA to fully discriminate the selected clusters.Based on the new visualization generated in this way, users can better identify which clusters the new point belongs to."}, {"x": 536, "text": "On completing the visually-supported classification process, users can assign a label to the new data item and optionally include the newly labeled data in future LDA computations, which is initiated only when users want to recompute them.The reason we do not force users to include every new data in LDA computations is that users confidence level of the assigned label may not be high enough for some reason such as noise."}, {"x": 543, "text": "Next, let us look at the heat maps of the LDA dimensions shown in Figs.33-34.The first dimension turns out to reflect the most distinct clusters in the original space.In addition, the heat maps in the LDA dimensions have mostly blue-colored elements, i.e., almost zero, except for a few rows and columns, which indicates that each of the LDA dimensions tends to discriminate only a few clusters."}, {"x": 544, "text": "Next, Fig.35 shows the image reconstruction of the first six LDA bases for both data sets.It is interesting to see that in both cases, the forehead part is heavily weighted in the first dimension,3 and then in the second dimension, the forehead part is differentiated into upper and lower parts.This indicates that the forehead part is the most prominent factor for facial recognition based on LDA in our data."}, {"x": 547, "text": "6.5.2 Interactive Classification"}, {"x": 550, "text": "Once some of the new data are assigned their labels, users can recompute LDA by taking into account the newly labeled data.Fig.39 shows the distributions of the new data whose label is 0 before and after LDA recomputation with a newly labeled data item.As we can see, the rest of the unseen data in cluster 0 becomes closer to its centroid after LDA recomputation, which indicates that the updated LDA dimensions potentially better discriminates the unseen data."}, {"x": 552, "text": "In this study, we have presented iVisClassifier, a visual analytics system for clustered data and classification.Our system enables users to explore high-dimensional data through LDA, which is a supervised dimension reduction method.We interpret the effect of regularization in visualization and provide an effective user-interface in which users can control the cluster radii depending on whether they focus on the cluster- or the data-level relationships.In addition, iVisClassifier facilitates the interpretability of the computational model applied to their data.Various views such as parallel coordinates, the scatter plot, and heat maps interactively show rich aspects of the data.Finally, we showed that iVisClassifier can efficiently support a user-driven classification process by reducing humans search space, e.g., recomputing LDA with a user-selected subset of data and mutual filtering in parallel coordinates and the scatter plot."}, {"x": 555, "text": "Finally, the computation of LDA can be burdensome for user interactions when we have a large-scale data.Novel interactions with LDA provided by iVisClassifier motivate the new types of dynamic updating algorithms based on the previous LDA results in various situations.For instance, updating the LDA results when changing the regularization parameter value has not been studied before.Thus, we are currently exploring for various situations and their corresponding updating algorithms when computational algorithms are integrated into user-interactive systems."}, {"x": 556, "text": "Figure 31: The overview of the system.SCface data with 30 randomly chosen persons images were used, and different colors correspond to different clusters, e.g., persons.The arrow indicates a clicking operation.(A) Parallel coordinates view.The LDA results are represented in 29 dimensions.(B) Basis view.The LDA basis vectors are reconstructed in the original data domain, which in this case is an image.(C) Heat map view.The pairwise distances between cluster centroids are visualized.The leftmost one is computed from the original space, and the rest from each of the LDA dimensions.Upon clicking, the full-size of a heat map is shown (D), and clicking each square shows the existing data in the corresponding pair of clusters (E).(F) Scatter plot view.A 2D scatter plot is visualized using two user-selected dimensions.When clicking a particular data point, its original data item is shown (G).(H) Control interfaces.Users can change the transparency and the colors in parallel coordinates.Data can be filtered at the data level as well as at the cluster level.The interfaces for unseen data visualize them one by one, interactively classify them, and finally update the LDA model.A horizontal slide bar for the regularization parameter value in LDA controls the scattering of each cluster.(I) shows the legend about cluster labels in terms of their assigned colors and enumerations."}, {"x": 561, "text": "Figure 33: Heat map view of the pairwise cluster distances of the Weizmann data set."}, {"x": 563, "text": "Figure 34: Heat map view of the pairwise cluster distances of the SCface data set."}, {"x": 566, "text": "Figure 35: Reconstructed images of the first six LDA bases."}, {"x": 569, "text": "Figure 37: Interactive classification by computational zoom-in.Recursive visualiza- tion by recomputing LDA for interactively selected subsets of data guides a new point into its corresponding cluster.The thick arrow indicates the new point position."}, {"x": 571, "text": "Figure 38: Interactive classification by mutual filtering.Filtering both in parallel coordinates and the scatter plot leads to a single cluster.The thick arrow indicates the new point position."}, {"x": 573, "text": "Figure 39: Effects of LDA recomputation when including a newly labeled point in the existing data.The arrow indicates the newly labeled point, and the red circles represent the distribution of the remaining unseen data in cluster 0."}, {"x": 574, "text": "CHAPTER VII VISIRR: AN INTERACTIVE VISUAL INFORMATION RETRIEVAL AND RECOMMENDER SYSTEM FOR LARGE-SCALE DOCUMENT DATA"}, {"x": 579, "text": "This problem regime is highly under-explored, compared to the billions that have been invested in the related paradigm of web search.Instead, the researcher or analyst is solving a subtle investigative problem for which each of several documents provides clues.By seeing this as an information retrieval (IR) problem, the focus in this chapter is on the long tail, or recall (making sure that few relevant documents are missed), while in web search the focus is generally on the quicker gratification of precision (making sure the first page of hits or so contain very relevant documents)."}, {"x": 581, "text": "In the context of visual analytics, document analyses have long been one of the main areas studied.Visual analytics systems for document data, such as IN-SPIRE <138> and JIGSAW <121>, can help to give an overall understanding about a set of documents as well as revealing their intra-set relationships that would have been difficult and time-consuming without the help of interactive visualization.However, despite the fact that personalized recommendations seem to be a natural fit with interactive visualization in that it directly utilizes the history of user interactions, there are few instances of such work in the visual analytic community."}, {"x": 582, "text": "As one of the milestones to fill this gap, we present a novel document visual an- alytics system called VisIRR, an interactive Visual Information Retrieval and Recommendation for document data, which effectively combines traditional query- based information retrieval and personalized recommendation.Basically, as seen in Fig.40, VisIRR adopts a scatter plot as a main visualization form similar to IN- SPIRE.In other words, the documents to be visualized are first clustered into several groups via a clustering algorithm and then projected to a 2D space via a dimension reduction algorithm.However, VisIRR features various novel aspects compared to existing systems, as follows."}, {"x": 584, "text": "Advanced clustering and dimension reduction techniques: As core computational modules, VisIRR adopts state-of-the-art techniques such as nonnegative matrix factorization (NMF) for clustering and linear discriminant analysis (LDA) for dimension reduction.These techniques give the results with a much better quality as well as with a faster computational time than traditional methods including k-means, principal component analysis (PCA), and multidimensional scaling.Additionally, VisIRR provides an alignment capability for both clus- tering and dimension reduction to facilitate easy comparisons between different visualization snapshots."}, {"x": 586, "text": "To integrate all these capabilities into a mature visual analytics system, we incor- porate various building blocks for front-end GUIs and back-end computational al- gorithms.This chapter mainly presents these building blocks in more detail with detailed usage scenarios.The rest of this chapter is organized as follows.Section 7.2 discusses related work.Section 7.3 explains the front-end GUI modules and comprehensive usage scenarios that highlight the key capabilities of the system.Af- terwards, Section 7.4 mainly discusses how we efficiently handle all the necessary information from a large-scale data corpus with a scalable expansion, and Section 7.5 describes computational methods used in the back-end of the system.Section 7.6 briefly presents the user study we conducted to evaluate the system.Finally, Section 7.7 concludes the chapter and discusses about the future work."}, {"x": 589, "text": "In the context of exploratory interfaces, information foraging <106> and scent the- ory <105> suggest making clusters of related data clear and facilitating the process of finding new clusters of interest.To that end, many search result visualization sys- tems also work in concert with automated clustering algorithms, especially when the information space is extremely large or unstructured.The Pacific Northwest National Labs SPIRE system (and IN-SPIRE follow-on) uses clustering to extract common themes, and includes several visualization components <138>.Its Themescape compo- nent is an abstract 3D landscape depiction of a document space, with arrangements of hills and valleys representing the relatively strength of various themes in the docu- ment corpus and how those themes interrelate.Other systems have used this general clusters-in-landscapes (both 2D and 3D) as well <116, 21, 6>.iVisClustering <88> is an interactive document clustering system focused on the user interactions to improve cluster quality based on an advanced technique called latent Dirichlet allocation <20>.On the other hand, rather than providing user interactions customized to a partic- ular clustering technique, the Testbed system <32> offers a wide variety of clustering algorithms and easy comparisons between them via an alignment process VisIRR has adopted."}, {"x": 591, "text": "Unsurprisingly, visualization of document collections has been explored for some time in library science.A relatively early example is the Envision digital library, which includes a visualization system that places documents in a 2D grid according to user-selectable attributes <97>.Systems have used various information visualization techniques such as hyperbolic trees <76, 119> and treemaps <62, 41> to visualize results.Curated collections such as those found in digital libraries more often have pre-formed hierarchies to leverage in visual analytics applications, but simple clustering methods have been implemented as well <119>."}, {"x": 592, "text": "When document categories and groupings are not already extant, automated methods of clustering and classifying collections are key to exploratory tools, includ- ing those supporting visual analysis.A recent survey <4, 63> distinguishes between the visualization of a single document (e.g., tag clouds) and a document collection and between time- (e.g., TIARA <134>) and network-oriented collection systems.Because VisIRRs clustering system implicitly creates relationships among members (and its graph diffusion-based recommendation system explicitly uses such data), examples of the last category are most relevant.Jigsaw <121> visualizes network relationships between documents and various entities, e.g., actors, events, etc., automatically ex- tracted from them."}, {"x": 594, "text": "There has been significant commercial and academic interest in the topic of ex- ploratory search for scientific literature for some time.Several commercial tools are targeted to this problem, with a variety of automated and visual features.Google Scholar <1> automatically extracts research works and their citation networks, but has few visual or recommendation features.The Microsoft Academic Search system from Microsoft Research <2> is a similar offering that also includes more advanced network- style visualization of authorship connections as well as various ways of examining topical, institutional, and venue trends and rankings."}, {"x": 595, "text": "Direct introspection of the academic research process has been a common topic in academia as well.One variation is automated recommender/matching systems, often applied to the problem of matching individual papers from a corpus to indi- viduals from a slate of candidate reviewers <14, 132>.More relevant to VisIRR are those systems that are more exploratory or analytical in nature.The Action Science Explorer (ASE) <52> focuses on co-citation network visualization, with document clus- ters created manually or by heuristics <95>.It also includes full-text citation context features not available on VisIRR.The FacetAtlas system <24> automatically clusters document collections using a Kernel density estimation algorithm and provides multi- faceted links between document nodes (rather than just keyword or author searches as in VisIRR).CiteSpace II <28> is a visual tool for identifying new or old research trends in a given set of documents (assumed to be a relatively coherent set produced by a keyword query on a large corpus)."}, {"x": 600, "text": "The user interface of VisIRR is mainly composed of four parts.The Query Bar at the top (Fig.40(A)) enables users to issue queries dynamically using various fields such as a keyword, an author name, a publication year, and a citation count.The Scatter Plot view (with document details shown in the lower table) (Fig.40(B)) visualizes the retrieved documents (as well as any recommended documents) with their cluster summary labels.The color and the size of each node in a scatter plot represent the"}, {"x": 601, "text": "1A high-quality video introducing VisIRR is available at http://www.cc.gatech.edu/~joyfull/ vast13/visirr/visirr_final.html cluster it belongs to and its citation count, respectively.Such a view can also be gen- erated from any user-selected subset of data (Fig.40(D)).The Recommendation view on the top left (Fig.40(C)) provides tabular representations of the documents whose ratings have been assigned by users (Fig.40(C) upper table) as well as the resulting recommended documents (Fig.40(C) lower table).These recommended documents are also visualized in the Scatter Plot view as rectangles while the query-retrieved documents are shown as circles.Finally, the Label panel provides additional controls such as highlighting and/or hiding particular clusters, changing how cluster summary labels are chosen, and showing direct edge relationships from rated documents to their system-derived recommended documents (Fig.40(E))."}, {"x": 603, "text": "VisIRR has been implemented using a modified version of the ArnetMiner dataset, which contains approximately 430,000 academic research articles from a variety of disciplines and venues (primarily conferences, journals and books), as will be described in detail in Section 7.4.The following scenarios illustrate the utility of VisIRR for tasks related to this dataset."}, {"x": 605, "text": "The user starts by issuing queries from the Query Toolbar.Suppose the user issues a query of keyword disease from a title field.Once documents are retrieved due to this query, the clustering and dimension reduction steps are performed to generate the Scatter plot view (Fig.40(A)).Since most clusters contain the keyword disease, the user can adjust a slider in the Label panel in order to obtain more distinctive cluster summaries, as shown in Fig.41.From the Scatter plot view, the user can drill down to a cluster of interest, e.g., the clusters about gene expression data (the top right), and image analysis (the top left).By moving a mouse pointer to a data point, the user can check the document details via a tooltip text and also skim through"}, {"x": 606, "text": "(a) Default cluster summary (b) Distinct cluster summary"}, {"x": 609, "text": "7.3.2.2 Drilling Down via Computational Zoom-in"}, {"x": 615, "text": "In addition to exploring visualized clusters, the user can apply additional queries to further narrow down the retrieved document set.Suppose the user wanted to focus on those recently published in 2008 or later and thus created another filter from the Query Toolbar in conjunction with the previous keyword query disease.Given such a new set of documents, VisIRR creates another visualization with its own clustering and dimension reduction.The user could then compare between the new and the previous visualization results, as shown in Figs.43(a) and (b), respectively, by brushing-and- linking in order to identify, for example, which topic clusters were more/less popular"}, {"x": 616, "text": "(a) An unaligned view (b) A reference view (c) An aligned view"}, {"x": 620, "text": "Throughout analyses, the user can assign ratings to the documents he/she likes or dislikes.Among the retrieved documents, suppose the user found a document Au- tomatic tool for Alzheimers disease diagnosis using PCA and Bayesian classification rules interesting and assigned the document a 5-star rating(highly-like) by right- clicking the corresponding data point in the Scatter Plot View.Based on this user preference information, VisIRR identifies the recommended documents based on the content similarity.These rated and the recommended documents are displayed in a tabular form in the Recommendation view (Fig.40(C))."}, {"x": 621, "text": "From the list of recommended documents shown in the lower table, the user could obtain an idea that the research about Alzheimers disease mainly involves an image analysis, clustering, classification, etc.Notice that without such a recommendation capability of VisIRR, the user would not be able to obtain these documents since these documents were not included in the retrieved set by user queries.In the Scatter Plot view, the user can see these recommended documents at the upper left corner around the rated document and its nearby clusters.To obtain a better idea about the recommended documents, the user can create another visualization by only using this subset with a new clustering and dimension reduction (Fig.40(D)).From its own cluster summary and visualization, the user could see that the documents directly related to Alzheimers disease are mainly shown in the bottom half while the upper half in the Scatter Plot view, shows documents mainly related to image analysis such as content-based image retrieval, clustering, etc."}, {"x": 622, "text": "7.3.2.5 Citation- and Co-authorship-based Recommendation"}, {"x": 623, "text": "Now, among the recommended documents, the user chose another document Auto- matic Classification System for the Diagnosis of Alzheimer Disease Using Component- Based SVM Aggregations and assigned it a 5-star rating.This time, the user changes"}, {"x": 625, "text": "(b) A visualization of recommended doc- uments"}, {"x": 626, "text": "Figure 44: Citation-based recommendation results obtained by assigning a 5-star rating to the paper, Automatic Classification System for the Diagnosis of Alzheimer Disease Using Component-Based SVM Aggregations.VisIRR recommends various papers mostly with high citation counts, which are relevant to the rated paper."}, {"x": 627, "text": "(a) A visualization of retrieved and rec-(b) A visualization of only the recom- ommended documents mended documents"}, {"x": 628, "text": "Figure 45: Co-authorship-based recommendation results based on the paper, Auto- matic Classification System for the Diagnosis of Alzheimer Disease Using Component- Based SVM Aggregations.Edges show direct co-authorship relations from the rated document.its recommendation type to a citation-based one from the Recommendation panel in order to obtain highly-cited documents relevant to this document.As expected, VisIRRs top-ranked recommended documents are relatively highly cited papers, as shown in Fig.44(a).After generating another visualization only using these recom- mended items, the user can obtain a summary about them, the clusters of which are composed of image retrieval, object detection/recognition, face recognition, and texture analyses (Fig.44(b)).Notice that these types of recommendation results would not be easily obtained by a simple keyword search since these recommended documents do not contain specific keywords in common.Instead, they are only im- plicitly related with each other via a citation network on which VisIRR can perform a recommendation based."}, {"x": 629, "text": "In addition, the user also wanted to know what other topics or areas the authors of this paper are involved in.To this end, the user changed the recommendation type to a co-authorship-based one from the Recommendation view.In addition, to better show the direct co-authorship relationships from the rated paper, the user turned on the Edges checkbox by selecting the edge type as Co-authorship in the Label panel.The existing visualization of the retrieved documents now includes the recommended documents as well as the direct co-authorship relations from the rated document, as shown in Fig.45(a).Similar to the previous case, the user can generate another visualization of only the recommended items to have a better idea about the recommended documents.After varying the number of clusters, the user obtains a new visualization as shown in Fig.45(b).From this visualization, the user could gain an insight that the authors of the rated paper have written the papers, other than Alzheimers disease-related papers (the green cluster on the right), in the four areas corresponding to blind source separation, gene expression, speech processing, and neural networks.This potentially indicates that the user, who was originally interested in Alzheimers disease diagnosis, could expand his/her research by following the ways the authors of the rated paper have published in other domains."}, {"x": 630, "text": "7.4 Data Collection / Ingestion"}, {"x": 632, "text": "VisIRR is intended to efficiently handle a large-scale document corpus with a rich set of features.To this end, VisIRR begins with the ArnetMiner data set, which is composed of about half a million academic papers, books, etc.<124>.2 Although the data set is mainly used in citation network analyses, it includes a variety of both structured and unstructured information such as a title, keywords, an abstract, authors, a publication year, a venue, a document type such as a book, a paper, etc., papers in the reference list, papers citing this document, the number of references, and the number of citations."}, {"x": 633, "text": "However, the original data set has numerous missing values and inconsistencies such as different expressions of an authors name, a publication venue, etc.To clean up the data, we utilized the Microsoft Academic Search APIs.3 Specifically, we used a title of each document as a query in order to obtain the full information about the document from the Microsoft Academic Search API, which fills the missing values and rectifies the inconsistencies.Finally, VisIRR builds upon 432,605 documents spanning from year 1825 to 2011."}, {"x": 636, "text": "2The used data is available as DBLP-Citation-network V5 at http://arnetminer.org/ citation.manage the large-scale data in all these various forms, we carefully optimized vari- ous data processing/storage techniques via database construction, pre-computation of frequently used information, and balanced storage between disk and memory.Even- tually, the system is easily and widely deployable in typical commodity PCs instead of requiring high-performance parallel machines."}, {"x": 638, "text": "For efficient and flexible query support, we have encoded the original data as a SQL database including full-text search capabilities on a title, keywords, an abstract, and a venue fields.For clustering and dimension reduction steps, we have pre-computed the sparse vector representations of individual documents based on a title, keywords, and an abstract fields together via a bag-of-words encoding scheme.Each vector representation is stored as a single file in a disk, the file name of which is the document ID.In this way, VisIRR can retrieve the vector representations of documents using their document IDs in the time complexity of O(1)."}, {"x": 639, "text": "7.4.2.2 Vector Representation"}, {"x": 642, "text": "The recommendation module, which will be described in Section 7.5, requires an input graph where the nodes correspond to documents and the edges represent their pairwise similarities/relationships.We have pre-computed three such graphs for the entire data set using contents, a citation network, and co-authorship, respectively, in order to support various recommendation capabilities.For content-based graph, we initially computed the pairwise cosine similarities between all the pairs of documents using their vector representations.Since maintaining all the pairwise information requires O(n2) storage where n is the total number of documents, we identified the fixed number (10 in our case) of the most similar documents for each document and kept only the edges between them.For the citation graph, we formed edges between a pair of documents if either cites the other.For the co-authorship graph, edges are created if two documents share the common author(s).Since citation and co- authorship graphs are typically sparse, we stored all these edge information.For each graph, VisIRR maintains the mappings from an individual document to a list of edges in terms of the destination document and its edge value so that it can retrieve the edge information for particular documents in the time complexity of O(1)."}, {"x": 645, "text": "(a) Maximization of dis-(b) Minimization of ap- (c) LDA (d) PCA tances between clusterproximate cluster radii centroids"}, {"x": 646, "text": "Figure 46: A high-level idea of LDA and a comparison example betwee LDA and PCA.A different color corresponds to a different cluster, and c1 and c2 are the cluster centroids.LDA tries to find a reduced-dimensional representation of data by putting different clusters as far as possible (a) and representing each cluster as compact as possible (b).(c) and (d) show an example 2D scatter plots obtained by PCA and LDA, respectively, for artificial Gaussian mixture data with 7 clusters and 1,000 original dimensions.From a comparison between them, LDA is shown to reveal a much clearer cluster structure than PCA in a 2D space.indexed as additional dimensions.However, sparse vector representations of exist- ing documents would still remain the same, and thus we only need to compute the representation of new documents, which can also be easily done."}, {"x": 647, "text": "Finally, in the case of updating graph representations, the only tricky part is to update the content similarity graph, where the top 10 most similar documents and their cosine similarity values are maintained.Specifically, we have to compute the pairwise similarity between all the existing documents and all the new documents, and then compare these similarity values against the current top 10 similarity values.If any of the former similarity values are greater than the latter similarity values, the corresponding edges are replaced with those to the new documents.The computa- tional complexity of this process is O(n  nnew) where n and nnew are the numbers of the existing and the new documents, respectively."}, {"x": 651, "text": "Clustering plays a crucial role in providing a summary of a given set of documents as a manageable number of groups based on their semantic meanings.The resulting cluster indices are used to color-code documents in a scatter plot with their cluster summaries in terms of the most frequently shown keywords (Fig.40(B)(E)).VisIRR adopts a state-of-the-art technique called nonnegative matrix factorization (NMF) <80>, which have shown superior performances in document clustering over traditional methods such as k-means <81, 141>."}, {"x": 652, "text": "Given a nonnegative matrix X  Rmn, and an integer k  min(m, n), NMF finds a lower-rank approximation given by"}, {"x": 653, "text": "X  WH, (40) where W  Rmk and H  Rkn are nonnegative factors.NMF can be formulated using the Frobenius norm as for the i-th document, and by taking the index the value of which is the largest, the cluster index of the document can be obtained."}, {"x": 654, "text": "The specific NMF algorithm we have used is based on a recently proposed block principal pivoting algorithm <82>,4 which is found to be one of the fastest and reliable algorithms.Although not reported, we have conducted an extensive amount of com- parison of NMF against traditional clustering techniques such as k-means, and we found that NMF mostly gives semantically more meaningful clusters than any other methods while requiring a significantly faster computational time."}, {"x": 655, "text": "7.5.2 Dimension Reduction"}, {"x": 656, "text": "Given high-dimensional vector representations of documents, dimension reduction computes their 2D representations so that they can be visualized in a scatter plot (Fig.40(B)).From the scatter plot, users can get an idea about how clusters/documents are related with each other.VisIRR adopts an advanced dimension reduction method called linear discriminant analysis (LDA) <68>."}, {"x": 657, "text": "Unlike traditional methods such as principal component analysis and multidi- mensional scaling, LDA explicitly utilizes additional cluster label information, which are taken from the clustering module, associated with the input high-dimensional vectors.Using this information, LDA tries to preserve the cluster structure in the low-dimensional space such that the dimension-reduced result can clearly reveal the underlying cluster structure in the input data.In this manner, as shown in Fig.46, LDA has an advantage over most traditional methods such as PCA and MDS in that it can provide a clear cluster structure in the data when the cluster label information is given."}, {"x": 658, "text": "Furthermore, VisIRR provides a slider interface for controlling how compactly each cluster is represented by using regularization on LDA, which enables users to"}, {"x": 661, "text": "In VisIRR, users can create multiple scatter plots for (1) new parameter values, e.g., the number of clusters in NMF, a regularization value in LDA, and (2) a new set of data from different queries or arbitrary selection by users.In order to maintain con- sistency between different scatter plots and facilitate their easy comparisons, VisIRR provides alignment capabilities on different clustering and dimension reduction re- sults.By aligning clustering results, users can expect that the same cluster index and color indicate semantically similar meanings.On the other hand, by aligning dimension reduction results, users can expect that the same data point is located in a similar position in the 2D space between different scatter plots."}, {"x": 662, "text": "To align different clustering results, VisIRR utilizes the Hungarian algorithm <85>.Given two sets of cluster assignments for the same set of documents, the Hungarian algorithm finds the optimal pairwise matching of cluster indices between the two sets so that the number of common data items within matching cluster pairs can be maximized.Based on the resulting matching, VisIRR changes the cluster indices and the colors of the newly created scatter plot with respect to those of the used reference scatter plot.In this manner, VisIRR maintains the cluster indices/colors with their consistent semantic meanings throughout multiple visualization results."}, {"x": 663, "text": "The alignment of different dimension reduction results is based on Procrustes analysis <69, 53>, which best maps one result to the other with only a rotation matrix.In addition, VisIRR extends the original Procrustes analysis by incorporating trans- lation and isotropic scaling factors as well.That is, given two reduced-dimensional matrices X, Y Rmn, where m is the number of dimensions and n is the number of data points, VisIRR solves"}, {"x": 667, "text": "Given these user preference information, VisIRR identifies the recommended doc- uments by performing a PageRank-style graph diffusion algorithm on a weighted graph of the entire document set.As briefly discussed in Section 7.4, such a graph can be based on either contents, a citation network, or co-authorship depending on users choice.Particularly, VisIRR has adopted a heat-kernel-based algorithm <40>, which gives a much faster convergence than the other traditional algorithms.In de- tail, given an input graph W  RNN between N documents, where each column of W is normalized such that its sum is equal to one, and a user preference vector p  RN1, where the i-th component pi is the preference value, VisIRR computes the An intuitive explanation of this formulation is that the preference value piof node i is propagated to its neighbor nodes with the corresponding weights specified in the graph W at the first iteration, and then the resulting values are then propagated again with the same graph W with the scale factor (1  ) at the next iteration, and so on.Finally, those values computed from each iteration is added up, forming a final recommendation score vector r. Once the computation is done, VisIRR presents the documents with the biggest scores in r as the recommended ones."}, {"x": 670, "text": "The system is mainly implemented in JAVA for front-end UI and rendering modules, which are partly based on the FODAVA Testbed system <32>.NetBeans Rich Client Platform and IDE5 have been used for flexible window management.The back-end computational modules NMF and LDA are originally written in MATLAB but we have wrapped them into a JAVA library by using a Matlab built-in functionality called Javabuilder.6 Since the library made in this manner is self-contained, VisIRR does not require an actual Matlab to be installed.For querying and accessing with the database, we have used H2 library.7"}, {"x": 673, "text": "The design of this study is evidence-by-existence.That is, our goal is to provide some support of our implicit VisIRR design claims.For example, we seek to show that recommendations outside the initial query set are useful to some people and they can find useful documents with VisIRR.It is not an experimental design as it includes no control condition, so we cannot and do not make any relative claims about VisIRRs effectiveness compared to other research or commercial alternatives (e.g., Google Scholar).Instead, our purpose is modest: demonstrate VisIRR can meet its intended purpose for real users (providing evidence that our imagined user scenarios above are valid), and provide direction for a future, comprehensive experimental or quasi-experimental design."}, {"x": 675, "text": "Participants in the study used VisIRR implemented with the same ArnetMiner-based set of academic articles described in the usage scenarios above.After completing a consent form and a brief demographics questionnaire, they were provided a live demo of the system usage scenario (lasting 5-10 minutes, depending on questions).Partici- pants then used the system to conduct searches of their own choosing and to complete a set of pre-defined tasks concerning either ubiquitous computing or information visu- alization (e.g., Describe any apparent subfields or application areas of information visualization.).Finally, we deployed a version of the IBM Computer System Usabil- ity Questionnaire (CSUQ) <90> along with a few other subjective assessment questions specific to VisIRR."}, {"x": 676, "text": "The system was installed on a workstation with dual 2.5GHz Intel Xeon processors and 128GB RAM running 64-bit Windows 7, though the Java VM memory limit was set to only 8 GB.It was connected to both a 30 monitor (1920x1200) and a 19 monitor (1280x1024); users were free to arrange windows on either monitor, but most chose to use the majority of the 30 screen for the VisIRR windows and dialogs with the task response window on the 19 screen."}, {"x": 677, "text": "We recruited 7 male Ph.D.students between the ages of 24-40 enrolled in various technical degree programs (engineering, computer science, robotics).As such, they all had experience doing academic literature searches using online resources such as Google, Google Scholar, the IEEE/ACM digital libraries, etc.We asked participants to self-rate their familiarity with information visualization and ubiquitous computing literature; all self-rated 4 or less on a 7-point Likert scale for information visualization and 6 of the 7 did so for ubiquitous computing.Participants completed tasks for the area with which they were less familiar.The VisIRR system was instrumented to log the UI actions shown in Table 9.We non-intrusively observed users while they completed the tasks."}, {"x": 681, "text": "All users made at least 9 distinct document ratings (again, across all tasks), and interestingly did so relatively evenly from different portions of the UI (the recom- mended, rating and query lists, and the scatterplot).Document details were dispro- portionately triggered from the visualization (112/146), indicating both that partici- pants interacted with the visualization and drilled down into document details from there.This matches both our subjective observations and post-test user comments like Its good to have that first clustering result ... Its easy to go deeper down from one or two clusters.Unfortunately, the logging does not distinguish between regular and recommended document nodes in the scatter plot."}, {"x": 682, "text": "On the subjective CSUQ, scores were generally 5 or higher, with the lowest rated scores coming on the questions The system has all the functions and capabilities I expect it to have; The system gives error messages that clearly tell me how to fix problems; and Whenever I make a mistake using the system, I recover easily and quickly.We suspect these ratings reflect occasional software bugs and crashes that occurred during some of the participant sessions."}, {"x": 684, "text": "Of course, we would hypothesize that rating-based refinement is more productive since it does require less user expertise at generating useful keyword sequences; at least one user agreed, saying that VisIRR ... is definitely much better than blindly searching Google Scholar or basic search engines using just a few keywords."}, {"x": 686, "text": "In this chapter, we have presented a visual analytics system called VisIRR, an inter- active visual information retrieval and recommendation system for document discov- ery.One of the primary contributions of VisIRR is that it has effectively combined both paradigms of passive query process and active recommendation by reflecting the user preference feedback.In addition, VisIRR directly tackles a large-scale docu- ment corpus via efficient data management and new data updating as well as a suite of state-of-the-art computational methods such as NMF, LDA, and graph diffusion- based recommendation."}, {"x": 694, "text": "1.A theoretical framework of visualizing clustered high-dimensional data via di- mension reduction.The proposed two-stage framework enables various com- binations and their interpretations of several well-known supervised and unsu- pervised dimension reduction methods to obtain appropriate 2D/3D represen- tations of high-dimensional data.<35>."}, {"x": 696, "text": "3.Iteration-wise integration framework of computational methods for real-time visualization and interaction.The presented framework and several applications of this idea in existing visual analytics systems, such as Jigsaw, iVisClustering, and the Testbed system, shows the effectiveness of the proposed framework using widely-used computational methods such as PCA, MDS, t-SNE, k-means, and latent Dirichlet allocation <31>."}, {"x": 698, "text": "1.Testbed: an interactive visual testbed system for various dimension reduction and clustering methods.The Testbed system brings a wide variety of tradi- tional and state-of-the-art dimension reduction and clustering methods to vi- sual analytics.The Testbed system provides full control of these methods with interactive visual access to their results.In addition, our system offers a flexible extensibility for new data types and methods <32>."}, {"x": 701, "text": "Figure 47: Hierarchical precision refinement of PCA computational results.1,420 facial image data represented as 11,264-dimensional vectors have been visualized with their person ID color-coded."}, {"x": 703, "text": "3.VisIRR: an interactive visual information retrieval and recommender system for large-scale document data.VisIRR integrates two main notions of information retrieval and personalized recommendation into a single visual analytics system.VisIRR is well-engineered to handle large-scale data and streaming data and utilizes the state-of-the-art clustering and dimension reduction methods such as NMF and LDA.The recommendation module works on an efficient graph diffusion algorithm on large-scale sparse graphs based on various criteria such as content, co-authorhship, and citation network.<33>."}, {"x": 708, "text": "Another potential idea to achieve real-time interactivity is to confine the data scale.As clearly seen in Fig.47(a), the finite resolution in the screen space introduces the limitation in the number of data items that can be visualized.Suppose there are much more data items than the total number of pixels available.In this case, it does not make sense to compute algorithms on the entire set of data items even though there is no possible way to visualize all of them.This approach is particularly useful when it comes to the computational complexity of algorithms.In principle, as the number of data items increases, the algorithm complexity cannot be more efficient than O(n), which assumes that every data item is processed at least once.Even with such an ideal complexity, a computational bottleneck can exist in real-time visual analytics.The notion of a fixed number of available pixels can turn the algorithm complexity into O(1) in the sense that we can visualize only a specific number of data items at most.One of the easiest ways to select this subset of data is random sampling, although one could adopt other more carefully designed sampling methods that better represent the entire data set."}, {"x": 709, "text": "However, some user interactions such as zoom-in/out may require the computa- tional results on the rest of the data items whose results have yet to be computed.However in this case, one can handle the situation via different efficient computa- tions.For example, suppose one wants to perform clustering on a large-scale data set, and the computations have been performed only on a certain subset of data.Then, to obtain the cluster labels of the other data items, one could apply a simple classification method based on already computed clusters.In addition, in the case of dimension reduction, suppose PCA has been computed on a subset of data.Then, the rest of the data can be projected onto the same space via a linear transformation matrix given by PCA, which is a much more efficient process than computing PCA on the entire data set.Although these approximated approaches cannot give the exact same results as the ones generated by using the entire data from the beginning, it is a viable approach to ensure real-time visual analytics for large-scale data."}, {"x": 712, "text": "For example in dimension reduction, MDS gives the reduced-dimensional re- sult that best preserves the original pairwise dimension reduction under the low- dimensional space within a given dimension.Fig.48 shows how these pairwise distances in the lower-dimensional space are degraded compared to those in the original high-dimensional space as the target dimension decreases.Each depicted line in Fig.48 represents the pairwise distances as their original values in the high- dimensional space decrease along with a horizontal axis.Fig.48 indicates that MDS significantly distorts the original data relationships by severely decreasing particular pairwise distances while some others are almost preserved.Even though MDS sup- posedly gives the best result in preserving the pairwise distances, one would not be able to trust the result considering such a significant distortion."}, {"x": 713, "text": "There are two ways to tackle these problems.The first one would be to de- sign a new computational method based on improved criteria that perceptually make more sense.For instance, one could come up with a new criterion for an alternative method to MDS so that distance losses can be evenly distributed throughout all the pairwise distances.However, such perception-friendly criteria may cause additional computational complexities.Therefore, as another way to tackle the trustworthiness problem, visual analytics could at least provide users with the information about how trustworthy the computational results are by showing the perceptual quality measures.Studying these new computational methods as well as corresponding per- ceptual quality measures would be another promising research direction."}], "scenes": [["Testbed"], ["Gaussian_function"], ["Ambiguity"], ["Visual_analytics"], ["Latent_Dirichlet_allocation"], ["Principal_component_analysis"], ["Holotype"], ["Visual_analytics"], ["Dimensionality_reduction"], ["Dimension", "Cartesian_coordinate_system"], ["Holotype"], ["Dimension", "Cartesian_coordinate_system"], ["Iteration", "Information_visualization"], ["Testbed"], ["Testbed"], ["Testbed"], ["Testbed"], ["Stage_(stratigraphy)", "Software_framework", "Dimension"], ["Two-dimensional_space"], ["Two-dimensional_space", "Dimension"], ["Self-organizing_map", "Multidimensional_scaling", "Principal_component_analysis"], ["Outlier"], ["Principal_component_analysis"], ["Self-organizing_map"], ["Two-dimensional_space"], ["Linear_discriminant_analysis"], ["Linear_discriminant_analysis"], ["Principal_component_analysis"], ["Principal_component_analysis"], ["Linear_discriminant_analysis", "Principal_component_analysis"], ["Two-dimensional_space", "Multidimensional_scaling"], ["Linear_discriminant_analysis", "Principal_component_analysis"], ["Mathematical_optimization", "Dimensionality_reduction", "Trace_(linear_algebra)"], ["Graph_theory"], ["Nickel"], ["Antimony"], ["Antimony"], ["Graph_theory", "Sinclair_Broadcast_Group"], ["Antimony"], ["Graph_theory"], ["Principal_component_analysis"], ["Graph_theory"], ["Oil_cleansing_method", "Linear_discriminant_analysis", "Principal_component_analysis"], ["Linear_discriminant_analysis"], ["Graph_theory", "Linear_discriminant_analysis", "Sinclair_Broadcast_Group"], ["Linear_discriminant_analysis"], ["Graph_theory", "Sinclair_Broadcast_Group"], ["Graph_theory"], ["Linear_discriminant_analysis"], ["Graph_theory", "Linear_discriminant_analysis", "Antimony"], ["Oil_cleansing_method", "Centroid"], ["Graph_theory", "Oil_cleansing_method", "Sinclair_Broadcast_Group"], ["Graph_theory", "Sinclair_Broadcast_Group"], ["Graph_theory"], ["Graph_theory", "Antimony", "Sinclair_Broadcast_Group"], ["Antimony"], ["Antimony"], ["Graph_theory", "Antimony", "Sinclair_Broadcast_Group"], ["Graph_theory", "Antimony", "Sinclair_Broadcast_Group"], ["Antimony"], ["Graph_theory", "Oil_cleansing_method", "Sinclair_Broadcast_Group", "QR_decomposition"], ["Oil_cleansing_method"], ["QR_decomposition"], ["Oil_cleansing_method"], ["Oil_cleansing_method", "Antimony"], ["Oil_cleansing_method"], ["Oil_cleansing_method", "Norm_(mathematics)"], ["Principal_component_analysis"], ["Principal_component_analysis"], ["Graph_theory"], ["Graph_theory"], ["Graph_theory"], ["Principal_component_analysis"], ["Guanosine_triphosphate", "Principal_component_analysis"], ["Principal_component_analysis"], ["Principal_component_analysis"], ["Antimony"], ["Oil_cleansing_method", "Linear_discriminant_analysis", "Antimony", "Principal_component_analysis"], ["Principal_component_analysis"], ["Principal_component_analysis"], ["Graph_theory"], ["Norm_(mathematics)", "Matrix_norm"], ["Graph_theory"], ["Graph_theory"], ["Graph_theory"], ["Oil_cleansing_method", "Linear_discriminant_analysis"], ["Graph_theory"], ["Oil_cleansing_method", "Linear_discriminant_analysis"], ["Principal_component_analysis"], ["Norm_(mathematics)", "Matrix_norm"], ["Linear_discriminant_analysis"], ["Linear_discriminant_analysis"], ["Linear_discriminant_analysis"], ["Norm_(mathematics)", "Matrix_norm", "Principal_component_analysis"], ["Oil_cleansing_method"], ["Oil_cleansing_method", "Antimony"], ["Norm_(mathematics)", "Matrix_norm", "Principal_component_analysis"], ["Antimony", "Principal_component_analysis"], ["India"], ["Regularization_(mathematics)", "Linear_discriminant_analysis"], ["Linear_discriminant_analysis"], ["Linear_discriminant_analysis", "Generalized_singular_value_decomposition"], ["Grounded_theory", "Linear_discriminant_analysis", "Generalized_singular_value_decomposition", "Sinclair_Broadcast_Group"], ["Linear_discriminant_analysis"], ["Grounded_theory", "Linear_discriminant_analysis"], ["Grounded_theory"], ["Linear_discriminant_analysis"], ["Grounded_theory", "Sinclair_Broadcast_Group", "Principal_component_analysis"], ["Usenet_newsgroup", "MEDLINE", "Reuters", "Normal_distribution"], ["Normal_distribution"], ["Common_fig"], ["MEDLINE"], ["Common_fig"], ["Usenet_newsgroup"], ["Common_fig"], ["Reuters"], ["Common_fig"], ["MEDLINE"], ["MEDLINE"], ["Principal_component_analysis"], ["Principal_component_analysis"], ["Linear_discriminant_analysis"], ["Linear_discriminant_analysis"], ["Usenet_newsgroup"], ["MEDLINE", "Reuters"], ["Linear_discriminant_analysis"], ["Linear_discriminant_analysis"], ["Linear_discriminant_analysis"], ["Linear_discriminant_analysis"], ["Principal_component_analysis"], ["Grounded_theory", "Sinclair_Broadcast_Group", "Principal_component_analysis"], ["Grounded_theory", "Sinclair_Broadcast_Group", "Principal_component_analysis"], ["Common_fig"], ["Common_fig"], ["Linear_discriminant_analysis"], ["Common_fig"], ["Linear_discriminant_analysis"], ["Euclidean_space", "Linear_discriminant_analysis", "Matrix_norm", "Principal_component_analysis"], ["Parameter"], ["Isomap"], ["Isomap"], ["Isomap"], ["Isomap"], ["Switzerland", "Isomap"], ["Classical_antiquity", "Multidimensional_scaling"], ["Euclidean_distance", "Multidimensional_scaling", "Isomap"], ["Multidimensional_scaling"], ["Euclidean_distance"], ["Isomap"], ["Isomap"], ["Isomap"], ["Isomap"], ["Isomap"], ["Isomap"], ["Isomap"], ["Isomap"], ["Switzerland"], ["Isomap"], ["Floyd\u2013Warshall_algorithm"], ["Dijkstra's_algorithm"], ["Isomap"], ["Isomap"], ["Isomap"], ["Isomap"], ["Isomap"], ["Isomap"], ["Euclidean_distance"], ["Multidimensional_scaling", "Isomap"], ["Isomap"], ["Isomap"], ["Europe", "Euclidean_distance"], ["Euclidean_distance", "Isomap"], ["Recurrent_neural_network", "Isomap", "Data_General"], ["Data_General"], ["Data_General"], ["Algorithm"], ["Time_complexity"], ["Isomap"], ["Big_O_notation"], ["Big_O_notation"], ["Shortest_path_problem"], ["Data_General"], ["Philippines", "Recurrent_neural_network", "Williams'_p_+_1_algorithm"], ["Time_complexity"], ["Algorithm"], ["T.I."], ["Algorithm"], ["Big_O_notation"], ["Algorithm"], ["Big_O_notation"], ["Data_General"], ["Cornelius_Lanczos", "Isomap"], ["Time_complexity", "Krylov_subspace", "Lanczos_algorithm"], ["Cornelius_Lanczos"], ["Krylov_subspace"], ["Lanczos_algorithm"], ["Krylov_subspace"], ["Lanczos_algorithm"], ["Isomap"], ["Dijkstra's_algorithm", "Isomap"], ["MATLAB"], ["Fortran", "Isomap", "MATLAB", "Lanczos_algorithm"], ["Windows_Vista", "Central_processing_unit", "MATLAB", "Isomap"], ["Switzerland", "RAND_Corporation", "MEDLINE", "Isomap"], ["National_Institutes_of_Health", "MEDLINE"], ["Isomap"], ["Isomap"], ["Isomap"], ["Switzerland"], ["RAND_Corporation"], ["Isomap"], ["Dijkstra's_algorithm", "Isomap"], ["Data_mining"], ["Isomap"], ["Isomap"], ["Ficus"], ["Ficus"], ["Ficus"], ["Multidimensional_scaling"], ["Isomap"], ["Switzerland"], ["Isomap"], ["Ficus"], ["Ficus"], ["Ficus"], ["Ficus"], ["Isomap"], ["Software_framework", "Computer", "Integral"], ["Iteration", "Information_visualization"], ["Latent_Dirichlet_allocation"], ["Latent_Dirichlet_allocation"], ["Multidimensional_scaling", "Principal_component_analysis"], ["Iteration"], ["Multidimensional_scaling", "Principal_component_analysis"], ["Latent_Dirichlet_allocation"], ["Interactivity"], ["Ronald_Fisher"], ["Graphics_processing_unit"], ["Human\u2013computer_interaction"], ["Iteration", "Information_visualization"], ["User_(computing)", "Principal_component_analysis"], ["Central_processing_unit"], ["Atlantic_Reporter"], ["Principal_component_analysis"], ["Principal_component_analysis"], ["Lanczos_algorithm", "QR_algorithm"], ["Krylov_subspace", "Lanczos_algorithm"], ["Lanczos_algorithm", "QR_algorithm"], ["Principal_component_analysis"], ["Multidimensional_scaling"], ["Multidimensional_scaling"], ["Euclidean_distance"], ["Multidimensional_scaling"], ["Human\u2013computer_interaction"], ["Prefuse", "Multidimensional_scaling"], ["Multidimensional_scaling"], ["Multidimensional_scaling"], ["Multidimensional_scaling"], ["Multidimensional_scaling"], ["Multidimensional_scaling"], ["Multidimensional_scaling"], ["Stochastic", "Embedding"], ["T-distributed_stochastic_neighbor_embedding"], ["Multidimensional_scaling"], ["Human\u2013computer_interaction"], ["Multidimensional_scaling"], ["Multidimensional_scaling"], ["Principal_component_analysis"], ["Principal_component_analysis"], ["Principal_component_analysis"], ["Euclidean_space"], ["Human\u2013computer_interaction"], ["Multidimensional_scaling"], ["Latent_Dirichlet_allocation"], ["Latent_Dirichlet_allocation"], ["Latent_Dirichlet_allocation"], ["National_Science_Foundation"], ["T-distributed_stochastic_neighbor_embedding"], ["T-distributed_stochastic_neighbor_embedding"], ["Latent_Dirichlet_allocation"], ["Latent_Dirichlet_allocation"], ["Iteration"], ["Latent_Dirichlet_allocation"], ["Reduction_(complexity)", "Cluster_analysis", "Dimension"], ["Visual_analytics"], ["Testbed"], ["Testbed"], ["Visual_analytics"], ["Dimensionality_reduction"], ["Dimension", "Cartesian_coordinate_system"], ["Testbed"], ["Testbed"], ["Testbed"], ["Testbed"], ["Testbed"], ["Testbed"], ["Testbed"], ["Scatter_plot", "Linear_discriminant_analysis", "Multidimensional_scaling", "Cartesian_coordinate_system"], ["Dimensionality_reduction", "Cluster_analysis"], ["Linear_discriminant_analysis", "Multidimensional_scaling", "Principal_component_analysis"], ["Three-dimensional_space_(mathematics)", "Cartesian_coordinate_system"], ["Dimension", "Cartesian_coordinate_system"], ["Isomap"], ["Dimension", "Cartesian_coordinate_system"], ["Dimension", "Cartesian_coordinate_system"], ["Multidimensional_scaling"], ["Linear_discriminant_analysis"], ["Linear_discriminant_analysis"], ["Linear_discriminant_analysis"], ["Dimension", "Cartesian_coordinate_system"], ["Cartesian_coordinate_system"], ["Normal_distribution"], ["Latent_Dirichlet_allocation", "Non-negative_matrix_factorization"], ["Latent_Dirichlet_allocation"], ["Analytics", "Dimensionality_reduction", "Cluster_analysis"], ["India", "GGobi", "Java_version_history", "Weka_(machine_learning)"], ["India"], ["Principal_component_analysis"], ["Cartesian_coordinate_system"], ["Cartesian_coordinate_system"], ["India", "Java_version_history", "Principal_component_analysis"], ["Automatic_transmission", "Java_version_history"], ["Java_version_history"], ["User_(computing)"], ["Java_version_history"], ["Java_version_history"], ["GGobi"], ["Cartesian_coordinate_system"], ["Principal_component_analysis"], ["Dimension", "Cartesian_coordinate_system"], ["Three-dimensional_space_(mathematics)", "Cartesian_coordinate_system"], ["Principal_component_analysis"], ["Weka_(machine_learning)"], ["Weka_(machine_learning)"], ["Weka_(machine_learning)", "Testbed"], ["Weka_(machine_learning)"], ["Testbed"], ["Testbed"], ["Testbed"], ["Testbed"], ["Testbed"], ["Testbed"], ["Testbed"], ["Testbed"], ["Testbed"], ["User_(computing)"], ["Testbed", "Comma-separated_values"], ["Testbed"], ["Testbed"], ["Tf\u2013idf"], ["Testbed"], ["Testbed"], ["Non-negative_matrix_factorization"], ["Dimension"], ["Testbed"], ["Linear_discriminant_analysis"], ["Oil_cleansing_method"], ["Linear_discriminant_analysis"], ["Sammon_mapping"], ["Isomap"], ["Minimum-variance_unbiased_estimator", "Local_tangent_space_alignment"], ["Kernel_(operating_system)"], ["Parallel_coordinates"], ["Testbed"], ["Testbed"], ["Scatter_plot"], ["Testbed", "Cartesian_coordinate_system"], ["Testbed"], ["Label"], ["Testbed"], ["Testbed"], ["Testbed"], ["Testbed"], ["Testbed"], ["Hungarian_algorithm", "Testbed"], ["Hungarian_algorithm"], ["Hungarian_algorithm", "Testbed"], ["Procrustes_analysis", "Testbed"], ["Procrustes_analysis"], ["Euclidean_space"], ["Testbed"], ["JavaScript", "Testbed", "Graphical_user_interface"], ["NetBeans", "Rich_client_platform"], ["MATLAB"], ["JavaScript", "MATLAB"], ["MATLAB"], ["MATLAB"], ["Microsoft_Windows", "MATLAB"], ["Euclidean_space", "MATLAB"], ["JavaScript", "MATLAB", "Testbed"], ["Testbed"], ["Testbed"], ["Testbed"], ["ICO_(file_format)"], ["JavaScript", "MATLAB", "Testbed"], ["Testbed"], ["Institute_of_Electrical_and_Electronics_Engineers", "Viewer_Access_Satellite_Television", "Information_visualization"], ["Parallel_coordinates"], ["Testbed"], ["Principal_component_analysis"], ["FaceBase"], ["Principal_component_analysis"], ["Testbed"], ["Isomap"], ["Isomap"], ["Isomap"], ["Dimensionality_reduction"], ["Isomap"], ["Isomap"], ["Holotype", "Isomap"], ["Cluster_analysis"], ["Testbed"], ["Non-negative_matrix_factorization"], ["Non-negative_matrix_factorization"], ["Non-negative_matrix_factorization"], ["Non-negative_matrix_factorization"], ["Testbed"], ["Testbed"], ["Procrustes_analysis"], ["Principal_component_analysis"], ["Isomap"], ["Isomap"], ["Isomap"], ["Isomap"], ["Non-negative_matrix_factorization"], ["Chapter_(religion)"], ["Statistical_classification", "Visual_perception", "Supervised_learning", "System", "Dimension"], ["Linear_discriminant_analysis"], ["Linear_discriminant_analysis"], ["Linear_discriminant_analysis"], ["Scatter_plot", "Normal_distribution", "Linear_discriminant_analysis", "Principal_component_analysis"], ["Linear_discriminant_analysis"], ["Multidimensional_scaling", "Principal_component_analysis"], ["Linear_discriminant_analysis"], ["Linear_discriminant_analysis"], ["Linear_discriminant_analysis"], ["Linear_discriminant_analysis", "Principal_component_analysis"], ["Linear_discriminant_analysis"], ["Linear_discriminant_analysis"], ["Linear_discriminant_analysis"], ["Linear_discriminant_analysis"], ["Linear_discriminant_analysis"], ["Linear_discriminant_analysis"], ["Novel"], ["Multidimensional_scaling"], ["Principal_component_analysis"], ["Principal_component_analysis"], ["Linear_discriminant_analysis"], ["Discriminant"], ["Linear_discriminant_analysis"], ["Linear_discriminant_analysis"], ["Linear_discriminant_analysis"], ["Linear_discriminant_analysis"], ["Principal_component_analysis"], ["Linear_discriminant_analysis"], ["Linear_discriminant_analysis"], ["Linear_discriminant_analysis"], ["Visualization_(computer_graphics)", "Cartesian_coordinate_system"], ["Linear_discriminant_analysis"], ["Regularization_(mathematics)"], ["Linear_discriminant_analysis"], ["Linear_discriminant_analysis"], ["Linear_discriminant_analysis"], ["Linear_discriminant_analysis", "Generalized_singular_value_decomposition"], ["Linear_discriminant_analysis", "Generalized_singular_value_decomposition"], ["Linear_discriminant_analysis"], ["Linear_discriminant_analysis", "Principal_component_analysis"], ["Cholesky_decomposition", "QR_decomposition"], ["Linear_discriminant_analysis", "Generalized_singular_value_decomposition"], ["Linear_discriminant_analysis", "User_(computing)"], ["Linear_discriminant_analysis"], ["Linear_discriminant_analysis"], ["Linear_discriminant_analysis", "Principal_component_analysis"], ["Linear_discriminant_analysis"], ["Cartesian_coordinate_system"], ["Linear_discriminant_analysis"], ["Linear_discriminant_analysis"], ["Linear_discriminant_analysis"], ["Linear_discriminant_analysis"], ["Linear_discriminant_analysis"], ["Linear_discriminant_analysis"], ["Linear_discriminant_analysis"], ["Linear_discriminant_analysis"], ["Linear_discriminant_analysis"], ["Linear_discriminant_analysis"], ["Linear_discriminant_analysis"], ["Principal_component_analysis"], ["Linear_discriminant_analysis"], ["Heat"], ["Linear_discriminant_analysis"], ["Linear_discriminant_analysis"], ["Linear_discriminant_analysis"], ["Linear_discriminant_analysis"], ["Linear_discriminant_analysis"], ["Linear_discriminant_analysis"], ["Linear_discriminant_analysis"], ["Linear_discriminant_analysis"], ["Linear_discriminant_analysis"], ["Linear_discriminant_analysis"], ["Interactivity"], ["Latent_Dirichlet_allocation"], ["Latent_Dirichlet_allocation"], ["Latent_Dirichlet_allocation"], ["Latent_Dirichlet_allocation"], ["Latent_Dirichlet_allocation"], ["Novel", "Latent_Dirichlet_allocation"], ["Latent_Dirichlet_allocation"], ["Latent_Dirichlet_allocation"], ["Latent_Dirichlet_allocation"], ["Latent_Dirichlet_allocation"], ["Heat_map"], ["Latent_Dirichlet_allocation"], ["Latent_Dirichlet_allocation"], ["Latent_Dirichlet_allocation"], ["Heat_map"], ["Heat_map"], ["Latent_Dirichlet_allocation"], ["Interactivity"], ["Latent_Dirichlet_allocation"], ["Interactivity"], ["Latent_Dirichlet_allocation"], ["Document", "Visual_system", "Information_retrieval", "Recommender_system"], ["Information_retrieval"], ["Visual_analytics"], ["Salesforce.com", "India"], ["Information_retrieval"], ["India"], ["2D_computer_graphics"], ["Non-negative_matrix_factorization"], ["Principal_component_analysis"], ["Graphical_user_interface"], ["Graphical_user_interface"], ["India", "Pacific_Northwest_National_Laboratory"], ["3D_computer_graphics"], ["3D_computer_graphics", "2D_computer_graphics"], ["Latent_Dirichlet_allocation"], ["Testbed"], ["2D_computer_graphics"], ["Tiara"], ["Google_Scholar"], ["Microsoft_Academic_Search", "Microsoft_Research"], ["Science_(journal)", "Internet_Explorer"], ["Kernel_density_estimation"], ["Medal_bar"], ["Scattering"], ["Scattering"], ["Arnetminer"], ["Toolbar"], ["Scatter_plot"], ["Scatter_plot"], ["Default_(computer_science)"], ["Down_feather"], ["Toolbar"], ["Bachelor's_degree"], ["Naive_Bayes_classifier", "Principal_component_analysis", "Holotype", "Audio_Units", "Alzheimer's_disease", "Scatter_plot"], ["Alzheimer's_disease"], ["Scatter_plot"], ["Scatter_plot", "Alzheimer's_disease"], ["Citation"], ["Support_vector_machine", "Component-based_software_engineering", "Alzheimer's_disease"], ["Bachelor's_degree"], ["Support_vector_machine", "Automation", "Component-based_software_engineering", "Alzheimer's_disease"], ["Bachelor's_degree"], ["Support_vector_machine", "Component-based_software_engineering", "Alzheimer's_disease"], ["Label"], ["Alzheimer's_disease"], ["Alzheimer's_disease"], ["Data_collection"], ["Arnetminer"], ["Microsoft_Academic_Search", "Application_programming_interface"], ["DBLP"], ["SQL"], ["Big_O_notation"], ["Vector_space"], ["Big_O_notation"], ["Foreach_loop"], ["Big_O_notation"], ["Linear_discriminant_analysis", "Principal_component_analysis"], ["Linear_discriminant_analysis", "Principal_component_analysis"], ["Linear_discriminant_analysis"], ["Normal_distribution", "Linear_discriminant_analysis", "Cartesian_coordinate_system", "Principal_component_analysis"], ["Linear_discriminant_analysis", "Cartesian_coordinate_system", "Principal_component_analysis"], ["O(n)"], ["Non-negative_matrix_factorization"], ["Non-negative_matrix_factorization"], ["Non-negative_matrix_factorization"], ["Matrix_norm"], ["Non-negative_matrix_factorization"], ["Non-negative_matrix_factorization"], ["Dimension"], ["Cartesian_coordinate_system"], ["Linear_discriminant_analysis"], ["Linear_discriminant_analysis"], ["Linear_discriminant_analysis"], ["Linear_discriminant_analysis", "Principal_component_analysis"], ["Linear_discriminant_analysis"], ["Linear_discriminant_analysis", "Non-negative_matrix_factorization"], ["Cartesian_coordinate_system"], ["Hungarian_algorithm"], ["Hungarian_algorithm", "Holotype"], ["Procrustes_analysis"], ["Procrustes_analysis"], ["Two-dimensional_space"], ["Recurrent_neural_network"], ["Testbed", "Java_(programming_language)", "NetBeans", "Graphical_user_interface"], ["Rich_client_platform"], ["Java_(programming_language)", "Non-negative_matrix_factorization", "MATLAB"], ["H2_(DBMS)"], ["Google_Scholar"], ["Questionnaire", "IBM"], ["Windows_7", "Random-access_memory", "Java_virtual_machine", "Xeon", "Gigabyte"], ["Institute_of_Electrical_and_Electronics_Engineers", "Association_for_Computing_Machinery", "Google", "Google_Scholar"], ["Likert_scale"], ["Graphical_user_interface"], ["Graphical_user_interface"], ["IBM_System_i"], ["Google_Scholar"], ["Non-negative_matrix_factorization"], ["3D_computer_graphics"], ["Salesforce.com", "Latent_Dirichlet_allocation", "Testbed", "Principal_component_analysis"], ["Testbed"], ["Testbed"], ["Principal_component_analysis"], ["Non-negative_matrix_factorization"], ["Big_O_notation"], ["Big_O_notation"], ["Principal_component_analysis"], ["Principal_component_analysis"], ["Multidimensional_scaling"], ["Multidimensional_scaling"], ["Multidimensional_scaling"], ["Multidimensional_scaling"]], "chapters": [{"text": "SUMMARY", "sentence_id": "s_0", "sentence_rank": "0", "paragraph_id": "p_0", "paragraph_rank": 0}, {"text": "CHAPTER I INTRODUCTION", "sentence_id": "s_11", "sentence_rank": "11", "paragraph_id": "p_4", "paragraph_rank": 4}, {"text": "1.1 Motivation", "sentence_id": "s_12", "sentence_rank": "12", "paragraph_id": "p_5", "paragraph_rank": 5}, {"text": "1.2 Objective", "sentence_id": "s_73", "sentence_rank": "73", "paragraph_id": "p_17", "paragraph_rank": 17}, {"text": "1.3 Contributions", "sentence_id": "s_89", "sentence_rank": "89", "paragraph_id": "p_25", "paragraph_rank": 25}, {"text": "1.4 Organization", "sentence_id": "s_104", "sentence_rank": "104", "paragraph_id": "p_34", "paragraph_rank": 34}, {"text": "CHAPTER II", "sentence_id": "s_131", "sentence_rank": "131", "paragraph_id": "p_43", "paragraph_rank": 43}, {"text": "2.1 Introduction", "sentence_id": "s_140", "sentence_rank": "140", "paragraph_id": "p_46", "paragraph_rank": 46}, {"text": "2.2 Motivation", "sentence_id": "s_162", "sentence_rank": "162", "paragraph_id": "p_50", "paragraph_rank": 50}, {"text": "2.3 Dimension Reduction as Trace Optimization Problem", "sentence_id": "s_190", "sentence_rank": "190", "paragraph_id": "p_58", "paragraph_rank": 58}, {"text": "2.3.1 Linear Discriminant Analysis (LDA)", "sentence_id": "s_207", "sentence_rank": "207", "paragraph_id": "p_67", "paragraph_rank": 67}, {"text": "2.3.2 Orthogonal Centroid Method (OCM)", "sentence_id": "s_222", "sentence_rank": "222", "paragraph_id": "p_76", "paragraph_rank": 76}, {"text": "2.3.3 Principal Component Analysis (PCA)", "sentence_id": "s_250", "sentence_rank": "250", "paragraph_id": "p_90", "paragraph_rank": 90}, {"text": "2.4 Formulation of Two-stage Framework for Visualiza- tion", "sentence_id": "s_274", "sentence_rank": "274", "paragraph_id": "p_103", "paragraph_rank": 103}, {"text": "2.5 Two-stage Methods for 2D Visualization", "sentence_id": "s_291", "sentence_rank": "291", "paragraph_id": "p_114", "paragraph_rank": 114}, {"text": "2.5.1 Rank-2 LDA", "sentence_id": "s_304", "sentence_rank": "304", "paragraph_id": "p_117", "paragraph_rank": 117}, {"text": "2.5.2 LDA followed by PCA", "sentence_id": "s_312", "sentence_rank": "312", "paragraph_id": "p_122", "paragraph_rank": 122}, {"text": "2.5.3 OCM followed by PCA", "sentence_id": "s_316", "sentence_rank": "316", "paragraph_id": "p_125", "paragraph_rank": 125}, {"text": "2.5.4 Rank-2 PCA on Sb", "sentence_id": "s_322", "sentence_rank": "322", "paragraph_id": "p_130", "paragraph_rank": 130}, {"text": "2.6 Experiments", "sentence_id": "s_332", "sentence_rank": "332", "paragraph_id": "p_136", "paragraph_rank": 136}, {"text": "2.6.3 Effects of Data Centering", "sentence_id": "s_380", "sentence_rank": "380", "paragraph_id": "p_157", "paragraph_rank": 157}, {"text": "2.6.4 Comparison of Visualization Results", "sentence_id": "s_389", "sentence_rank": "389", "paragraph_id": "p_159", "paragraph_rank": 159}, {"text": "2.7 Conclusions", "sentence_id": "s_419", "sentence_rank": "419", "paragraph_id": "p_172", "paragraph_rank": 172}, {"text": "CHAPTER III EFFICIENT UPDATING OF COMPUTATIONAL METHODS DUE TO PARAMETER CHANGES", "sentence_id": "s_431", "sentence_rank": "431", "paragraph_id": "p_175", "paragraph_rank": 175}, {"text": "3.2 ISOMAP", "sentence_id": "s_479", "sentence_rank": "479", "paragraph_id": "p_186", "paragraph_rank": 186}, {"text": "3.4.1.1 Time Complexity", "sentence_id": "s_501", "sentence_rank": "501", "paragraph_id": "p_191", "paragraph_rank": 191}, {"text": "3.4.2 Shortest Path Update", "sentence_id": "s_506", "sentence_rank": "506", "paragraph_id": "p_193", "paragraph_rank": 193}, {"text": "3.4.3 Eigenvalue/vector Update", "sentence_id": "s_531", "sentence_rank": "531", "paragraph_id": "p_209", "paragraph_rank": 209}, {"text": "3.5 Experiments and Applications", "sentence_id": "s_570", "sentence_rank": "570", "paragraph_id": "p_224", "paragraph_rank": 224}, {"text": "3.5.1 Computation Time", "sentence_id": "s_578", "sentence_rank": "578", "paragraph_id": "p_227", "paragraph_rank": 227}, {"text": "3.5.2 Knowledge Discovery via Visualization using p-ISOMAP", "sentence_id": "s_604", "sentence_rank": "604", "paragraph_id": "p_232", "paragraph_rank": 232}, {"text": "3.6 Conclusions", "sentence_id": "s_689", "sentence_rank": "689", "paragraph_id": "p_239", "paragraph_rank": 239}, {"text": "CHAPTER IV", "sentence_id": "s_694", "sentence_rank": "694", "paragraph_id": "p_241", "paragraph_rank": 241}, {"text": "4.1 Introduction", "sentence_id": "s_703", "sentence_rank": "703", "paragraph_id": "p_244", "paragraph_rank": 244}, {"text": "4.2 Related Work", "sentence_id": "s_751", "sentence_rank": "751", "paragraph_id": "p_261", "paragraph_rank": 261}, {"text": "4.2.1 Efficient Interactive Visualization", "sentence_id": "s_753", "sentence_rank": "753", "paragraph_id": "p_263", "paragraph_rank": 263}, {"text": "4.2.2 User Interaction with Computational Methods", "sentence_id": "s_779", "sentence_rank": "779", "paragraph_id": "p_269", "paragraph_rank": 269}, {"text": "4.3 Per-Iteration Visualization Environment (PIVE)", "sentence_id": "s_784", "sentence_rank": "784", "paragraph_id": "p_272", "paragraph_rank": 272}, {"text": "4.3.1 Issues and Solutions", "sentence_id": "s_811", "sentence_rank": "811", "paragraph_id": "p_281", "paragraph_rank": 281}, {"text": "4.3.1.1 Computational Overhead and Multi-threading", "sentence_id": "s_812", "sentence_rank": "812", "paragraph_id": "p_282", "paragraph_rank": 282}, {"text": "4.3.1.2 Visual Inconsistency and User Control", "sentence_id": "s_840", "sentence_rank": "840", "paragraph_id": "p_288", "paragraph_rank": 288}, {"text": "4.4 Customized Methods under PIVE", "sentence_id": "s_854", "sentence_rank": "854", "paragraph_id": "p_291", "paragraph_rank": 291}, {"text": "4.4.1 Principal Component Analysis (PCA)", "sentence_id": "s_865", "sentence_rank": "865", "paragraph_id": "p_297", "paragraph_rank": 297}, {"text": "4.4.2 Multidimensional Scaling (MDS)", "sentence_id": "s_871", "sentence_rank": "871", "paragraph_id": "p_301", "paragraph_rank": 301}, {"text": "4.4.2.1 User Interaction Capabilities", "sentence_id": "s_879", "sentence_rank": "879", "paragraph_id": "p_304", "paragraph_rank": 304}, {"text": "4.4.3 t-Distributed Stochastic Neighbor Embedding (t-SNE)", "sentence_id": "s_892", "sentence_rank": "892", "paragraph_id": "p_308", "paragraph_rank": 308}, {"text": "4.4.3.1 User Interaction Capabilities", "sentence_id": "s_899", "sentence_rank": "899", "paragraph_id": "p_311", "paragraph_rank": 311}, {"text": "4.4.4 k-means", "sentence_id": "s_905", "sentence_rank": "905", "paragraph_id": "p_313", "paragraph_rank": 313}, {"text": "4.4.4.1 User Interaction Capabilities", "sentence_id": "s_918", "sentence_rank": "918", "paragraph_id": "p_320", "paragraph_rank": 320}, {"text": "4.4.5 Latent Dirichlet Allocation (LDA)", "sentence_id": "s_924", "sentence_rank": "924", "paragraph_id": "p_323", "paragraph_rank": 323}, {"text": "4.5.2 Real-time User Interactions", "sentence_id": "s_942", "sentence_rank": "942", "paragraph_id": "p_330", "paragraph_rank": 330}, {"text": "4.5.2.1 Moving data points in t-SNE", "sentence_id": "s_952", "sentence_rank": "952", "paragraph_id": "p_335", "paragraph_rank": 335}, {"text": "4.5.2.2 Fixing cluster assignments in k-means", "sentence_id": "s_967", "sentence_rank": "967", "paragraph_id": "p_341", "paragraph_rank": 341}, {"text": "4.5.2.3 Split/merge clusters in k-means", "sentence_id": "s_989", "sentence_rank": "989", "paragraph_id": "p_349", "paragraph_rank": 349}, {"text": "4.5.2.4 Filtering noisy documents to improve topics in LDA", "sentence_id": "s_1004", "sentence_rank": "1004", "paragraph_id": "p_351", "paragraph_rank": 351}, {"text": "4.6 Conclusions", "sentence_id": "s_1012", "sentence_rank": "1012", "paragraph_id": "p_353", "paragraph_rank": 353}, {"text": "CHAPTER V", "sentence_id": "s_1022", "sentence_rank": "1022", "paragraph_id": "p_356", "paragraph_rank": 356}, {"text": "5.1 Introduction", "sentence_id": "s_1032", "sentence_rank": "1032", "paragraph_id": "p_359", "paragraph_rank": 359}, {"text": "5.2 Related Work", "sentence_id": "s_1071", "sentence_rank": "1071", "paragraph_id": "p_369", "paragraph_rank": 369}, {"text": "5.2.2 Visual Analytic Systems using Dimension Reduction and Cluster- ing", "sentence_id": "s_1115", "sentence_rank": "1115", "paragraph_id": "p_381", "paragraph_rank": 381}, {"text": "5.3 Testbed System", "sentence_id": "s_1150", "sentence_rank": "1150", "paragraph_id": "p_389", "paragraph_rank": 389}, {"text": "5.3.1 Basic Workflow", "sentence_id": "s_1155", "sentence_rank": "1155", "paragraph_id": "p_391", "paragraph_rank": 391}, {"text": "5.3.2 Computational Modules", "sentence_id": "s_1172", "sentence_rank": "1172", "paragraph_id": "p_396", "paragraph_rank": 396}, {"text": "5.3.2.1 Vector Encoding", "sentence_id": "s_1173", "sentence_rank": "1173", "paragraph_id": "p_397", "paragraph_rank": 397}, {"text": "5.3.2.2 Pre-processing", "sentence_id": "s_1179", "sentence_rank": "1179", "paragraph_id": "p_399", "paragraph_rank": 399}, {"text": "5.3.2.3 Clustering", "sentence_id": "s_1186", "sentence_rank": "1186", "paragraph_id": "p_403", "paragraph_rank": 403}, {"text": "5.3.2.4 Dimension Reduction", "sentence_id": "s_1195", "sentence_rank": "1195", "paragraph_id": "p_406", "paragraph_rank": 406}, {"text": "5.3.3 Interactive Visualization Modules", "sentence_id": "s_1210", "sentence_rank": "1210", "paragraph_id": "p_409", "paragraph_rank": 409}, {"text": "5.3.3.1 Parallel Coordinates View", "sentence_id": "s_1211", "sentence_rank": "1211", "paragraph_id": "p_410", "paragraph_rank": 410}, {"text": "5.3.3.2 Scatter Plot View", "sentence_id": "s_1215", "sentence_rank": "1215", "paragraph_id": "p_412", "paragraph_rank": 412}, {"text": "5.3.3.3 Cluster Label View", "sentence_id": "s_1223", "sentence_rank": "1223", "paragraph_id": "p_415", "paragraph_rank": 415}, {"text": "5.3.3.4 Accessing Original Data", "sentence_id": "s_1229", "sentence_rank": "1229", "paragraph_id": "p_417", "paragraph_rank": 417}, {"text": "5.3.3.5 Supporting Multi-view Exploration", "sentence_id": "s_1239", "sentence_rank": "1239", "paragraph_id": "p_420", "paragraph_rank": 420}, {"text": "5.3.3.6 Aligning Different Views", "sentence_id": "s_1250", "sentence_rank": "1250", "paragraph_id": "p_423", "paragraph_rank": 423}, {"text": "5.3.4 Implementation and Extensibility", "sentence_id": "s_1263", "sentence_rank": "1263", "paragraph_id": "p_427", "paragraph_rank": 427}, {"text": "5.4 Usage Scenarios", "sentence_id": "s_1283", "sentence_rank": "1283", "paragraph_id": "p_434", "paragraph_rank": 434}, {"text": "5.4.1 Data Sets", "sentence_id": "s_1284", "sentence_rank": "1284", "paragraph_id": "p_435", "paragraph_rank": 435}, {"text": "5.4.2 Parallel Coordinates: Guiding beyond Two Leading Dimensions", "sentence_id": "s_1301", "sentence_rank": "1301", "paragraph_id": "p_439", "paragraph_rank": 439}, {"text": "5.4.3 Effects of Alignment: Helping Comparisons between Visualizations", "sentence_id": "s_1314", "sentence_rank": "1314", "paragraph_id": "p_444", "paragraph_rank": 444}, {"text": "5.4.4 Dimension Reduction: Supporting Multiple Perspectives", "sentence_id": "s_1332", "sentence_rank": "1332", "paragraph_id": "p_448", "paragraph_rank": 448}, {"text": "5.4.5 Clustering: Combining Knowledge from Different Clustering", "sentence_id": "s_1350", "sentence_rank": "1350", "paragraph_id": "p_451", "paragraph_rank": 451}, {"text": "5.5 Conclusions", "sentence_id": "s_1383", "sentence_rank": "1383", "paragraph_id": "p_472", "paragraph_rank": 472}, {"text": "CHAPTER VI IVISCLASSIFIER: AN INTERACTIVE VISUAL CLASSIFICATION SYSTEM USING SUPERVISED DIMENSION REDUCTION", "sentence_id": "s_1425", "sentence_rank": "1425", "paragraph_id": "p_487", "paragraph_rank": 487}, {"text": "6.1 Introduction", "sentence_id": "s_1434", "sentence_rank": "1434", "paragraph_id": "p_489", "paragraph_rank": 489}, {"text": "6.2 Related Work", "sentence_id": "s_1476", "sentence_rank": "1476", "paragraph_id": "p_495", "paragraph_rank": 495}, {"text": "6.3 Linear Discriminant Analysis", "sentence_id": "s_1495", "sentence_rank": "1495", "paragraph_id": "p_500", "paragraph_rank": 500}, {"text": "6.3.1 Concepts", "sentence_id": "s_1498", "sentence_rank": "1498", "paragraph_id": "p_502", "paragraph_rank": 502}, {"text": "6.3.2 Regularization to Control the Cluster Radius", "sentence_id": "s_1514", "sentence_rank": "1514", "paragraph_id": "p_508", "paragraph_rank": 508}, {"text": "6.3.3 Algorithms", "sentence_id": "s_1529", "sentence_rank": "1529", "paragraph_id": "p_513", "paragraph_rank": 513}, {"text": "6.4 System Description", "sentence_id": "s_1535", "sentence_rank": "1535", "paragraph_id": "p_515", "paragraph_rank": 515}, {"text": "6.4.1 Data Encoding", "sentence_id": "s_1536", "sentence_rank": "1536", "paragraph_id": "p_516", "paragraph_rank": 516}, {"text": "6.4.2 Visualization Modules", "sentence_id": "s_1543", "sentence_rank": "1543", "paragraph_id": "p_519", "paragraph_rank": 519}, {"text": "6.4.2.1 Parallel coordinates", "sentence_id": "s_1554", "sentence_rank": "1554", "paragraph_id": "p_521", "paragraph_rank": 521}, {"text": "6.4.2.2 Basis view", "sentence_id": "s_1570", "sentence_rank": "1570", "paragraph_id": "p_524", "paragraph_rank": 524}, {"text": "6.4.2.3 Heat maps", "sentence_id": "s_1586", "sentence_rank": "1586", "paragraph_id": "p_527", "paragraph_rank": 527}, {"text": "6.4.2.4 Scatter plots", "sentence_id": "s_1598", "sentence_rank": "1598", "paragraph_id": "p_530", "paragraph_rank": 530}, {"text": "6.4.3 Classification Modules", "sentence_id": "s_1605", "sentence_rank": "1605", "paragraph_id": "p_534", "paragraph_rank": 534}, {"text": "6.5 Case Studies", "sentence_id": "s_1617", "sentence_rank": "1617", "paragraph_id": "p_537", "paragraph_rank": 537}, {"text": "6.5.1 Exploratory Data Analysis", "sentence_id": "s_1630", "sentence_rank": "1630", "paragraph_id": "p_541", "paragraph_rank": 541}, {"text": "6.5.2 Interactive Classification", "sentence_id": "s_1655", "sentence_rank": "1655", "paragraph_id": "p_547", "paragraph_rank": 547}, {"text": "6.6 Conclusions", "sentence_id": "s_1677", "sentence_rank": "1677", "paragraph_id": "p_551", "paragraph_rank": 551}, {"text": "CHAPTER VII VISIRR: AN INTERACTIVE VISUAL INFORMATION RETRIEVAL AND RECOMMENDER SYSTEM FOR LARGE-SCALE DOCUMENT DATA", "sentence_id": "s_1736", "sentence_rank": "1736", "paragraph_id": "p_574", "paragraph_rank": 574}, {"text": "7.1 Introduction", "sentence_id": "s_1745", "sentence_rank": "1745", "paragraph_id": "p_576", "paragraph_rank": 576}, {"text": "7.2 Related Work", "sentence_id": "s_1788", "sentence_rank": "1788", "paragraph_id": "p_587", "paragraph_rank": 587}, {"text": "7.3 VisIRR Design and Function", "sentence_id": "s_1827", "sentence_rank": "1827", "paragraph_id": "p_597", "paragraph_rank": 597}, {"text": "7.3.1 User Interface", "sentence_id": "s_1829", "sentence_rank": "1829", "paragraph_id": "p_599", "paragraph_rank": 599}, {"text": "7.3.2 Usage Scenarios", "sentence_id": "s_1846", "sentence_rank": "1846", "paragraph_id": "p_602", "paragraph_rank": 602}, {"text": "7.3.2.1 A Visual Overview of Query-Retrieved Documents", "sentence_id": "s_1849", "sentence_rank": "1849", "paragraph_id": "p_604", "paragraph_rank": 604}, {"text": "7.3.2.2 Drilling Down via Computational Zoom-in", "sentence_id": "s_1864", "sentence_rank": "1864", "paragraph_id": "p_609", "paragraph_rank": 609}, {"text": "7.3.2.3 Dynamic Queries and Multi-view Alignment", "sentence_id": "s_1876", "sentence_rank": "1876", "paragraph_id": "p_614", "paragraph_rank": 614}, {"text": "7.3.2.4 Content-based Recommendation", "sentence_id": "s_1894", "sentence_rank": "1894", "paragraph_id": "p_619", "paragraph_rank": 619}, {"text": "7.3.2.5 Citation- and Co-authorship-based Recommendation", "sentence_id": "s_1906", "sentence_rank": "1906", "paragraph_id": "p_622", "paragraph_rank": 622}, {"text": "7.4 Data Collection / Ingestion", "sentence_id": "s_1933", "sentence_rank": "1933", "paragraph_id": "p_630", "paragraph_rank": 630}, {"text": "7.4.1 Initial Data Collection", "sentence_id": "s_1934", "sentence_rank": "1934", "paragraph_id": "p_631", "paragraph_rank": 631}, {"text": "7.4.2 Data Ingestion", "sentence_id": "s_1941", "sentence_rank": "1941", "paragraph_id": "p_634", "paragraph_rank": 634}, {"text": "7.4.2.1 Original Field of Data", "sentence_id": "s_1948", "sentence_rank": "1948", "paragraph_id": "p_637", "paragraph_rank": 637}, {"text": "7.4.2.2 Vector Representation", "sentence_id": "s_1953", "sentence_rank": "1953", "paragraph_id": "p_639", "paragraph_rank": 639}, {"text": "7.4.2.3 Graph Representation", "sentence_id": "s_1960", "sentence_rank": "1960", "paragraph_id": "p_641", "paragraph_rank": 641}, {"text": "7.4.3 Scalable Update for New Data", "sentence_id": "s_1969", "sentence_rank": "1969", "paragraph_id": "p_643", "paragraph_rank": 643}, {"text": "7.5 Computational Methods", "sentence_id": "s_1986", "sentence_rank": "1986", "paragraph_id": "p_648", "paragraph_rank": 648}, {"text": "7.5.1 Clustering", "sentence_id": "s_1989", "sentence_rank": "1989", "paragraph_id": "p_650", "paragraph_rank": 650}, {"text": "7.5.2 Dimension Reduction", "sentence_id": "s_1999", "sentence_rank": "1999", "paragraph_id": "p_655", "paragraph_rank": 655}, {"text": "7.5.3 Alignment", "sentence_id": "s_2012", "sentence_rank": "2012", "paragraph_id": "p_660", "paragraph_rank": 660}, {"text": "7.5.4 Recommendation", "sentence_id": "s_2028", "sentence_rank": "2028", "paragraph_id": "p_665", "paragraph_rank": 665}, {"text": "7.5.5 Implementation", "sentence_id": "s_2044", "sentence_rank": "2044", "paragraph_id": "p_669", "paragraph_rank": 669}, {"text": "7.6 Confirmatory User Study", "sentence_id": "s_2049", "sentence_rank": "2049", "paragraph_id": "p_671", "paragraph_rank": 671}, {"text": "7.6.1 Method and Limitations", "sentence_id": "s_2058", "sentence_rank": "2058", "paragraph_id": "p_674", "paragraph_rank": 674}, {"text": "7.6.2 Results and Discussion", "sentence_id": "s_2074", "sentence_rank": "2074", "paragraph_id": "p_679", "paragraph_rank": 679}, {"text": "7.7 Conclusions", "sentence_id": "s_2088", "sentence_rank": "2088", "paragraph_id": "p_685", "paragraph_rank": 685}, {"text": "CHAPTER VIII CONCLUSIONS AND FUTURE WORK", "sentence_id": "s_2100", "sentence_rank": "2100", "paragraph_id": "p_690", "paragraph_rank": 690}, {"text": "8.1 Summary of Contributions", "sentence_id": "s_2101", "sentence_rank": "2101", "paragraph_id": "p_691", "paragraph_rank": 691}, {"text": "8.2 Future Directions", "sentence_id": "s_2140", "sentence_rank": "2140", "paragraph_id": "p_704", "paragraph_rank": 704}, {"text": "8.2.1 Real-time Interactivity", "sentence_id": "s_2145", "sentence_rank": "2145", "paragraph_id": "p_706", "paragraph_rank": 706}, {"text": "8.2.2 Output Trustworthiness", "sentence_id": "s_2173", "sentence_rank": "2173", "paragraph_id": "p_710", "paragraph_rank": 710}], "all_paragraphs": [{"paragraph_info": {"end": 7, "start": 0, "text": "SUMMARY", "rank": 0, "paragraph_comparative_number": 1, "entities": [], "id": "p_0"}, "sentences": [{"end": 7, "text": "SUMMARY", "rank": 0, "start": 0, "IsComparative": "1", "id": "st_0"}]}, {"paragraph_info": {"end": 612, "start": 7, "text": "With the increasing amount of collected data, large-scale high-dimensional data analysis is becoming essential in many areas.These data can be analyzed either by using fully computational methods or by leveraging human capabilities via interactive visualization.However, each method has its drawbacks.While a fully computational method can deal with large amounts of data, it lacks depth in its understanding of the data, which is critical to the analysis.With the interactive visualization method, the user can give a deeper insight on the data but suffers when large amounts of data need to be analyzed.", "rank": 1, "paragraph_comparative_number": 2, "entities": [], "id": "p_1"}, "sentences": [{"end": 132, "text": "With the increasing amount of collected data, large-scale high-dimensional data analysis is becoming essential in many areas.", "rank": 1, "start": 7, "IsComparative": "0", "id": "st_1"}, {"end": 269, "text": "These data can be analyzed either by using fully computational methods or by leveraging human capabilities via interactive visualization.", "rank": 2, "start": 132, "IsComparative": "0", "id": "st_2"}, {"end": 308, "text": "However, each method has its drawbacks.", "rank": 3, "start": 269, "IsComparative": "1", "id": "st_3"}, {"end": 463, "text": "While a fully computational method can deal with large amounts of data, it lacks depth in its understanding of the data, which is critical to the analysis.", "rank": 4, "start": 308, "IsComparative": "1", "id": "st_4"}, {"end": 612, "text": "With the interactive visualization method, the user can give a deeper insight on the data but suffers when large amounts of data need to be analyzed.", "rank": 5, "start": 463, "IsComparative": "0", "id": "st_5"}]}, {"paragraph_info": {"end": 1314, "start": 612, "text": "Even with an apparent need for these two approaches to be integrated, little progress has been made.As ways to tackle this problem, computational methods have to be re-designed both theoretically and algorithmically, and the visual ana- lytics system has to expose these computational methods to users so that they can choose the proper algorithms and settings.To achieve an appropriate integration between computational methods and visual analytics, the thesis focuses on essential computational methods for visualization, such as dimension reduction and clustering, and it presents fundamental development of computational methods as well as visual analytic systems involving newly developed methods.", "rank": 2, "paragraph_comparative_number": 1, "entities": [], "id": "p_2"}, "sentences": [{"end": 712, "text": "Even with an apparent need for these two approaches to be integrated, little progress has been made.", "rank": 6, "start": 612, "IsComparative": "1", "id": "st_6"}, {"end": 973, "text": "As ways to tackle this problem, computational methods have to be re-designed both theoretically and algorithmically, and the visual ana- lytics system has to expose these computational methods to users so that they can choose the proper algorithms and settings.", "rank": 7, "start": 712, "IsComparative": "0", "id": "st_7"}, {"end": 1314, "text": "To achieve an appropriate integration between computational methods and visual analytics, the thesis focuses on essential computational methods for visualization, such as dimension reduction and clustering, and it presents fundamental development of computational methods as well as visual analytic systems involving newly developed methods.", "rank": 8, "start": 973, "IsComparative": "0", "id": "st_8"}]}, {"paragraph_info": {"end": 2160, "start": 1314, "text": "The contributions of the thesis include (1) the two-stage dimension reduction framework that better handles significant information loss in visualization of high- dimensional data, (2) efficient parametric updating of computational methods for fast and smooth user interactions, and (3) an iteration-wise integration framework of computational methods in real-time visual analytics.The latter parts of the thesis focus on the development of visual analytics systems involving the presented compu- tational methods, such as (1) Testbed: an interactive visual testbed system for various dimension reduction and clustering methods, (2) iVisClassifier: an interactive visual classification system using supervised dimension reduction, and (3) VisIRR: an inter- active visual information retrieval and recommender system for large-scale document data.", "rank": 3, "paragraph_comparative_number": 1, "entities": [], "id": "p_3"}, "sentences": [{"end": 1696, "text": "The contributions of the thesis include (1) the two-stage dimension reduction framework that better handles significant information loss in visualization of high- dimensional data, (2) efficient parametric updating of computational methods for fast and smooth user interactions, and (3) an iteration-wise integration framework of computational methods in real-time visual analytics.", "rank": 9, "start": 1314, "IsComparative": "0", "id": "st_9"}, {"end": 2160, "text": "The latter parts of the thesis focus on the development of visual analytics systems involving the presented compu- tational methods, such as (1) Testbed: an interactive visual testbed system for various dimension reduction and clustering methods, (2) iVisClassifier: an interactive visual classification system using supervised dimension reduction, and (3) VisIRR: an inter- active visual information retrieval and recommender system for large-scale document data.", "rank": 10, "start": 1696, "IsComparative": "1", "id": "st_10"}]}, {"paragraph_info": {"end": 2182, "start": 2160, "text": "CHAPTER I INTRODUCTION", "rank": 4, "paragraph_comparative_number": 0, "entities": [], "id": "p_4"}, "sentences": [{"end": 2182, "text": "CHAPTER I INTRODUCTION", "rank": 11, "start": 2160, "IsComparative": "0", "id": "st_11"}]}, {"paragraph_info": {"end": 2196, "start": 2182, "text": "1.1 Motivation", "rank": 5, "paragraph_comparative_number": 0, "entities": [], "id": "p_5"}, "sentences": [{"end": 2196, "text": "1.1 Motivation", "rank": 12, "start": 2182, "IsComparative": "0", "id": "st_12"}]}, {"paragraph_info": {"end": 3658, "start": 2196, "text": "In these days, an increasing amount of data is being generated in various forms such as documents, images, etc.To analyze these data, the raw data are encoded as high-dimensional vectors and then computational methods are typically applied in the context of statistical machine learning and data mining.For instance, in order to perform the facial recognition given a set of facial image data, the image data are first encoded as a bag-of-feature-points scheme <91>, and then a certain classification technique, such as support vector machines <118>, is applied.In many cases, however, these computational methods are done in a fully automated manner, and such approaches bear many limitations as follows: data rithms exist, and users may not know which one to apply.Furthermore, each method imposes its own assumptions on the data and involves a set of parame- ters to be carefully determined.However, these issues are by no means straight- forward to solve.For instance, the characteristics of the data do not meet the underlying assumption in the algorithms.Recent manifold learning algorithms <125, 111, 17> assume the low-dimensional curvi-linear manifold structure, but there is no guarantee that the data at hand have such a structure.As an- other example, even though the recent nonlinear kernel-based methods sound appealing, how to determine the optimal kernel parameters, e.g., a bandwidth parameter in Gaussian kernels, is also dependent on the data.", "rank": 6, "paragraph_comparative_number": 4, "entities": [], "id": "p_6"}, "sentences": [{"end": 2307, "text": "In these days, an increasing amount of data is being generated in various forms such as documents, images, etc.", "rank": 13, "start": 2196, "IsComparative": "0", "id": "st_13"}, {"end": 2499, "text": "To analyze these data, the raw data are encoded as high-dimensional vectors and then computational methods are typically applied in the context of statistical machine learning and data mining.", "rank": 14, "start": 2307, "IsComparative": "1", "id": "st_14"}, {"end": 2758, "text": "For instance, in order to perform the facial recognition given a set of facial image data, the image data are first encoded as a bag-of-feature-points scheme <91>, and then a certain classification technique, such as support vector machines <118>, is applied.", "rank": 15, "start": 2499, "IsComparative": "1", "id": "st_15"}, {"end": 2963, "text": "In many cases, however, these computational methods are done in a fully automated manner, and such approaches bear many limitations as follows: data rithms exist, and users may not know which one to apply.", "rank": 16, "start": 2758, "IsComparative": "0", "id": "st_16"}, {"end": 3090, "text": "Furthermore, each method imposes its own assumptions on the data and involves a set of parame- ters to be carefully determined.", "rank": 17, "start": 2963, "IsComparative": "1", "id": "st_17"}, {"end": 3155, "text": "However, these issues are by no means straight- forward to solve.", "rank": 18, "start": 3090, "IsComparative": "0", "id": "st_18"}, {"end": 3257, "text": "For instance, the characteristics of the data do not meet the underlying assumption in the algorithms.", "rank": 19, "start": 3155, "IsComparative": "0", "id": "st_19"}, {"end": 3438, "text": "Recent manifold learning algorithms <125, 111, 17> assume the low-dimensional curvi-linear manifold structure, but there is no guarantee that the data at hand have such a structure.", "rank": 20, "start": 3257, "IsComparative": "0", "id": "st_20"}, {"end": 3658, "text": "As an- other example, even though the recent nonlinear kernel-based methods sound appealing, how to determine the optimal kernel parameters, e.g., a bandwidth parameter in Gaussian kernels, is also dependent on the data.", "rank": 21, "start": 3438, "IsComparative": "1", "id": "st_21"}]}, {"paragraph_info": {"end": 3670, "start": 3658, "text": "1.Difficulty", "rank": 7, "paragraph_comparative_number": 2, "entities": [], "id": "p_7"}, "sentences": [{"end": 3660, "text": "1.", "rank": 22, "start": 3658, "IsComparative": "1", "id": "st_22"}, {"end": 3670, "text": "Difficulty", "rank": 23, "start": 3660, "IsComparative": "1", "id": "st_23"}]}, {"paragraph_info": {"end": 3801, "start": 3670, "text": "choosing the encoding scheme and algorithm against the in at hand.For certain types of tasks, e.g., classification, countless algo-", "rank": 8, "paragraph_comparative_number": 0, "entities": [], "id": "p_8"}, "sentences": [{"end": 3736, "text": "choosing the encoding scheme and algorithm against the in at hand.", "rank": 24, "start": 3670, "IsComparative": "0", "id": "st_24"}, {"end": 3801, "text": "For certain types of tasks, e.g., classification, countless algo-", "rank": 25, "start": 3736, "IsComparative": "0", "id": "st_25"}]}, {"paragraph_info": {"end": 3814, "start": 3801, "text": "2.Discrepancy", "rank": 9, "paragraph_comparative_number": 2, "entities": [], "id": "p_9"}, "sentences": [{"end": 3803, "text": "2.", "rank": 26, "start": 3801, "IsComparative": "1", "id": "st_26"}, {"end": 3814, "text": "Discrepancy", "rank": 27, "start": 3803, "IsComparative": "1", "id": "st_27"}]}, {"paragraph_info": {"end": 4635, "start": 3814, "text": "task objective.Many algorithm criteria of computational modules do not nec- essarily reflect humans semantics and intuition.Instead, the algorithm criteria are often driven by other aspects such as computational efficiency, tractabil- ity, closed-form solutions, etc.For example, a squared loss function employed in many computational modules is widely used due to its simple optimization processes, but it may not always give the best results in practical data analysis scenarios in that, say, the squared loss is generally prone to outliers.Even if the algorithm criteria suit humans needs well, the performance may not be sufficient.To be specific, many carefully-designed criteria often make it hard to achieve the satisfactory criteria value due to their intensive computation and existence of multiple local minima.", "rank": 10, "paragraph_comparative_number": 2, "entities": [], "id": "p_10"}, "sentences": [{"end": 3829, "text": "task objective.", "rank": 28, "start": 3814, "IsComparative": "0", "id": "st_28"}, {"end": 3938, "text": "Many algorithm criteria of computational modules do not nec- essarily reflect humans semantics and intuition.", "rank": 29, "start": 3829, "IsComparative": "0", "id": "st_29"}, {"end": 4081, "text": "Instead, the algorithm criteria are often driven by other aspects such as computational efficiency, tractabil- ity, closed-form solutions, etc.", "rank": 30, "start": 3938, "IsComparative": "0", "id": "st_30"}, {"end": 4357, "text": "For example, a squared loss function employed in many computational modules is widely used due to its simple optimization processes, but it may not always give the best results in practical data analysis scenarios in that, say, the squared loss is generally prone to outliers.", "rank": 31, "start": 4081, "IsComparative": "1", "id": "st_31"}, {"end": 4450, "text": "Even if the algorithm criteria suit humans needs well, the performance may not be sufficient.", "rank": 32, "start": 4357, "IsComparative": "0", "id": "st_32"}, {"end": 4635, "text": "To be specific, many carefully-designed criteria often make it hard to achieve the satisfactory criteria value due to their intensive computation and existence of multiple local minima.", "rank": 33, "start": 4450, "IsComparative": "1", "id": "st_33"}]}, {"paragraph_info": {"end": 5379, "start": 4635, "text": "3.Ambiguity in task formulation.Large-scale data make it hard to explore and understand them, and sometimes they even obscure what to solve and what to be able to do with our data.In this situation, people may seek for some insight about the data as to which data items may behave differently from the rest.In addition, even if people have a clear goal in mind, it is often the case that the mathematical formulation required to apply computational methods is not straightforward.For instance, suppose one wants to analyze social network data to identify which person or group has caused a certain movement.This task may not be simply interpreted as a mathematical objective function or fit to the existing formulation of computational methods.", "rank": 11, "paragraph_comparative_number": 5, "entities": [], "id": "p_11"}, "sentences": [{"end": 4637, "text": "3.", "rank": 34, "start": 4635, "IsComparative": "1", "id": "st_34"}, {"end": 4667, "text": "Ambiguity in task formulation.", "rank": 35, "start": 4637, "IsComparative": "0", "id": "st_35"}, {"end": 4815, "text": "Large-scale data make it hard to explore and understand them, and sometimes they even obscure what to solve and what to be able to do with our data.", "rank": 36, "start": 4667, "IsComparative": "1", "id": "st_36"}, {"end": 4942, "text": "In this situation, people may seek for some insight about the data as to which data items may behave differently from the rest.", "rank": 37, "start": 4815, "IsComparative": "1", "id": "st_37"}, {"end": 5115, "text": "In addition, even if people have a clear goal in mind, it is often the case that the mathematical formulation required to apply computational methods is not straightforward.", "rank": 38, "start": 4942, "IsComparative": "1", "id": "st_38"}, {"end": 5242, "text": "For instance, suppose one wants to analyze social network data to identify which person or group has caused a certain movement.", "rank": 39, "start": 5115, "IsComparative": "1", "id": "st_39"}, {"end": 5379, "text": "This task may not be simply interpreted as a mathematical objective function or fit to the existing formulation of computational methods.", "rank": 40, "start": 5242, "IsComparative": "0", "id": "st_40"}]}, {"paragraph_info": {"end": 6647, "start": 5379, "text": "In contrast to the fully automated computational approaches that lack deep under- standing and careful treatment of the data, the area of visual analytics <127, 78>, which is defined as the science of analytical reasoning facilitated by interactive visual inter- faces, has gained increasing interest.Visual analytics has fascinating characteristics that leverage humans ability of quick visual insight in the data analysis and decision processes, and compared with the well-established literature of information visual- ization, visual analytics typically focuses on reasoning and decision-making processes rather than just understanding the data visually.Unfortunately, however, most of the state-of-the-art visual analytics techniques or systems do not properly accommodate large-scale data.One of the reasons is that although humans are good at quick visual insight, such an ability deteriorates when the number of visualized objects, either data items or features, is large.Furthermore, the limited screen space tends to create visual clutter when visualizing large-scale high-dimensional data.For instance, paral- lel coordinates, a widely-used visualization technique for multi-dimensional data, do not scale when the dimension exceeds several tens or hundreds.", "rank": 12, "paragraph_comparative_number": 3, "entities": [], "id": "p_12"}, "sentences": [{"end": 5680, "text": "In contrast to the fully automated computational approaches that lack deep under- standing and careful treatment of the data, the area of visual analytics <127, 78>, which is defined as the science of analytical reasoning facilitated by interactive visual inter- faces, has gained increasing interest.", "rank": 41, "start": 5379, "IsComparative": "1", "id": "st_41"}, {"end": 6036, "text": "Visual analytics has fascinating characteristics that leverage humans ability of quick visual insight in the data analysis and decision processes, and compared with the well-established literature of information visual- ization, visual analytics typically focuses on reasoning and decision-making processes rather than just understanding the data visually.", "rank": 42, "start": 5680, "IsComparative": "0", "id": "st_42"}, {"end": 6173, "text": "Unfortunately, however, most of the state-of-the-art visual analytics techniques or systems do not properly accommodate large-scale data.", "rank": 43, "start": 6036, "IsComparative": "0", "id": "st_43"}, {"end": 6358, "text": "One of the reasons is that although humans are good at quick visual insight, such an ability deteriorates when the number of visualized objects, either data items or features, is large.", "rank": 44, "start": 6173, "IsComparative": "0", "id": "st_44"}, {"end": 6478, "text": "Furthermore, the limited screen space tends to create visual clutter when visualizing large-scale high-dimensional data.", "rank": 45, "start": 6358, "IsComparative": "1", "id": "st_45"}, {"end": 6647, "text": "For instance, paral- lel coordinates, a widely-used visualization technique for multi-dimensional data, do not scale when the dimension exceeds several tens or hundreds.", "rank": 46, "start": 6478, "IsComparative": "1", "id": "st_46"}]}, {"paragraph_info": {"end": 7362, "start": 6647, "text": "To improve the scalability issue, computational methods can support visual ana- lytics by transforming the raw data into more compact and meaningful information.For instance, dimension reduction and clustering can reduce the numbers of features and data items into manageable sizes for both visualization and human perception.Beyond such reduction aspects, computational methods can provide more intelligent information about the data via their formulations based on long-studied statistical or probabilistic theories in the context of machine learning and data mining.Examples of such tasks include facial/speech recognition <16>, document topic modeling <20>, sentiment analysis <99>, and recommender systems <3>.", "rank": 13, "paragraph_comparative_number": 2, "entities": [], "id": "p_13"}, "sentences": [{"end": 6808, "text": "To improve the scalability issue, computational methods can support visual ana- lytics by transforming the raw data into more compact and meaningful information.", "rank": 47, "start": 6647, "IsComparative": "1", "id": "st_47"}, {"end": 6973, "text": "For instance, dimension reduction and clustering can reduce the numbers of features and data items into manageable sizes for both visualization and human perception.", "rank": 48, "start": 6808, "IsComparative": "0", "id": "st_48"}, {"end": 7216, "text": "Beyond such reduction aspects, computational methods can provide more intelligent information about the data via their formulations based on long-studied statistical or probabilistic theories in the context of machine learning and data mining.", "rank": 49, "start": 6973, "IsComparative": "1", "id": "st_49"}, {"end": 7362, "text": "Examples of such tasks include facial/speech recognition <16>, document topic modeling <20>, sentiment analysis <99>, and recommender systems <3>.", "rank": 50, "start": 7216, "IsComparative": "0", "id": "st_50"}]}, {"paragraph_info": {"end": 8737, "start": 7362, "text": "Such appealing capabilities of computational methods motivated people towards the tight integration of them with visual analytics for large-scale data.For example, Seo et al.<117> have provided an interactive visualization system to explore the clustering results obtained by the widely-used hierarchical clustering method.A recently proposed method, latent Dirichlet allocation, has been utilized in visual analytics tools for text documents <134, 50, 38, 37>.Even though various efforts have been made for utilizing computational methods in visual analytics, there is still significant room to improve such utility.That is, even though numerous advanced computational methods are currently being proposed and some of them claim that they can be easily adopted in visualization applications, practical visual analytics systems do not seem to currently take full advantage of these advanced methods.As a result, people still tend to only use a few of the basic computational methods, e.g., principal component analysis (PCA) <74> for dimension reduction and k-means<19> for clustering in many real-world analysis tasks.In addition, the above-mentioned intelligent information could be useful in interactive visualization approaches, but its usage in this direction is still limited.In this thesis, I address several hurdles in achieving an appropriate integration as follows:", "rank": 14, "paragraph_comparative_number": 4, "entities": [], "id": "p_14"}, "sentences": [{"end": 7513, "text": "Such appealing capabilities of computational methods motivated people towards the tight integration of them with visual analytics for large-scale data.", "rank": 51, "start": 7362, "IsComparative": "0", "id": "st_51"}, {"end": 7536, "text": "For example, Seo et al.", "rank": 52, "start": 7513, "IsComparative": "0", "id": "st_52"}, {"end": 7685, "text": "<117> have provided an interactive visualization system to explore the clustering results obtained by the widely-used hierarchical clustering method.", "rank": 53, "start": 7536, "IsComparative": "1", "id": "st_53"}, {"end": 7823, "text": "A recently proposed method, latent Dirichlet allocation, has been utilized in visual analytics tools for text documents <134, 50, 38, 37>.", "rank": 54, "start": 7685, "IsComparative": "0", "id": "st_54"}, {"end": 7979, "text": "Even though various efforts have been made for utilizing computational methods in visual analytics, there is still significant room to improve such utility.", "rank": 55, "start": 7823, "IsComparative": "0", "id": "st_55"}, {"end": 8261, "text": "That is, even though numerous advanced computational methods are currently being proposed and some of them claim that they can be easily adopted in visualization applications, practical visual analytics systems do not seem to currently take full advantage of these advanced methods.", "rank": 56, "start": 7979, "IsComparative": "0", "id": "st_56"}, {"end": 8481, "text": "As a result, people still tend to only use a few of the basic computational methods, e.g., principal component analysis (PCA) <74> for dimension reduction and k-means<19> for clustering in many real-world analysis tasks.", "rank": 57, "start": 8261, "IsComparative": "1", "id": "st_57"}, {"end": 8644, "text": "In addition, the above-mentioned intelligent information could be useful in interactive visualization approaches, but its usage in this direction is still limited.", "rank": 58, "start": 8481, "IsComparative": "1", "id": "st_58"}, {"end": 8737, "text": "In this thesis, I address several hurdles in achieving an appropriate integration as follows:", "rank": 59, "start": 8644, "IsComparative": "1", "id": "st_59"}]}, {"paragraph_info": {"end": 10146, "start": 8737, "text": "Without deep knowledge about the computational methods and the data, the output generated by the computational methods is often more difficult to un- derstand than the original raw data.Many modern computational algorithms are complex, and often for the sake of algorithm flexibility, they involve pa- rameters that have to be carefully determined.However, domain experts may improperly set the parameters values due to their lack of understanding the function of the parameters.Consequently, many visual analytic systems choose specific computational methods and treat them as a black box while focusing on the subsequent analysis of their output.Without a proper understanding of the algorithm and its parameters, the performance of the computational module may not be satisfactory enough to start an analysis with.computational methods involve heavy computations.In fact, as most meth- ods become more advanced and capable, they tend to require more intensive computations, which usually have a squared or cubic order of computational complexity in terms of the number of data items and/or features.Therefore, when dealing with large-scale data, the significant amount of computational computational module and its output are difficult to understand.methods require a significant amount of time.Most time required hinders real-time visualization and subsequent interaction with these computational modules.", "rank": 15, "paragraph_comparative_number": 6, "entities": [], "id": "p_15"}, "sentences": [{"end": 8923, "text": "Without deep knowledge about the computational methods and the data, the output generated by the computational methods is often more difficult to un- derstand than the original raw data.", "rank": 60, "start": 8737, "IsComparative": "0", "id": "st_60"}, {"end": 9085, "text": "Many modern computational algorithms are complex, and often for the sake of algorithm flexibility, they involve pa- rameters that have to be carefully determined.", "rank": 61, "start": 8923, "IsComparative": "1", "id": "st_61"}, {"end": 9216, "text": "However, domain experts may improperly set the parameters values due to their lack of understanding the function of the parameters.", "rank": 62, "start": 9085, "IsComparative": "1", "id": "st_62"}, {"end": 9385, "text": "Consequently, many visual analytic systems choose specific computational methods and treat them as a black box while focusing on the subsequent analysis of their output.", "rank": 63, "start": 9216, "IsComparative": "1", "id": "st_63"}, {"end": 9554, "text": "Without a proper understanding of the algorithm and its parameters, the performance of the computational module may not be satisfactory enough to start an analysis with.", "rank": 64, "start": 9385, "IsComparative": "1", "id": "st_64"}, {"end": 9603, "text": "computational methods involve heavy computations.", "rank": 65, "start": 9554, "IsComparative": "0", "id": "st_65"}, {"end": 9839, "text": "In fact, as most meth- ods become more advanced and capable, they tend to require more intensive computations, which usually have a squared or cubic order of computational complexity in terms of the number of data items and/or features.", "rank": 66, "start": 9603, "IsComparative": "1", "id": "st_66"}, {"end": 9990, "text": "Therefore, when dealing with large-scale data, the significant amount of computational computational module and its output are difficult to understand.", "rank": 67, "start": 9839, "IsComparative": "1", "id": "st_67"}, {"end": 10035, "text": "methods require a significant amount of time.", "rank": 68, "start": 9990, "IsComparative": "0", "id": "st_68"}, {"end": 10146, "text": "Most time required hinders real-time visualization and subsequent interaction with these computational modules.", "rank": 69, "start": 10035, "IsComparative": "0", "id": "st_69"}]}, {"paragraph_info": {"end": 10824, "start": 10146, "text": "This thesis aims to overcome these hurdles and achieve the tight integration between computational methods and visual analytics in modern data analysis scenarios.I claim that to this end, the computational methods have to be customized and even be re-invented for use in visual analytics, and at the same time, the visual analytics systems have to expose them to users out of a black box via interactive capabilities of choosing the right methods and the best parameters.Based on this claim, the thesis provides (1) several novel approaches for customizing computational methods and (2) the visual analytics systems integrating such customization in various application domains.", "rank": 16, "paragraph_comparative_number": 3, "entities": [], "id": "p_16"}, "sentences": [{"end": 10308, "text": "This thesis aims to overcome these hurdles and achieve the tight integration between computational methods and visual analytics in modern data analysis scenarios.", "rank": 70, "start": 10146, "IsComparative": "1", "id": "st_70"}, {"end": 10617, "text": "I claim that to this end, the computational methods have to be customized and even be re-invented for use in visual analytics, and at the same time, the visual analytics systems have to expose them to users out of a black box via interactive capabilities of choosing the right methods and the best parameters.", "rank": 71, "start": 10308, "IsComparative": "1", "id": "st_71"}, {"end": 10824, "text": "Based on this claim, the thesis provides (1) several novel approaches for customizing computational methods and (2) the visual analytics systems integrating such customization in various application domains.", "rank": 72, "start": 10617, "IsComparative": "1", "id": "st_72"}]}, {"paragraph_info": {"end": 10837, "start": 10824, "text": "1.2 Objective", "rank": 17, "paragraph_comparative_number": 1, "entities": [], "id": "p_17"}, "sentences": [{"end": 10837, "text": "1.2 Objective", "rank": 73, "start": 10824, "IsComparative": "1", "id": "st_73"}]}, {"paragraph_info": {"end": 10898, "start": 10837, "text": "In summary, the thesis statement can be described as follows:", "rank": 18, "paragraph_comparative_number": 0, "entities": [], "id": "p_18"}, "sentences": [{"end": 10898, "text": "In summary, the thesis statement can be described as follows:", "rank": 74, "start": 10837, "IsComparative": "0", "id": "st_74"}]}, {"paragraph_info": {"end": 11285, "start": 10898, "text": "The theoretical and algorithmic customization of computational methods will enable their appropriate integration with visual analytics for analyses of complex large-scale high-dimensional data.Visual analytics systems equipped with interactive capa- bilities with various computational methods will help analysts better understand the data at hand and solve complicated anal- ysis tasks.", "rank": 19, "paragraph_comparative_number": 2, "entities": [], "id": "p_19"}, "sentences": [{"end": 11091, "text": "The theoretical and algorithmic customization of computational methods will enable their appropriate integration with visual analytics for analyses of complex large-scale high-dimensional data.", "rank": 75, "start": 10898, "IsComparative": "1", "id": "st_75"}, {"end": 11285, "text": "Visual analytics systems equipped with interactive capa- bilities with various computational methods will help analysts better understand the data at hand and solve complicated anal- ysis tasks.", "rank": 76, "start": 11091, "IsComparative": "1", "id": "st_76"}]}, {"paragraph_info": {"end": 12099, "start": 11285, "text": "Under this statement, the thesis aims at achieving the true visual analytics where the computational analyses and the user-driven interactive visual exploration are tightly integrated.More specifically, the thesis mainly focuses on two key categories of computational methods: dimension reduction and clustering.Dimension reduction and clustering play an essential role in dealing with large-scale high-dimensional data in visual analytics by reducing the data dimension and the number of data items.In other words, dimension reduction can reveal meaningful dimensions or features out of numerous original dimensions as well as provide a means of visualizing high- dimensional data in visual 2D/3D spaces so that analysts can obtain the insight about data relationships with respect to geometric locations of data.", "rank": 20, "paragraph_comparative_number": 1, "entities": [], "id": "p_20"}, "sentences": [{"end": 11469, "text": "Under this statement, the thesis aims at achieving the true visual analytics where the computational analyses and the user-driven interactive visual exploration are tightly integrated.", "rank": 77, "start": 11285, "IsComparative": "0", "id": "st_77"}, {"end": 11597, "text": "More specifically, the thesis mainly focuses on two key categories of computational methods: dimension reduction and clustering.", "rank": 78, "start": 11469, "IsComparative": "0", "id": "st_78"}, {"end": 11785, "text": "Dimension reduction and clustering play an essential role in dealing with large-scale high-dimensional data in visual analytics by reducing the data dimension and the number of data items.", "rank": 79, "start": 11597, "IsComparative": "0", "id": "st_79"}, {"end": 12099, "text": "In other words, dimension reduction can reveal meaningful dimensions or features out of numerous original dimensions as well as provide a means of visualizing high- dimensional data in visual 2D/3D spaces so that analysts can obtain the insight about data relationships with respect to geometric locations of data.", "rank": 80, "start": 11785, "IsComparative": "1", "id": "st_80"}]}, {"paragraph_info": {"end": 12378, "start": 12099, "text": "On the other hand, clustering provides an overview of large-scale data in terms of a manageable number of groups based on their semantic coherences.Such cluster information can then guide analysts to a proper data group of interest on which they can further focus their analysis.", "rank": 21, "paragraph_comparative_number": 1, "entities": [], "id": "p_21"}, "sentences": [{"end": 12247, "text": "On the other hand, clustering provides an overview of large-scale data in terms of a manageable number of groups based on their semantic coherences.", "rank": 81, "start": 12099, "IsComparative": "0", "id": "st_81"}, {"end": 12378, "text": "Such cluster information can then guide analysts to a proper data group of interest on which they can further focus their analysis.", "rank": 82, "start": 12247, "IsComparative": "1", "id": "st_82"}]}, {"paragraph_info": {"end": 12532, "start": 12378, "text": "To be specific, the thesis addresses the following research questions in regards to integration of dimension reduction and clustering to visual analytics.", "rank": 22, "paragraph_comparative_number": 1, "entities": [], "id": "p_22"}, "sentences": [{"end": 12532, "text": "To be specific, the thesis addresses the following research questions in regards to integration of dimension reduction and clustering to visual analytics.", "rank": 83, "start": 12378, "IsComparative": "1", "id": "st_83"}]}, {"paragraph_info": {"end": 12792, "start": 12532, "text": "1.Which characteristics in terms of data, algorithms, and humans, should be exploited in order to make computational methods better support visual ana- lytics?Based on these characteristics, how can the computational methods be re-designed in visual analytics?", "rank": 23, "paragraph_comparative_number": 2, "entities": [], "id": "p_23"}, "sentences": [{"end": 12534, "text": "1.", "rank": 84, "start": 12532, "IsComparative": "1", "id": "st_84"}, {"end": 12691, "text": "Which characteristics in terms of data, algorithms, and humans, should be exploited in order to make computational methods better support visual ana- lytics?", "rank": 85, "start": 12534, "IsComparative": "1", "id": "st_85"}, {"end": 12792, "text": "Based on these characteristics, how can the computational methods be re-designed in visual analytics?", "rank": 86, "start": 12691, "IsComparative": "0", "id": "st_86"}]}, {"paragraph_info": {"end": 12946, "start": 12792, "text": "2.How can visual analytics systems utilizing these improved computational meth- ods be realized and what analytic benefits can we claim from such systems?", "rank": 24, "paragraph_comparative_number": 1, "entities": [], "id": "p_24"}, "sentences": [{"end": 12794, "text": "2.", "rank": 87, "start": 12792, "IsComparative": "1", "id": "st_87"}, {"end": 12946, "text": "How can visual analytics systems utilizing these improved computational meth- ods be realized and what analytic benefits can we claim from such systems?", "rank": 88, "start": 12794, "IsComparative": "0", "id": "st_88"}]}, {"paragraph_info": {"end": 12963, "start": 12946, "text": "1.3 Contributions", "rank": 25, "paragraph_comparative_number": 0, "entities": [], "id": "p_25"}, "sentences": [{"end": 12963, "text": "1.3 Contributions", "rank": 89, "start": 12946, "IsComparative": "0", "id": "st_89"}]}, {"paragraph_info": {"end": 13269, "start": 12963, "text": "I present as the main contributions of the thesis various ways to tackle each of the two addressed research questions.Basically, in response to the first question, the thesis discusses several theoretical and algorithmic improvements of computational methods when they support visual analytics, as follows:", "rank": 26, "paragraph_comparative_number": 0, "entities": [], "id": "p_26"}, "sentences": [{"end": 13081, "text": "I present as the main contributions of the thesis various ways to tackle each of the two addressed research questions.", "rank": 90, "start": 12963, "IsComparative": "0", "id": "st_90"}, {"end": 13269, "text": "Basically, in response to the first question, the thesis discusses several theoretical and algorithmic improvements of computational methods when they support visual analytics, as follows:", "rank": 91, "start": 13081, "IsComparative": "0", "id": "st_91"}]}, {"paragraph_info": {"end": 13409, "start": 13269, "text": "1.Two-stage dimension reduction framework that better handles significant infor- mation loss in visualization of high-dimensional data <35>.", "rank": 27, "paragraph_comparative_number": 2, "entities": [], "id": "p_27"}, "sentences": [{"end": 13271, "text": "1.", "rank": 92, "start": 13269, "IsComparative": "1", "id": "st_92"}, {"end": 13409, "text": "Two-stage dimension reduction framework that better handles significant infor- mation loss in visualization of high-dimensional data <35>.", "rank": 93, "start": 13271, "IsComparative": "1", "id": "st_93"}]}, {"paragraph_info": {"end": 13509, "start": 13409, "text": "2.Efficient parametric updating of computational methods for fast and smooth user interactions <34>.", "rank": 28, "paragraph_comparative_number": 1, "entities": [], "id": "p_28"}, "sentences": [{"end": 13411, "text": "2.", "rank": 94, "start": 13409, "IsComparative": "1", "id": "st_94"}, {"end": 13509, "text": "Efficient parametric updating of computational methods for fast and smooth user interactions <34>.", "rank": 95, "start": 13411, "IsComparative": "0", "id": "st_95"}]}, {"paragraph_info": {"end": 13610, "start": 13509, "text": "3.Iteration-wise integration framework of computational methods in real-time vi- sual analytics <31>.", "rank": 29, "paragraph_comparative_number": 1, "entities": [], "id": "p_29"}, "sentences": [{"end": 13511, "text": "3.", "rank": 96, "start": 13509, "IsComparative": "1", "id": "st_96"}, {"end": 13610, "text": "Iteration-wise integration framework of computational methods in real-time vi- sual analytics <31>.", "rank": 97, "start": 13511, "IsComparative": "0", "id": "st_97"}]}, {"paragraph_info": {"end": 13781, "start": 13610, "text": "On the other hand, the latter part of the thesis contribution lies mainly in the devel- opment of visual analytics systems involving the presented improvements as follows:", "rank": 30, "paragraph_comparative_number": 0, "entities": [], "id": "p_30"}, "sentences": [{"end": 13781, "text": "On the other hand, the latter part of the thesis contribution lies mainly in the devel- opment of visual analytics systems involving the presented improvements as follows:", "rank": 98, "start": 13610, "IsComparative": "0", "id": "st_98"}]}, {"paragraph_info": {"end": 13948, "start": 13781, "text": "1.Testbed: an interactive visual testbed system providing users with an easy access to various dimension reduction and clustering methods for their own data sets <32>.", "rank": 31, "paragraph_comparative_number": 2, "entities": [], "id": "p_31"}, "sentences": [{"end": 13783, "text": "1.", "rank": 99, "start": 13781, "IsComparative": "1", "id": "st_99"}, {"end": 13948, "text": "Testbed: an interactive visual testbed system providing users with an easy access to various dimension reduction and clustering methods for their own data sets <32>.", "rank": 100, "start": 13783, "IsComparative": "1", "id": "st_100"}]}, {"paragraph_info": {"end": 14112, "start": 13948, "text": "2. iVisClassifier: an interactive visual classification system via a supervised di- mension reduction for improving classification models in a user-driven way <36>.", "rank": 32, "paragraph_comparative_number": 1, "entities": [], "id": "p_32"}, "sentences": [{"end": 14112, "text": "2. iVisClassifier: an interactive visual classification system via a supervised di- mension reduction for improving classification models in a user-driven way <36>.", "rank": 101, "start": 13948, "IsComparative": "1", "id": "st_101"}]}, {"paragraph_info": {"end": 14289, "start": 14112, "text": "3.VisIRR: an interactive visual information retrieval and recommender system for large-scale document data that expands the documents of interest based on user preferences <33>.", "rank": 33, "paragraph_comparative_number": 2, "entities": [], "id": "p_33"}, "sentences": [{"end": 14114, "text": "3.", "rank": 102, "start": 14112, "IsComparative": "1", "id": "st_102"}, {"end": 14289, "text": "VisIRR: an interactive visual information retrieval and recommender system for large-scale document data that expands the documents of interest based on user preferences <33>.", "rank": 103, "start": 14114, "IsComparative": "1", "id": "st_103"}]}, {"paragraph_info": {"end": 14305, "start": 14289, "text": "1.4 Organization", "rank": 34, "paragraph_comparative_number": 0, "entities": [], "id": "p_34"}, "sentences": [{"end": 14305, "text": "1.4 Organization", "rank": 104, "start": 14289, "IsComparative": "0", "id": "st_104"}]}, {"paragraph_info": {"end": 14983, "start": 14305, "text": "The rest of the thesis presents each of the above-listed contributions in more detail.Chapter 2 presents a novel framework of two-stage dimension reduction for visu- alization of high-dimensional data.It is inevitable that significant information will be lost when reducing the original high dimension of data into 2D/3D in visualization.By using the formulations in terms of cluster-wise measures, the two-stage dimension reduction framework, which separates the criteria of the original dimension reduction methods and the further information loss, are presented.The thesis presents the de- tailed criteria using widely-used dimension reduction methods and their visualization", "rank": 35, "paragraph_comparative_number": 2, "entities": [], "id": "p_35"}, "sentences": [{"end": 14391, "text": "The rest of the thesis presents each of the above-listed contributions in more detail.", "rank": 105, "start": 14305, "IsComparative": "0", "id": "st_105"}, {"end": 14506, "text": "Chapter 2 presents a novel framework of two-stage dimension reduction for visu- alization of high-dimensional data.", "rank": 106, "start": 14391, "IsComparative": "1", "id": "st_106"}, {"end": 14643, "text": "It is inevitable that significant information will be lost when reducing the original high dimension of data into 2D/3D in visualization.", "rank": 107, "start": 14506, "IsComparative": "0", "id": "st_107"}, {"end": 14870, "text": "By using the formulations in terms of cluster-wise measures, the two-stage dimension reduction framework, which separates the criteria of the original dimension reduction methods and the further information loss, are presented.", "rank": 108, "start": 14643, "IsComparative": "0", "id": "st_108"}, {"end": 14983, "text": "The thesis presents the de- tailed criteria using widely-used dimension reduction methods and their visualization", "rank": 109, "start": 14870, "IsComparative": "1", "id": "st_109"}]}, {"paragraph_info": {"end": 15011, "start": 14983, "text": "examples on real-world data.", "rank": 36, "paragraph_comparative_number": 0, "entities": [], "id": "p_36"}, "sentences": [{"end": 15011, "text": "examples on real-world data.", "rank": 110, "start": 14983, "IsComparative": "0", "id": "st_110"}]}, {"paragraph_info": {"end": 15491, "start": 15011, "text": "Chapter 3 focuses on improving basic interactions with compuational methods, i.e., changing their parameters in visual analytics.When dealing with real-world data, it is not trivial to determine the parameter values of used computational methods.Thus, users could naturally change the parameters and see what aspects of data the computational methods may reveal.In order to accelerate such interactions, the thesis presents efficient parametric updating algorithms and their uses.", "rank": 37, "paragraph_comparative_number": 3, "entities": [], "id": "p_37"}, "sentences": [{"end": 15140, "text": "Chapter 3 focuses on improving basic interactions with compuational methods, i.e., changing their parameters in visual analytics.", "rank": 111, "start": 15011, "IsComparative": "1", "id": "st_111"}, {"end": 15257, "text": "When dealing with real-world data, it is not trivial to determine the parameter values of used computational methods.", "rank": 112, "start": 15140, "IsComparative": "1", "id": "st_112"}, {"end": 15373, "text": "Thus, users could naturally change the parameters and see what aspects of data the computational methods may reveal.", "rank": 113, "start": 15257, "IsComparative": "0", "id": "st_113"}, {"end": 15491, "text": "In order to accelerate such interactions, the thesis presents efficient parametric updating algorithms and their uses.", "rank": 114, "start": 15373, "IsComparative": "1", "id": "st_114"}]}, {"paragraph_info": {"end": 16201, "start": 15491, "text": "Chapter 4 presents another novel approach called PIVE (Per-Iteration Visualization Environment for supporting real-time interactive visualization with computational methods) to make computational methods significantly efficient in visual analytics.In this chapter, the presented approach exploits the fact that most computational methods are built upon iterative algorithms.Rather than waiting for the entire it- eration to finish, the iteration-wise framework visualizes the intermediate results of computational methods and enables users to interact with them in real time during it- erations.The details of the proposed framework and its applications using well-known visual analytics systems are presented.", "rank": 38, "paragraph_comparative_number": 3, "entities": [], "id": "p_38"}, "sentences": [{"end": 15739, "text": "Chapter 4 presents another novel approach called PIVE (Per-Iteration Visualization Environment for supporting real-time interactive visualization with computational methods) to make computational methods significantly efficient in visual analytics.", "rank": 115, "start": 15491, "IsComparative": "1", "id": "st_115"}, {"end": 15865, "text": "In this chapter, the presented approach exploits the fact that most computational methods are built upon iterative algorithms.", "rank": 116, "start": 15739, "IsComparative": "0", "id": "st_116"}, {"end": 16086, "text": "Rather than waiting for the entire it- eration to finish, the iteration-wise framework visualizes the intermediate results of computational methods and enables users to interact with them in real time during it- erations.", "rank": 117, "start": 15865, "IsComparative": "1", "id": "st_117"}, {"end": 16201, "text": "The details of the proposed framework and its applications using well-known visual analytics systems are presented.", "rank": 118, "start": 16086, "IsComparative": "1", "id": "st_118"}]}, {"paragraph_info": {"end": 16753, "start": 16201, "text": "Chapter 5 describes the fundamental visual analytics system called the Testbed system, which makes various traditional and advanced dimension reduction and clus- tering algorithms readily available in visual analytics scenarios.In addition to the basic but crucial capabilities of selecting different methods and their parameters, ex- ploring raw data, and brushing-and-linking, the system features a novel capability of alignment between multiple visualization results for easy comparisons.The system details and several usage scenarios are discussed.", "rank": 39, "paragraph_comparative_number": 2, "entities": [], "id": "p_39"}, "sentences": [{"end": 16429, "text": "Chapter 5 describes the fundamental visual analytics system called the Testbed system, which makes various traditional and advanced dimension reduction and clus- tering algorithms readily available in visual analytics scenarios.", "rank": 119, "start": 16201, "IsComparative": "1", "id": "st_119"}, {"end": 16692, "text": "In addition to the basic but crucial capabilities of selecting different methods and their parameters, ex- ploring raw data, and brushing-and-linking, the system features a novel capability of alignment between multiple visualization results for easy comparisons.", "rank": 120, "start": 16429, "IsComparative": "1", "id": "st_120"}, {"end": 16753, "text": "The system details and several usage scenarios are discussed.", "rank": 121, "start": 16692, "IsComparative": "0", "id": "st_121"}]}, {"paragraph_info": {"end": 17390, "start": 16753, "text": "Chapter 6 presents iVisClassifier, another visual analytics system developed based on the Testbed system.iVisClassifier is a customized system for classification appli- cations, which mainly utilizes a specific dimension reduction among those available in the Testbed system.iVisClassifier supports ample features in order to help users un- derstand data, which are, for example, what classes are confusing against each other, which data items are easy/difficult to classify, as well as enables users to intervene in the classification processes.The system details and the usage scenarios in the facial recognition context are described.", "rank": 40, "paragraph_comparative_number": 1, "entities": [], "id": "p_40"}, "sentences": [{"end": 16858, "text": "Chapter 6 presents iVisClassifier, another visual analytics system developed based on the Testbed system.", "rank": 122, "start": 16753, "IsComparative": "1", "id": "st_122"}, {"end": 17028, "text": "iVisClassifier is a customized system for classification appli- cations, which mainly utilizes a specific dimension reduction among those available in the Testbed system.", "rank": 123, "start": 16858, "IsComparative": "0", "id": "st_123"}, {"end": 17299, "text": "iVisClassifier supports ample features in order to help users un- derstand data, which are, for example, what classes are confusing against each other, which data items are easy/difficult to classify, as well as enables users to intervene in the classification processes.", "rank": 124, "start": 17028, "IsComparative": "0", "id": "st_124"}, {"end": 17390, "text": "The system details and the usage scenarios in the facial recognition context are described.", "rank": 125, "start": 17299, "IsComparative": "0", "id": "st_125"}]}, {"paragraph_info": {"end": 17944, "start": 17390, "text": "Chapter 6 presents VisIRR, an interactive visual information retrieval and recom- mender system based on the Testbed system.This system directly tackles the large scale of data by starting with more than 400,000 data items.Besides the basic capa- bilities of clustering and visualizing the retrieved documents based on users queries, the system has the capability to provide recommendations based on users prefer- ence information.The details of the computational methods used and visualization processes are presented along with several usage scenarios.", "rank": 41, "paragraph_comparative_number": 1, "entities": [], "id": "p_41"}, "sentences": [{"end": 17514, "text": "Chapter 6 presents VisIRR, an interactive visual information retrieval and recom- mender system based on the Testbed system.", "rank": 126, "start": 17390, "IsComparative": "0", "id": "st_126"}, {"end": 17613, "text": "This system directly tackles the large scale of data by starting with more than 400,000 data items.", "rank": 127, "start": 17514, "IsComparative": "1", "id": "st_127"}, {"end": 17821, "text": "Besides the basic capa- bilities of clustering and visualizing the retrieved documents based on users queries, the system has the capability to provide recommendations based on users prefer- ence information.", "rank": 128, "start": 17613, "IsComparative": "0", "id": "st_128"}, {"end": 17944, "text": "The details of the computational methods used and visualization processes are presented along with several usage scenarios.", "rank": 129, "start": 17821, "IsComparative": "0", "id": "st_129"}]}, {"paragraph_info": {"end": 18032, "start": 17944, "text": "Finally, Chapter 8 concludes the thesis and presents interesting future research topics.", "rank": 42, "paragraph_comparative_number": 1, "entities": [], "id": "p_42"}, "sentences": [{"end": 18032, "text": "Finally, Chapter 8 concludes the thesis and presents interesting future research topics.", "rank": 130, "start": 17944, "IsComparative": "1", "id": "st_130"}]}, {"paragraph_info": {"end": 18042, "start": 18032, "text": "CHAPTER II", "rank": 43, "paragraph_comparative_number": 0, "entities": [], "id": "p_43"}, "sentences": [{"end": 18042, "text": "CHAPTER II", "rank": 131, "start": 18032, "IsComparative": "0", "id": "st_131"}]}, {"paragraph_info": {"end": 18114, "start": 18042, "text": "TWO-STAGE FRAMEWORK FOR VISUALIZATION OF CLUSTERED HIGH-DIMENSIONAL DATA", "rank": 44, "paragraph_comparative_number": 0, "entities": [], "id": "p_44"}, "sentences": [{"end": 18114, "text": "TWO-STAGE FRAMEWORK FOR VISUALIZATION OF CLUSTERED HIGH-DIMENSIONAL DATA", "rank": 132, "start": 18042, "IsComparative": "0", "id": "st_132"}]}, {"paragraph_info": {"end": 19124, "start": 18114, "text": "In this chapter, we will discuss dimension reduction methods for 2D visualization of high dimensional clustered data.We propose a two-stage framework for visualizing such data based on dimension reduction methods.In the first stage, we obtain the reduced dimensional data by applying a supervised dimension reduction method such as linear discriminant analysis which preserves the original cluster structure in terms of its criteria.The resulting optimal reduced dimension depends on the optimization criteria and is often larger than two.In the second stage, the dimension is further reduced to two for visualization purposes by another dimension reduction method such as principal component analysis.The role of the second stage is to minimize the loss of information due to reducing the dimension all the way to two.Using this framework, we propose several two-stage methods, and present their theoretical characteristics as well as experimental comparisons on both artificial and real-world text data sets.", "rank": 45, "paragraph_comparative_number": 3, "entities": [], "id": "p_45"}, "sentences": [{"end": 18231, "text": "In this chapter, we will discuss dimension reduction methods for 2D visualization of high dimensional clustered data.", "rank": 133, "start": 18114, "IsComparative": "1", "id": "st_133"}, {"end": 18327, "text": "We propose a two-stage framework for visualizing such data based on dimension reduction methods.", "rank": 134, "start": 18231, "IsComparative": "0", "id": "st_134"}, {"end": 18547, "text": "In the first stage, we obtain the reduced dimensional data by applying a supervised dimension reduction method such as linear discriminant analysis which preserves the original cluster structure in terms of its criteria.", "rank": 135, "start": 18327, "IsComparative": "0", "id": "st_135"}, {"end": 18653, "text": "The resulting optimal reduced dimension depends on the optimization criteria and is often larger than two.", "rank": 136, "start": 18547, "IsComparative": "1", "id": "st_136"}, {"end": 18816, "text": "In the second stage, the dimension is further reduced to two for visualization purposes by another dimension reduction method such as principal component analysis.", "rank": 137, "start": 18653, "IsComparative": "0", "id": "st_137"}, {"end": 18933, "text": "The role of the second stage is to minimize the loss of information due to reducing the dimension all the way to two.", "rank": 138, "start": 18816, "IsComparative": "0", "id": "st_138"}, {"end": 19124, "text": "Using this framework, we propose several two-stage methods, and present their theoretical characteristics as well as experimental comparisons on both artificial and real-world text data sets.", "rank": 139, "start": 18933, "IsComparative": "1", "id": "st_139"}]}, {"paragraph_info": {"end": 19140, "start": 19124, "text": "2.1 Introduction", "rank": 46, "paragraph_comparative_number": 0, "entities": [], "id": "p_46"}, "sentences": [{"end": 19140, "text": "2.1 Introduction", "rank": 140, "start": 19124, "IsComparative": "0", "id": "st_140"}]}, {"paragraph_info": {"end": 20229, "start": 19140, "text": "Within the visual analytics community, various types of information content are rep- resented using high dimensional signatures.To make these signatures useful they often need to be transformed into a lower dimension (i.e., 2D or 3D) for a variety of visual representations such as scatter plots.Many researchers in this community have used a wide assortment of dimension reduction techniques, e.g., self-organizing map (SOM) <83>, principal component analysis (PCA) <75>, multidimensional scaling (MDS) <45>, etc.However, it is not always clear why a certain technique has been chosen over another, especially to the end user.Typically, the goal of dimension reduction techniques can be viewed in terms of two aspects: efficiency and accuracy.Efficiency as defined here is the time to compute the reduction, but accuracy may not be as simple to quantify.Many would amiably agree to quantify accuracy as a measure of the relationship preservation in the high dimensional space to the reduced dimensional space.Note that most techniques either directly or indirectly work on this principle.", "rank": 47, "paragraph_comparative_number": 3, "entities": [], "id": "p_47"}, "sentences": [{"end": 19268, "text": "Within the visual analytics community, various types of information content are rep- resented using high dimensional signatures.", "rank": 141, "start": 19140, "IsComparative": "0", "id": "st_141"}, {"end": 19436, "text": "To make these signatures useful they often need to be transformed into a lower dimension (i.e., 2D or 3D) for a variety of visual representations such as scatter plots.", "rank": 142, "start": 19268, "IsComparative": "1", "id": "st_142"}, {"end": 19654, "text": "Many researchers in this community have used a wide assortment of dimension reduction techniques, e.g., self-organizing map (SOM) <83>, principal component analysis (PCA) <75>, multidimensional scaling (MDS) <45>, etc.", "rank": 143, "start": 19436, "IsComparative": "0", "id": "st_143"}, {"end": 19767, "text": "However, it is not always clear why a certain technique has been chosen over another, especially to the end user.", "rank": 144, "start": 19654, "IsComparative": "0", "id": "st_144"}, {"end": 19884, "text": "Typically, the goal of dimension reduction techniques can be viewed in terms of two aspects: efficiency and accuracy.", "rank": 145, "start": 19767, "IsComparative": "1", "id": "st_145"}, {"end": 19995, "text": "Efficiency as defined here is the time to compute the reduction, but accuracy may not be as simple to quantify.", "rank": 146, "start": 19884, "IsComparative": "0", "id": "st_146"}, {"end": 20150, "text": "Many would amiably agree to quantify accuracy as a measure of the relationship preservation in the high dimensional space to the reduced dimensional space.", "rank": 147, "start": 19995, "IsComparative": "0", "id": "st_147"}, {"end": 20229, "text": "Note that most techniques either directly or indirectly work on this principle.", "rank": 148, "start": 20150, "IsComparative": "1", "id": "st_148"}]}, {"paragraph_info": {"end": 21647, "start": 20229, "text": "There are other properties that are important to those interpreting the semantics of the reduced space.Specifically, we note that while local neighbor preservation is important it depends upon the analysis task.No single reduction technique will provide the complete view as various properties of the space are obscured or lost.We have mentioned that typically the primary objective is relationship preservation.However, there are at least two others: outlier and macro structure visualization.Outliers are conceptually easy (i.e., a variance beyond some threshold), but more difficult to quantify, as we do not necessarily know which set of outliers are important to accentuate to the user.Certain techniques (e.g., PCA) tend to show outliers more readily, however tend to compress the reduced space at the expense of showcasing the outliers.Other techniques (e.g., SOM) maximize space usage well, but do so at the expense of masking or even hiding those outliers.Likewise, macro structures of the high dimensional space may be masked or massively distorted during the reduction.Macro structures are those larger order groupings (e.g., clusters) that exist in the original dimensional space.We recognize they are important in dimension reduction research and to those in the visual analytics community.However, few of them focus on data representation especially for visualization of the clustered data <147, 84, 49>.", "rank": 48, "paragraph_comparative_number": 3, "entities": [], "id": "p_48"}, "sentences": [{"end": 20332, "text": "There are other properties that are important to those interpreting the semantics of the reduced space.", "rank": 149, "start": 20229, "IsComparative": "0", "id": "st_149"}, {"end": 20440, "text": "Specifically, we note that while local neighbor preservation is important it depends upon the analysis task.", "rank": 150, "start": 20332, "IsComparative": "0", "id": "st_150"}, {"end": 20557, "text": "No single reduction technique will provide the complete view as various properties of the space are obscured or lost.", "rank": 151, "start": 20440, "IsComparative": "0", "id": "st_151"}, {"end": 20641, "text": "We have mentioned that typically the primary objective is relationship preservation.", "rank": 152, "start": 20557, "IsComparative": "0", "id": "st_152"}, {"end": 20723, "text": "However, there are at least two others: outlier and macro structure visualization.", "rank": 153, "start": 20641, "IsComparative": "1", "id": "st_153"}, {"end": 20920, "text": "Outliers are conceptually easy (i.e., a variance beyond some threshold), but more difficult to quantify, as we do not necessarily know which set of outliers are important to accentuate to the user.", "rank": 154, "start": 20723, "IsComparative": "0", "id": "st_154"}, {"end": 21072, "text": "Certain techniques (e.g., PCA) tend to show outliers more readily, however tend to compress the reduced space at the expense of showcasing the outliers.", "rank": 155, "start": 20920, "IsComparative": "1", "id": "st_155"}, {"end": 21194, "text": "Other techniques (e.g., SOM) maximize space usage well, but do so at the expense of masking or even hiding those outliers.", "rank": 156, "start": 21072, "IsComparative": "1", "id": "st_156"}, {"end": 21309, "text": "Likewise, macro structures of the high dimensional space may be masked or massively distorted during the reduction.", "rank": 157, "start": 21194, "IsComparative": "0", "id": "st_157"}, {"end": 21421, "text": "Macro structures are those larger order groupings (e.g., clusters) that exist in the original dimensional space.", "rank": 158, "start": 21309, "IsComparative": "0", "id": "st_158"}, {"end": 21532, "text": "We recognize they are important in dimension reduction research and to those in the visual analytics community.", "rank": 159, "start": 21421, "IsComparative": "0", "id": "st_159"}, {"end": 21647, "text": "However, few of them focus on data representation especially for visualization of the clustered data <147, 84, 49>.", "rank": 160, "start": 21532, "IsComparative": "0", "id": "st_160"}]}, {"paragraph_info": {"end": 21891, "start": 21647, "text": "We propose theoretical measures for these properties and efficient algorithms which will aid not only the researchers but ultimately the users/analysts to better understand which balance of properties are important and for which analytic tasks.", "rank": 49, "paragraph_comparative_number": 0, "entities": [], "id": "p_49"}, "sentences": [{"end": 21891, "text": "We propose theoretical measures for these properties and efficient algorithms which will aid not only the researchers but ultimately the users/analysts to better understand which balance of properties are important and for which analytic tasks.", "rank": 161, "start": 21647, "IsComparative": "0", "id": "st_161"}]}, {"paragraph_info": {"end": 21905, "start": 21891, "text": "2.2 Motivation", "rank": 50, "paragraph_comparative_number": 0, "entities": [], "id": "p_50"}, "sentences": [{"end": 21905, "text": "2.2 Motivation", "rank": 162, "start": 21891, "IsComparative": "0", "id": "st_162"}]}, {"paragraph_info": {"end": 22455, "start": 21905, "text": "The focus of this chapter is the fundamental characteristics of dimension reduction techniques for visualizing high dimensional data in the form of a 2D scatter plot when the data has cluster structure.The role of dimension reduction here is to give a 2-dimensional representation of data while preserving cluster structure as much as possible.To this end, supervised dimension reduction methods that incorporate cluster information such as linear discriminant analysis (LDA) <60> or orthogonal centroid method (OCM) <71> can be naturally considered.", "rank": 51, "paragraph_comparative_number": 2, "entities": [], "id": "p_51"}, "sentences": [{"end": 22107, "text": "The focus of this chapter is the fundamental characteristics of dimension reduction techniques for visualizing high dimensional data in the form of a 2D scatter plot when the data has cluster structure.", "rank": 163, "start": 21905, "IsComparative": "0", "id": "st_163"}, {"end": 22249, "text": "The role of dimension reduction here is to give a 2-dimensional representation of data while preserving cluster structure as much as possible.", "rank": 164, "start": 22107, "IsComparative": "1", "id": "st_164"}, {"end": 22455, "text": "To this end, supervised dimension reduction methods that incorporate cluster information such as linear discriminant analysis (LDA) <60> or orthogonal centroid method (OCM) <71> can be naturally considered.", "rank": 165, "start": 22249, "IsComparative": "1", "id": "st_165"}]}, {"paragraph_info": {"end": 23002, "start": 22455, "text": "However, one of the issues is that with many dimension reduction methods de- signed to preserve the cluster structure in the data, the theoretically optimal reduced dimension, which is the smallest dimension that is acceptable with respect to the optimization criteria of the dimension reduction method, is usually larger than 2.For example, in LDA, the minimum reduced dimension that preserves the cluster struc- ture quality measure defined as a trace maximization problem is one less than the number of clusters in the data in general <68, 67>.", "rank": 52, "paragraph_comparative_number": 0, "entities": [], "id": "p_52"}, "sentences": [{"end": 22784, "text": "However, one of the issues is that with many dimension reduction methods de- signed to preserve the cluster structure in the data, the theoretically optimal reduced dimension, which is the smallest dimension that is acceptable with respect to the optimization criteria of the dimension reduction method, is usually larger than 2.", "rank": 166, "start": 22455, "IsComparative": "0", "id": "st_166"}, {"end": 23002, "text": "For example, in LDA, the minimum reduced dimension that preserves the cluster struc- ture quality measure defined as a trace maximization problem is one less than the number of clusters in the data in general <68, 67>.", "rank": 167, "start": 22784, "IsComparative": "0", "id": "st_167"}]}, {"paragraph_info": {"end": 23785, "start": 23002, "text": "In this case, one may simply choose the two dimensions that contribute most to such a measure.However, with only two dimensions, such a measure may become significantly smaller than the original quantity after dimension reduction.This results in loss of information that hinders visualization in properly reflecting the true cluster relationship of the data.A similar situation may occur when using PCA for visualizing the data not having a cluster structure.Even though PCA finds the principal axes that maximally capture the variance of the data, when the resulting 2-dimensional representation of the data maintains only a small fraction of the total variance, the relationships of the data in 2 dimension are likely to be highly inconsistent with those in the original dimension.", "rank": 53, "paragraph_comparative_number": 3, "entities": [], "id": "p_53"}, "sentences": [{"end": 23096, "text": "In this case, one may simply choose the two dimensions that contribute most to such a measure.", "rank": 168, "start": 23002, "IsComparative": "1", "id": "st_168"}, {"end": 23232, "text": "However, with only two dimensions, such a measure may become significantly smaller than the original quantity after dimension reduction.", "rank": 169, "start": 23096, "IsComparative": "1", "id": "st_169"}, {"end": 23360, "text": "This results in loss of information that hinders visualization in properly reflecting the true cluster relationship of the data.", "rank": 170, "start": 23232, "IsComparative": "0", "id": "st_170"}, {"end": 23461, "text": "A similar situation may occur when using PCA for visualizing the data not having a cluster structure.", "rank": 171, "start": 23360, "IsComparative": "0", "id": "st_171"}, {"end": 23785, "text": "Even though PCA finds the principal axes that maximally capture the variance of the data, when the resulting 2-dimensional representation of the data maintains only a small fraction of the total variance, the relationships of the data in 2 dimension are likely to be highly inconsistent with those in the original dimension.", "rank": 172, "start": 23461, "IsComparative": "1", "id": "st_172"}]}, {"paragraph_info": {"end": 25065, "start": 23785, "text": "Such loss of information is inevitable in that the dimension has to be reduced to 2.Our main motivation is to deal with such loss more carefully by separating the loss-introducing stage from the original dimension reduction methods.Based on this idea, we propose the two-stage framework of dimension reduction for visualization.In this framework, a supervised dimension reduction method is applied in the first stage so that the original dimension is reduced to the minimum dimension achievable while preserving the quality of cluster measure as defined in a dimension reduction method.The reduced dimension achieved in the first stage is often larger than 2.Thus in the second stage, we find another dimension reducing transformation that minimizes the loss introduced in further reducing the dimension all the way to 2.This two-stage framework provides us with a means to flexibly apply different types of dimension reduction techniques in each stage and to systematically analyze their effects, which provides understanding the effects of the overall dimension reduction methods for visualization of clustered data.The issues then are the design of the most appropriate dimension reduction methods, the modeling of optimization criteria, and the corresponding solution methods.", "rank": 54, "paragraph_comparative_number": 3, "entities": [], "id": "p_54"}, "sentences": [{"end": 23869, "text": "Such loss of information is inevitable in that the dimension has to be reduced to 2.", "rank": 173, "start": 23785, "IsComparative": "0", "id": "st_173"}, {"end": 24017, "text": "Our main motivation is to deal with such loss more carefully by separating the loss-introducing stage from the original dimension reduction methods.", "rank": 174, "start": 23869, "IsComparative": "1", "id": "st_174"}, {"end": 24113, "text": "Based on this idea, we propose the two-stage framework of dimension reduction for visualization.", "rank": 175, "start": 24017, "IsComparative": "0", "id": "st_175"}, {"end": 24371, "text": "In this framework, a supervised dimension reduction method is applied in the first stage so that the original dimension is reduced to the minimum dimension achievable while preserving the quality of cluster measure as defined in a dimension reduction method.", "rank": 176, "start": 24113, "IsComparative": "0", "id": "st_176"}, {"end": 24444, "text": "The reduced dimension achieved in the first stage is often larger than 2.", "rank": 177, "start": 24371, "IsComparative": "0", "id": "st_177"}, {"end": 24606, "text": "Thus in the second stage, we find another dimension reducing transformation that minimizes the loss introduced in further reducing the dimension all the way to 2.", "rank": 178, "start": 24444, "IsComparative": "0", "id": "st_178"}, {"end": 24903, "text": "This two-stage framework provides us with a means to flexibly apply different types of dimension reduction techniques in each stage and to systematically analyze their effects, which provides understanding the effects of the overall dimension reduction methods for visualization of clustered data.", "rank": 179, "start": 24606, "IsComparative": "1", "id": "st_179"}, {"end": 25065, "text": "The issues then are the design of the most appropriate dimension reduction methods, the modeling of optimization criteria, and the corresponding solution methods.", "rank": 180, "start": 24903, "IsComparative": "1", "id": "st_180"}]}, {"paragraph_info": {"end": 26208, "start": 25065, "text": "In this chapter, we present both theoretical and empirical answers to these is- sues.Specifically, we propose several two-stage methods utilizing linear dimension reduction methods such as LDA, orthogonal centroid method (OCM), and principal component analysis (PCA), and we present their theoretical justifications by mod- eling the optimization criteria for which each method provides the optimal solution.Also, we illustrate and compare the effectiveness of the proposed methods by show- ing empirical visualization on synthetic and real-world data sets.Although nonlinear dimension reduction methods such as MDS or other manifold learning methods such as isometric feature mapping <125> and locally linear embedding <111> may also be utilized for the effective 2D visualization of high dimensional data, our focus in this chapter is on linear methods.The linear methods are computationally more efficient in general, and unlike most of the manifold learning methods, they also provide di- mension reducing transformations that can be applied to map and visualize unseen data points in the same space where the existing data are visualized.", "rank": 55, "paragraph_comparative_number": 0, "entities": [], "id": "p_55"}, "sentences": [{"end": 25150, "text": "In this chapter, we present both theoretical and empirical answers to these is- sues.", "rank": 181, "start": 25065, "IsComparative": "0", "id": "st_181"}, {"end": 25473, "text": "Specifically, we propose several two-stage methods utilizing linear dimension reduction methods such as LDA, orthogonal centroid method (OCM), and principal component analysis (PCA), and we present their theoretical justifications by mod- eling the optimization criteria for which each method provides the optimal solution.", "rank": 182, "start": 25150, "IsComparative": "0", "id": "st_182"}, {"end": 25920, "text": "Also, we illustrate and compare the effectiveness of the proposed methods by show- ing empirical visualization on synthetic and real-world data sets.Although nonlinear dimension reduction methods such as MDS or other manifold learning methods such as isometric feature mapping <125> and locally linear embedding <111> may also be utilized for the effective 2D visualization of high dimensional data, our focus in this chapter is on linear methods.", "rank": 183, "start": 25473, "IsComparative": "0", "id": "st_183"}, {"end": 26208, "text": "The linear methods are computationally more efficient in general, and unlike most of the manifold learning methods, they also provide di- mension reducing transformations that can be applied to map and visualize unseen data points in the same space where the existing data are visualized.", "rank": 184, "start": 25920, "IsComparative": "0", "id": "st_184"}]}, {"paragraph_info": {"end": 26480, "start": 26208, "text": "Our approach to successively apply two dimension reduction methods should be discerned from the previous work <144, 145, 150> in that they usually aim for improving computational efficiency, scalability, or applicability of a certain dimension reduction method, e.g., LDA.", "rank": 56, "paragraph_comparative_number": 1, "entities": [], "id": "p_56"}, "sentences": [{"end": 26480, "text": "Our approach to successively apply two dimension reduction methods should be discerned from the previous work <144, 145, 150> in that they usually aim for improving computational efficiency, scalability, or applicability of a certain dimension reduction method, e.g., LDA.", "rank": 185, "start": 26208, "IsComparative": "1", "id": "st_185"}]}, {"paragraph_info": {"end": 26986, "start": 26480, "text": "The rest of this chapter is organized as follows.In Section 2.3, LDA, OCM, and PCA are described based on a unified framework of the scatter matrices and their trace optimization problems.In Section 2.4, we formulate two-stage dimension reduction methods, and in Section 2.5, several two-stage methods for visualization are proposed and compared along with their criteria.Experimental comparisons are given using artificial and real-world data sets in Section 2.6, and conclusions are drawn in Section 2.7.", "rank": 57, "paragraph_comparative_number": 1, "entities": [], "id": "p_57"}, "sentences": [{"end": 26529, "text": "The rest of this chapter is organized as follows.", "rank": 186, "start": 26480, "IsComparative": "0", "id": "st_186"}, {"end": 26668, "text": "In Section 2.3, LDA, OCM, and PCA are described based on a unified framework of the scatter matrices and their trace optimization problems.", "rank": 187, "start": 26529, "IsComparative": "1", "id": "st_187"}, {"end": 26852, "text": "In Section 2.4, we formulate two-stage dimension reduction methods, and in Section 2.5, several two-stage methods for visualization are proposed and compared along with their criteria.", "rank": 188, "start": 26668, "IsComparative": "0", "id": "st_188"}, {"end": 26986, "text": "Experimental comparisons are given using artificial and real-world data sets in Section 2.6, and conclusions are drawn in Section 2.7.", "rank": 189, "start": 26852, "IsComparative": "0", "id": "st_189"}]}, {"paragraph_info": {"end": 27039, "start": 26986, "text": "2.3 Dimension Reduction as Trace Optimization Problem", "rank": 58, "paragraph_comparative_number": 0, "entities": [], "id": "p_58"}, "sentences": [{"end": 27039, "text": "2.3 Dimension Reduction as Trace Optimization Problem", "rank": 190, "start": 26986, "IsComparative": "0", "id": "st_190"}]}, {"paragraph_info": {"end": 27184, "start": 27039, "text": "In this section, we introduce the notions of scatter matrices used in defining cluster quality and optimization criteria for dimension reduction.", "rank": 59, "paragraph_comparative_number": 0, "entities": [], "id": "p_59"}, "sentences": [{"end": 27184, "text": "In this section, we introduce the notions of scatter matrices used in defining cluster quality and optimization criteria for dimension reduction.", "rank": 191, "start": 27039, "IsComparative": "0", "id": "st_191"}]}, {"paragraph_info": {"end": 27328, "start": 27184, "text": "Suppose a dimension reducing linear transformation GT  Rlm maps an m- dimensional data vector x to a vector z in an l-dimensional space (m > l):", "rank": 60, "paragraph_comparative_number": 0, "entities": [], "id": "p_60"}, "sentences": [{"end": 27328, "text": "Suppose a dimension reducing linear transformation GT  Rlm maps an m- dimensional data vector x to a vector z in an l-dimensional space (m > l):", "rank": 192, "start": 27184, "IsComparative": "0", "id": "st_192"}]}, {"paragraph_info": {"end": 27774, "start": 27328, "text": "Let Ni denote the set of column indices that belong to cluster i, and ni the size of Ni.The i-th cluster centroid c(i) and the global centroid c are defined, respectively, as we obtain values that can be used to measure the cluster quality.Note that from Eqs.(8) and (9), trace(Sb) can be viewed as the squared sum of the pairwise distances between cluster centroids as well as that of the distances between each centroid and the global centroid.", "rank": 61, "paragraph_comparative_number": 2, "entities": [], "id": "p_61"}, "sentences": [{"end": 27416, "text": "Let Ni denote the set of column indices that belong to cluster i, and ni the size of Ni.", "rank": 193, "start": 27328, "IsComparative": "1", "id": "st_193"}, {"end": 27568, "text": "The i-th cluster centroid c(i) and the global centroid c are defined, respectively, as we obtain values that can be used to measure the cluster quality.", "rank": 194, "start": 27416, "IsComparative": "1", "id": "st_194"}, {"end": 27587, "text": "Note that from Eqs.", "rank": 195, "start": 27568, "IsComparative": "0", "id": "st_195"}, {"end": 27774, "text": "(8) and (9), trace(Sb) can be viewed as the squared sum of the pairwise distances between cluster centroids as well as that of the distances between each centroid and the global centroid.", "rank": 196, "start": 27587, "IsComparative": "0", "id": "st_196"}]}, {"paragraph_info": {"end": 28276, "start": 27774, "text": "The cluster structure quality can be defined by analyzing how well each clus- ter can be discriminated from each other.High quality clusters usually have small trace(Sw) and large trace(Sb), relating to the small variance within each cluster and the large distances between clusters.Subsequently, dimension reduction methods may be intended to maximize trace(GT SbG) and minimize trace(GT SwG) in the reduced dimensional space.This simultaneous optimization can be approximated to a single criterion as", "rank": 62, "paragraph_comparative_number": 1, "entities": [], "id": "p_62"}, "sentences": [{"end": 27893, "text": "The cluster structure quality can be defined by analyzing how well each clus- ter can be discriminated from each other.", "rank": 197, "start": 27774, "IsComparative": "0", "id": "st_197"}, {"end": 28057, "text": "High quality clusters usually have small trace(Sw) and large trace(Sb), relating to the small variance within each cluster and the large distances between clusters.", "rank": 198, "start": 27893, "IsComparative": "1", "id": "st_198"}, {"end": 28201, "text": "Subsequently, dimension reduction methods may be intended to maximize trace(GT SbG) and minimize trace(GT SwG) in the reduced dimensional space.", "rank": 199, "start": 28057, "IsComparative": "0", "id": "st_199"}, {"end": 28276, "text": "This simultaneous optimization can be approximated to a single criterion as", "rank": 200, "start": 28201, "IsComparative": "0", "id": "st_200"}]}, {"paragraph_info": {"end": 28405, "start": 28276, "text": "On the other hand, regardless of cluster dependent terms, Sw and Sb, the trace of the total scatter matrix St can be maximized as", "rank": 63, "paragraph_comparative_number": 0, "entities": [], "id": "p_63"}, "sentences": [{"end": 28405, "text": "On the other hand, regardless of cluster dependent terms, Sw and Sb, the trace of the total scatter matrix St can be maximized as", "rank": 201, "start": 28276, "IsComparative": "0", "id": "st_201"}]}, {"paragraph_info": {"end": 28443, "start": 28405, "text": "Jt(G) = max trace(GT StG), (13) GT G=I", "rank": 64, "paragraph_comparative_number": 0, "entities": [], "id": "p_64"}, "sentences": [{"end": 28443, "text": "Jt(G) = max trace(GT StG), (13) GT G=I", "rank": 202, "start": 28405, "IsComparative": "0", "id": "st_202"}]}, {"paragraph_info": {"end": 28587, "start": 28443, "text": "which turns out to be the criterion of PCA.In Eqs.(12) and (13), without the constraint, GT G = I, Jb(G) and Jt(G) can become arbitrarily large.", "rank": 65, "paragraph_comparative_number": 0, "entities": [], "id": "p_65"}, "sentences": [{"end": 28486, "text": "which turns out to be the criterion of PCA.", "rank": 203, "start": 28443, "IsComparative": "0", "id": "st_203"}, {"end": 28493, "text": "In Eqs.", "rank": 204, "start": 28486, "IsComparative": "0", "id": "st_204"}, {"end": 28587, "text": "(12) and (13), without the constraint, GT G = I, Jb(G) and Jt(G) can become arbitrarily large.", "rank": 205, "start": 28493, "IsComparative": "0", "id": "st_205"}]}, {"paragraph_info": {"end": 28735, "start": 28587, "text": "In what follows, LDA, OCM, and PCA are discussed based on such maximization criteria, and their properties relevant to visualization are identified.", "rank": 66, "paragraph_comparative_number": 1, "entities": [], "id": "p_66"}, "sentences": [{"end": 28735, "text": "In what follows, LDA, OCM, and PCA are discussed based on such maximization criteria, and their properties relevant to visualization are identified.", "rank": 206, "start": 28587, "IsComparative": "1", "id": "st_206"}]}, {"paragraph_info": {"end": 28775, "start": 28735, "text": "2.3.1 Linear Discriminant Analysis (LDA)", "rank": 67, "paragraph_comparative_number": 0, "entities": [], "id": "p_67"}, "sentences": [{"end": 28775, "text": "2.3.1 Linear Discriminant Analysis (LDA)", "rank": 207, "start": 28735, "IsComparative": "0", "id": "st_207"}]}, {"paragraph_info": {"end": 29101, "start": 28775, "text": "Conceptually, in LDA, we are looking for a dimension reducing transformation that keeps the between-cluster relationship as remote as possible by maximizing trace(GT SbG) while keeping the within cluster relationship as compact as possible by minimizing trace(GTSwG).As shown in Eq.(11), the criterion of LDA can be written as", "rank": 68, "paragraph_comparative_number": 0, "entities": [], "id": "p_68"}, "sentences": [{"end": 29042, "text": "Conceptually, in LDA, we are looking for a dimension reducing transformation that keeps the between-cluster relationship as remote as possible by maximizing trace(GT SbG) while keeping the within cluster relationship as compact as possible by minimizing trace(GTSwG).", "rank": 208, "start": 28775, "IsComparative": "0", "id": "st_208"}, {"end": 29057, "text": "As shown in Eq.", "rank": 209, "start": 29042, "IsComparative": "0", "id": "st_209"}, {"end": 29101, "text": "(11), the criterion of LDA can be written as", "rank": 210, "start": 29057, "IsComparative": "0", "id": "st_210"}]}, {"paragraph_info": {"end": 29181, "start": 29101, "text": "Jb/w(G) = max trace((GT SwG)1(GT SbG)).(14) ItcanbeshownthatforanyGRml wherem>l,", "rank": 69, "paragraph_comparative_number": 0, "entities": [], "id": "p_69"}, "sentences": [{"end": 29140, "text": "Jb/w(G) = max trace((GT SwG)1(GT SbG)).", "rank": 211, "start": 29101, "IsComparative": "0", "id": "st_211"}, {"end": 29181, "text": "(14) ItcanbeshownthatforanyGRml wherem>l,", "rank": 212, "start": 29140, "IsComparative": "0", "id": "st_212"}]}, {"paragraph_info": {"end": 29229, "start": 29181, "text": "trace((GT S G)1(GT S G))  trace(S1S ), (15) wbwb", "rank": 70, "paragraph_comparative_number": 0, "entities": [], "id": "p_70"}, "sentences": [{"end": 29229, "text": "trace((GT S G)1(GT S G))  trace(S1S ), (15) wbwb", "rank": 213, "start": 29181, "IsComparative": "0", "id": "st_213"}]}, {"paragraph_info": {"end": 29312, "start": 29229, "text": "meaning that the cluster structure quality measured by trace(S1S ) cannot be in- wb", "rank": 71, "paragraph_comparative_number": 0, "entities": [], "id": "p_71"}, "sentences": [{"end": 29312, "text": "meaning that the cluster structure quality measured by trace(S1S ) cannot be in- wb", "rank": 214, "start": 29229, "IsComparative": "0", "id": "st_214"}]}, {"paragraph_info": {"end": 29638, "start": 29312, "text": "creased after dimension reduction <60>.By setting the derivative of Eq.(14) with respect to G to zero, which gives the first order optimality condition, it can be shown that the solution of LDA, where we denote it as GLDA, has the columns which are the leading generalized eigenvectors u of the generalized eigenvalue problem,", "rank": 72, "paragraph_comparative_number": 1, "entities": [], "id": "p_72"}, "sentences": [{"end": 29351, "text": "creased after dimension reduction <60>.", "rank": 215, "start": 29312, "IsComparative": "0", "id": "st_215"}, {"end": 29383, "text": "By setting the derivative of Eq.", "rank": 216, "start": 29351, "IsComparative": "0", "id": "st_216"}, {"end": 29638, "text": "(14) with respect to G to zero, which gives the first order optimality condition, it can be shown that the solution of LDA, where we denote it as GLDA, has the columns which are the leading generalized eigenvectors u of the generalized eigenvalue problem,", "rank": 217, "start": 29383, "IsComparative": "1", "id": "st_217"}]}, {"paragraph_info": {"end": 29736, "start": 29638, "text": "Sbu = Swu.(16) Since the rank of Sb is at most k1, LDA achieves the upper bound of trace((GT SwG)1", "rank": 73, "paragraph_comparative_number": 1, "entities": [], "id": "p_73"}, "sentences": [{"end": 29648, "text": "Sbu = Swu.", "rank": 218, "start": 29638, "IsComparative": "0", "id": "st_218"}, {"end": 29736, "text": "(16) Since the rank of Sb is at most k1, LDA achieves the upper bound of trace((GT SwG)1", "rank": 219, "start": 29648, "IsComparative": "1", "id": "st_219"}]}, {"paragraph_info": {"end": 29822, "start": 29736, "text": "which indicates trace(S1S ) is preserved between the original space and the reduced wb", "rank": 74, "paragraph_comparative_number": 0, "entities": [], "id": "p_74"}, "sentences": [{"end": 29822, "text": "which indicates trace(S1S ) is preserved between the original space and the reduced wb", "rank": 220, "start": 29736, "IsComparative": "0", "id": "st_220"}]}, {"paragraph_info": {"end": 29857, "start": 29822, "text": "dimensional space obtained by GLDA.", "rank": 75, "paragraph_comparative_number": 0, "entities": [], "id": "p_75"}, "sentences": [{"end": 29857, "text": "dimensional space obtained by GLDA.", "rank": 221, "start": 29822, "IsComparative": "0", "id": "st_221"}]}, {"paragraph_info": {"end": 29895, "start": 29857, "text": "2.3.2 Orthogonal Centroid Method (OCM)", "rank": 76, "paragraph_comparative_number": 0, "entities": [], "id": "p_76"}, "sentences": [{"end": 29895, "text": "2.3.2 Orthogonal Centroid Method (OCM)", "rank": 222, "start": 29857, "IsComparative": "0", "id": "st_222"}]}, {"paragraph_info": {"end": 30040, "start": 29895, "text": "Orthogonal centroid method (OCM) <71> focuses only on maximizing trace(GT SbG) under the constraint of GT G = I. The criterion of OCM is shown as", "rank": 77, "paragraph_comparative_number": 1, "entities": [], "id": "p_77"}, "sentences": [{"end": 30040, "text": "Orthogonal centroid method (OCM) <71> focuses only on maximizing trace(GT SbG) under the constraint of GT G = I. The criterion of OCM is shown as", "rank": 223, "start": 29895, "IsComparative": "1", "id": "st_223"}]}, {"paragraph_info": {"end": 30077, "start": 30040, "text": "Jb(G) = max trace(GT SbG).(18) GT G=I", "rank": 78, "paragraph_comparative_number": 0, "entities": [], "id": "p_78"}, "sentences": [{"end": 30066, "text": "Jb(G) = max trace(GT SbG).", "rank": 224, "start": 30040, "IsComparative": "0", "id": "st_224"}, {"end": 30077, "text": "(18) GT G=I", "rank": 225, "start": 30066, "IsComparative": "0", "id": "st_225"}]}, {"paragraph_info": {"end": 30123, "start": 30077, "text": "ItisknownthatforanyGRml wherem>lsuchthatGTG=I,", "rank": 79, "paragraph_comparative_number": 0, "entities": [], "id": "p_79"}, "sentences": [{"end": 30123, "text": "ItisknownthatforanyGRml wherem>lsuchthatGTG=I,", "rank": 226, "start": 30077, "IsComparative": "0", "id": "st_226"}]}, {"paragraph_info": {"end": 30153, "start": 30123, "text": "trace(GT SbG)  trace(Sb), (19)", "rank": 80, "paragraph_comparative_number": 0, "entities": [], "id": "p_80"}, "sentences": [{"end": 30153, "text": "trace(GT SbG)  trace(Sb), (19)", "rank": 227, "start": 30123, "IsComparative": "0", "id": "st_227"}]}, {"paragraph_info": {"end": 30506, "start": 30153, "text": "which means the cluster structure quality measured by trace(Sb) cannot be increased after dimension reduction.The solution of Eq.(18) can be obtained by setting the columns of G as the leading eigenvectors of Sb.Since Sb has at most k  1 nonzero eigenvalues, the upper bound of trace(GT SbG) in Eq.(19) can be achieved for any l such that l  k  1, i.e.,", "rank": 81, "paragraph_comparative_number": 1, "entities": [], "id": "p_81"}, "sentences": [{"end": 30263, "text": "which means the cluster structure quality measured by trace(Sb) cannot be increased after dimension reduction.", "rank": 228, "start": 30153, "IsComparative": "0", "id": "st_228"}, {"end": 30282, "text": "The solution of Eq.", "rank": 229, "start": 30263, "IsComparative": "0", "id": "st_229"}, {"end": 30365, "text": "(18) can be obtained by setting the columns of G as the leading eigenvectors of Sb.", "rank": 230, "start": 30282, "IsComparative": "0", "id": "st_230"}, {"end": 30451, "text": "Since Sb has at most k  1 nonzero eigenvalues, the upper bound of trace(GT SbG) in Eq.", "rank": 231, "start": 30365, "IsComparative": "1", "id": "st_231"}, {"end": 30506, "text": "(19) can be achieved for any l such that l  k  1, i.e.,", "rank": 232, "start": 30451, "IsComparative": "0", "id": "st_232"}]}, {"paragraph_info": {"end": 30551, "start": 30506, "text": "trace(GT SbG) = trace(Sb) for l  k  1.(20) 18", "rank": 82, "paragraph_comparative_number": 1, "entities": [], "id": "p_82"}, "sentences": [{"end": 30544, "text": "trace(GT SbG) = trace(Sb) for l  k  1.", "rank": 233, "start": 30506, "IsComparative": "0", "id": "st_233"}, {"end": 30551, "text": "(20) 18", "rank": 234, "start": 30544, "IsComparative": "1", "id": "st_234"}]}, {"paragraph_info": {"end": 30650, "start": 30551, "text": "Eq.(20) indicates trace(Sb) is preserved between the original and the reduced dimen- sional spaces.", "rank": 83, "paragraph_comparative_number": 0, "entities": [], "id": "p_83"}, "sentences": [{"end": 30554, "text": "Eq.", "rank": 235, "start": 30551, "IsComparative": "0", "id": "st_235"}, {"end": 30650, "text": "(20) indicates trace(Sb) is preserved between the original and the reduced dimen- sional spaces.", "rank": 236, "start": 30554, "IsComparative": "0", "id": "st_236"}]}, {"paragraph_info": {"end": 31137, "start": 30650, "text": "An advantage of OCM is that it achieves an upper bound of trace(GT SbG) more efficiently by using QR decomposition, avoiding the eigendecomposition.The algo- rithm of OCM is as follows.First the centroid matrix C is formed so that each column of C is composed of each clusters centroid vector, i.e., C =   c1 c2    ck  .Then the reduced QR decomposition <61> of C is computed for C = QkR where Qk  Rmk with QTk Qk = I and R  Rkk is upper triangular.The solution of OCM, GOCM, is found as", "rank": 84, "paragraph_comparative_number": 1, "entities": [], "id": "p_84"}, "sentences": [{"end": 30798, "text": "An advantage of OCM is that it achieves an upper bound of trace(GT SbG) more efficiently by using QR decomposition, avoiding the eigendecomposition.", "rank": 237, "start": 30650, "IsComparative": "1", "id": "st_237"}, {"end": 30835, "text": "The algo- rithm of OCM is as follows.", "rank": 238, "start": 30798, "IsComparative": "0", "id": "st_238"}, {"end": 30970, "text": "First the centroid matrix C is formed so that each column of C is composed of each clusters centroid vector, i.e., C =   c1 c2    ck  .", "rank": 239, "start": 30835, "IsComparative": "0", "id": "st_239"}, {"end": 31099, "text": "Then the reduced QR decomposition <61> of C is computed for C = QkR where Qk  Rmk with QTk Qk = I and R  Rkk is upper triangular.", "rank": 240, "start": 30970, "IsComparative": "0", "id": "st_240"}, {"end": 31137, "text": "The solution of OCM, GOCM, is found as", "rank": 241, "start": 31099, "IsComparative": "0", "id": "st_241"}]}, {"paragraph_info": {"end": 31146, "start": 31137, "text": "GOCM =Qk.", "rank": 85, "paragraph_comparative_number": 0, "entities": [], "id": "p_85"}, "sentences": [{"end": 31146, "text": "GOCM =Qk.", "rank": 242, "start": 31137, "IsComparative": "0", "id": "st_242"}]}, {"paragraph_info": {"end": 31229, "start": 31146, "text": "Note that the columns of GOCM are composed of the orthogonal bases for the subspace", "rank": 86, "paragraph_comparative_number": 0, "entities": [], "id": "p_86"}, "sentences": [{"end": 31229, "text": "Note that the columns of GOCM are composed of the orthogonal bases for the subspace", "rank": 243, "start": 31146, "IsComparative": "0", "id": "st_243"}]}, {"paragraph_info": {"end": 31346, "start": 31229, "text": "spanned by the centroids, and l = k in this case.Finally, OCM achieves trace(GTOCM SbGOCM ) = trace(Sb), where l = k.", "rank": 87, "paragraph_comparative_number": 0, "entities": [], "id": "p_87"}, "sentences": [{"end": 31278, "text": "spanned by the centroids, and l = k in this case.", "rank": 244, "start": 31229, "IsComparative": "0", "id": "st_244"}, {"end": 31346, "text": "Finally, OCM achieves trace(GTOCM SbGOCM ) = trace(Sb), where l = k.", "rank": 245, "start": 31278, "IsComparative": "0", "id": "st_245"}]}, {"paragraph_info": {"end": 31533, "start": 31346, "text": "By using the equivalence between Eqs.(3) and (4), one can prove that each pair- wise distance between cluster centroids is also preserved in the reduced dimensional space obtained by OCM.", "rank": 88, "paragraph_comparative_number": 0, "entities": [], "id": "p_88"}, "sentences": [{"end": 31383, "text": "By using the equivalence between Eqs.", "rank": 246, "start": 31346, "IsComparative": "0", "id": "st_246"}, {"end": 31533, "text": "(3) and (4), one can prove that each pair- wise distance between cluster centroids is also preserved in the reduced dimensional space obtained by OCM.", "rank": 247, "start": 31383, "IsComparative": "0", "id": "st_247"}]}, {"paragraph_info": {"end": 31869, "start": 31533, "text": "Another important property of OCM is that by projecting data into the subspace spanned by the centroids, the order of similarities between any particular point and centroids are preserved in terms of Euclidean norm and cosine similarity measure <71, 67>.In other words, for any vector q  Rm1 and cluster centroids c(i) and c(j), we have", "rank": 89, "paragraph_comparative_number": 1, "entities": [], "id": "p_89"}, "sentences": [{"end": 31787, "text": "Another important property of OCM is that by projecting data into the subspace spanned by the centroids, the order of similarities between any particular point and centroids are preserved in terms of Euclidean norm and cosine similarity measure <71, 67>.", "rank": 248, "start": 31533, "IsComparative": "0", "id": "st_248"}, {"end": 31869, "text": "In other words, for any vector q  Rm1 and cluster centroids c(i) and c(j), we have", "rank": 249, "start": 31787, "IsComparative": "1", "id": "st_249"}]}, {"paragraph_info": {"end": 31909, "start": 31869, "text": "2.3.3 Principal Component Analysis (PCA)", "rank": 90, "paragraph_comparative_number": 0, "entities": [], "id": "p_90"}, "sentences": [{"end": 31909, "text": "2.3.3 Principal Component Analysis (PCA)", "rank": 250, "start": 31869, "IsComparative": "0", "id": "st_250"}]}, {"paragraph_info": {"end": 32041, "start": 31909, "text": "PCA is a well-known dimension reduction method that captures the maximal variance in the data.The criterion of PCA can be written as", "rank": 91, "paragraph_comparative_number": 1, "entities": [], "id": "p_91"}, "sentences": [{"end": 32003, "text": "PCA is a well-known dimension reduction method that captures the maximal variance in the data.", "rank": 251, "start": 31909, "IsComparative": "1", "id": "st_251"}, {"end": 32041, "text": "The criterion of PCA can be written as", "rank": 252, "start": 32003, "IsComparative": "0", "id": "st_252"}]}, {"paragraph_info": {"end": 32078, "start": 32041, "text": "Jt(G) = max trace(GT StG).(21) GT G=I", "rank": 92, "paragraph_comparative_number": 0, "entities": [], "id": "p_92"}, "sentences": [{"end": 32067, "text": "Jt(G) = max trace(GT StG).", "rank": 253, "start": 32041, "IsComparative": "0", "id": "st_253"}, {"end": 32078, "text": "(21) GT G=I", "rank": 254, "start": 32067, "IsComparative": "0", "id": "st_254"}]}, {"paragraph_info": {"end": 32117, "start": 32078, "text": "ForanyGRml wherem>lsuchthatGTG=I,wehave", "rank": 93, "paragraph_comparative_number": 0, "entities": [], "id": "p_93"}, "sentences": [{"end": 32117, "text": "ForanyGRml wherem>lsuchthatGTG=I,wehave", "rank": 255, "start": 32078, "IsComparative": "0", "id": "st_255"}]}, {"paragraph_info": {"end": 32147, "start": 32117, "text": "trace(GT StG)  trace(St), (22)", "rank": 94, "paragraph_comparative_number": 0, "entities": [], "id": "p_94"}, "sentences": [{"end": 32147, "text": "trace(GT StG)  trace(St), (22)", "rank": 256, "start": 32117, "IsComparative": "0", "id": "st_256"}]}, {"paragraph_info": {"end": 32535, "start": 32147, "text": "which means trace(St) cannot be increased after dimension reduction.The solution of Eq.(21), where we denote it as GPCA, can be obtained by setting the columns of G as the leading eigenvectors of St.Since the rank of St is at most min(m, n), PCA achieves the upper bound of trace(GTStG) in Eq.(22) for any l such that l  min(m, n), i.e., trace(GTP CAStGP CA) = trace(St) for l  min(m, n).", "rank": 95, "paragraph_comparative_number": 1, "entities": [], "id": "p_95"}, "sentences": [{"end": 32215, "text": "which means trace(St) cannot be increased after dimension reduction.", "rank": 257, "start": 32147, "IsComparative": "0", "id": "st_257"}, {"end": 32234, "text": "The solution of Eq.", "rank": 258, "start": 32215, "IsComparative": "0", "id": "st_258"}, {"end": 32346, "text": "(21), where we denote it as GPCA, can be obtained by setting the columns of G as the leading eigenvectors of St.", "rank": 259, "start": 32234, "IsComparative": "0", "id": "st_259"}, {"end": 32440, "text": "Since the rank of St is at most min(m, n), PCA achieves the upper bound of trace(GTStG) in Eq.", "rank": 260, "start": 32346, "IsComparative": "1", "id": "st_260"}, {"end": 32535, "text": "(22) for any l such that l  min(m, n), i.e., trace(GTP CAStGP CA) = trace(St) for l  min(m, n).", "rank": 261, "start": 32440, "IsComparative": "0", "id": "st_261"}]}, {"paragraph_info": {"end": 32828, "start": 32535, "text": "In many applications of PCA, however, l is usually chosen as a fixed value less than the ranke of St for the purpose of dimension reduction or noise reduction.This noisy subspace corresponds to the smallest eigenvectors of St, and they are removed by PCA for better representation of the data.", "rank": 96, "paragraph_comparative_number": 1, "entities": [], "id": "p_96"}, "sentences": [{"end": 32694, "text": "In many applications of PCA, however, l is usually chosen as a fixed value less than the ranke of St for the purpose of dimension reduction or noise reduction.", "rank": 262, "start": 32535, "IsComparative": "0", "id": "st_262"}, {"end": 32828, "text": "This noisy subspace corresponds to the smallest eigenvectors of St, and they are removed by PCA for better representation of the data.", "rank": 263, "start": 32694, "IsComparative": "1", "id": "st_263"}]}, {"paragraph_info": {"end": 33110, "start": 32828, "text": "Although St is related to Sb and Sw as in Eq.(6), St as it is does not contain any information on cluster labels.That is, unlike LDA and OCM, PCA ignores the cluster structure represented by Sb and/or Sw, which is why PCA is considered as an unsupervised dimension reduction method.", "rank": 97, "paragraph_comparative_number": 1, "entities": [], "id": "p_97"}, "sentences": [{"end": 32873, "text": "Although St is related to Sb and Sw as in Eq.", "rank": 264, "start": 32828, "IsComparative": "0", "id": "st_264"}, {"end": 32941, "text": "(6), St as it is does not contain any information on cluster labels.", "rank": 265, "start": 32873, "IsComparative": "1", "id": "st_265"}, {"end": 33110, "text": "That is, unlike LDA and OCM, PCA ignores the cluster structure represented by Sb and/or Sw, which is why PCA is considered as an unsupervised dimension reduction method.", "rank": 266, "start": 32941, "IsComparative": "0", "id": "st_266"}]}, {"paragraph_info": {"end": 33341, "start": 33110, "text": "Usually, PCA assumes that the global centroid is zero by subtracting the empirical mean of the data from each data vector.The centered data can be represented as A  ceT , where e is n-dimensional vector whose components are all 1s.", "rank": 98, "paragraph_comparative_number": 1, "entities": [], "id": "p_98"}, "sentences": [{"end": 33232, "text": "Usually, PCA assumes that the global centroid is zero by subtracting the empirical mean of the data from each data vector.", "rank": 267, "start": 33110, "IsComparative": "1", "id": "st_267"}, {"end": 33341, "text": "The centered data can be represented as A  ceT , where e is n-dimensional vector whose components are all 1s.", "rank": 268, "start": 33232, "IsComparative": "0", "id": "st_268"}]}, {"paragraph_info": {"end": 33503, "start": 33341, "text": "PCA has a unique property that, given a fixed l, it produces the best reduced dimensional representation that minimizes the difference between the centered matrix", "rank": 99, "paragraph_comparative_number": 0, "entities": [], "id": "p_99"}, "sentences": [{"end": 33503, "text": "PCA has a unique property that, given a fixed l, it produces the best reduced dimensional representation that minimizes the difference between the centered matrix", "rank": 269, "start": 33341, "IsComparative": "0", "id": "st_269"}]}, {"paragraph_info": {"end": 33598, "start": 33503, "text": "AceT anditsprojectiontothereduceddimensionalspaceGGT(AceT)whereG has orthonormal columns, i.e.,", "rank": 100, "paragraph_comparative_number": 0, "entities": [], "id": "p_100"}, "sentences": [{"end": 33598, "text": "AceT anditsprojectiontothereduceddimensionalspaceGGT(AceT)whereG has orthonormal columns, i.e.,", "rank": 270, "start": 33503, "IsComparative": "0", "id": "st_270"}]}, {"paragraph_info": {"end": 33639, "start": 33598, "text": "GPCA =arg min GGT(AceT)(AceT), G, GT G=Il", "rank": 101, "paragraph_comparative_number": 0, "entities": [], "id": "p_101"}, "sentences": [{"end": 33639, "text": "GPCA =arg min GGT(AceT)(AceT), G, GT G=Il", "rank": 271, "start": 33598, "IsComparative": "0", "id": "st_271"}]}, {"paragraph_info": {"end": 33765, "start": 33639, "text": "where the matrix norm    is either a Frobenius norm or a Euclidean norm.The three discussed methods are summarized in Table 1.", "rank": 102, "paragraph_comparative_number": 0, "entities": [], "id": "p_102"}, "sentences": [{"end": 33711, "text": "where the matrix norm    is either a Frobenius norm or a Euclidean norm.", "rank": 272, "start": 33639, "IsComparative": "0", "id": "st_272"}, {"end": 33765, "text": "The three discussed methods are summarized in Table 1.", "rank": 273, "start": 33711, "IsComparative": "0", "id": "st_273"}]}, {"paragraph_info": {"end": 33823, "start": 33765, "text": "2.4 Formulation of Two-stage Framework for Visualiza- tion", "rank": 103, "paragraph_comparative_number": 0, "entities": [], "id": "p_103"}, "sentences": [{"end": 33823, "text": "2.4 Formulation of Two-stage Framework for Visualiza- tion", "rank": 274, "start": 33765, "IsComparative": "0", "id": "st_274"}]}, {"paragraph_info": {"end": 33986, "start": 33823, "text": "Suppose we want to find a dimension reducing linear transformation V T  R2m that maps an m-dimensional data vector x to a vector z in a 2-dimensional space (m  2):", "rank": 104, "paragraph_comparative_number": 0, "entities": [], "id": "p_104"}, "sentences": [{"end": 33986, "text": "Suppose we want to find a dimension reducing linear transformation V T  R2m that maps an m-dimensional data vector x to a vector z in a 2-dimensional space (m  2):", "rank": 275, "start": 33823, "IsComparative": "0", "id": "st_275"}]}, {"paragraph_info": {"end": 34008, "start": 33986, "text": "VT :xRm1 z=VTxR21.(23)", "rank": 105, "paragraph_comparative_number": 1, "entities": [], "id": "p_105"}, "sentences": [{"end": 34004, "text": "VT :xRm1 z=VTxR21.", "rank": 276, "start": 33986, "IsComparative": "0", "id": "st_276"}, {"end": 34008, "text": "(23)", "rank": 277, "start": 34004, "IsComparative": "1", "id": "st_277"}]}, {"paragraph_info": {"end": 34247, "start": 34008, "text": "Further assume that it is composed of two stages of dimension reductions as follows.In the first stage, a dimension reducing linear transformation GT  Rlm maps an m-dimensional data vector x to a vector y in the l-dimensional space (l  m):", "rank": 106, "paragraph_comparative_number": 1, "entities": [], "id": "p_106"}, "sentences": [{"end": 34092, "text": "Further assume that it is composed of two stages of dimension reductions as follows.", "rank": 278, "start": 34008, "IsComparative": "1", "id": "st_278"}, {"end": 34247, "text": "In the first stage, a dimension reducing linear transformation GT  Rlm maps an m-dimensional data vector x to a vector y in the l-dimensional space (l  m):", "rank": 279, "start": 34092, "IsComparative": "0", "id": "st_279"}]}, {"paragraph_info": {"end": 34270, "start": 34247, "text": "GT :xRm1 y=GTxRl1, (24)", "rank": 107, "paragraph_comparative_number": 0, "entities": [], "id": "p_107"}, "sentences": [{"end": 34270, "text": "GT :xRm1 y=GTxRl1, (24)", "rank": 280, "start": 34247, "IsComparative": "0", "id": "st_280"}]}, {"paragraph_info": {"end": 34533, "start": 34270, "text": "where l is fixed as its minimum optimal dimension by the first-stage criterion.When l  2, we have no further dimension reduction to do after the first step.However, an optimal l in many methods and for many data sets is larger than 2, and so we assume that l > 2.", "rank": 108, "paragraph_comparative_number": 2, "entities": [], "id": "p_108"}, "sentences": [{"end": 34349, "text": "where l is fixed as its minimum optimal dimension by the first-stage criterion.", "rank": 281, "start": 34270, "IsComparative": "0", "id": "st_281"}, {"end": 34426, "text": "When l  2, we have no further dimension reduction to do after the first step.", "rank": 282, "start": 34349, "IsComparative": "1", "id": "st_282"}, {"end": 34533, "text": "However, an optimal l in many methods and for many data sets is larger than 2, and so we assume that l > 2.", "rank": 283, "start": 34426, "IsComparative": "1", "id": "st_283"}]}, {"paragraph_info": {"end": 34695, "start": 34533, "text": "In the second stage, another dimension reducing linear transformation HT  R2l maps an l-dimensional data vector y to a vector z in the 2-dimensional space(l > 2):", "rank": 109, "paragraph_comparative_number": 0, "entities": [], "id": "p_109"}, "sentences": [{"end": 34695, "text": "In the second stage, another dimension reducing linear transformation HT  R2l maps an l-dimensional data vector y to a vector z in the 2-dimensional space(l > 2):", "rank": 284, "start": 34533, "IsComparative": "0", "id": "st_284"}]}, {"paragraph_info": {"end": 34793, "start": 34695, "text": "HT :yRl1 z=HTyR21.(25) Such consecutive dimension reductions performed by GT followed by HT can be", "rank": 110, "paragraph_comparative_number": 0, "entities": [], "id": "p_110"}, "sentences": [{"end": 34713, "text": "HT :yRl1 z=HTyR21.", "rank": 285, "start": 34695, "IsComparative": "0", "id": "st_285"}, {"end": 34793, "text": "(25) Such consecutive dimension reductions performed by GT followed by HT can be", "rank": 286, "start": 34713, "IsComparative": "0", "id": "st_286"}]}, {"paragraph_info": {"end": 34865, "start": 34793, "text": "combined, resulting in a single dimension reducing transformation V T as", "rank": 111, "paragraph_comparative_number": 0, "entities": [], "id": "p_111"}, "sentences": [{"end": 34865, "text": "combined, resulting in a single dimension reducing transformation V T as", "rank": 287, "start": 34793, "IsComparative": "0", "id": "st_287"}]}, {"paragraph_info": {"end": 34878, "start": 34865, "text": "VT =HTGT.(26)", "rank": 112, "paragraph_comparative_number": 1, "entities": [], "id": "p_112"}, "sentences": [{"end": 34874, "text": "VT =HTGT.", "rank": 288, "start": 34865, "IsComparative": "0", "id": "st_288"}, {"end": 34878, "text": "(26)", "rank": 289, "start": 34874, "IsComparative": "1", "id": "st_289"}]}, {"paragraph_info": {"end": 35217, "start": 34878, "text": "In the next section, discussion will be focused on various ways for choosing the first stage dimension reducing transformation G and the second stage dimension transfor- mation H with a purpose to construct combined dimension reducing transformation V T = HT GT for 2-dimensional visualization according to various optimization crite- ria.", "rank": 113, "paragraph_comparative_number": 0, "entities": [], "id": "p_113"}, "sentences": [{"end": 35217, "text": "In the next section, discussion will be focused on various ways for choosing the first stage dimension reducing transformation G and the second stage dimension transfor- mation H with a purpose to construct combined dimension reducing transformation V T = HT GT for 2-dimensional visualization according to various optimization crite- ria.", "rank": 290, "start": 34878, "IsComparative": "0", "id": "st_290"}]}, {"paragraph_info": {"end": 35259, "start": 35217, "text": "2.5 Two-stage Methods for 2D Visualization", "rank": 114, "paragraph_comparative_number": 0, "entities": [], "id": "p_114"}, "sentences": [{"end": 35259, "text": "2.5 Two-stage Methods for 2D Visualization", "rank": 291, "start": 35217, "IsComparative": "0", "id": "st_291"}]}, {"paragraph_info": {"end": 36066, "start": 35259, "text": "All the proposed two-stage methods start from one of the supervised dimension re-duction methods such as LDA or OCM that are designed for clustered data.In the first stage (by GT  Rlm in Eq.(24)), the dimension is reduced by LDA or OCM to the smallest dimension that satisfies Eq.(17) or (20), respectively.Therefore in the first stage, the cluster structure quality measured either by trace(S1S ) or trace(S ) wbb is preserved.Then we perform the second-stage dimension reduction (by HT  R2l in Eq.(25)) that minimizes the loss of information either by applying the same cri- terion used in the first stage or by using Jt in Eq.(21), i.e., that of PCA.As seen in Section 3.3, Eq.(21) gives the best approximation of the first-stage results that minimize the difference in terms of Frobenius/Euclidean norm.", "rank": 115, "paragraph_comparative_number": 2, "entities": [], "id": "p_115"}, "sentences": [{"end": 35412, "text": "All the proposed two-stage methods start from one of the supervised dimension re-duction methods such as LDA or OCM that are designed for clustered data.", "rank": 292, "start": 35259, "IsComparative": "0", "id": "st_292"}, {"end": 35449, "text": "In the first stage (by GT  Rlm in Eq.", "rank": 293, "start": 35412, "IsComparative": "0", "id": "st_293"}, {"end": 35539, "text": "(24)), the dimension is reduced by LDA or OCM to the smallest dimension that satisfies Eq.", "rank": 294, "start": 35449, "IsComparative": "0", "id": "st_294"}, {"end": 35566, "text": "(17) or (20), respectively.", "rank": 295, "start": 35539, "IsComparative": "1", "id": "st_295"}, {"end": 35687, "text": "Therefore in the first stage, the cluster structure quality measured either by trace(S1S ) or trace(S ) wbb is preserved.", "rank": 296, "start": 35566, "IsComparative": "0", "id": "st_296"}, {"end": 35758, "text": "Then we perform the second-stage dimension reduction (by HT  R2l in Eq.", "rank": 297, "start": 35687, "IsComparative": "0", "id": "st_297"}, {"end": 35888, "text": "(25)) that minimizes the loss of information either by applying the same cri- terion used in the first stage or by using Jt in Eq.", "rank": 298, "start": 35758, "IsComparative": "0", "id": "st_298"}, {"end": 35912, "text": "(21), i.e., that of PCA.", "rank": 299, "start": 35888, "IsComparative": "0", "id": "st_299"}, {"end": 35939, "text": "As seen in Section 3.3, Eq.", "rank": 300, "start": 35912, "IsComparative": "0", "id": "st_300"}, {"end": 36066, "text": "(21) gives the best approximation of the first-stage results that minimize the difference in terms of Frobenius/Euclidean norm.", "rank": 301, "start": 35939, "IsComparative": "1", "id": "st_301"}]}, {"paragraph_info": {"end": 36229, "start": 36066, "text": "In what follows, we describe each of the two-stage methods in detail, and derive their equivalent single-stage methods (by V T  R2m in Eq.(23)) in case they exist.", "rank": 116, "paragraph_comparative_number": 1, "entities": [], "id": "p_116"}, "sentences": [{"end": 36204, "text": "In what follows, we describe each of the two-stage methods in detail, and derive their equivalent single-stage methods (by V T  R2m in Eq.", "rank": 302, "start": 36066, "IsComparative": "1", "id": "st_302"}, {"end": 36229, "text": "(23)) in case they exist.", "rank": 303, "start": 36204, "IsComparative": "0", "id": "st_303"}]}, {"paragraph_info": {"end": 36245, "start": 36229, "text": "2.5.1 Rank-2 LDA", "rank": 117, "paragraph_comparative_number": 0, "entities": [], "id": "p_117"}, "sentences": [{"end": 36245, "text": "2.5.1 Rank-2 LDA", "rank": 304, "start": 36229, "IsComparative": "0", "id": "st_304"}]}, {"paragraph_info": {"end": 36334, "start": 36245, "text": "In this method, LDA is applied in the first stage, and trace(S1S ) is preserved in the wb", "rank": 118, "paragraph_comparative_number": 0, "entities": [], "id": "p_118"}, "sentences": [{"end": 36334, "text": "In this method, LDA is applied in the first stage, and trace(S1S ) is preserved in the wb", "rank": 305, "start": 36245, "IsComparative": "0", "id": "st_305"}]}, {"paragraph_info": {"end": 36495, "start": 36334, "text": "l-dimensional space where l = k  1.In the second stage, the same criterion Jb/w(H) is used to reduce the l-dimensional first-stage results to 2-dimensional data.", "rank": 119, "paragraph_comparative_number": 0, "entities": [], "id": "p_119"}, "sentences": [{"end": 36369, "text": "l-dimensional space where l = k  1.", "rank": 306, "start": 36334, "IsComparative": "0", "id": "st_306"}, {"end": 36495, "text": "In the second stage, the same criterion Jb/w(H) is used to reduce the l-dimensional first-stage results to 2-dimensional data.", "rank": 307, "start": 36369, "IsComparative": "0", "id": "st_307"}]}, {"paragraph_info": {"end": 36574, "start": 36495, "text": "The criterion of the second-stage dimension reducing matrix H can be formulated", "rank": 120, "paragraph_comparative_number": 0, "entities": [], "id": "p_120"}, "sentences": [{"end": 36574, "text": "The criterion of the second-stage dimension reducing matrix H can be formulated", "rank": 308, "start": 36495, "IsComparative": "0", "id": "st_308"}]}, {"paragraph_info": {"end": 36719, "start": 36574, "text": "where u1and u2 are the leading generalized eigenvectors of Eq.(16).This solution is also known as reduced-rank linear discriminant analysis <66>.", "rank": 121, "paragraph_comparative_number": 1, "entities": [], "id": "p_121"}, "sentences": [{"end": 36636, "text": "where u1and u2 are the leading generalized eigenvectors of Eq.", "rank": 309, "start": 36574, "IsComparative": "0", "id": "st_309"}, {"end": 36641, "text": "(16).", "rank": 310, "start": 36636, "IsComparative": "1", "id": "st_310"}, {"end": 36719, "text": "This solution is also known as reduced-rank linear discriminant analysis <66>.", "rank": 311, "start": 36641, "IsComparative": "0", "id": "st_311"}]}, {"paragraph_info": {"end": 36744, "start": 36719, "text": "2.5.2 LDA followed by PCA", "rank": 122, "paragraph_comparative_number": 0, "entities": [], "id": "p_122"}, "sentences": [{"end": 36744, "text": "2.5.2 LDA followed by PCA", "rank": 312, "start": 36719, "IsComparative": "0", "id": "st_312"}]}, {"paragraph_info": {"end": 37024, "start": 36744, "text": "In this method, LDA is applied in the first stage, and trace(S1Sb) is preserved in w the l-dimensional space where l = k  1.In the second stage, PCA is applied in order to obtain the best approximation of the l-dimensional first-stage results in terms of Frobenius/Euclidean norm.", "rank": 123, "paragraph_comparative_number": 0, "entities": [], "id": "p_123"}, "sentences": [{"end": 36868, "text": "In this method, LDA is applied in the first stage, and trace(S1Sb) is preserved in w the l-dimensional space where l = k  1.", "rank": 313, "start": 36744, "IsComparative": "0", "id": "st_313"}, {"end": 37024, "text": "In the second stage, PCA is applied in order to obtain the best approximation of the l-dimensional first-stage results in terms of Frobenius/Euclidean norm.", "rank": 314, "start": 36868, "IsComparative": "0", "id": "st_314"}]}, {"paragraph_info": {"end": 37091, "start": 37024, "text": "The second-stage dimension reducing matrix H is obtained by solving", "rank": 124, "paragraph_comparative_number": 0, "entities": [], "id": "p_124"}, "sentences": [{"end": 37091, "text": "The second-stage dimension reducing matrix H is obtained by solving", "rank": 315, "start": 37024, "IsComparative": "0", "id": "st_315"}]}, {"paragraph_info": {"end": 37116, "start": 37091, "text": "2.5.3 OCM followed by PCA", "rank": 125, "paragraph_comparative_number": 0, "entities": [], "id": "p_125"}, "sentences": [{"end": 37116, "text": "2.5.3 OCM followed by PCA", "rank": 316, "start": 37091, "IsComparative": "0", "id": "st_316"}]}, {"paragraph_info": {"end": 37389, "start": 37116, "text": "In this method, OCM is applied in the first stage, and trace(Sb) is preserved in the l-dimensional space where l = k.In the second stage, PCA is applied in order to obtain the best approximation of the l-dimensional first-stage results in terms of Frobenius/Euclidean norm.", "rank": 126, "paragraph_comparative_number": 0, "entities": [], "id": "p_126"}, "sentences": [{"end": 37233, "text": "In this method, OCM is applied in the first stage, and trace(Sb) is preserved in the l-dimensional space where l = k.", "rank": 317, "start": 37116, "IsComparative": "0", "id": "st_317"}, {"end": 37389, "text": "In the second stage, PCA is applied in order to obtain the best approximation of the l-dimensional first-stage results in terms of Frobenius/Euclidean norm.", "rank": 318, "start": 37233, "IsComparative": "0", "id": "st_318"}]}, {"paragraph_info": {"end": 37475, "start": 37389, "text": "As in Section 5.2, the second-stage dimension reducing matrix H is obtained by solving", "rank": 127, "paragraph_comparative_number": 0, "entities": [], "id": "p_127"}, "sentences": [{"end": 37475, "text": "As in Section 5.2, the second-stage dimension reducing matrix H is obtained by solving", "rank": 319, "start": 37389, "IsComparative": "0", "id": "st_319"}]}, {"paragraph_info": {"end": 37528, "start": 37475, "text": "Ht =arg max trace(HT(GTOCMStGOCM)H), (31) HRl2,HT H=I", "rank": 128, "paragraph_comparative_number": 0, "entities": [], "id": "p_128"}, "sentences": [{"end": 37528, "text": "Ht =arg max trace(HT(GTOCMStGOCM)H), (31) HRl2,HT H=I", "rank": 320, "start": 37475, "IsComparative": "0", "id": "st_320"}]}, {"paragraph_info": {"end": 37648, "start": 37528, "text": "where the solution is the two leading eigenvectors of the total scatter matrix of the first-stage result, GTOCM StGOCM .", "rank": 129, "paragraph_comparative_number": 0, "entities": [], "id": "p_129"}, "sentences": [{"end": 37648, "text": "where the solution is the two leading eigenvectors of the total scatter matrix of the first-stage result, GTOCM StGOCM .", "rank": 321, "start": 37528, "IsComparative": "0", "id": "st_321"}]}, {"paragraph_info": {"end": 37670, "start": 37648, "text": "2.5.4 Rank-2 PCA on Sb", "rank": 130, "paragraph_comparative_number": 0, "entities": [], "id": "p_130"}, "sentences": [{"end": 37670, "text": "2.5.4 Rank-2 PCA on Sb", "rank": 322, "start": 37648, "IsComparative": "0", "id": "st_322"}]}, {"paragraph_info": {"end": 37911, "start": 37670, "text": "In this method, OCM is applied in the first stage, and trace(Sb) is preserved in the l-dimensional space where l = k.In the second stage, the same criterion Jb(H) is used to reduce the l-dimensional first-stage results to 2-dimensional data.", "rank": 131, "paragraph_comparative_number": 0, "entities": [], "id": "p_131"}, "sentences": [{"end": 37787, "text": "In this method, OCM is applied in the first stage, and trace(Sb) is preserved in the l-dimensional space where l = k.", "rank": 323, "start": 37670, "IsComparative": "0", "id": "st_323"}, {"end": 37911, "text": "In the second stage, the same criterion Jb(H) is used to reduce the l-dimensional first-stage results to 2-dimensional data.", "rank": 324, "start": 37787, "IsComparative": "0", "id": "st_324"}]}, {"paragraph_info": {"end": 37978, "start": 37911, "text": "The second-stage dimension reducing matrix H is obtained by solving", "rank": 132, "paragraph_comparative_number": 0, "entities": [], "id": "p_132"}, "sentences": [{"end": 37978, "text": "The second-stage dimension reducing matrix H is obtained by solving", "rank": 325, "start": 37911, "IsComparative": "0", "id": "st_325"}]}, {"paragraph_info": {"end": 38019, "start": 37978, "text": "Hb =arg max trace(HT(GTOCMSbGOCM)H), (33)", "rank": 133, "paragraph_comparative_number": 0, "entities": [], "id": "p_133"}, "sentences": [{"end": 38019, "text": "Hb =arg max trace(HT(GTOCMSbGOCM)H), (33)", "rank": 326, "start": 37978, "IsComparative": "0", "id": "st_326"}]}, {"paragraph_info": {"end": 38250, "start": 38019, "text": "Eq.(35) holds since the eigenvectors of Sb, u1 and u2, are in the range space of GOCM .The criterion of Eq.(36) has been used in one of the successful visual analytic systems, IN-SPIRE, for 2D representation of document data <138>.", "rank": 134, "paragraph_comparative_number": 1, "entities": [], "id": "p_134"}, "sentences": [{"end": 38022, "text": "Eq.", "rank": 327, "start": 38019, "IsComparative": "0", "id": "st_327"}, {"end": 38106, "text": "(35) holds since the eigenvectors of Sb, u1 and u2, are in the range space of GOCM .", "rank": 328, "start": 38022, "IsComparative": "0", "id": "st_328"}, {"end": 38126, "text": "The criterion of Eq.", "rank": 329, "start": 38106, "IsComparative": "0", "id": "st_329"}, {"end": 38250, "text": "(36) has been used in one of the successful visual analytic systems, IN-SPIRE, for 2D representation of document data <138>.", "rank": 330, "start": 38126, "IsComparative": "1", "id": "st_330"}]}, {"paragraph_info": {"end": 38308, "start": 38250, "text": "The discussed two-stage methods are summarized in Table 2.", "rank": 135, "paragraph_comparative_number": 1, "entities": [], "id": "p_135"}, "sentences": [{"end": 38308, "text": "The discussed two-stage methods are summarized in Table 2.", "rank": 331, "start": 38250, "IsComparative": "1", "id": "st_331"}]}, {"paragraph_info": {"end": 38323, "start": 38308, "text": "2.6 Experiments", "rank": 136, "paragraph_comparative_number": 0, "entities": [], "id": "p_136"}, "sentences": [{"end": 38323, "text": "2.6 Experiments", "rank": 332, "start": 38308, "IsComparative": "0", "id": "st_332"}]}, {"paragraph_info": {"end": 38599, "start": 38323, "text": "In this section, we present visualization results using the proposed methods for several data sets, especially focusing on undersampled text data visualization where the data item is represented in m-dimensional space and the number of the data items n is less than m (m > n).", "rank": 137, "paragraph_comparative_number": 1, "entities": [], "id": "p_137"}, "sentences": [{"end": 38599, "text": "In this section, we present visualization results using the proposed methods for several data sets, especially focusing on undersampled text data visualization where the data item is represented in m-dimensional space and the number of the data items n is less than m (m > n).", "rank": 333, "start": 38323, "IsComparative": "1", "id": "st_333"}]}, {"paragraph_info": {"end": 38648, "start": 38599, "text": "2.6.1 Regularization on LDA for undersampled data", "rank": 138, "paragraph_comparative_number": 1, "entities": [], "id": "p_138"}, "sentences": [{"end": 38648, "text": "2.6.1 Regularization on LDA for undersampled data", "rank": 334, "start": 38599, "IsComparative": "1", "id": "st_334"}]}, {"paragraph_info": {"end": 39511, "start": 38648, "text": "In undersampled cases, the LDA criterion shown in Eq.(14) cannot be applied di- rectly because Sw is singular.In order to overcome this singularity problem, Howland et al.proposed a universal algorithmic framework of LDA using the generalized sin- gular value decomposition (LDA/GSVD) <68, 67>.Specifically, for the case when m  n  k, which is usual for most undersampled problems, LDA/GSVD gives the solution for G such that GT SwG = 0 while maintaining the maximum value of trace(GT SbG).This solution makes sense since LDA criterion is formulated to min- imize trace(GTSwG).However, it means that all of the data points belonging to a specific cluster are represented as a single point in the reduced dimensional space, which lessens the generalization ability for classification as well as for visualizing the individual data relationship within each cluster.", "rank": 139, "paragraph_comparative_number": 2, "entities": [], "id": "p_139"}, "sentences": [{"end": 38701, "text": "In undersampled cases, the LDA criterion shown in Eq.", "rank": 335, "start": 38648, "IsComparative": "0", "id": "st_335"}, {"end": 38758, "text": "(14) cannot be applied di- rectly because Sw is singular.", "rank": 336, "start": 38701, "IsComparative": "0", "id": "st_336"}, {"end": 38819, "text": "In order to overcome this singularity problem, Howland et al.", "rank": 337, "start": 38758, "IsComparative": "0", "id": "st_337"}, {"end": 38942, "text": "proposed a universal algorithmic framework of LDA using the generalized sin- gular value decomposition (LDA/GSVD) <68, 67>.", "rank": 338, "start": 38819, "IsComparative": "0", "id": "st_338"}, {"end": 39138, "text": "Specifically, for the case when m  n  k, which is usual for most undersampled problems, LDA/GSVD gives the solution for G such that GT SwG = 0 while maintaining the maximum value of trace(GT SbG).", "rank": 339, "start": 38942, "IsComparative": "1", "id": "st_339"}, {"end": 39225, "text": "This solution makes sense since LDA criterion is formulated to min- imize trace(GTSwG).", "rank": 340, "start": 39138, "IsComparative": "0", "id": "st_340"}, {"end": 39511, "text": "However, it means that all of the data points belonging to a specific cluster are represented as a single point in the reduced dimensional space, which lessens the generalization ability for classification as well as for visualizing the individual data relationship within each cluster.", "rank": 341, "start": 39225, "IsComparative": "1", "id": "st_341"}]}, {"paragraph_info": {"end": 40625, "start": 39511, "text": "On the contrary, the fact that LDA makes GTSwG = 0 can be viewed as an advantage for visualization purposes since LDA has the capability to fully minimize trace(GT Sw G).By means of regularization on Sw one can control trace(GT Sw G), which determines the scatter of the data points within each cluster.In regularized LDA which was originally proposed to avoid the singularity of Sw in classification context, Sw is replaced by a nonsingular matrix Sw + I where I is an identity matrix, and  is a control parameter.In general, as  is increased, the within- cluster distance, trace(GTSwG), also becomes larger with data points being more scattered around their corresponding centroids.As  is decreased, the within-cluster distance becomes smaller, and the data points gather closer around their centroids.Such manipulation of  can be exploited in a visualization context because one can choose an appropriate value of  so that the second-stage method such as PCA, which maximizes trace(GT StG) = trace(GT SbG + GT SwG), does not focus too much on trace(GT SwG).The results that follow are based on such choices of .", "rank": 140, "paragraph_comparative_number": 4, "entities": [], "id": "p_140"}, "sentences": [{"end": 39681, "text": "On the contrary, the fact that LDA makes GTSwG = 0 can be viewed as an advantage for visualization purposes since LDA has the capability to fully minimize trace(GT Sw G).", "rank": 342, "start": 39511, "IsComparative": "1", "id": "st_342"}, {"end": 39814, "text": "By means of regularization on Sw one can control trace(GT Sw G), which determines the scatter of the data points within each cluster.", "rank": 343, "start": 39681, "IsComparative": "1", "id": "st_343"}, {"end": 40026, "text": "In regularized LDA which was originally proposed to avoid the singularity of Sw in classification context, Sw is replaced by a nonsingular matrix Sw + I where I is an identity matrix, and  is a control parameter.", "rank": 344, "start": 39814, "IsComparative": "0", "id": "st_344"}, {"end": 40195, "text": "In general, as  is increased, the within- cluster distance, trace(GTSwG), also becomes larger with data points being more scattered around their corresponding centroids.", "rank": 345, "start": 40026, "IsComparative": "1", "id": "st_345"}, {"end": 40315, "text": "As  is decreased, the within-cluster distance becomes smaller, and the data points gather closer around their centroids.", "rank": 346, "start": 40195, "IsComparative": "1", "id": "st_346"}, {"end": 40571, "text": "Such manipulation of  can be exploited in a visualization context because one can choose an appropriate value of  so that the second-stage method such as PCA, which maximizes trace(GT StG) = trace(GT SbG + GT SwG), does not focus too much on trace(GT SwG).", "rank": 347, "start": 40315, "IsComparative": "0", "id": "st_347"}, {"end": 40625, "text": "The results that follow are based on such choices of .", "rank": 348, "start": 40571, "IsComparative": "0", "id": "st_348"}]}, {"paragraph_info": {"end": 40640, "start": 40625, "text": "2.6.2 Data Sets", "rank": 141, "paragraph_comparative_number": 1, "entities": [], "id": "p_141"}, "sentences": [{"end": 40640, "text": "2.6.2 Data Sets", "rank": 349, "start": 40625, "IsComparative": "1", "id": "st_349"}]}, {"paragraph_info": {"end": 41237, "start": 40640, "text": "The data sets tested are composed of one artificially-generated Gaussian-mixture dataset (GAUSSIAN) and three real-world text data sets (MEDLINE, NEWSGROUPS, and REUTERS) that are clustered based on their topics.All the text documents are encoded as term-document matrices where each dimension corresponds to a particular word, and the value of a certain dimension represents the frequency of the correspond- ing word shown in the document.Each data set is set to have an equal number of data per cluster, and have a mean of zero which is attained by subtracting the global mean.(See Section 6.3.)", "rank": 142, "paragraph_comparative_number": 2, "entities": [], "id": "p_142"}, "sentences": [{"end": 40852, "text": "The data sets tested are composed of one artificially-generated Gaussian-mixture dataset (GAUSSIAN) and three real-world text data sets (MEDLINE, NEWSGROUPS, and REUTERS) that are clustered based on their topics.", "rank": 350, "start": 40640, "IsComparative": "1", "id": "st_350"}, {"end": 41080, "text": "All the text documents are encoded as term-document matrices where each dimension corresponds to a particular word, and the value of a certain dimension represents the frequency of the correspond- ing word shown in the document.", "rank": 351, "start": 40852, "IsComparative": "1", "id": "st_351"}, {"end": 41219, "text": "Each data set is set to have an equal number of data per cluster, and have a mean of zero which is attained by subtracting the global mean.", "rank": 352, "start": 41080, "IsComparative": "0", "id": "st_352"}, {"end": 41237, "text": "(See Section 6.3.)", "rank": 353, "start": 41219, "IsComparative": "0", "id": "st_353"}]}, {"paragraph_info": {"end": 41321, "start": 41237, "text": "The descriptions of data sets, which are also summarized in Table 3, are as follows.", "rank": 143, "paragraph_comparative_number": 0, "entities": [], "id": "p_143"}, "sentences": [{"end": 41321, "text": "The descriptions of data sets, which are also summarized in Table 3, are as follows.", "rank": 354, "start": 41237, "IsComparative": "0", "id": "st_354"}]}, {"paragraph_info": {"end": 41648, "start": 41321, "text": "The GAUSSIAN data set is a randomly generated Gaussian mixture with 10 clusters.Each cluster is made up of 100 data vectors, which add up to 1000 in total, and the dimension is set to 1100, which is slightly more than the number of the data items.In its visualization shown in Fig.(1), the clusters are labeled using letters as", "rank": 144, "paragraph_comparative_number": 2, "entities": [], "id": "p_144"}, "sentences": [{"end": 41401, "text": "The GAUSSIAN data set is a randomly generated Gaussian mixture with 10 clusters.", "rank": 355, "start": 41321, "IsComparative": "1", "id": "st_355"}, {"end": 41568, "text": "Each cluster is made up of 100 data vectors, which add up to 1000 in total, and the dimension is set to 1100, which is slightly more than the number of the data items.", "rank": 356, "start": 41401, "IsComparative": "1", "id": "st_356"}, {"end": 41602, "text": "In its visualization shown in Fig.", "rank": 357, "start": 41568, "IsComparative": "0", "id": "st_357"}, {"end": 41648, "text": "(1), the clusters are labeled using letters as", "rank": 358, "start": 41602, "IsComparative": "0", "id": "st_358"}]}, {"paragraph_info": {"end": 41725, "start": 41648, "text": "The MEDLINE data set is a document corpus related to medical science from the", "rank": 145, "paragraph_comparative_number": 1, "entities": [], "id": "p_145"}, "sentences": [{"end": 41725, "text": "The MEDLINE data set is a document corpus related to medical science from the", "rank": 359, "start": 41648, "IsComparative": "1", "id": "st_359"}]}, {"paragraph_info": {"end": 41929, "start": 41725, "text": "National Institutes of Health1.The original dimension is 22095, and the number of clusters is 5, where each cluster has 100 documents.The cluster labels that correspond to the document topics are shown as", "rank": 146, "paragraph_comparative_number": 1, "entities": [], "id": "p_146"}, "sentences": [{"end": 41756, "text": "National Institutes of Health1.", "rank": 360, "start": 41725, "IsComparative": "0", "id": "st_360"}, {"end": 41859, "text": "The original dimension is 22095, and the number of clusters is 5, where each cluster has 100 documents.", "rank": 361, "start": 41756, "IsComparative": "1", "id": "st_361"}, {"end": 41929, "text": "The cluster labels that correspond to the document topics are shown as", "rank": 362, "start": 41859, "IsComparative": "0", "id": "st_362"}]}, {"paragraph_info": {"end": 42016, "start": 41929, "text": "heart attack (h), colon cancer (c), diabetes (d), oral cancer (o), and tooth decay (t),", "rank": 147, "paragraph_comparative_number": 0, "entities": [], "id": "p_147"}, "sentences": [{"end": 42016, "text": "heart attack (h), colon cancer (c), diabetes (d), oral cancer (o), and tooth decay (t),", "rank": 363, "start": 41929, "IsComparative": "0", "id": "st_363"}]}, {"paragraph_info": {"end": 42096, "start": 42016, "text": "where the letters in parentheses are used in the visualization shown in Fig.(2).", "rank": 148, "paragraph_comparative_number": 1, "entities": [], "id": "p_148"}, "sentences": [{"end": 42092, "text": "where the letters in parentheses are used in the visualization shown in Fig.", "rank": 364, "start": 42016, "IsComparative": "0", "id": "st_364"}, {"end": 42096, "text": "(2).", "rank": 365, "start": 42092, "IsComparative": "1", "id": "st_365"}]}, {"paragraph_info": {"end": 42336, "start": 42096, "text": "The NEWSGROUPS data set <11> is a collection of newsgroup documents, and originally composed of 20 topics.However, we have chosen 11 topics for visualization, and each cluster is set to have 70 documents.The original dimension is 16702, and", "rank": 149, "paragraph_comparative_number": 2, "entities": [], "id": "p_149"}, "sentences": [{"end": 42202, "text": "The NEWSGROUPS data set <11> is a collection of newsgroup documents, and originally composed of 20 topics.", "rank": 366, "start": 42096, "IsComparative": "1", "id": "st_366"}, {"end": 42300, "text": "However, we have chosen 11 topics for visualization, and each cluster is set to have 70 documents.", "rank": 367, "start": 42202, "IsComparative": "1", "id": "st_367"}, {"end": 42336, "text": "The original dimension is 16702, and", "rank": 368, "start": 42300, "IsComparative": "0", "id": "st_368"}]}, {"paragraph_info": {"end": 42367, "start": 42336, "text": "the cluster labels are shown as", "rank": 150, "paragraph_comparative_number": 0, "entities": [], "id": "p_150"}, "sentences": [{"end": 42367, "text": "the cluster labels are shown as", "rank": 369, "start": 42336, "IsComparative": "0", "id": "st_369"}]}, {"paragraph_info": {"end": 42612, "start": 42367, "text": "comp.sys.ibm.pc.hardware(p),comp.sys.mac.hardware(a),misc.forsale(f),rec.sport.baseball (b), sci crypt (y), sci.electronics (e), sci.med (d), soc.religion.christian (c), talk.politics.guns (g), talk.politics.misc (p), and talk.religion.misc (r),", "rank": 151, "paragraph_comparative_number": 0, "entities": [], "id": "p_151"}, "sentences": [{"end": 42612, "text": "comp.sys.ibm.pc.hardware(p),comp.sys.mac.hardware(a),misc.forsale(f),rec.sport.baseball (b), sci crypt (y), sci.electronics (e), sci.med (d), soc.religion.christian (c), talk.politics.guns (g), talk.politics.misc (p), and talk.religion.misc (r),", "rank": 370, "start": 42367, "IsComparative": "0", "id": "st_370"}]}, {"paragraph_info": {"end": 42692, "start": 42612, "text": "where the letters in parentheses are used in the visualization shown in Fig.(3).", "rank": 152, "paragraph_comparative_number": 1, "entities": [], "id": "p_152"}, "sentences": [{"end": 42688, "text": "where the letters in parentheses are used in the visualization shown in Fig.", "rank": 371, "start": 42612, "IsComparative": "0", "id": "st_371"}, {"end": 42692, "text": "(3).", "rank": 372, "start": 42688, "IsComparative": "1", "id": "st_372"}]}, {"paragraph_info": {"end": 43011, "start": 42692, "text": "The REUTERS data set <11> is the document collection that appeared in the Reuters newswire in 1987, and originally composed of hundreds of topics.Among them, 10 topics related to economic subjects are chosen for visualization, and each cluster has 80 documents.The original dimension is 3907, and the cluster labels are", "rank": 153, "paragraph_comparative_number": 1, "entities": [], "id": "p_153"}, "sentences": [{"end": 42838, "text": "The REUTERS data set <11> is the document collection that appeared in the Reuters newswire in 1987, and originally composed of hundreds of topics.", "rank": 373, "start": 42692, "IsComparative": "0", "id": "st_373"}, {"end": 42953, "text": "Among them, 10 topics related to economic subjects are chosen for visualization, and each cluster has 80 documents.", "rank": 374, "start": 42838, "IsComparative": "1", "id": "st_374"}, {"end": 43011, "text": "The original dimension is 3907, and the cluster labels are", "rank": 375, "start": 42953, "IsComparative": "0", "id": "st_375"}]}, {"paragraph_info": {"end": 43019, "start": 43011, "text": "shown as", "rank": 154, "paragraph_comparative_number": 0, "entities": [], "id": "p_154"}, "sentences": [{"end": 43019, "text": "shown as", "rank": 376, "start": 43011, "IsComparative": "0", "id": "st_376"}]}, {"paragraph_info": {"end": 43142, "start": 43019, "text": "earn (e), acquisitions (a), money-fx (m), grain (g), crude (r), trade (t), interest (i), ship (s), wheat (w), and corn (c),", "rank": 155, "paragraph_comparative_number": 0, "entities": [], "id": "p_155"}, "sentences": [{"end": 43142, "text": "earn (e), acquisitions (a), money-fx (m), grain (g), crude (r), trade (t), interest (i), ship (s), wheat (w), and corn (c),", "rank": 377, "start": 43019, "IsComparative": "0", "id": "st_377"}]}, {"paragraph_info": {"end": 43222, "start": 43142, "text": "where the letters in parentheses are used in the visualization shown in Fig.(4).", "rank": 156, "paragraph_comparative_number": 1, "entities": [], "id": "p_156"}, "sentences": [{"end": 43218, "text": "where the letters in parentheses are used in the visualization shown in Fig.", "rank": 378, "start": 43142, "IsComparative": "0", "id": "st_378"}, {"end": 43222, "text": "(4).", "rank": 379, "start": 43218, "IsComparative": "1", "id": "st_379"}]}, {"paragraph_info": {"end": 43253, "start": 43222, "text": "2.6.3 Effects of Data Centering", "rank": 157, "paragraph_comparative_number": 0, "entities": [], "id": "p_157"}, "sentences": [{"end": 43253, "text": "2.6.3 Effects of Data Centering", "rank": 380, "start": 43222, "IsComparative": "0", "id": "st_380"}]}, {"paragraph_info": {"end": 44258, "start": 43253, "text": "Fig.5 is the example of applying OCM+PCA to the MEDLINE data set with and without data centering.Once the MEDLINE data set is encoded as a term-document matrix, every component has a non-negative value, which results in the global centroid that is significantly far from the origin.Then performing PCA without data centering might give the first principal axis as the one reflecting the global centroid rather than that discriminating clusters.If we consider projecting the data onto each of the horizontal and the vertical axes in Fig.5, the former, which corresponds to the first principal axis, does not help in showing the cluster structure clearly, and only the vertical axis, which corresponds to the second principal axis from PCA, discriminates clusters.We have found that such undesirable behavior is common in many cases without data centering, which is why we assume that data is centered throughout this chapter.Accordingly, all the results shown in Figs 1-4 are obtained after data centering.", "rank": 158, "paragraph_comparative_number": 2, "entities": [], "id": "p_158"}, "sentences": [{"end": 43257, "text": "Fig.", "rank": 381, "start": 43253, "IsComparative": "0", "id": "st_381"}, {"end": 43350, "text": "5 is the example of applying OCM+PCA to the MEDLINE data set with and without data centering.", "rank": 382, "start": 43257, "IsComparative": "1", "id": "st_382"}, {"end": 43535, "text": "Once the MEDLINE data set is encoded as a term-document matrix, every component has a non-negative value, which results in the global centroid that is significantly far from the origin.", "rank": 383, "start": 43350, "IsComparative": "0", "id": "st_383"}, {"end": 43697, "text": "Then performing PCA without data centering might give the first principal axis as the one reflecting the global centroid rather than that discriminating clusters.", "rank": 384, "start": 43535, "IsComparative": "0", "id": "st_384"}, {"end": 43789, "text": "If we consider projecting the data onto each of the horizontal and the vertical axes in Fig.", "rank": 385, "start": 43697, "IsComparative": "0", "id": "st_385"}, {"end": 44015, "text": "5, the former, which corresponds to the first principal axis, does not help in showing the cluster structure clearly, and only the vertical axis, which corresponds to the second principal axis from PCA, discriminates clusters.", "rank": 386, "start": 43789, "IsComparative": "1", "id": "st_386"}, {"end": 44177, "text": "We have found that such undesirable behavior is common in many cases without data centering, which is why we assume that data is centered throughout this chapter.", "rank": 387, "start": 44015, "IsComparative": "0", "id": "st_387"}, {"end": 44258, "text": "Accordingly, all the results shown in Figs 1-4 are obtained after data centering.", "rank": 388, "start": 44177, "IsComparative": "0", "id": "st_388"}]}, {"paragraph_info": {"end": 44299, "start": 44258, "text": "2.6.4 Comparison of Visualization Results", "rank": 159, "paragraph_comparative_number": 1, "entities": [], "id": "p_159"}, "sentences": [{"end": 44299, "text": "2.6.4 Comparison of Visualization Results", "rank": 389, "start": 44258, "IsComparative": "1", "id": "st_389"}]}, {"paragraph_info": {"end": 44879, "start": 44299, "text": "The results of four two-stage methods for the tested data sets are shown in Figs.1-42.In all cases, LDA-based methods show cluster structures more clearly than OCM- based methods.This proves the effectiveness of LDA that considers both within- and between-cluster measures while OCM only takes into account the latter.Due to this difference, OCM generally produces a widely-scattered data representation within each cluster.As a result, in the NEWSGROUPS dataset, such a wide within-cluster variance significantly deteriorates the cluster structure visualization even if OCM still", "rank": 160, "paragraph_comparative_number": 2, "entities": [], "id": "p_160"}, "sentences": [{"end": 44385, "text": "The results of four two-stage methods for the tested data sets are shown in Figs.1-42.", "rank": 390, "start": 44299, "IsComparative": "0", "id": "st_390"}, {"end": 44478, "text": "In all cases, LDA-based methods show cluster structures more clearly than OCM- based methods.", "rank": 391, "start": 44385, "IsComparative": "0", "id": "st_391"}, {"end": 44617, "text": "This proves the effectiveness of LDA that considers both within- and between-cluster measures while OCM only takes into account the latter.", "rank": 392, "start": 44478, "IsComparative": "1", "id": "st_392"}, {"end": 44723, "text": "Due to this difference, OCM generally produces a widely-scattered data representation within each cluster.", "rank": 393, "start": 44617, "IsComparative": "1", "id": "st_393"}, {"end": 44879, "text": "As a result, in the NEWSGROUPS dataset, such a wide within-cluster variance significantly deteriorates the cluster structure visualization even if OCM still", "rank": 394, "start": 44723, "IsComparative": "0", "id": "st_394"}]}, {"paragraph_info": {"end": 44929, "start": 44879, "text": "attempts to maximize the between-cluster distance.", "rank": 161, "paragraph_comparative_number": 0, "entities": [], "id": "p_161"}, "sentences": [{"end": 44929, "text": "attempts to maximize the between-cluster distance.", "rank": 395, "start": 44879, "IsComparative": "0", "id": "st_395"}]}, {"paragraph_info": {"end": 45002, "start": 44929, "text": "In the MEDLINE and the REUTERS data sets, all of the four methods produce", "rank": 162, "paragraph_comparative_number": 1, "entities": [], "id": "p_162"}, "sentences": [{"end": 45002, "text": "In the MEDLINE and the REUTERS data sets, all of the four methods produce", "rank": 396, "start": 44929, "IsComparative": "1", "id": "st_396"}]}, {"paragraph_info": {"end": 45333, "start": 45002, "text": "relatively similar results.However, we have controlled the within-cluster variance in LDA-based methods using the regularization term I. In addition, the fact that rank- 2 LDA and LDA+PCA produce almost identical results indicates that GTLDAStGLDA is dominated by GTLDASbGLDA after LDA is applied in the first stage as we expected.", "rank": 163, "paragraph_comparative_number": 0, "entities": [], "id": "p_163"}, "sentences": [{"end": 45029, "text": "relatively similar results.", "rank": 397, "start": 45002, "IsComparative": "0", "id": "st_397"}, {"end": 45333, "text": "However, we have controlled the within-cluster variance in LDA-based methods using the regularization term I. In addition, the fact that rank- 2 LDA and LDA+PCA produce almost identical results indicates that GTLDAStGLDA is dominated by GTLDASbGLDA after LDA is applied in the first stage as we expected.", "rank": 398, "start": 45029, "IsComparative": "0", "id": "st_398"}]}, {"paragraph_info": {"end": 45449, "start": 45333, "text": "2Those figures can be arbitrarily magnified without losing the resolution in the electronic version of this chapter.", "rank": 164, "paragraph_comparative_number": 1, "entities": [], "id": "p_164"}, "sentences": [{"end": 45449, "text": "2Those figures can be arbitrarily magnified without losing the resolution in the electronic version of this chapter.", "rank": 399, "start": 45333, "IsComparative": "1", "id": "st_399"}]}, {"paragraph_info": {"end": 46037, "start": 45449, "text": "Rank-2 LDA represents each cluster most compactly by minimizing the within- cluster radii both in the first and the second stage.However, it may reduce the between-cluster distances as well because Jb/w maximizes the conceptual ratio of two scatter measures.As can be seen in the two LDA-based methods applied to the NEWGROUPS data set, while rank-2 LDA minimizes the within-cluster radii, it also places the centroids closer to each other as compared to those in LDA+PCA.Due to this effect, which one is preferable between rank-2 LDA and LDA+PCA depends on the data set to be visualized.", "rank": 165, "paragraph_comparative_number": 1, "entities": [], "id": "p_165"}, "sentences": [{"end": 45578, "text": "Rank-2 LDA represents each cluster most compactly by minimizing the within- cluster radii both in the first and the second stage.", "rank": 400, "start": 45449, "IsComparative": "0", "id": "st_400"}, {"end": 45707, "text": "However, it may reduce the between-cluster distances as well because Jb/w maximizes the conceptual ratio of two scatter measures.", "rank": 401, "start": 45578, "IsComparative": "1", "id": "st_401"}, {"end": 45921, "text": "As can be seen in the two LDA-based methods applied to the NEWGROUPS data set, while rank-2 LDA minimizes the within-cluster radii, it also places the centroids closer to each other as compared to those in LDA+PCA.", "rank": 402, "start": 45707, "IsComparative": "0", "id": "st_402"}, {"end": 46037, "text": "Due to this effect, which one is preferable between rank-2 LDA and LDA+PCA depends on the data set to be visualized.", "rank": 403, "start": 45921, "IsComparative": "0", "id": "st_403"}]}, {"paragraph_info": {"end": 46416, "start": 46037, "text": "Overall, OCM+PCA and Rank-2 PCA on Sb show similar results.It means GT SbG  GT StG in that the difference between two methods lies in whether PCA is applied to GT SbG or GT StG in the second stage.Since performing PCA on GT SbG is computationally more efficient than PCA on GT StG, Rank-2 PCA on Sb can be a good alternative to OCM+PCA in case efficient computation is important.", "rank": 166, "paragraph_comparative_number": 2, "entities": [], "id": "p_166"}, "sentences": [{"end": 46096, "text": "Overall, OCM+PCA and Rank-2 PCA on Sb show similar results.", "rank": 404, "start": 46037, "IsComparative": "0", "id": "st_404"}, {"end": 46234, "text": "It means GT SbG  GT StG in that the difference between two methods lies in whether PCA is applied to GT SbG or GT StG in the second stage.", "rank": 405, "start": 46096, "IsComparative": "1", "id": "st_405"}, {"end": 46416, "text": "Since performing PCA on GT SbG is computationally more efficient than PCA on GT StG, Rank-2 PCA on Sb can be a good alternative to OCM+PCA in case efficient computation is important.", "rank": 406, "start": 46234, "IsComparative": "1", "id": "st_406"}]}, {"paragraph_info": {"end": 46588, "start": 46416, "text": "Finally, these visualization results reveal the interesting cluster relationships un- derlying in the data.In Fig.(2), the clusters for colon cancer (c) and oral cancer (o)", "rank": 167, "paragraph_comparative_number": 1, "entities": [], "id": "p_167"}, "sentences": [{"end": 46523, "text": "Finally, these visualization results reveal the interesting cluster relationships un- derlying in the data.", "rank": 407, "start": 46416, "IsComparative": "1", "id": "st_407"}, {"end": 46530, "text": "In Fig.", "rank": 408, "start": 46523, "IsComparative": "0", "id": "st_408"}, {"end": 46588, "text": "(2), the clusters for colon cancer (c) and oral cancer (o)", "rank": 409, "start": 46530, "IsComparative": "0", "id": "st_409"}]}, {"paragraph_info": {"end": 46659, "start": 46588, "text": "are shown close to each other.In Fig.(3), the clusters of soc.religion.", "rank": 168, "paragraph_comparative_number": 1, "entities": [], "id": "p_168"}, "sentences": [{"end": 46618, "text": "are shown close to each other.", "rank": 410, "start": 46588, "IsComparative": "1", "id": "st_410"}, {"end": 46625, "text": "In Fig.", "rank": 411, "start": 46618, "IsComparative": "0", "id": "st_411"}, {"end": 46659, "text": "(3), the clusters of soc.religion.", "rank": 412, "start": 46625, "IsComparative": "0", "id": "st_412"}]}, {"paragraph_info": {"end": 46668, "start": 46659, "text": "and talk.", "rank": 169, "paragraph_comparative_number": 0, "entities": [], "id": "p_169"}, "sentences": [{"end": 46668, "text": "and talk.", "rank": 413, "start": 46659, "IsComparative": "0", "id": "st_413"}]}, {"paragraph_info": {"end": 46922, "start": 46668, "text": "(a), and those of sci.crypt (y) and sci.med (d) are closely located respectively in LDA-based methods.In addition, the two clusters, misc.forsale (f) and rec.sport.baseball (b), are shown to be the most distinctive, which makes sense because those topics", "rank": 170, "paragraph_comparative_number": 1, "entities": [], "id": "p_170"}, "sentences": [{"end": 46770, "text": "(a), and those of sci.crypt (y) and sci.med (d) are closely located respectively in LDA-based methods.", "rank": 414, "start": 46668, "IsComparative": "0", "id": "st_414"}, {"end": 46922, "text": "In addition, the two clusters, misc.forsale (f) and rec.sport.baseball (b), are shown to be the most distinctive, which makes sense because those topics", "rank": 415, "start": 46770, "IsComparative": "1", "id": "st_415"}]}, {"paragraph_info": {"end": 47096, "start": 46922, "text": "are quite irrelevant to the others.In Fig.(4), the clusters of grain (g), wheat (w), and corn (c) as well as those of money-fx (m) and interest (i) are visualized very close.", "rank": 171, "paragraph_comparative_number": 0, "entities": [], "id": "p_171"}, "sentences": [{"end": 46957, "text": "are quite irrelevant to the others.", "rank": 416, "start": 46922, "IsComparative": "0", "id": "st_416"}, {"end": 46964, "text": "In Fig.", "rank": 417, "start": 46957, "IsComparative": "0", "id": "st_417"}, {"end": 47096, "text": "(4), the clusters of grain (g), wheat (w), and corn (c) as well as those of money-fx (m) and interest (i) are visualized very close.", "rank": 418, "start": 46964, "IsComparative": "0", "id": "st_418"}]}, {"paragraph_info": {"end": 47111, "start": 47096, "text": "2.7 Conclusions", "rank": 172, "paragraph_comparative_number": 0, "entities": [], "id": "p_172"}, "sentences": [{"end": 47111, "text": "2.7 Conclusions", "rank": 419, "start": 47096, "IsComparative": "0", "id": "st_419"}]}, {"paragraph_info": {"end": 47569, "start": 47111, "text": "According to our results, LDA-based methods are shown to be superior to OCM-based methods since both within- and between-cluster relationships are taken into account religion.misc (r), those of comp.sys.ibm.in LDA.Especially, combined with PCA in the second stage, LDA+PCA achieves a clear discrimination between clusters as well as the best approximation of the results of LDA when the distance between data is measured in terms of Frobenius/Euclidean norm.", "rank": 173, "paragraph_comparative_number": 0, "entities": [], "id": "p_173"}, "sentences": [{"end": 47318, "text": "According to our results, LDA-based methods are shown to be superior to OCM-based methods since both within- and between-cluster relationships are taken into account religion.misc (r), those of comp.sys.ibm.", "rank": 420, "start": 47111, "IsComparative": "0", "id": "st_420"}, {"end": 47325, "text": "in LDA.", "rank": 421, "start": 47318, "IsComparative": "0", "id": "st_421"}, {"end": 47569, "text": "Especially, combined with PCA in the second stage, LDA+PCA achieves a clear discrimination between clusters as well as the best approximation of the results of LDA when the distance between data is measured in terms of Frobenius/Euclidean norm.", "rank": 422, "start": 47325, "IsComparative": "0", "id": "st_422"}]}, {"paragraph_info": {"end": 48658, "start": 47569, "text": "However, many classes except for few of them that are clearly unrelated tend to be overlapped especially when dealing with large numbers of data points and clusters.This is inherently due to the nature of the second-stage dimension reduction in which only the two axes are chosen so that the classes which contribute most to the second stage criteria can be well-discriminated.Such behavior can exaggerate the distances between particular clusters, and more elaboration towards new criteria that fits in visualization is required.In the MEDLINE and the REUTERS datasets, visualization results seem to have a tail-shape along specific directions.We often found this phenomenon to occur in many other data sets.It is still unclear as to what causes this and how it affects the visualization, e.g.characteristics of information loss in the second stage.Finally, in order to determine how much loss of information is introduced by each method, more rigorous analysis based on various quantitative measures such as pairwise between-cluster distance and within-cluster radii should be conducted.", "rank": 174, "paragraph_comparative_number": 4, "entities": [], "id": "p_174"}, "sentences": [{"end": 47734, "text": "However, many classes except for few of them that are clearly unrelated tend to be overlapped especially when dealing with large numbers of data points and clusters.", "rank": 423, "start": 47569, "IsComparative": "0", "id": "st_423"}, {"end": 47946, "text": "This is inherently due to the nature of the second-stage dimension reduction in which only the two axes are chosen so that the classes which contribute most to the second stage criteria can be well-discriminated.", "rank": 424, "start": 47734, "IsComparative": "1", "id": "st_424"}, {"end": 48099, "text": "Such behavior can exaggerate the distances between particular clusters, and more elaboration towards new criteria that fits in visualization is required.", "rank": 425, "start": 47946, "IsComparative": "0", "id": "st_425"}, {"end": 48214, "text": "In the MEDLINE and the REUTERS datasets, visualization results seem to have a tail-shape along specific directions.", "rank": 426, "start": 48099, "IsComparative": "1", "id": "st_426"}, {"end": 48278, "text": "We often found this phenomenon to occur in many other data sets.", "rank": 427, "start": 48214, "IsComparative": "1", "id": "st_427"}, {"end": 48363, "text": "It is still unclear as to what causes this and how it affects the visualization, e.g.", "rank": 428, "start": 48278, "IsComparative": "1", "id": "st_428"}, {"end": 48419, "text": "characteristics of information loss in the second stage.", "rank": 429, "start": 48363, "IsComparative": "0", "id": "st_429"}, {"end": 48658, "text": "Finally, in order to determine how much loss of information is introduced by each method, more rigorous analysis based on various quantitative measures such as pairwise between-cluster distance and within-cluster radii should be conducted.", "rank": 430, "start": 48419, "IsComparative": "0", "id": "st_430"}]}, {"paragraph_info": {"end": 48738, "start": 48658, "text": "CHAPTER III EFFICIENT UPDATING OF COMPUTATIONAL METHODS DUE TO PARAMETER CHANGES", "rank": 175, "paragraph_comparative_number": 1, "entities": [], "id": "p_175"}, "sentences": [{"end": 48738, "text": "CHAPTER III EFFICIENT UPDATING OF COMPUTATIONAL METHODS DUE TO PARAMETER CHANGES", "rank": 431, "start": 48658, "IsComparative": "1", "id": "st_431"}]}, {"paragraph_info": {"end": 49809, "start": 48738, "text": "One of the most widely-used nonlinear data embedding methods is ISOMAP.Based on a manifold learning framework, ISOMAP has a parameter k or o that controls how many edges a neighborhood graph has.However, a suitable parameter value is often difficult to determine because of a time-consuming optimization process based on certain criteria, which may not be clearly justified.When ISOMAP is used to visualize data, users might want to test different parameter values in order to gain various insights about data, but such interaction between humans and such visual- izations requires reasonably efficient updating, even for large-scale data.To tackle these problems, we propose an efficient updating algorithm for ISOMAP with pa- rameter changes, called p-ISOMAP.We present not only a complexity analysis but also an empirical running time comparison, which show the advantage of p-ISOMAP.We also show interesting visualization applications of p-ISOMAP and demonstrate how to discover various characteristics of data through visualizations using different parameter values.", "rank": 176, "paragraph_comparative_number": 3, "entities": [], "id": "p_176"}, "sentences": [{"end": 48809, "text": "One of the most widely-used nonlinear data embedding methods is ISOMAP.", "rank": 432, "start": 48738, "IsComparative": "0", "id": "st_432"}, {"end": 48933, "text": "Based on a manifold learning framework, ISOMAP has a parameter k or o that controls how many edges a neighborhood graph has.", "rank": 433, "start": 48809, "IsComparative": "0", "id": "st_433"}, {"end": 49112, "text": "However, a suitable parameter value is often difficult to determine because of a time-consuming optimization process based on certain criteria, which may not be clearly justified.", "rank": 434, "start": 48933, "IsComparative": "0", "id": "st_434"}, {"end": 49377, "text": "When ISOMAP is used to visualize data, users might want to test different parameter values in order to gain various insights about data, but such interaction between humans and such visual- izations requires reasonably efficient updating, even for large-scale data.", "rank": 435, "start": 49112, "IsComparative": "1", "id": "st_435"}, {"end": 49499, "text": "To tackle these problems, we propose an efficient updating algorithm for ISOMAP with pa- rameter changes, called p-ISOMAP.", "rank": 436, "start": 49377, "IsComparative": "1", "id": "st_436"}, {"end": 49625, "text": "We present not only a complexity analysis but also an empirical running time comparison, which show the advantage of p-ISOMAP.", "rank": 437, "start": 49499, "IsComparative": "0", "id": "st_437"}, {"end": 49809, "text": "We also show interesting visualization applications of p-ISOMAP and demonstrate how to discover various characteristics of data through visualizations using different parameter values.", "rank": 438, "start": 49625, "IsComparative": "1", "id": "st_438"}]}, {"paragraph_info": {"end": 49823, "start": 49809, "text": "3.1 Motivation", "rank": 177, "paragraph_comparative_number": 0, "entities": [], "id": "p_177"}, "sentences": [{"end": 49823, "text": "3.1 Motivation", "rank": 439, "start": 49809, "IsComparative": "0", "id": "st_439"}]}, {"paragraph_info": {"end": 50608, "start": 49823, "text": "One of the most widely-used data mining techniques that reduce noise in data and im- prove efficiency in terms of computation time and memory usage is dimension reduc- tion.Recently, nonlinear dimension reduction techniques, which have been actively investigated, revealed the underlying nonlinear structure in data.Such nonlinearity is often considered as a curvilinear manifold with a much lower dimension than that in the original high-dimensional space.Among the most recent nonlinear dimension reduction methods, isometric feature mapping (ISOMAP) has shown its effectiveness in capturing the underlying manifold structure in the reduced dimensional space by being successfully applied to synthetic data such as Swiss roll data and real-world data such as facial image data <125>.", "rank": 178, "paragraph_comparative_number": 1, "entities": [], "id": "p_178"}, "sentences": [{"end": 49996, "text": "One of the most widely-used data mining techniques that reduce noise in data and im- prove efficiency in terms of computation time and memory usage is dimension reduc- tion.", "rank": 440, "start": 49823, "IsComparative": "0", "id": "st_440"}, {"end": 50139, "text": "Recently, nonlinear dimension reduction techniques, which have been actively investigated, revealed the underlying nonlinear structure in data.", "rank": 441, "start": 49996, "IsComparative": "0", "id": "st_441"}, {"end": 50280, "text": "Such nonlinearity is often considered as a curvilinear manifold with a much lower dimension than that in the original high-dimensional space.", "rank": 442, "start": 50139, "IsComparative": "0", "id": "st_442"}, {"end": 50608, "text": "Among the most recent nonlinear dimension reduction methods, isometric feature mapping (ISOMAP) has shown its effectiveness in capturing the underlying manifold structure in the reduced dimensional space by being successfully applied to synthetic data such as Swiss roll data and real-world data such as facial image data <125>.", "rank": 443, "start": 50280, "IsComparative": "1", "id": "st_443"}]}, {"paragraph_info": {"end": 51514, "start": 50608, "text": "ISOMAP shares the basic idea with a traditional technique, classical multidimen- sional scaling (MDS).Classical MDS first constructs the pairwise similarity matrix, which is usually measured by the Euclidean distance, and computes the reduced dimensional mapping that maximally preserves such a similarity matrix in a given reduced dimension.ISOMAP differs from classical MDS in that it constructs the pairwise similarity matrix based on the geodesic distance estimated by the short- est path in the neighborhood graph of data.The neighborhood graph is formed by having vertices as data points and setting each edge weight between the nodes as their Euclidean distance only if at least one node is one of the k-nearest neighbors (k-NN) of the other node (k-ISOMAP) or if their Euclidean distance is smaller than o (o-ISOMAP).Hence, ISOMAP has an either parameter k or o to construct the neighborhood graph.", "rank": 179, "paragraph_comparative_number": 1, "entities": [], "id": "p_179"}, "sentences": [{"end": 50710, "text": "ISOMAP shares the basic idea with a traditional technique, classical multidimen- sional scaling (MDS).", "rank": 444, "start": 50608, "IsComparative": "0", "id": "st_444"}, {"end": 50950, "text": "Classical MDS first constructs the pairwise similarity matrix, which is usually measured by the Euclidean distance, and computes the reduced dimensional mapping that maximally preserves such a similarity matrix in a given reduced dimension.", "rank": 445, "start": 50710, "IsComparative": "0", "id": "st_445"}, {"end": 51135, "text": "ISOMAP differs from classical MDS in that it constructs the pairwise similarity matrix based on the geodesic distance estimated by the short- est path in the neighborhood graph of data.", "rank": 446, "start": 50950, "IsComparative": "0", "id": "st_446"}, {"end": 51433, "text": "The neighborhood graph is formed by having vertices as data points and setting each edge weight between the nodes as their Euclidean distance only if at least one node is one of the k-nearest neighbors (k-NN) of the other node (k-ISOMAP) or if their Euclidean distance is smaller than o (o-ISOMAP).", "rank": 447, "start": 51135, "IsComparative": "1", "id": "st_447"}, {"end": 51514, "text": "Hence, ISOMAP has an either parameter k or o to construct the neighborhood graph.", "rank": 448, "start": 51433, "IsComparative": "0", "id": "st_448"}]}, {"paragraph_info": {"end": 52545, "start": 51514, "text": "This chapter focuses on the algorithm and applications of the dynamic updating of ISOMAP when the value of k or o varies.It is generally known that in ISOMAP, if k or o is too small, the graph becomes sparse, resulting in infinite geodesic distances between some pairs of data points, and if k or o is too large, it is prone to short circuit the true geometry of the manifold.However, it is not always easy to figure out which value of k or o is appropriate for the data at hand.One way of optimizing these parameters is using certain quantitative measures such as residual variance <12, 125, 112> and finding the elbow point at which the residual variance curve stops decreasing significantly as the parameter value changes.However, running ISOMAP repeatedly using different parameter values for k or o may be time-consuming since it involves computationally intensive processes such as the all-pairs shortest path computation and the eigendecomposition, whose complexity is usually O(n3) in which n is the number of data points.1", "rank": 180, "paragraph_comparative_number": 3, "entities": [], "id": "p_180"}, "sentences": [{"end": 51635, "text": "This chapter focuses on the algorithm and applications of the dynamic updating of ISOMAP when the value of k or o varies.", "rank": 449, "start": 51514, "IsComparative": "0", "id": "st_449"}, {"end": 51890, "text": "It is generally known that in ISOMAP, if k or o is too small, the graph becomes sparse, resulting in infinite geodesic distances between some pairs of data points, and if k or o is too large, it is prone to short circuit the true geometry of the manifold.", "rank": 450, "start": 51635, "IsComparative": "0", "id": "st_450"}, {"end": 51993, "text": "However, it is not always easy to figure out which value of k or o is appropriate for the data at hand.", "rank": 451, "start": 51890, "IsComparative": "1", "id": "st_451"}, {"end": 52239, "text": "One way of optimizing these parameters is using certain quantitative measures such as residual variance <12, 125, 112> and finding the elbow point at which the residual variance curve stops decreasing significantly as the parameter value changes.", "rank": 452, "start": 51993, "IsComparative": "1", "id": "st_452"}, {"end": 52545, "text": "However, running ISOMAP repeatedly using different parameter values for k or o may be time-consuming since it involves computationally intensive processes such as the all-pairs shortest path computation and the eigendecomposition, whose complexity is usually O(n3) in which n is the number of data points.1", "rank": 453, "start": 52239, "IsComparative": "1", "id": "st_453"}]}, {"paragraph_info": {"end": 54209, "start": 52545, "text": "In practice, there is also often no guarantee of the existence of the underlying well-defined manifold structure in data, and thus, one may not be sure if manifold learning methods such as ISOMAP are suitable for the data at hand.Even so, one may still want to try ISOMAP or another manifold learning method in order to see if it serves ones purpose.In this case, however, it may not be a good idea to rely on a particular value of k or o to achieve a reasonable dimension reduction since the optimal value tends to be indistinct in terms of a certain measure.When it comes to the visualization of high-dimensional data in two- or three-dimensional space, we can acquire different insights on the data by using various dimension reduction techniques <35>.This statement also holds true even when we use just a single dimension reduc- tion method, e.g., ISOMAP, while we test its various parameter values.In short, visualizations using ISOMAP with different parameter values for k or o can provide us with various aspects of our data.In instances of the Swiss roll and toroidal helix data sets shown in Fig.6, one may want to visualize them based on the unfolded version of its manifold, as shown in Figs.6(b) and 6(f), but sometimes one may also want to see how the underlying manifold is curved in the original space, i.e., the curvature of the manifold itself, as shown in Figs.6(d) and 6(h).2 It is also possible that visualizations of the transition between these two cases, shown in Figs.6(c) and 6(g), imply different insight about data.In this sense, it is worthwhile for users to test different parameter values in ISOMAP to visualize data in various ways.", "rank": 181, "paragraph_comparative_number": 7, "entities": [], "id": "p_181"}, "sentences": [{"end": 52775, "text": "In practice, there is also often no guarantee of the existence of the underlying well-defined manifold structure in data, and thus, one may not be sure if manifold learning methods such as ISOMAP are suitable for the data at hand.", "rank": 454, "start": 52545, "IsComparative": "0", "id": "st_454"}, {"end": 52895, "text": "Even so, one may still want to try ISOMAP or another manifold learning method in order to see if it serves ones purpose.", "rank": 455, "start": 52775, "IsComparative": "0", "id": "st_455"}, {"end": 53105, "text": "In this case, however, it may not be a good idea to rely on a particular value of k or o to achieve a reasonable dimension reduction since the optimal value tends to be indistinct in terms of a certain measure.", "rank": 456, "start": 52895, "IsComparative": "0", "id": "st_456"}, {"end": 53300, "text": "When it comes to the visualization of high-dimensional data in two- or three-dimensional space, we can acquire different insights on the data by using various dimension reduction techniques <35>.", "rank": 457, "start": 53105, "IsComparative": "1", "id": "st_457"}, {"end": 53449, "text": "This statement also holds true even when we use just a single dimension reduc- tion method, e.g., ISOMAP, while we test its various parameter values.", "rank": 458, "start": 53300, "IsComparative": "1", "id": "st_458"}, {"end": 53578, "text": "In short, visualizations using ISOMAP with different parameter values for k or o can provide us with various aspects of our data.", "rank": 459, "start": 53449, "IsComparative": "1", "id": "st_459"}, {"end": 53651, "text": "In instances of the Swiss roll and toroidal helix data sets shown in Fig.", "rank": 460, "start": 53578, "IsComparative": "0", "id": "st_460"}, {"end": 53749, "text": "6, one may want to visualize them based on the unfolded version of its manifold, as shown in Figs.", "rank": 461, "start": 53651, "IsComparative": "1", "id": "st_461"}, {"end": 53925, "text": "6(b) and 6(f), but sometimes one may also want to see how the underlying manifold is curved in the original space, i.e., the curvature of the manifold itself, as shown in Figs.", "rank": 462, "start": 53749, "IsComparative": "0", "id": "st_462"}, {"end": 54038, "text": "6(d) and 6(h).2 It is also possible that visualizations of the transition between these two cases, shown in Figs.", "rank": 463, "start": 53925, "IsComparative": "1", "id": "st_463"}, {"end": 54088, "text": "6(c) and 6(g), imply different insight about data.", "rank": 464, "start": 54038, "IsComparative": "1", "id": "st_464"}, {"end": 54209, "text": "In this sense, it is worthwhile for users to test different parameter values in ISOMAP to visualize data in various ways.", "rank": 465, "start": 54088, "IsComparative": "1", "id": "st_465"}]}, {"paragraph_info": {"end": 54448, "start": 54209, "text": "1The complexity of the (all-pairs) shortest path computation depends on the algorithm.Floyd- Warshall algorithm requires O(n3) computations while Dijkstras algorithm does O(|e|nlogn) com- putations <13> in which |e| is the number of edges.", "rank": 182, "paragraph_comparative_number": 1, "entities": [], "id": "p_182"}, "sentences": [{"end": 54295, "text": "1The complexity of the (all-pairs) shortest path computation depends on the algorithm.", "rank": 466, "start": 54209, "IsComparative": "0", "id": "st_466"}, {"end": 54448, "text": "Floyd- Warshall algorithm requires O(n3) computations while Dijkstras algorithm does O(|e|nlogn) com- putations <13> in which |e| is the number of edges.", "rank": 467, "start": 54295, "IsComparative": "1", "id": "st_467"}]}, {"paragraph_info": {"end": 54645, "start": 54448, "text": "2It may not be possible to visualize the manifold curvature perfectly without using the original dimension, but at least we can obtain some insights about it from a lower dimensional visualization.", "rank": 183, "paragraph_comparative_number": 1, "entities": [], "id": "p_183"}, "sentences": [{"end": 54645, "text": "2It may not be possible to visualize the manifold curvature perfectly without using the original dimension, but at least we can obtain some insights about it from a lower dimensional visualization.", "rank": 468, "start": 54448, "IsComparative": "1", "id": "st_468"}]}, {"paragraph_info": {"end": 55729, "start": 54645, "text": "Such visualizations, however, should provide users with smooth and prompt in- teraction that requires fast and efficient computations of the results.In other words, when users change the parameter value, if they have to wait for a significant amount of time, then such interaction would not be practical.Motivated by the above men- tioned cases, we propose p-ISOMAP, an efficient dynamic updating algorithm for ISOMAP when the parameter value changes.Given the ISOMAP result from a par- ticular parameter value, our proposed algorithm updates the previous result to obtain another ISOMAP result of the same data with a new parameter value instead of re- computing ISOMAP for different parameter values from scratch.We present the complexity analysis of our algorithms as well as the experimental comparison of their computation times.In addition, we demonstrate several visualization examples by varying the parameters in ISOMAP, which not only show the interesting aspects of the tested data but also help us thoroughly understand the behavior of ISOMAP in terms of parameter values.", "rank": 184, "paragraph_comparative_number": 3, "entities": [], "id": "p_184"}, "sentences": [{"end": 54794, "text": "Such visualizations, however, should provide users with smooth and prompt in- teraction that requires fast and efficient computations of the results.", "rank": 469, "start": 54645, "IsComparative": "0", "id": "st_469"}, {"end": 54949, "text": "In other words, when users change the parameter value, if they have to wait for a significant amount of time, then such interaction would not be practical.", "rank": 470, "start": 54794, "IsComparative": "0", "id": "st_470"}, {"end": 55096, "text": "Motivated by the above men- tioned cases, we propose p-ISOMAP, an efficient dynamic updating algorithm for ISOMAP when the parameter value changes.", "rank": 471, "start": 54949, "IsComparative": "0", "id": "st_471"}, {"end": 55360, "text": "Given the ISOMAP result from a par- ticular parameter value, our proposed algorithm updates the previous result to obtain another ISOMAP result of the same data with a new parameter value instead of re- computing ISOMAP for different parameter values from scratch.", "rank": 472, "start": 55096, "IsComparative": "1", "id": "st_472"}, {"end": 55479, "text": "We present the complexity analysis of our algorithms as well as the experimental comparison of their computation times.", "rank": 473, "start": 55360, "IsComparative": "1", "id": "st_473"}, {"end": 55729, "text": "In addition, we demonstrate several visualization examples by varying the parameters in ISOMAP, which not only show the interesting aspects of the tested data but also help us thoroughly understand the behavior of ISOMAP in terms of parameter values.", "rank": 474, "start": 55479, "IsComparative": "1", "id": "st_474"}]}, {"paragraph_info": {"end": 56218, "start": 55729, "text": "The rest of this chapter is organized as follows.Section 3.2 briefly introduces ISOMAP and its algorithm, and Section 3.3 discusses previous work related to p- ISOMAP.Section 3.4 describes the algorithmic details and the complexity analysis of p-ISOMAP, and Section 3.5 presents not only the experimental results that compare the computation times of ISOMAP and p-ISOMAP but also interesting visualization examples of real-world data using p-ISOMAP.Finally, Section 3.6 concludes our work.", "rank": 185, "paragraph_comparative_number": 3, "entities": [], "id": "p_185"}, "sentences": [{"end": 55778, "text": "The rest of this chapter is organized as follows.", "rank": 475, "start": 55729, "IsComparative": "0", "id": "st_475"}, {"end": 55896, "text": "Section 3.2 briefly introduces ISOMAP and its algorithm, and Section 3.3 discusses previous work related to p- ISOMAP.", "rank": 476, "start": 55778, "IsComparative": "1", "id": "st_476"}, {"end": 56178, "text": "Section 3.4 describes the algorithmic details and the complexity analysis of p-ISOMAP, and Section 3.5 presents not only the experimental results that compare the computation times of ISOMAP and p-ISOMAP but also interesting visualization examples of real-world data using p-ISOMAP.", "rank": 477, "start": 55896, "IsComparative": "1", "id": "st_477"}, {"end": 56218, "text": "Finally, Section 3.6 concludes our work.", "rank": 478, "start": 56178, "IsComparative": "1", "id": "st_478"}]}, {"paragraph_info": {"end": 56228, "start": 56218, "text": "3.2 ISOMAP", "rank": 186, "paragraph_comparative_number": 0, "entities": [], "id": "p_186"}, "sentences": [{"end": 56228, "text": "3.2 ISOMAP", "rank": 479, "start": 56218, "IsComparative": "0", "id": "st_479"}]}, {"paragraph_info": {"end": 56996, "start": 56228, "text": "Given a set of data points represented as M-dimensional vectors xi  RM for i = 1, ..., n, ISOMAP assumes a lower dimensional manifold structure in which the data are embedded.It yields the m-dimensional representation of xi as yi  Rm (m  M) such that the Euclidean distance between yi and yj approximates their geodesic distance along the underlying manifold as much as possible.Such an approximation builds on the classical MDS framework, but unlike MDS, ISOMAP has the capability of handling nonlinearity existing in the original space since a geodesic distance reflects an arbitrary curvilinear shape of the manifold.On input, ISOMAP takes a data matrix X =   x1 x2  xn    RMn, a reduced dimension m, and a parameter k or o.The algorithm is composed of three steps:", "rank": 187, "paragraph_comparative_number": 2, "entities": [], "id": "p_187"}, "sentences": [{"end": 56311, "text": "Given a set of data points represented as M-dimensional vectors xi  RM for i = 1, .", "rank": 480, "start": 56228, "IsComparative": "0", "id": "st_480"}, {"end": 56312, "text": ".", "rank": 481, "start": 56311, "IsComparative": "0", "id": "st_481"}, {"end": 56313, "text": ".", "rank": 482, "start": 56312, "IsComparative": "0", "id": "st_482"}, {"end": 56403, "text": ", n, ISOMAP assumes a lower dimensional manifold structure in which the data are embedded.", "rank": 483, "start": 56313, "IsComparative": "0", "id": "st_483"}, {"end": 56607, "text": "It yields the m-dimensional representation of xi as yi  Rm (m  M) such that the Euclidean distance between yi and yj approximates their geodesic distance along the underlying manifold as much as possible.", "rank": 484, "start": 56403, "IsComparative": "1", "id": "st_484"}, {"end": 56848, "text": "Such an approximation builds on the classical MDS framework, but unlike MDS, ISOMAP has the capability of handling nonlinearity existing in the original space since a geodesic distance reflects an arbitrary curvilinear shape of the manifold.", "rank": 485, "start": 56607, "IsComparative": "0", "id": "st_485"}, {"end": 56955, "text": "On input, ISOMAP takes a data matrix X =   x1 x2  xn    RMn, a reduced dimension m, and a parameter k or o.", "rank": 486, "start": 56848, "IsComparative": "0", "id": "st_486"}, {"end": 56996, "text": "The algorithm is composed of three steps:", "rank": 487, "start": 56955, "IsComparative": "1", "id": "st_487"}]}, {"paragraph_info": {"end": 57564, "start": 56996, "text": "1.Neighborhood graph construction.ISOMAP first computes the pairwise Eu- clidean distance matrix, DX  Rnn, in which DX (i, j) is the Euclidean dis- tance between xi and xj.Then it determines the set of neighbors for each point either by k-nearest neighbors or by those within a fixed radius o. Be- tween a point xi and each of its neighbors xj, an edge e(i, j) is assigned with a weight equivalent to their Euclidean distance, and in this way, ISOMAP forms a weighted undirected neighborhood graph G = (V, E), where the vertices in V correspond to the data points xis.", "rank": 188, "paragraph_comparative_number": 2, "entities": [], "id": "p_188"}, "sentences": [{"end": 56998, "text": "1.", "rank": 488, "start": 56996, "IsComparative": "1", "id": "st_488"}, {"end": 57030, "text": "Neighborhood graph construction.", "rank": 489, "start": 56998, "IsComparative": "0", "id": "st_489"}, {"end": 57168, "text": "ISOMAP first computes the pairwise Eu- clidean distance matrix, DX  Rnn, in which DX (i, j) is the Euclidean dis- tance between xi and xj.", "rank": 490, "start": 57030, "IsComparative": "0", "id": "st_490"}, {"end": 57564, "text": "Then it determines the set of neighbors for each point either by k-nearest neighbors or by those within a fixed radius o. Be- tween a point xi and each of its neighbors xj, an edge e(i, j) is assigned with a weight equivalent to their Euclidean distance, and in this way, ISOMAP forms a weighted undirected neighborhood graph G = (V, E), where the vertices in V correspond to the data points xis.", "rank": 491, "start": 57168, "IsComparative": "1", "id": "st_491"}]}, {"paragraph_info": {"end": 57863, "start": 57564, "text": "2.Geodesic distance estimation.In the second step, ISOMAP estimates the pair- wise geodesic distance based on the shortest path length for every vertex pair along the neighborhood graph G, which is represented as a matrix DG  Rnn in which DG(i, j) is the shortest path length between xi and xj in G.", "rank": 189, "paragraph_comparative_number": 1, "entities": [], "id": "p_189"}, "sentences": [{"end": 57566, "text": "2.", "rank": 492, "start": 57564, "IsComparative": "1", "id": "st_492"}, {"end": 57595, "text": "Geodesic distance estimation.", "rank": 493, "start": 57566, "IsComparative": "0", "id": "st_493"}, {"end": 57863, "text": "In the second step, ISOMAP estimates the pair- wise geodesic distance based on the shortest path length for every vertex pair along the neighborhood graph G, which is represented as a matrix DG  Rnn in which DG(i, j) is the shortest path length between xi and xj in G.", "rank": 494, "start": 57595, "IsComparative": "0", "id": "st_494"}]}, {"paragraph_info": {"end": 58483, "start": 57863, "text": "3.Lower dimensional embedding.The final step performs classical MDS on DG, producing m-dimensional data embedding, Y =   y1 y2  yn    Rmn.First, the pairwise geodesic distance matrix DG is converted to an inner product matrix BG as avoid ambiguity about which changes of neighbors in a directed graph cause actual edge changes in an undirected one in which we have to actually compute the shortest paths.The detailed procedure of the neighborhood graph update are described in Algorithm 1.As an output, it produces the set of effectively inserted/removed edges, which is, in turn, used in the shortest path update stage.", "rank": 190, "paragraph_comparative_number": 2, "entities": [], "id": "p_190"}, "sentences": [{"end": 57865, "text": "3.", "rank": 495, "start": 57863, "IsComparative": "1", "id": "st_495"}, {"end": 57893, "text": "Lower dimensional embedding.", "rank": 496, "start": 57865, "IsComparative": "0", "id": "st_496"}, {"end": 58001, "text": "The final step performs classical MDS on DG, producing m-dimensional data embedding, Y =   y1 y2  yn    Rmn.", "rank": 497, "start": 57893, "IsComparative": "1", "id": "st_497"}, {"end": 58267, "text": "First, the pairwise geodesic distance matrix DG is converted to an inner product matrix BG as avoid ambiguity about which changes of neighbors in a directed graph cause actual edge changes in an undirected one in which we have to actually compute the shortest paths.", "rank": 498, "start": 58001, "IsComparative": "0", "id": "st_498"}, {"end": 58352, "text": "The detailed procedure of the neighborhood graph update are described in Algorithm 1.", "rank": 499, "start": 58267, "IsComparative": "0", "id": "st_499"}, {"end": 58483, "text": "As an output, it produces the set of effectively inserted/removed edges, which is, in turn, used in the shortest path update stage.", "rank": 500, "start": 58352, "IsComparative": "0", "id": "st_500"}]}, {"paragraph_info": {"end": 58506, "start": 58483, "text": "3.4.1.1 Time Complexity", "rank": 191, "paragraph_comparative_number": 0, "entities": [], "id": "p_191"}, "sentences": [{"end": 58506, "text": "3.4.1.1 Time Complexity", "rank": 501, "start": 58483, "IsComparative": "0", "id": "st_501"}]}, {"paragraph_info": {"end": 58958, "start": 58506, "text": "In ISOMAP, the time complexity in constructing a neighborhood graph is as fol- lows.It starts with a sort operation for a given data set whose time complexity is O(n2 log n).Then obtaining G and G requires O(nq), in which q is the maximum degree of vertices in the graph G.In p-ISOMAP, the time complexity required in the neighborhood graph update is bounded by O(n  maxi |ei|), in which |ei| is the number of inserted/removed edges associated with xi.", "rank": 192, "paragraph_comparative_number": 1, "entities": [], "id": "p_192"}, "sentences": [{"end": 58590, "text": "In ISOMAP, the time complexity in constructing a neighborhood graph is as fol- lows.", "rank": 502, "start": 58506, "IsComparative": "0", "id": "st_502"}, {"end": 58680, "text": "It starts with a sort operation for a given data set whose time complexity is O(n2 log n).", "rank": 503, "start": 58590, "IsComparative": "1", "id": "st_503"}, {"end": 58779, "text": "Then obtaining G and G requires O(nq), in which q is the maximum degree of vertices in the graph G.", "rank": 504, "start": 58680, "IsComparative": "0", "id": "st_504"}, {"end": 58958, "text": "In p-ISOMAP, the time complexity required in the neighborhood graph update is bounded by O(n  maxi |ei|), in which |ei| is the number of inserted/removed edges associated with xi.", "rank": 505, "start": 58779, "IsComparative": "0", "id": "st_505"}]}, {"paragraph_info": {"end": 58984, "start": 58958, "text": "3.4.2 Shortest Path Update", "rank": 193, "paragraph_comparative_number": 0, "entities": [], "id": "p_193"}, "sentences": [{"end": 58984, "text": "3.4.2 Shortest Path Update", "rank": 506, "start": 58958, "IsComparative": "0", "id": "st_506"}]}, {"paragraph_info": {"end": 59625, "start": 58984, "text": "The shortest path update stage, which is one of the most computationally inten- sive steps in p-ISOMAP, takes the input as either A or D and updates the shortest path length matrix DG.In order to facilitate this process, p-ISOMAP maintains and updates the information about the shortest path itself with a minimal memory requirement in the form of a predecessor matrix P  Rnn, in which P(i, j) stores the node index immediately preceding xj in the shortest path from xi to xj.3 For instance, if the shortest path from x1 to x2 is composed of x1  x4  x3  x2, then we set P (1, 2) = 3.For the shortest path update, p-ISOMAP performs two steps:", "rank": 194, "paragraph_comparative_number": 1, "entities": [], "id": "p_194"}, "sentences": [{"end": 59168, "text": "The shortest path update stage, which is one of the most computationally inten- sive steps in p-ISOMAP, takes the input as either A or D and updates the shortest path length matrix DG.", "rank": 507, "start": 58984, "IsComparative": "0", "id": "st_507"}, {"end": 59567, "text": "In order to facilitate this process, p-ISOMAP maintains and updates the information about the shortest path itself with a minimal memory requirement in the form of a predecessor matrix P  Rnn, in which P(i, j) stores the node index immediately preceding xj in the shortest path from xi to xj.3 For instance, if the shortest path from x1 to x2 is composed of x1  x4  x3  x2, then we set P (1, 2) = 3.", "rank": 508, "start": 59168, "IsComparative": "0", "id": "st_508"}, {"end": 59625, "text": "For the shortest path update, p-ISOMAP performs two steps:", "rank": 509, "start": 59567, "IsComparative": "1", "id": "st_509"}]}, {"paragraph_info": {"end": 59784, "start": 59625, "text": "1.It identifies the set, F , of the affected vertex pairs, whose shortest paths need to be recomputed due to the inserted edges in A or the removed edges in D.", "rank": 195, "paragraph_comparative_number": 2, "entities": [], "id": "p_195"}, "sentences": [{"end": 59627, "text": "1.", "rank": 510, "start": 59625, "IsComparative": "1", "id": "st_510"}, {"end": 59784, "text": "It identifies the set, F , of the affected vertex pairs, whose shortest paths need to be recomputed due to the inserted edges in A or the removed edges in D.", "rank": 511, "start": 59627, "IsComparative": "1", "id": "st_511"}]}, {"paragraph_info": {"end": 59893, "start": 59784, "text": "3Here we assume the shortest path is unique for every vertex pair, which is almost always the case in ISOMAP.", "rank": 196, "paragraph_comparative_number": 0, "entities": [], "id": "p_196"}, "sentences": [{"end": 59893, "text": "3Here we assume the shortest path is unique for every vertex pair, which is almost always the case in ISOMAP.", "rank": 512, "start": 59784, "IsComparative": "0", "id": "st_512"}]}, {"paragraph_info": {"end": 59971, "start": 59893, "text": "summarizes the shortest path update process based on F.3.4.2.3 Time Complexity", "rank": 197, "paragraph_comparative_number": 0, "entities": [], "id": "p_197"}, "sentences": [{"end": 59948, "text": "summarizes the shortest path update process based on F.", "rank": 513, "start": 59893, "IsComparative": "0", "id": "st_513"}, {"end": 59971, "text": "3.4.2.3 Time Complexity", "rank": 514, "start": 59948, "IsComparative": "0", "id": "st_514"}]}, {"paragraph_info": {"end": 60050, "start": 59971, "text": "When a parameter increases, Algorithm 2 requires the time complexity of O(|A|nq", "rank": 198, "paragraph_comparative_number": 1, "entities": [], "id": "p_198"}, "sentences": [{"end": 60050, "text": "When a parameter increases, Algorithm 2 requires the time complexity of O(|A|nq", "rank": 515, "start": 59971, "IsComparative": "1", "id": "st_515"}]}, {"paragraph_info": {"end": 60134, "start": 60050, "text": "maxi,a |T (i; a)|) in which maxi,a |T (i; a)| is the maximum number of nodes in sub-", "rank": 199, "paragraph_comparative_number": 0, "entities": [], "id": "p_199"}, "sentences": [{"end": 60134, "text": "maxi,a |T (i; a)|) in which maxi,a |T (i; a)| is the maximum number of nodes in sub-", "rank": 516, "start": 60050, "IsComparative": "0", "id": "st_516"}]}, {"paragraph_info": {"end": 60216, "start": 60134, "text": "tree T(i;a) over all xis and inserted edge e(a, b)s.This complexity can be loosely", "rank": 200, "paragraph_comparative_number": 1, "entities": [], "id": "p_200"}, "sentences": [{"end": 60186, "text": "tree T(i;a) over all xis and inserted edge e(a, b)s.", "rank": 517, "start": 60134, "IsComparative": "1", "id": "st_517"}, {"end": 60216, "text": "This complexity can be loosely", "rank": 518, "start": 60186, "IsComparative": "0", "id": "st_518"}]}, {"paragraph_info": {"end": 60297, "start": 60216, "text": "bounded by O(|A|q|F|) where |F| is the number of affected vertex pairs due to the", "rank": 201, "paragraph_comparative_number": 1, "entities": [], "id": "p_201"}, "sentences": [{"end": 60297, "text": "bounded by O(|A|q|F|) where |F| is the number of affected vertex pairs due to the", "rank": 519, "start": 60216, "IsComparative": "1", "id": "st_519"}]}, {"paragraph_info": {"end": 60379, "start": 60297, "text": "inserted edges in A.For a decreasing parameter, the time complexity of Algorithm 3", "rank": 202, "paragraph_comparative_number": 0, "entities": [], "id": "p_202"}, "sentences": [{"end": 60317, "text": "inserted edges in A.", "rank": 520, "start": 60297, "IsComparative": "0", "id": "st_520"}, {"end": 60379, "text": "For a decreasing parameter, the time complexity of Algorithm 3", "rank": 521, "start": 60317, "IsComparative": "0", "id": "st_521"}]}, {"paragraph_info": {"end": 60462, "start": 60379, "text": "requires O(n2) computations since it visits every vertex pair exactly once.Now, let", "rank": 203, "paragraph_comparative_number": 1, "entities": [], "id": "p_203"}, "sentences": [{"end": 60454, "text": "requires O(n2) computations since it visits every vertex pair exactly once.", "rank": 522, "start": 60379, "IsComparative": "0", "id": "st_522"}, {"end": 60462, "text": "Now, let", "rank": 523, "start": 60454, "IsComparative": "1", "id": "st_523"}]}, {"paragraph_info": {"end": 60552, "start": 60462, "text": "us partition the entire vertices into two disjoint sets Vd(i) and Vdc(i) such that Vd(i) =", "rank": 204, "paragraph_comparative_number": 0, "entities": [], "id": "p_204"}, "sentences": [{"end": 60552, "text": "us partition the entire vertices into two disjoint sets Vd(i) and Vdc(i) such that Vd(i) =", "rank": 524, "start": 60462, "IsComparative": "0", "id": "st_524"}]}, {"paragraph_info": {"end": 60636, "start": 60552, "text": "<xj|(xi, xj)  F> for a certain xi.Then, the complexity of Algorithm 4 is represented", "rank": 205, "paragraph_comparative_number": 1, "entities": [], "id": "p_205"}, "sentences": [{"end": 60586, "text": "<xj|(xi, xj)  F> for a certain xi.", "rank": 525, "start": 60552, "IsComparative": "0", "id": "st_525"}, {"end": 60636, "text": "Then, the complexity of Algorithm 4 is represented", "rank": 526, "start": 60586, "IsComparative": "1", "id": "st_526"}]}, {"paragraph_info": {"end": 60721, "start": 60636, "text": "as O(n  maxi(|E | log |Vd(i)| + (|E |)) in which E = <e(xa, xb)  G|xa, xb  Vd(i)> iii", "rank": 206, "paragraph_comparative_number": 0, "entities": [], "id": "p_206"}, "sentences": [{"end": 60721, "text": "as O(n  maxi(|E | log |Vd(i)| + (|E |)) in which E = <e(xa, xb)  G|xa, xb  Vd(i)> iii", "rank": 527, "start": 60636, "IsComparative": "0", "id": "st_527"}]}, {"paragraph_info": {"end": 60787, "start": 60721, "text": "andE =<e(x,x)G|x V(i),x /V(i)>.Inbothcases,forsmallchanges iabadbd", "rank": 207, "paragraph_comparative_number": 0, "entities": [], "id": "p_207"}, "sentences": [{"end": 60752, "text": "andE =<e(x,x)G|x V(i),x /V(i)>.", "rank": 528, "start": 60721, "IsComparative": "0", "id": "st_528"}, {"end": 60787, "text": "Inbothcases,forsmallchanges iabadbd", "rank": 529, "start": 60752, "IsComparative": "0", "id": "st_529"}]}, {"paragraph_info": {"end": 60901, "start": 60787, "text": "in the neighborhood graph, |F| is expected to be much smaller than n2, which is the maximum possible value of |F|.", "rank": 208, "paragraph_comparative_number": 1, "entities": [], "id": "p_208"}, "sentences": [{"end": 60901, "text": "in the neighborhood graph, |F| is expected to be much smaller than n2, which is the maximum possible value of |F|.", "rank": 530, "start": 60787, "IsComparative": "1", "id": "st_530"}]}, {"paragraph_info": {"end": 60931, "start": 60901, "text": "3.4.3 Eigenvalue/vector Update", "rank": 209, "paragraph_comparative_number": 0, "entities": [], "id": "p_209"}, "sentences": [{"end": 60931, "text": "3.4.3 Eigenvalue/vector Update", "rank": 531, "start": 60901, "IsComparative": "0", "id": "st_531"}]}, {"paragraph_info": {"end": 61103, "start": 60931, "text": "Let us denote the updated DG after the shortest path update described in Section 4.2 as Dnew.In this step, Dnew is first converted into the pairwise inner product matrix GG", "rank": 210, "paragraph_comparative_number": 0, "entities": [], "id": "p_210"}, "sentences": [{"end": 61024, "text": "Let us denote the updated DG after the shortest path update described in Section 4.2 as Dnew.", "rank": 532, "start": 60931, "IsComparative": "0", "id": "st_532"}, {"end": 61103, "text": "In this step, Dnew is first converted into the pairwise inner product matrix GG", "rank": 533, "start": 61024, "IsComparative": "0", "id": "st_533"}]}, {"paragraph_info": {"end": 61267, "start": 61103, "text": "Bnew by Eq.(37).To get a lower dimensional embedding as shown in Eq.(38), G we need to obtain m eigenvalue/vector pairs (new, vnew), ..., (new, vnew) for Bnew.11mmG", "rank": 211, "paragraph_comparative_number": 3, "entities": [], "id": "p_211"}, "sentences": [{"end": 61114, "text": "Bnew by Eq.", "rank": 534, "start": 61103, "IsComparative": "0", "id": "st_534"}, {"end": 61119, "text": "(37).", "rank": 535, "start": 61114, "IsComparative": "1", "id": "st_535"}, {"end": 61171, "text": "To get a lower dimensional embedding as shown in Eq.", "rank": 536, "start": 61119, "IsComparative": "0", "id": "st_536"}, {"end": 61237, "text": "(38), G we need to obtain m eigenvalue/vector pairs (new, vnew), .", "rank": 537, "start": 61171, "IsComparative": "1", "id": "st_537"}, {"end": 61238, "text": ".", "rank": 538, "start": 61237, "IsComparative": "0", "id": "st_538"}, {"end": 61239, "text": ".", "rank": 539, "start": 61238, "IsComparative": "0", "id": "st_539"}, {"end": 61262, "text": ", (new, vnew) for Bnew.", "rank": 540, "start": 61239, "IsComparative": "0", "id": "st_540"}, {"end": 61267, "text": "11mmG", "rank": 541, "start": 61262, "IsComparative": "1", "id": "st_541"}]}, {"paragraph_info": {"end": 61520, "start": 61267, "text": "In this computation, the available information that we can exploit is the previous m eigenvalue/vector pairs (1, v1), ..., (m, vm) of BG.In fact, they can be good initial guesses for m eigenvalue/vector pairs for Bnew, assuming the two matrices B and GG", "rank": 212, "paragraph_comparative_number": 2, "entities": [], "id": "p_212"}, "sentences": [{"end": 61386, "text": "In this computation, the available information that we can exploit is the previous m eigenvalue/vector pairs (1, v1), .", "rank": 542, "start": 61267, "IsComparative": "1", "id": "st_542"}, {"end": 61387, "text": ".", "rank": 543, "start": 61386, "IsComparative": "0", "id": "st_543"}, {"end": 61388, "text": ".", "rank": 544, "start": 61387, "IsComparative": "0", "id": "st_544"}, {"end": 61404, "text": ", (m, vm) of BG.", "rank": 545, "start": 61388, "IsComparative": "0", "id": "st_545"}, {"end": 61520, "text": "In fact, they can be good initial guesses for m eigenvalue/vector pairs for Bnew, assuming the two matrices B and GG", "rank": 546, "start": 61404, "IsComparative": "1", "id": "st_546"}]}, {"paragraph_info": {"end": 61964, "start": 61520, "text": "Bnew are not much different in any sense.The original ISOMAP uses the Lanczos G algorithm <61>, which is an iterative method that is appropriate for solving the first few leading eigenvalue/vector pairs.The Lanczos algorithm iteratively refines the solution in the Krylov subspace that grows from an initial vector by multiplying itTable 6: Computation time in seconds required to determine the optimal k value by minimizing residual variances.", "rank": 213, "paragraph_comparative_number": 2, "entities": [], "id": "p_213"}, "sentences": [{"end": 61561, "text": "Bnew are not much different in any sense.", "rank": 547, "start": 61520, "IsComparative": "0", "id": "st_547"}, {"end": 61723, "text": "The original ISOMAP uses the Lanczos G algorithm <61>, which is an iterative method that is appropriate for solving the first few leading eigenvalue/vector pairs.", "rank": 548, "start": 61561, "IsComparative": "1", "id": "st_548"}, {"end": 61964, "text": "The Lanczos algorithm iteratively refines the solution in the Krylov subspace that grows from an initial vector by multiplying itTable 6: Computation time in seconds required to determine the optimal k value by minimizing residual variances.", "rank": 549, "start": 61723, "IsComparative": "1", "id": "st_549"}]}, {"paragraph_info": {"end": 62050, "start": 61964, "text": "with the matrix, i.e., span(b, Bnewb, (Bnew)2b, ...).The performance of the Lanczos GG", "rank": 214, "paragraph_comparative_number": 0, "entities": [], "id": "p_214"}, "sentences": [{"end": 62013, "text": "with the matrix, i.e., span(b, Bnewb, (Bnew)2b, .", "rank": 550, "start": 61964, "IsComparative": "0", "id": "st_550"}, {"end": 62014, "text": ".", "rank": 551, "start": 62013, "IsComparative": "0", "id": "st_551"}, {"end": 62015, "text": ".", "rank": 552, "start": 62014, "IsComparative": "0", "id": "st_552"}, {"end": 62017, "text": ").", "rank": 553, "start": 62015, "IsComparative": "0", "id": "st_553"}, {"end": 62050, "text": "The performance of the Lanczos GG", "rank": 554, "start": 62017, "IsComparative": "0", "id": "st_554"}]}, {"paragraph_info": {"end": 62133, "start": 62050, "text": "algorithm largely depends on how quickly such a Krylov subspace covers that spanned", "rank": 215, "paragraph_comparative_number": 0, "entities": [], "id": "p_215"}, "sentences": [{"end": 62133, "text": "algorithm largely depends on how quickly such a Krylov subspace covers that spanned", "rank": 555, "start": 62050, "IsComparative": "0", "id": "st_555"}]}, {"paragraph_info": {"end": 62212, "start": 62133, "text": "by the eigenvectors.Another characteristic of the Lanczos algorithm is that the", "rank": 216, "paragraph_comparative_number": 0, "entities": [], "id": "p_216"}, "sentences": [{"end": 62153, "text": "by the eigenvectors.", "rank": 556, "start": 62133, "IsComparative": "0", "id": "st_556"}, {"end": 62212, "text": "Another characteristic of the Lanczos algorithm is that the", "rank": 557, "start": 62153, "IsComparative": "0", "id": "st_557"}]}, {"paragraph_info": {"end": 62297, "start": 62212, "text": "least leading eigenvalue/vector pair converges slowest within a particular tolerance.", "rank": 217, "paragraph_comparative_number": 0, "entities": [], "id": "p_217"}, "sentences": [{"end": 62297, "text": "least leading eigenvalue/vector pair converges slowest within a particular tolerance.", "rank": 558, "start": 62212, "IsComparative": "0", "id": "st_558"}]}, {"paragraph_info": {"end": 62377, "start": 62297, "text": "In other words, when the Krylov subspace becomes k dimensions, the first leading", "rank": 218, "paragraph_comparative_number": 0, "entities": [], "id": "p_218"}, "sentences": [{"end": 62377, "text": "In other words, when the Krylov subspace becomes k dimensions, the first leading", "rank": 559, "start": 62297, "IsComparative": "0", "id": "st_559"}]}, {"paragraph_info": {"end": 62464, "start": 62377, "text": "eigenvalue is refined k times, the second one (k  1) times, the third one (k  2) times,", "rank": 219, "paragraph_comparative_number": 1, "entities": [], "id": "p_219"}, "sentences": [{"end": 62464, "text": "eigenvalue is refined k times, the second one (k  1) times, the third one (k  2) times,", "rank": 560, "start": 62377, "IsComparative": "1", "id": "st_560"}]}, {"paragraph_info": {"end": 62545, "start": 62464, "text": "and so on.In this sense, we suggest using an initial vector from which the Krylov", "rank": 220, "paragraph_comparative_number": 1, "entities": [], "id": "p_220"}, "sentences": [{"end": 62474, "text": "and so on.", "rank": 561, "start": 62464, "IsComparative": "1", "id": "st_561"}, {"end": 62545, "text": "In this sense, we suggest using an initial vector from which the Krylov", "rank": 562, "start": 62474, "IsComparative": "0", "id": "st_562"}]}, {"paragraph_info": {"end": 62634, "start": 62545, "text": "subspace grows as v , i.e., span(v , Bnewv , (Bnew)2v , ...), which possibly best m mGmGm", "rank": 221, "paragraph_comparative_number": 0, "entities": [], "id": "p_221"}, "sentences": [{"end": 62602, "text": "subspace grows as v , i.e., span(v , Bnewv , (Bnew)2v , .", "rank": 563, "start": 62545, "IsComparative": "0", "id": "st_563"}, {"end": 62603, "text": ".", "rank": 564, "start": 62602, "IsComparative": "0", "id": "st_564"}, {"end": 62604, "text": ".", "rank": 565, "start": 62603, "IsComparative": "0", "id": "st_565"}, {"end": 62634, "text": "), which possibly best m mGmGm", "rank": 566, "start": 62604, "IsComparative": "0", "id": "st_566"}]}, {"paragraph_info": {"end": 62719, "start": 62634, "text": "recovers (new, vnew).As a result, we can expect the Lanczos algorithm to terminate mm", "rank": 222, "paragraph_comparative_number": 1, "entities": [], "id": "p_222"}, "sentences": [{"end": 62655, "text": "recovers (new, vnew).", "rank": 567, "start": 62634, "IsComparative": "0", "id": "st_567"}, {"end": 62719, "text": "As a result, we can expect the Lanczos algorithm to terminate mm", "rank": 568, "start": 62655, "IsComparative": "1", "id": "st_568"}]}, {"paragraph_info": {"end": 62772, "start": 62719, "text": "in less number of iterations than in any other cases.", "rank": 223, "paragraph_comparative_number": 0, "entities": [], "id": "p_223"}, "sentences": [{"end": 62772, "text": "in less number of iterations than in any other cases.", "rank": 569, "start": 62719, "IsComparative": "0", "id": "st_569"}]}, {"paragraph_info": {"end": 62804, "start": 62772, "text": "3.5 Experiments and Applications", "rank": 224, "paragraph_comparative_number": 1, "entities": [], "id": "p_224"}, "sentences": [{"end": 62804, "text": "3.5 Experiments and Applications", "rank": 570, "start": 62772, "IsComparative": "1", "id": "st_570"}]}, {"paragraph_info": {"end": 63596, "start": 62804, "text": "In this section, we present an empirical comparison between the computation times of ISOMAP and those of p-ISOMAP using both synthetic and real-world data sets.In addition, we show visualization applications of p-ISOMAP for real-world data sets.In our experiments, we used the code of ISOMAP provided by the original author.4 However, the original code does not take advantage of sparse graphs, so we compared p-ISOMAP with an improved version of ISOMAP that runs Dijkstras algorithm in C++ with a sparse representation of the graph.p-ISOMAP was implemented mainly in MATLAB except for the shortest path update part, which runs in C++.In both ISOMAP and p-ISOMAP, the eigendecomposition was done by MATLAB built- in function eigs, which performs the Lanczos algorithm by using Fortran library", "rank": 225, "paragraph_comparative_number": 1, "entities": [], "id": "p_225"}, "sentences": [{"end": 62964, "text": "In this section, we present an empirical comparison between the computation times of ISOMAP and those of p-ISOMAP using both synthetic and real-world data sets.", "rank": 571, "start": 62804, "IsComparative": "0", "id": "st_571"}, {"end": 63049, "text": "In addition, we show visualization applications of p-ISOMAP for real-world data sets.", "rank": 572, "start": 62964, "IsComparative": "0", "id": "st_572"}, {"end": 63337, "text": "In our experiments, we used the code of ISOMAP provided by the original author.4 However, the original code does not take advantage of sparse graphs, so we compared p-ISOMAP with an improved version of ISOMAP that runs Dijkstras algorithm in C++ with a sparse representation of the graph.", "rank": 573, "start": 63049, "IsComparative": "1", "id": "st_573"}, {"end": 63439, "text": "p-ISOMAP was implemented mainly in MATLAB except for the shortest path update part, which runs in C++.", "rank": 574, "start": 63337, "IsComparative": "0", "id": "st_574"}, {"end": 63596, "text": "In both ISOMAP and p-ISOMAP, the eigendecomposition was done by MATLAB built- in function eigs, which performs the Lanczos algorithm by using Fortran library", "rank": 575, "start": 63439, "IsComparative": "0", "id": "st_575"}]}, {"paragraph_info": {"end": 63900, "start": 63596, "text": "ARPACK <89>.Throughout all experiments, we used the ISOMAP parameter as k, where the neighborhood graph is constructed by k-NN, since we can easily bound |A| or |D| by O(nk) in which k = knew  k. All the experiments were done using MATLAB 7.7.0 on Windows Vista 64bit with 3.0GHz CPU with a 4.0GB memory.", "rank": 226, "paragraph_comparative_number": 1, "entities": [], "id": "p_226"}, "sentences": [{"end": 63608, "text": "ARPACK <89>.", "rank": 576, "start": 63596, "IsComparative": "0", "id": "st_576"}, {"end": 63900, "text": "Throughout all experiments, we used the ISOMAP parameter as k, where the neighborhood graph is constructed by k-NN, since we can easily bound |A| or |D| by O(nk) in which k = knew  k. All the experiments were done using MATLAB 7.7.0 on Windows Vista 64bit with 3.0GHz CPU with a 4.0GB memory.", "rank": 577, "start": 63608, "IsComparative": "1", "id": "st_577"}]}, {"paragraph_info": {"end": 63922, "start": 63900, "text": "3.5.1 Computation Time", "rank": 227, "paragraph_comparative_number": 0, "entities": [], "id": "p_227"}, "sentences": [{"end": 63922, "text": "3.5.1 Computation Time", "rank": 578, "start": 63900, "IsComparative": "0", "id": "st_578"}]}, {"paragraph_info": {"end": 65473, "start": 63922, "text": "To compare the computation times between ISOMAP and p-ISOMAP, we tested two synthetic data sets (Rand and Swiss roll) and two real-world data sets (Pendigits and Medline).Rand data set was made by sampling a uniform distribution in a 5,000- dimensional hypercube, <0, 1>5000, where the number of data is 3,500.Swiss roll data set has 4,000 data points in three-dimensional space.Pendigits data set5 contains 10,992 handwritten digit data in a form of pen traces in 16-dimensional space <11>, but we selected 3,000 data with an equal number of data per cluster because of memory constraints.Finally, Medline data set6 is a document corpus related to medical science from the National Institutes of Health, and it has 2,500 documents encoded in 22,095- dimensional space.Table 5 compares computation times of ISOMAP with those of p-ISOMAP for each data set.In most cases, p-ISOMAP runs significantly faster than ISOMAP.However, as the number of vertex pairs whose shortest paths need to be updated increases, the computational advantage of p-ISOMAP over ISOMAP gradually vanishes.Nonetheless, except for Swiss roll data set, which involves a large number of the shortest path update even with a slight parameter change, most data sets require only about 10-40% the shortest path update for a reasonable parameter change, e.g., within 5.Fig.7 shows the behaviors of p-ISOMAP depending on the number of data, k, and an initial k value.We selected Rand data since it was the most suitable one to clearly observe its behaviors.Fig.7(a) shows the computation", "rank": 228, "paragraph_comparative_number": 10, "entities": [], "id": "p_228"}, "sentences": [{"end": 64093, "text": "To compare the computation times between ISOMAP and p-ISOMAP, we tested two synthetic data sets (Rand and Swiss roll) and two real-world data sets (Pendigits and Medline).", "rank": 579, "start": 63922, "IsComparative": "1", "id": "st_579"}, {"end": 64232, "text": "Rand data set was made by sampling a uniform distribution in a 5,000- dimensional hypercube, <0, 1>5000, where the number of data is 3,500.", "rank": 580, "start": 64093, "IsComparative": "1", "id": "st_580"}, {"end": 64301, "text": "Swiss roll data set has 4,000 data points in three-dimensional space.", "rank": 581, "start": 64232, "IsComparative": "1", "id": "st_581"}, {"end": 64512, "text": "Pendigits data set5 contains 10,992 handwritten digit data in a form of pen traces in 16-dimensional space <11>, but we selected 3,000 data with an equal number of data per cluster because of memory constraints.", "rank": 582, "start": 64301, "IsComparative": "1", "id": "st_582"}, {"end": 64691, "text": "Finally, Medline data set6 is a document corpus related to medical science from the National Institutes of Health, and it has 2,500 documents encoded in 22,095- dimensional space.", "rank": 583, "start": 64512, "IsComparative": "1", "id": "st_583"}, {"end": 64777, "text": "Table 5 compares computation times of ISOMAP with those of p-ISOMAP for each data set.", "rank": 584, "start": 64691, "IsComparative": "1", "id": "st_584"}, {"end": 64839, "text": "In most cases, p-ISOMAP runs significantly faster than ISOMAP.", "rank": 585, "start": 64777, "IsComparative": "0", "id": "st_585"}, {"end": 65000, "text": "However, as the number of vertex pairs whose shortest paths need to be updated increases, the computational advantage of p-ISOMAP over ISOMAP gradually vanishes.", "rank": 586, "start": 64839, "IsComparative": "1", "id": "st_586"}, {"end": 65256, "text": "Nonetheless, except for Swiss roll data set, which involves a large number of the shortest path update even with a slight parameter change, most data sets require only about 10-40% the shortest path update for a reasonable parameter change, e.g., within 5.", "rank": 587, "start": 65000, "IsComparative": "1", "id": "st_587"}, {"end": 65260, "text": "Fig.", "rank": 588, "start": 65256, "IsComparative": "0", "id": "st_588"}, {"end": 65353, "text": "7 shows the behaviors of p-ISOMAP depending on the number of data, k, and an initial k value.", "rank": 589, "start": 65260, "IsComparative": "1", "id": "st_589"}, {"end": 65443, "text": "We selected Rand data since it was the most suitable one to clearly observe its behaviors.", "rank": 590, "start": 65353, "IsComparative": "1", "id": "st_590"}, {"end": 65447, "text": "Fig.", "rank": 591, "start": 65443, "IsComparative": "0", "id": "st_591"}, {"end": 65473, "text": "7(a) shows the computation", "rank": 592, "start": 65447, "IsComparative": "0", "id": "st_592"}]}, {"paragraph_info": {"end": 65557, "start": 65473, "text": "5http://archive.ics.uci.edu/ml/datasets/Pen-Based+Recognition+of+Handwritten+ Digits", "rank": 229, "paragraph_comparative_number": 0, "entities": [], "id": "p_229"}, "sentences": [{"end": 65557, "text": "5http://archive.ics.uci.edu/ml/datasets/Pen-Based+Recognition+of+Handwritten+ Digits", "rank": 593, "start": 65473, "IsComparative": "0", "id": "st_593"}]}, {"paragraph_info": {"end": 65599, "start": 65557, "text": "6http://www.cc.gatech.edu/~hpark/data.html", "rank": 230, "paragraph_comparative_number": 0, "entities": [], "id": "p_230"}, "sentences": [{"end": 65599, "text": "6http://www.cc.gatech.edu/~hpark/data.html", "rank": 594, "start": 65557, "IsComparative": "0", "id": "st_594"}]}, {"paragraph_info": {"end": 66546, "start": 65599, "text": "time in terms of the number of data.As we can see, p-ISOMAP scales well in terms of the number of data compared to ISOMAP.In Fig.7(b), as the parameter change k gets bigger, the running time of p-ISOMAP increases linearly, which tells that |A| or |D|, which is proportional to k, has a dominant influence on the performance of p-ISOMAP.Finally, Fig.7(c) shows an increasing performance gap between two methods as an initial k value grows.This is mainly because the original Dijkstras algorithm used in ISOMAP needs more computations as the graph gets denser while p-ISOMAP depends only on |A|, |D|, or correspondingly |F|, which probably does not increase over different initial k values.Finally, for each data set, we measured the computation times to take to determine the optimal k value that minimizes residual variances <12>.As shown in Table 6, we could significantly reduce the computation times by utilizing the dynamic update of p-ISOMAP.", "rank": 231, "paragraph_comparative_number": 5, "entities": [], "id": "p_231"}, "sentences": [{"end": 65635, "text": "time in terms of the number of data.", "rank": 595, "start": 65599, "IsComparative": "0", "id": "st_595"}, {"end": 65721, "text": "As we can see, p-ISOMAP scales well in terms of the number of data compared to ISOMAP.", "rank": 596, "start": 65635, "IsComparative": "1", "id": "st_596"}, {"end": 65728, "text": "In Fig.", "rank": 597, "start": 65721, "IsComparative": "0", "id": "st_597"}, {"end": 65935, "text": "7(b), as the parameter change k gets bigger, the running time of p-ISOMAP increases linearly, which tells that |A| or |D|, which is proportional to k, has a dominant influence on the performance of p-ISOMAP.", "rank": 598, "start": 65728, "IsComparative": "1", "id": "st_598"}, {"end": 65948, "text": "Finally, Fig.", "rank": 599, "start": 65935, "IsComparative": "0", "id": "st_599"}, {"end": 66037, "text": "7(c) shows an increasing performance gap between two methods as an initial k value grows.", "rank": 600, "start": 65948, "IsComparative": "1", "id": "st_600"}, {"end": 66287, "text": "This is mainly because the original Dijkstras algorithm used in ISOMAP needs more computations as the graph gets denser while p-ISOMAP depends only on |A|, |D|, or correspondingly |F|, which probably does not increase over different initial k values.", "rank": 601, "start": 66037, "IsComparative": "0", "id": "st_601"}, {"end": 66429, "text": "Finally, for each data set, we measured the computation times to take to determine the optimal k value that minimizes residual variances <12>.", "rank": 602, "start": 66287, "IsComparative": "1", "id": "st_602"}, {"end": 66546, "text": "As shown in Table 6, we could significantly reduce the computation times by utilizing the dynamic update of p-ISOMAP.", "rank": 603, "start": 66429, "IsComparative": "1", "id": "st_603"}]}, {"paragraph_info": {"end": 66604, "start": 66546, "text": "3.5.2 Knowledge Discovery via Visualization using p-ISOMAP", "rank": 232, "paragraph_comparative_number": 0, "entities": [], "id": "p_232"}, "sentences": [{"end": 66604, "text": "3.5.2 Knowledge Discovery via Visualization using p-ISOMAP", "rank": 604, "start": 66546, "IsComparative": "0", "id": "st_604"}]}, {"paragraph_info": {"end": 67599, "start": 66604, "text": "In this section, we present interesting visualization examples of real-world data sets using p-ISOMAP.To be specific, we show how ISOMAP with different parameters can discover various knowledge about data and how the information acquired through visualization can facilitate traditional data mining problems such as a classification task.p-ISOMAP was used to efficiently update ISOMAP results throughout all the visualization experiments.To begin with, we have chosen three real-world data sets (Weizmann, Medline, and Pendigits) that have cluster structures in order to make it easy to analyze their visualization.Weizmann data set is a facial image data set7 that has 28 persons images with various angles, illuminations, and facial expressions.To obtain an understandable visualization, we have chosen three particular persons images with three different viewing angles as shown in Fig.8(a), in which each com- bination of a particular person and a viewing angle contains multiple images that", "rank": 233, "paragraph_comparative_number": 4, "entities": [], "id": "p_233"}, "sentences": [{"end": 66706, "text": "In this section, we present interesting visualization examples of real-world data sets using p-ISOMAP.", "rank": 605, "start": 66604, "IsComparative": "0", "id": "st_605"}, {"end": 66942, "text": "To be specific, we show how ISOMAP with different parameters can discover various knowledge about data and how the information acquired through visualization can facilitate traditional data mining problems such as a classification task.", "rank": 606, "start": 66706, "IsComparative": "1", "id": "st_606"}, {"end": 67042, "text": "p-ISOMAP was used to efficiently update ISOMAP results throughout all the visualization experiments.", "rank": 607, "start": 66942, "IsComparative": "1", "id": "st_607"}, {"end": 67219, "text": "To begin with, we have chosen three real-world data sets (Weizmann, Medline, and Pendigits) that have cluster structures in order to make it easy to analyze their visualization.", "rank": 608, "start": 67042, "IsComparative": "1", "id": "st_608"}, {"end": 67351, "text": "Weizmann data set is a facial image data set7 that has 28 persons images with various angles, illuminations, and facial expressions.", "rank": 609, "start": 67219, "IsComparative": "1", "id": "st_609"}, {"end": 67493, "text": "To obtain an understandable visualization, we have chosen three particular persons images with three different viewing angles as shown in Fig.", "rank": 610, "start": 67351, "IsComparative": "0", "id": "st_610"}, {"end": 67599, "text": "8(a), in which each com- bination of a particular person and a viewing angle contains multiple images that", "rank": 611, "start": 67493, "IsComparative": "0", "id": "st_611"}]}, {"paragraph_info": {"end": 67644, "start": 67599, "text": "7ftp://ftp.wisdom.weizmann.ac.il/pub/facebase", "rank": 234, "paragraph_comparative_number": 0, "entities": [], "id": "p_234"}, "sentences": [{"end": 67644, "text": "7ftp://ftp.wisdom.weizmann.ac.il/pub/facebase", "rank": 612, "start": 67599, "IsComparative": "0", "id": "st_612"}]}, {"paragraph_info": {"end": 69754, "start": 67644, "text": "vary based on other factors such as illuminations and facial expressions.In their visualizations shown in Figs.8(b)-(d), each of these images is represented as a letter that corresponds to its cluster from Fig.8(a).Medline data set, which is a document collection, has 5 topic clusters, heart attack (h), colon cancer (c), diabetes (d), oral cancer (o), and tooth decay (t), in which the letters in parentheses are used in its visualization in Fig.9.Pendigits data set, which is described in Section 5.1, has 10 clusters in terms of which digit each data item corresponds to, i.e., 0, 1, ..., 9.Several interesting visualization examples of these data based on p-ISOMAP are shown in Figs.8-108 where cluster centroids and neighborhood connections are also shown in the form of letters in rectangles and grey lines in the background, re- spectively.Among visualization examples of Weizmann data set, Fig.8(c), which well resembles the layout of clusters in Fig.8(a), successfully straightens its intrinsic manifold defined by the two factors, a person and an angle.This is mainly because of the neighborhood graph constructed by a proper k value that forms its edges either within a particular person or within a particular angle, which is why we mostly see horizonal and vertical neighborhood connections as well as gaps between grid-shaped cluster centroids in Fig.8(c).Regarding a comparison between Figs.8(b) and 8(d), fewer neighbors in Fig.8(b) bring connections only within images with the same angle, which in turn results in a clustered form of visualization based on angles.This indi- cates that even if we prefer the similarity in terms of a person to that in terms of an angle, the actual distances in the vector space into which the images are transformed are dominated by an angle.On the other hand, Fig.8(d) connects almost all the data points between each other, which would reflect the Euclidean distances in the original space just like MDS does.In addition, we can consider the layout of cluster structure shown in Fig.8(d) as a curved version of manifold as it appears in the original space,", "rank": 235, "paragraph_comparative_number": 7, "entities": [], "id": "p_235"}, "sentences": [{"end": 67717, "text": "vary based on other factors such as illuminations and facial expressions.", "rank": 613, "start": 67644, "IsComparative": "0", "id": "st_613"}, {"end": 67755, "text": "In their visualizations shown in Figs.", "rank": 614, "start": 67717, "IsComparative": "0", "id": "st_614"}, {"end": 67854, "text": "8(b)-(d), each of these images is represented as a letter that corresponds to its cluster from Fig.", "rank": 615, "start": 67755, "IsComparative": "1", "id": "st_615"}, {"end": 67859, "text": "8(a).", "rank": 616, "start": 67854, "IsComparative": "1", "id": "st_616"}, {"end": 68092, "text": "Medline data set, which is a document collection, has 5 topic clusters, heart attack (h), colon cancer (c), diabetes (d), oral cancer (o), and tooth decay (t), in which the letters in parentheses are used in its visualization in Fig.", "rank": 617, "start": 67859, "IsComparative": "1", "id": "st_617"}, {"end": 68094, "text": "9.", "rank": 618, "start": 68092, "IsComparative": "1", "id": "st_618"}, {"end": 68233, "text": "Pendigits data set, which is described in Section 5.1, has 10 clusters in terms of which digit each data item corresponds to, i.e., 0, 1, .", "rank": 619, "start": 68094, "IsComparative": "0", "id": "st_619"}, {"end": 68234, "text": ".", "rank": 620, "start": 68233, "IsComparative": "0", "id": "st_620"}, {"end": 68235, "text": ".", "rank": 621, "start": 68234, "IsComparative": "0", "id": "st_621"}, {"end": 68239, "text": ", 9.", "rank": 622, "start": 68235, "IsComparative": "1", "id": "st_622"}, {"end": 68332, "text": "Several interesting visualization examples of these data based on p-ISOMAP are shown in Figs.", "rank": 623, "start": 68239, "IsComparative": "0", "id": "st_623"}, {"end": 68492, "text": "8-108 where cluster centroids and neighborhood connections are also shown in the form of letters in rectangles and grey lines in the background, re- spectively.", "rank": 624, "start": 68332, "IsComparative": "0", "id": "st_624"}, {"end": 68547, "text": "Among visualization examples of Weizmann data set, Fig.", "rank": 625, "start": 68492, "IsComparative": "0", "id": "st_625"}, {"end": 68604, "text": "8(c), which well resembles the layout of clusters in Fig.", "rank": 626, "start": 68547, "IsComparative": "0", "id": "st_626"}, {"end": 68708, "text": "8(a), successfully straightens its intrinsic manifold defined by the two factors, a person and an angle.", "rank": 627, "start": 68604, "IsComparative": "1", "id": "st_627"}, {"end": 69010, "text": "This is mainly because of the neighborhood graph constructed by a proper k value that forms its edges either within a particular person or within a particular angle, which is why we mostly see horizonal and vertical neighborhood connections as well as gaps between grid-shaped cluster centroids in Fig.", "rank": 628, "start": 68708, "IsComparative": "1", "id": "st_628"}, {"end": 69015, "text": "8(c).", "rank": 629, "start": 69010, "IsComparative": "0", "id": "st_629"}, {"end": 69051, "text": "Regarding a comparison between Figs.", "rank": 630, "start": 69015, "IsComparative": "0", "id": "st_630"}, {"end": 69089, "text": "8(b) and 8(d), fewer neighbors in Fig.", "rank": 631, "start": 69051, "IsComparative": "0", "id": "st_631"}, {"end": 69227, "text": "8(b) bring connections only within images with the same angle, which in turn results in a clustered form of visualization based on angles.", "rank": 632, "start": 69089, "IsComparative": "0", "id": "st_632"}, {"end": 69438, "text": "This indi- cates that even if we prefer the similarity in terms of a person to that in terms of an angle, the actual distances in the vector space into which the images are transformed are dominated by an angle.", "rank": 633, "start": 69227, "IsComparative": "0", "id": "st_633"}, {"end": 69461, "text": "On the other hand, Fig.", "rank": 634, "start": 69438, "IsComparative": "0", "id": "st_634"}, {"end": 69607, "text": "8(d) connects almost all the data points between each other, which would reflect the Euclidean distances in the original space just like MDS does.", "rank": 635, "start": 69461, "IsComparative": "0", "id": "st_635"}, {"end": 69681, "text": "In addition, we can consider the layout of cluster structure shown in Fig.", "rank": 636, "start": 69607, "IsComparative": "0", "id": "st_636"}, {"end": 69754, "text": "8(d) as a curved version of manifold as it appears in the original space,", "rank": 637, "start": 69681, "IsComparative": "0", "id": "st_637"}]}, {"paragraph_info": {"end": 74224, "start": 69754, "text": "8These figures can be arbitrarily magnified without losing the resolution in the electronic version of this thesis.which is analogous to what we discussed in Fig.6.Medline data shown in Fig.9 is not visualized in a well-clustered form by ISOMAP because it is usually difficult to find a well-defined manifold structure with few meaningful dimensions for document data.However, by manipulating k values, we can at least obtain various visualization results that possibly reveal different aspects of the data.For example, when k = 30 in Fig.8(b), the topic cluster, tooth decay (t), is shown distinct from the other clusters while so does the cluster, diabetes (d), in the other cases.In this situation, if one wants to focus on a certain cluster separately from the others, it would be necessary to change k values for a suitable visualization result.Visualizations of Pendigits data set shown in Fig.10 give numerous interesting characteristics.First of all, as the parameter k increases, the overall transition from Fig.10(a) to 10(f) is shown similar to that of Swiss roll data set from Fig.6(c) to 6(d).In other words, a larger k value places more data in a curved shape, which reflects the underlying curvature in the original space, while a smaller k value does more data in a linear shape, which corresponds to a straightened manifold.To be specific, starting from Fig.10(b), the cluster 8 gradually gets scattered and curved with an increasing k. Similarly, the cluster 0 maintains a linear shape before k = 50, and finally it becomes scattered in Fig.10(f).In short, ISOMAP with a small parameter value tends to unroll the curved manifold due to geodesic paths, but that with a large parameter better shows its curvature itself.In view of clustering, Fig.10(a) well separates the clusters 2 and 7 whereas the other visualizations gradually overlap them with increasing k values.In addition, the clusters 3 and 6 appears to overlap for a certain range of k between 9 and 11 as shown in Figs.10(b)-(d).Now let us discuss about subcluster/outlier discovery through various visualization examples.In most examples in Fig.10, the cluster 5 is shown to have two subclusters, one of which is near the cluster 8, and the other between the clusters 3 and 9.Based on this observation, we examined some sample data from each cluster and found out such subclusters are due to the different way to write 5.9 From the examples in these two subclusters shown in Figs.11(a)-(b), we can see that some people write 5 starting from the hat, which is the top horizontal line in 5, while others write the hat after finishing the bottom part.Similarly, the cluster 7 has a majority of data near the cluster 2, but it also has two minor groups of data near the cluster 1 and the cluster 6, respectively.(See, for example, the coordinates around (100, 50) and (50, 150) in Fig.10(c).)After looking at the actual data samples from these groups, we found that most people write 7 in a way shown in Fig.11(c).However, some people first write an additional small vertical line in the top-left part but by omitting the small horizontal line in the middle part as shown in Fig.11(d), which corresponds to the minor data near the cluster 1, but some others just reverse the direction to write the small horizontal line in the middle part of 7 as shown in Fig.11(e), which corresponds to those near the cluster 6.In addition, their different traces and shapes impose similarities to those of the clusters 1 and 6, respectively.Finally, in Fig.10(d), some data in the cluster 0 seems to deviate from its major line-shaped data in Figs.10(a)-(c).Figs.11(f) and 11(g) represent the latter and the former data, respectively.We can see that such deviated ones shown in Fig.11(g) start from the top-right corner rather than from the top-middle part when writing 0, which causes their connections to the clus- ter 5 that also starts from the top-right corner.Finally, we have incorporated the above findings in a handwritten digit recognition, which is a classification problem, using Pendigits data set.Based on the information that the cluster 5 has two clear subclusters, we modified the training data labels in the cluster 5 into two different labels and classified the test data that are assigned either label to the cluster 5.As a classification method, we have chosen the linear discriminant analysis combined with k-nearest neighbor classification, which is a common setting in classification.", "rank": 236, "paragraph_comparative_number": 19, "entities": [], "id": "p_236"}, "sentences": [{"end": 69869, "text": "8These figures can be arbitrarily magnified without losing the resolution in the electronic version of this thesis.", "rank": 638, "start": 69754, "IsComparative": "1", "id": "st_638"}, {"end": 69916, "text": "which is analogous to what we discussed in Fig.", "rank": 639, "start": 69869, "IsComparative": "1", "id": "st_639"}, {"end": 69918, "text": "6.", "rank": 640, "start": 69916, "IsComparative": "1", "id": "st_640"}, {"end": 69944, "text": "Medline data shown in Fig.", "rank": 641, "start": 69918, "IsComparative": "0", "id": "st_641"}, {"end": 70122, "text": "9 is not visualized in a well-clustered form by ISOMAP because it is usually difficult to find a well-defined manifold structure with few meaningful dimensions for document data.", "rank": 642, "start": 69944, "IsComparative": "0", "id": "st_642"}, {"end": 70261, "text": "However, by manipulating k values, we can at least obtain various visualization results that possibly reveal different aspects of the data.", "rank": 643, "start": 70122, "IsComparative": "1", "id": "st_643"}, {"end": 70293, "text": "For example, when k = 30 in Fig.", "rank": 644, "start": 70261, "IsComparative": "0", "id": "st_644"}, {"end": 70437, "text": "8(b), the topic cluster, tooth decay (t), is shown distinct from the other clusters while so does the cluster, diabetes (d), in the other cases.", "rank": 645, "start": 70293, "IsComparative": "0", "id": "st_645"}, {"end": 70604, "text": "In this situation, if one wants to focus on a certain cluster separately from the others, it would be necessary to change k values for a suitable visualization result.", "rank": 646, "start": 70437, "IsComparative": "1", "id": "st_646"}, {"end": 70654, "text": "Visualizations of Pendigits data set shown in Fig.", "rank": 647, "start": 70604, "IsComparative": "0", "id": "st_647"}, {"end": 70699, "text": "10 give numerous interesting characteristics.", "rank": 648, "start": 70654, "IsComparative": "0", "id": "st_648"}, {"end": 70775, "text": "First of all, as the parameter k increases, the overall transition from Fig.", "rank": 649, "start": 70699, "IsComparative": "1", "id": "st_649"}, {"end": 70847, "text": "10(a) to 10(f) is shown similar to that of Swiss roll data set from Fig.", "rank": 650, "start": 70775, "IsComparative": "1", "id": "st_650"}, {"end": 70860, "text": "6(c) to 6(d).", "rank": 651, "start": 70847, "IsComparative": "0", "id": "st_651"}, {"end": 71095, "text": "In other words, a larger k value places more data in a curved shape, which reflects the underlying curvature in the original space, while a smaller k value does more data in a linear shape, which corresponds to a straightened manifold.", "rank": 652, "start": 70860, "IsComparative": "1", "id": "st_652"}, {"end": 71129, "text": "To be specific, starting from Fig.", "rank": 653, "start": 71095, "IsComparative": "0", "id": "st_653"}, {"end": 71313, "text": "10(b), the cluster 8 gradually gets scattered and curved with an increasing k. Similarly, the cluster 0 maintains a linear shape before k = 50, and finally it becomes scattered in Fig.", "rank": 654, "start": 71129, "IsComparative": "0", "id": "st_654"}, {"end": 71319, "text": "10(f).", "rank": 655, "start": 71313, "IsComparative": "0", "id": "st_655"}, {"end": 71490, "text": "In short, ISOMAP with a small parameter value tends to unroll the curved manifold due to geodesic paths, but that with a large parameter better shows its curvature itself.", "rank": 656, "start": 71319, "IsComparative": "1", "id": "st_656"}, {"end": 71517, "text": "In view of clustering, Fig.", "rank": 657, "start": 71490, "IsComparative": "0", "id": "st_657"}, {"end": 71640, "text": "10(a) well separates the clusters 2 and 7 whereas the other visualizations gradually overlap them with increasing k values.", "rank": 658, "start": 71517, "IsComparative": "1", "id": "st_658"}, {"end": 71752, "text": "In addition, the clusters 3 and 6 appears to overlap for a certain range of k between 9 and 11 as shown in Figs.", "rank": 659, "start": 71640, "IsComparative": "1", "id": "st_659"}, {"end": 71762, "text": "10(b)-(d).", "rank": 660, "start": 71752, "IsComparative": "0", "id": "st_660"}, {"end": 71855, "text": "Now let us discuss about subcluster/outlier discovery through various visualization examples.", "rank": 661, "start": 71762, "IsComparative": "1", "id": "st_661"}, {"end": 71879, "text": "In most examples in Fig.", "rank": 662, "start": 71855, "IsComparative": "0", "id": "st_662"}, {"end": 72010, "text": "10, the cluster 5 is shown to have two subclusters, one of which is near the cluster 8, and the other between the clusters 3 and 9.", "rank": 663, "start": 71879, "IsComparative": "1", "id": "st_663"}, {"end": 72214, "text": "Based on this observation, we examined some sample data from each cluster and found out such subclusters are due to the different way to write 5.9 From the examples in these two subclusters shown in Figs.", "rank": 664, "start": 72010, "IsComparative": "1", "id": "st_664"}, {"end": 72382, "text": "11(a)-(b), we can see that some people write 5 starting from the hat, which is the top horizontal line in 5, while others write the hat after finishing the bottom part.", "rank": 665, "start": 72214, "IsComparative": "1", "id": "st_665"}, {"end": 72542, "text": "Similarly, the cluster 7 has a majority of data near the cluster 2, but it also has two minor groups of data near the cluster 1 and the cluster 6, respectively.", "rank": 666, "start": 72382, "IsComparative": "0", "id": "st_666"}, {"end": 72615, "text": "(See, for example, the coordinates around (100, 50) and (50, 150) in Fig.", "rank": 667, "start": 72542, "IsComparative": "0", "id": "st_667"}, {"end": 72622, "text": "10(c).)", "rank": 668, "start": 72615, "IsComparative": "0", "id": "st_668"}, {"end": 72738, "text": "After looking at the actual data samples from these groups, we found that most people write 7 in a way shown in Fig.", "rank": 669, "start": 72622, "IsComparative": "0", "id": "st_669"}, {"end": 72744, "text": "11(c).", "rank": 670, "start": 72738, "IsComparative": "0", "id": "st_670"}, {"end": 72909, "text": "However, some people first write an additional small vertical line in the top-left part but by omitting the small horizontal line in the middle part as shown in Fig.", "rank": 671, "start": 72744, "IsComparative": "0", "id": "st_671"}, {"end": 73090, "text": "11(d), which corresponds to the minor data near the cluster 1, but some others just reverse the direction to write the small horizontal line in the middle part of 7 as shown in Fig.", "rank": 672, "start": 72909, "IsComparative": "0", "id": "st_672"}, {"end": 73143, "text": "11(e), which corresponds to those near the cluster 6.", "rank": 673, "start": 73090, "IsComparative": "0", "id": "st_673"}, {"end": 73257, "text": "In addition, their different traces and shapes impose similarities to those of the clusters 1 and 6, respectively.", "rank": 674, "start": 73143, "IsComparative": "1", "id": "st_674"}, {"end": 73273, "text": "Finally, in Fig.", "rank": 675, "start": 73257, "IsComparative": "0", "id": "st_675"}, {"end": 73364, "text": "10(d), some data in the cluster 0 seems to deviate from its major line-shaped data in Figs.", "rank": 676, "start": 73273, "IsComparative": "1", "id": "st_676"}, {"end": 73374, "text": "10(a)-(c).", "rank": 677, "start": 73364, "IsComparative": "0", "id": "st_677"}, {"end": 73379, "text": "Figs.", "rank": 678, "start": 73374, "IsComparative": "0", "id": "st_678"}, {"end": 73450, "text": "11(f) and 11(g) represent the latter and the former data, respectively.", "rank": 679, "start": 73379, "IsComparative": "0", "id": "st_679"}, {"end": 73498, "text": "We can see that such deviated ones shown in Fig.", "rank": 680, "start": 73450, "IsComparative": "0", "id": "st_680"}, {"end": 73682, "text": "11(g) start from the top-right corner rather than from the top-middle part when writing 0, which causes their connections to the clus- ter 5 that also starts from the top-right corner.", "rank": 681, "start": 73498, "IsComparative": "1", "id": "st_681"}, {"end": 73827, "text": "Finally, we have incorporated the above findings in a handwritten digit recognition, which is a classification problem, using Pendigits data set.", "rank": 682, "start": 73682, "IsComparative": "0", "id": "st_682"}, {"end": 74055, "text": "Based on the information that the cluster 5 has two clear subclusters, we modified the training data labels in the cluster 5 into two different labels and classified the test data that are assigned either label to the cluster 5.", "rank": 683, "start": 73827, "IsComparative": "1", "id": "st_683"}, {"end": 74224, "text": "As a classification method, we have chosen the linear discriminant analysis combined with k-nearest neighbor classification, which is a common setting in classification.", "rank": 684, "start": 74055, "IsComparative": "0", "id": "st_684"}]}, {"paragraph_info": {"end": 74376, "start": 74224, "text": "9Note that Pendigits data set we used here is not just static image data but the traces of the pen, which is why the order matters in the feature space.", "rank": 237, "paragraph_comparative_number": 1, "entities": [], "id": "p_237"}, "sentences": [{"end": 74376, "text": "9Note that Pendigits data set we used here is not just static image data but the traces of the pen, which is why the order matters in the feature space.", "rank": 685, "start": 74224, "IsComparative": "1", "id": "st_685"}]}, {"paragraph_info": {"end": 74649, "start": 74376, "text": "As a result, the classification accuracy increased from 89% to 93%.In fact, this is a promising example of human-aided data mining processes through visualizations with intelligent interaction.The computational efficiency of p-ISOMAP makes such processes smooth and prompt.", "rank": 238, "paragraph_comparative_number": 1, "entities": [], "id": "p_238"}, "sentences": [{"end": 74443, "text": "As a result, the classification accuracy increased from 89% to 93%.", "rank": 686, "start": 74376, "IsComparative": "0", "id": "st_686"}, {"end": 74569, "text": "In fact, this is a promising example of human-aided data mining processes through visualizations with intelligent interaction.", "rank": 687, "start": 74443, "IsComparative": "0", "id": "st_687"}, {"end": 74649, "text": "The computational efficiency of p-ISOMAP makes such processes smooth and prompt.", "rank": 688, "start": 74569, "IsComparative": "1", "id": "st_688"}]}, {"paragraph_info": {"end": 74664, "start": 74649, "text": "3.6 Conclusions", "rank": 239, "paragraph_comparative_number": 0, "entities": [], "id": "p_239"}, "sentences": [{"end": 74664, "text": "3.6 Conclusions", "rank": 689, "start": 74649, "IsComparative": "0", "id": "st_689"}]}, {"paragraph_info": {"end": 75227, "start": 74664, "text": "In this chapter, we proposed p-ISOMAP, an efficient algorithmic framework to dynam- ically update ISOMAP embedding for varying parameter values.The experiments using both synthetic and real-world data with various settings validate its efficiency.This advantage of p-ISOMAP can not only speed up the parameter optimization pro- cesses but also enable users to interact with visual analytics systems more smoothly.Such interaction provides us with deep understanding about data, which can improve even the computational data mining problems such as classification.", "rank": 240, "paragraph_comparative_number": 2, "entities": [], "id": "p_240"}, "sentences": [{"end": 74808, "text": "In this chapter, we proposed p-ISOMAP, an efficient algorithmic framework to dynam- ically update ISOMAP embedding for varying parameter values.", "rank": 690, "start": 74664, "IsComparative": "1", "id": "st_690"}, {"end": 74911, "text": "The experiments using both synthetic and real-world data with various settings validate its efficiency.", "rank": 691, "start": 74808, "IsComparative": "0", "id": "st_691"}, {"end": 75077, "text": "This advantage of p-ISOMAP can not only speed up the parameter optimization pro- cesses but also enable users to interact with visual analytics systems more smoothly.", "rank": 692, "start": 74911, "IsComparative": "1", "id": "st_692"}, {"end": 75227, "text": "Such interaction provides us with deep understanding about data, which can improve even the computational data mining problems such as classification.", "rank": 693, "start": 75077, "IsComparative": "0", "id": "st_693"}]}, {"paragraph_info": {"end": 75237, "start": 75227, "text": "CHAPTER IV", "rank": 241, "paragraph_comparative_number": 0, "entities": [], "id": "p_241"}, "sentences": [{"end": 75237, "text": "CHAPTER IV", "rank": 694, "start": 75227, "IsComparative": "0", "id": "st_694"}]}, {"paragraph_info": {"end": 75298, "start": 75237, "text": "ITERATION-WISE INTEGRATION FRAMEWORK OF COMPUTATIONAL METHODS", "rank": 242, "paragraph_comparative_number": 1, "entities": [], "id": "p_242"}, "sentences": [{"end": 75298, "text": "ITERATION-WISE INTEGRATION FRAMEWORK OF COMPUTATIONAL METHODS", "rank": 695, "start": 75237, "IsComparative": "1", "id": "st_695"}]}, {"paragraph_info": {"end": 76580, "start": 75298, "text": "Visual analytics has been gaining increasing interest due to its fascinating charac- teristic that leverages both humans visual perception and the power of computing.Although various computational methods are being proposed, they do not properly support visual analytics.One of the biggest obstacles towards their real-time vi- sual analytic integration is their high computational complexity.As a way to tackle this problem, this chapter presents PIVE, a Per-Iteration Visualization Environment for supporting real-time interactive visualization with computational methods.The main idea behind PIVE is that most advanced computational methods work by re- fining the solution iteratively.By visually delivering the result from each iteration to users, the proposed framework enables users to quickly acquire the information that the computational method provides as well as the ability to perform continuous interactions with them in real time.We show the effectiveness of PIVE in terms of real-time visualization and interaction capabilities by customizing various dimension reduction methods such as principal component analysis, multidimensional scaling, and t-distributed stochastic neighborhood embedding, and clustering methods such as k-means and latent Dirichlet allocation.", "rank": 243, "paragraph_comparative_number": 4, "entities": [], "id": "p_243"}, "sentences": [{"end": 75464, "text": "Visual analytics has been gaining increasing interest due to its fascinating charac- teristic that leverages both humans visual perception and the power of computing.", "rank": 696, "start": 75298, "IsComparative": "1", "id": "st_696"}, {"end": 75569, "text": "Although various computational methods are being proposed, they do not properly support visual analytics.", "rank": 697, "start": 75464, "IsComparative": "0", "id": "st_697"}, {"end": 75691, "text": "One of the biggest obstacles towards their real-time vi- sual analytic integration is their high computational complexity.", "rank": 698, "start": 75569, "IsComparative": "1", "id": "st_698"}, {"end": 75872, "text": "As a way to tackle this problem, this chapter presents PIVE, a Per-Iteration Visualization Environment for supporting real-time interactive visualization with computational methods.", "rank": 699, "start": 75691, "IsComparative": "1", "id": "st_699"}, {"end": 75986, "text": "The main idea behind PIVE is that most advanced computational methods work by re- fining the solution iteratively.", "rank": 700, "start": 75872, "IsComparative": "1", "id": "st_700"}, {"end": 76242, "text": "By visually delivering the result from each iteration to users, the proposed framework enables users to quickly acquire the information that the computational method provides as well as the ability to perform continuous interactions with them in real time.", "rank": 701, "start": 75986, "IsComparative": "0", "id": "st_701"}, {"end": 76580, "text": "We show the effectiveness of PIVE in terms of real-time visualization and interaction capabilities by customizing various dimension reduction methods such as principal component analysis, multidimensional scaling, and t-distributed stochastic neighborhood embedding, and clustering methods such as k-means and latent Dirichlet allocation.", "rank": 702, "start": 76242, "IsComparative": "0", "id": "st_702"}]}, {"paragraph_info": {"end": 76596, "start": 76580, "text": "4.1 Introduction", "rank": 244, "paragraph_comparative_number": 0, "entities": [], "id": "p_244"}, "sentences": [{"end": 76596, "text": "4.1 Introduction", "rank": 703, "start": 76580, "IsComparative": "0", "id": "st_703"}]}, {"paragraph_info": {"end": 77395, "start": 76596, "text": "The innate ability of humans to quickly perceive insight through visual analysis and decision processes has been a key factor in the growth of visual analytic research <78, 127>.One of the most significant efforts made by visual analytics researchers is the integration of various computational methods from data mining and machine learning areas with visual analytics so that users can benefit from intelligent mean- ingful information generated by these techniques.For example, dimension reduction and clustering methods have been commonly used in high-dimensional data visual analytics <23, 117>.More recently, latent Dirichlet allocation (LDA) <20>, a popular method for document topic modeling, has been adopted in a wide variety of visual analytics systems for document analysis <134, 50, 88>.", "rank": 245, "paragraph_comparative_number": 1, "entities": [], "id": "p_245"}, "sentences": [{"end": 76774, "text": "The innate ability of humans to quickly perceive insight through visual analysis and decision processes has been a key factor in the growth of visual analytic research <78, 127>.", "rank": 704, "start": 76596, "IsComparative": "1", "id": "st_704"}, {"end": 77063, "text": "One of the most significant efforts made by visual analytics researchers is the integration of various computational methods from data mining and machine learning areas with visual analytics so that users can benefit from intelligent mean- ingful information generated by these techniques.", "rank": 705, "start": 76774, "IsComparative": "0", "id": "st_705"}, {"end": 77195, "text": "For example, dimension reduction and clustering methods have been commonly used in high-dimensional data visual analytics <23, 117>.", "rank": 706, "start": 77063, "IsComparative": "0", "id": "st_706"}, {"end": 77395, "text": "More recently, latent Dirichlet allocation (LDA) <20>, a popular method for document topic modeling, has been adopted in a wide variety of visual analytics systems for document analysis <134, 50, 88>.", "rank": 707, "start": 77195, "IsComparative": "0", "id": "st_707"}]}, {"paragraph_info": {"end": 78407, "start": 77395, "text": "However, a critical hurdle in the integration of computational methods into visual analytics is the significant amount of computational time required by these meth- ods.As computational methods become more advanced and capable, they usually run much slower, making it almost impossible to visualize and interact with them smoothly in real-time visual analytics.Due to this significant running time, even though numerous computational methods are currently being developed and some methods such as t-distributed stochastic neighbor embedding (t-SNE) <130> even claim their suitability directly in visualization applications, the state-of-the-art in visual analytics does not seem to fully utilize the advancements in computational methods.Consequently, in many domain areas, people still resort to only a few basic computational methods such as principal component analysis (PCA) <74> and multi- dimensional scaling (MDS) <45> for dimension reduction, hierarchical clustering and k-means <19> for clustering, etc.", "rank": 246, "paragraph_comparative_number": 2, "entities": [], "id": "p_246"}, "sentences": [{"end": 77564, "text": "However, a critical hurdle in the integration of computational methods into visual analytics is the significant amount of computational time required by these meth- ods.", "rank": 708, "start": 77395, "IsComparative": "0", "id": "st_708"}, {"end": 77756, "text": "As computational methods become more advanced and capable, they usually run much slower, making it almost impossible to visualize and interact with them smoothly in real-time visual analytics.", "rank": 709, "start": 77564, "IsComparative": "0", "id": "st_709"}, {"end": 78133, "text": "Due to this significant running time, even though numerous computational methods are currently being developed and some methods such as t-distributed stochastic neighbor embedding (t-SNE) <130> even claim their suitability directly in visualization applications, the state-of-the-art in visual analytics does not seem to fully utilize the advancements in computational methods.", "rank": 710, "start": 77756, "IsComparative": "1", "id": "st_710"}, {"end": 78407, "text": "Consequently, in many domain areas, people still resort to only a few basic computational methods such as principal component analysis (PCA) <74> and multi- dimensional scaling (MDS) <45> for dimension reduction, hierarchical clustering and k-means <19> for clustering, etc.", "rank": 711, "start": 78133, "IsComparative": "1", "id": "st_711"}]}, {"paragraph_info": {"end": 78995, "start": 78407, "text": "However, we believe that various important aspects have been largely ignored when integrating (advanced) computational methods into visual analytics.In a sense that such an integration essentially involves both humans and computational meth- ods, exploiting the characteristics of each side simultaneously may bring a synergetic effect for their tight integration that would not be possible otherwise.Motivated by this general idea, this chapter focuses on the following aspects from each side: (1) humans perceptual precision and (2) the iteration-wise behavior of computational methods.", "rank": 247, "paragraph_comparative_number": 1, "entities": [], "id": "p_247"}, "sentences": [{"end": 78556, "text": "However, we believe that various important aspects have been largely ignored when integrating (advanced) computational methods into visual analytics.", "rank": 712, "start": 78407, "IsComparative": "0", "id": "st_712"}, {"end": 78808, "text": "In a sense that such an integration essentially involves both humans and computational meth- ods, exploiting the characteristics of each side simultaneously may bring a synergetic effect for their tight integration that would not be possible otherwise.", "rank": 713, "start": 78556, "IsComparative": "1", "id": "st_713"}, {"end": 78995, "text": "Motivated by this general idea, this chapter focuses on the following aspects from each side: (1) humans perceptual precision and (2) the iteration-wise behavior of computational methods.", "rank": 714, "start": 78808, "IsComparative": "0", "id": "st_714"}]}, {"paragraph_info": {"end": 79816, "start": 78995, "text": "For humans perceptual precision, we highlight that when perceiving numbers, hu- mans do not require a high precision such as a double or a single precisions typically used in modern computers.For example, when perceiving the value of , most people know its approximate value, e.g., 3.14.In practice, perceiving it as a more accurate value, e.g., 3.1415926, does not make much difference.In a more analytic context, suppose the topic modeling has given a topic-wise representation of a particular doc- ument as (55.5852%, 38.8615%, 5.533%) with respect to three topics, e.g., science, sports, and economics.People may perceive its topic contribution at a tenth value at most, which is approximately (55.6%, 38.9%, 5.5%), but it would not change their perception significantly even if more accurate numbers were considered.", "rank": 248, "paragraph_comparative_number": 3, "entities": [], "id": "p_248"}, "sentences": [{"end": 79187, "text": "For humans perceptual precision, we highlight that when perceiving numbers, hu- mans do not require a high precision such as a double or a single precisions typically used in modern computers.", "rank": 715, "start": 78995, "IsComparative": "0", "id": "st_715"}, {"end": 79282, "text": "For example, when perceiving the value of , most people know its approximate value, e.g., 3.14.", "rank": 716, "start": 79187, "IsComparative": "1", "id": "st_716"}, {"end": 79382, "text": "In practice, perceiving it as a more accurate value, e.g., 3.1415926, does not make much difference.", "rank": 717, "start": 79282, "IsComparative": "0", "id": "st_717"}, {"end": 79601, "text": "In a more analytic context, suppose the topic modeling has given a topic-wise representation of a particular doc- ument as (55.5852%, 38.8615%, 5.533%) with respect to three topics, e.g., science, sports, and economics.", "rank": 718, "start": 79382, "IsComparative": "1", "id": "st_718"}, {"end": 79816, "text": "People may perceive its topic contribution at a tenth value at most, which is approximately (55.6%, 38.9%, 5.5%), but it would not change their perception significantly even if more accurate numbers were considered.", "rank": 719, "start": 79601, "IsComparative": "1", "id": "st_719"}]}, {"paragraph_info": {"end": 80851, "start": 79816, "text": "This substantially low perceptual precision compared to that of computational methods opens up a variety of possibilities to reduce the intensive computational time taken in running a computational method in a visual analytics environment.As a complementary characteristics of the computational methods to achieve this goal, we focus on their iteration-wise behavior.These days, many modern computational methods are performed through an iterative refinement process until reaching the final converged solution.An important observation found in most methods is that throughout the iterations, a major refinement of the solution typically occurs in early iterations while only minor changes occur in the later iterations.It indicates that the low-precision outputs of computational methods are dominated by their major refinement made during early iterations.In this respect, humans may be able to obtain most information from the computational method outputs in a much shorter amount of time than the full iterations until convergence.", "rank": 249, "paragraph_comparative_number": 1, "entities": [], "id": "p_249"}, "sentences": [{"end": 80055, "text": "This substantially low perceptual precision compared to that of computational methods opens up a variety of possibilities to reduce the intensive computational time taken in running a computational method in a visual analytics environment.", "rank": 720, "start": 79816, "IsComparative": "0", "id": "st_720"}, {"end": 80183, "text": "As a complementary characteristics of the computational methods to achieve this goal, we focus on their iteration-wise behavior.", "rank": 721, "start": 80055, "IsComparative": "0", "id": "st_721"}, {"end": 80327, "text": "These days, many modern computational methods are performed through an iterative refinement process until reaching the final converged solution.", "rank": 722, "start": 80183, "IsComparative": "0", "id": "st_722"}, {"end": 80536, "text": "An important observation found in most methods is that throughout the iterations, a major refinement of the solution typically occurs in early iterations while only minor changes occur in the later iterations.", "rank": 723, "start": 80327, "IsComparative": "0", "id": "st_723"}, {"end": 80674, "text": "It indicates that the low-precision outputs of computational methods are dominated by their major refinement made during early iterations.", "rank": 724, "start": 80536, "IsComparative": "0", "id": "st_724"}, {"end": 80851, "text": "In this respect, humans may be able to obtain most information from the computational method outputs in a much shorter amount of time than the full iterations until convergence.", "rank": 725, "start": 80674, "IsComparative": "1", "id": "st_725"}]}, {"paragraph_info": {"end": 81761, "start": 80851, "text": "However, apart from well-principled convergence criteria studied in most com- putational methods, it is not straightforward to determine when to terminate the iteration at which the result is reasonably accurate from the perspective of humans perceptual precision.Instead, we propose an alternative approach called PIVE (Per- Iteration Visualization Environment for supporting real-time interactive visualization with computational methods), which visualizes the intermediate result per iteration as soon as they become available.Unlike the previous approaches, which typically treat a particular computational method as a black box, the main novelty of PIVE lies in the idea to break computational methods down to the iteration level and tightly integrate it with the interactive visualization so that users can check the result of computational methods without any delays and interact with them in real-time.", "rank": 250, "paragraph_comparative_number": 2, "entities": [], "id": "p_250"}, "sentences": [{"end": 81115, "text": "However, apart from well-principled convergence criteria studied in most com- putational methods, it is not straightforward to determine when to terminate the iteration at which the result is reasonably accurate from the perspective of humans perceptual precision.", "rank": 726, "start": 80851, "IsComparative": "1", "id": "st_726"}, {"end": 81381, "text": "Instead, we propose an alternative approach called PIVE (Per- Iteration Visualization Environment for supporting real-time interactive visualization with computational methods), which visualizes the intermediate result per iteration as soon as they become available.", "rank": 727, "start": 81115, "IsComparative": "0", "id": "st_727"}, {"end": 81761, "text": "Unlike the previous approaches, which typically treat a particular computational method as a black box, the main novelty of PIVE lies in the idea to break computational methods down to the iteration level and tightly integrate it with the interactive visualization so that users can check the result of computational methods without any delays and interact with them in real-time.", "rank": 728, "start": 81381, "IsComparative": "1", "id": "st_728"}]}, {"paragraph_info": {"end": 83222, "start": 81761, "text": "Such real-time interaction capabilities based on this tight integration of compu- tational methods at an iteration level makes significant differences in terms of the approaches for handling how we interact with a computational method.That is, from a perspective of viewing it as a black box, the turn-around time required for a particular interaction is usually equivalent to the time taken in running the entire set of iterations until its convergence.Therefore, previous efforts in adding an interaction capability to a computational method interactive have mainly focused on the sophis- ticated algorithmic modifications that can maximally reflect the users intention from a single interaction.Accordingly, during a single interaction, it was generally recom- mended that users give the computational method a substantial amount of changes that are carefully made.Otherwise, users would be frustrated if the result due to a user interaction does not properly reflect their intention after a long time of waiting for the computational method to converge.On the contrary, in PIVE, a turn-around time for a single user interaction drastically decreases to the time taken in running a single or a small number of iterations at most instead of an entire set of iterations.In this respect, PIVE enables users to perform multiple small interactions contin- uously by quickly adjusting their interactions based on the real-time response of the computational method.", "rank": 251, "paragraph_comparative_number": 3, "entities": [], "id": "p_251"}, "sentences": [{"end": 81996, "text": "Such real-time interaction capabilities based on this tight integration of compu- tational methods at an iteration level makes significant differences in terms of the approaches for handling how we interact with a computational method.", "rank": 729, "start": 81761, "IsComparative": "0", "id": "st_729"}, {"end": 82215, "text": "That is, from a perspective of viewing it as a black box, the turn-around time required for a particular interaction is usually equivalent to the time taken in running the entire set of iterations until its convergence.", "rank": 730, "start": 81996, "IsComparative": "1", "id": "st_730"}, {"end": 82459, "text": "Therefore, previous efforts in adding an interaction capability to a computational method interactive have mainly focused on the sophis- ticated algorithmic modifications that can maximally reflect the users intention from a single interaction.", "rank": 731, "start": 82215, "IsComparative": "0", "id": "st_731"}, {"end": 82629, "text": "Accordingly, during a single interaction, it was generally recom- mended that users give the computational method a substantial amount of changes that are carefully made.", "rank": 732, "start": 82459, "IsComparative": "0", "id": "st_732"}, {"end": 82818, "text": "Otherwise, users would be frustrated if the result due to a user interaction does not properly reflect their intention after a long time of waiting for the computational method to converge.", "rank": 733, "start": 82629, "IsComparative": "1", "id": "st_733"}, {"end": 83032, "text": "On the contrary, in PIVE, a turn-around time for a single user interaction drastically decreases to the time taken in running a single or a small number of iterations at most instead of an entire set of iterations.", "rank": 734, "start": 82818, "IsComparative": "0", "id": "st_734"}, {"end": 83222, "text": "In this respect, PIVE enables users to perform multiple small interactions contin- uously by quickly adjusting their interactions based on the real-time response of the computational method.", "rank": 735, "start": 83032, "IsComparative": "1", "id": "st_735"}]}, {"paragraph_info": {"end": 83448, "start": 83222, "text": "Motivated by these ideas, this chapter discusses about PIVE in detail and present the example realizations of various well-known computational methods under PIVE.The main contributions of this chapter is summarized as follows:", "rank": 252, "paragraph_comparative_number": 0, "entities": [], "id": "p_252"}, "sentences": [{"end": 83384, "text": "Motivated by these ideas, this chapter discusses about PIVE in detail and present the example realizations of various well-known computational methods under PIVE.", "rank": 736, "start": 83222, "IsComparative": "0", "id": "st_736"}, {"end": 83448, "text": "The main contributions of this chapter is summarized as follows:", "rank": 737, "start": 83384, "IsComparative": "0", "id": "st_737"}]}, {"paragraph_info": {"end": 83574, "start": 83448, "text": "Presentation of PIVE as a general idea to tightly integrate computational meth- ods in visual analytics at an iteration level.", "rank": 253, "paragraph_comparative_number": 0, "entities": [], "id": "p_253"}, "sentences": [{"end": 83574, "text": "Presentation of PIVE as a general idea to tightly integrate computational meth- ods in visual analytics at an iteration level.", "rank": 738, "start": 83448, "IsComparative": "0", "id": "st_738"}]}, {"paragraph_info": {"end": 83648, "start": 83574, "text": "In-depth discussion about the potential issues and their solutions in PIVE", "rank": 254, "paragraph_comparative_number": 1, "entities": [], "id": "p_254"}, "sentences": [{"end": 83648, "text": "In-depth discussion about the potential issues and their solutions in PIVE", "rank": 739, "start": 83574, "IsComparative": "1", "id": "st_739"}]}, {"paragraph_info": {"end": 83720, "start": 83648, "text": "Realizations of PIVE with various well-known computational methods (PCA,", "rank": 255, "paragraph_comparative_number": 0, "entities": [], "id": "p_255"}, "sentences": [{"end": 83720, "text": "Realizations of PIVE with various well-known computational methods (PCA,", "rank": 740, "start": 83648, "IsComparative": "0", "id": "st_740"}]}, {"paragraph_info": {"end": 83789, "start": 83720, "text": "MDS, t-SNE, k-means, and LDA) in established visual analytics systems", "rank": 256, "paragraph_comparative_number": 1, "entities": [], "id": "p_256"}, "sentences": [{"end": 83789, "text": "MDS, t-SNE, k-means, and LDA) in established visual analytics systems", "rank": 741, "start": 83720, "IsComparative": "1", "id": "st_741"}]}, {"paragraph_info": {"end": 83868, "start": 83789, "text": "Customizations of the above methods for real-time user interaction capabilities", "rank": 257, "paragraph_comparative_number": 0, "entities": [], "id": "p_257"}, "sentences": [{"end": 83868, "text": "Customizations of the above methods for real-time user interaction capabilities", "rank": 742, "start": 83789, "IsComparative": "0", "id": "st_742"}]}, {"paragraph_info": {"end": 83878, "start": 83868, "text": "under PIVE", "rank": 258, "paragraph_comparative_number": 0, "entities": [], "id": "p_258"}, "sentences": [{"end": 83878, "text": "under PIVE", "rank": 743, "start": 83868, "IsComparative": "0", "id": "st_743"}]}, {"paragraph_info": {"end": 83954, "start": 83878, "text": "Use cases of the customized methods with real-time user interaction examples", "rank": 259, "paragraph_comparative_number": 0, "entities": [], "id": "p_259"}, "sentences": [{"end": 83954, "text": "Use cases of the customized methods with real-time user interaction examples", "rank": 744, "start": 83878, "IsComparative": "0", "id": "st_744"}]}, {"paragraph_info": {"end": 84568, "start": 83954, "text": "The rest of this chapter is organized as follows.Section 4.2 discusses related work.Section 4.3 describes PIVE in more detail and discuss its potential issues and their solutions.Section 4.4 presents various customized computational methods with their supported interactions in PIVE.Using these customized methods, Section 4.5 de- scribes the quantitative analyses about the iteration-wise behavior of computational methods and provide several use cases of the customized methods with their real- time interactions under PIVE in several well-known visual analytics systems.nally, Section 4.6 concludes the chapter.", "rank": 260, "paragraph_comparative_number": 4, "entities": [], "id": "p_260"}, "sentences": [{"end": 84003, "text": "The rest of this chapter is organized as follows.", "rank": 745, "start": 83954, "IsComparative": "0", "id": "st_745"}, {"end": 84038, "text": "Section 4.2 discusses related work.", "rank": 746, "start": 84003, "IsComparative": "1", "id": "st_746"}, {"end": 84133, "text": "Section 4.3 describes PIVE in more detail and discuss its potential issues and their solutions.", "rank": 747, "start": 84038, "IsComparative": "1", "id": "st_747"}, {"end": 84237, "text": "Section 4.4 presents various customized computational methods with their supported interactions in PIVE.", "rank": 748, "start": 84133, "IsComparative": "1", "id": "st_748"}, {"end": 84527, "text": "Using these customized methods, Section 4.5 de- scribes the quantitative analyses about the iteration-wise behavior of computational methods and provide several use cases of the customized methods with their real- time interactions under PIVE in several well-known visual analytics systems.", "rank": 749, "start": 84237, "IsComparative": "1", "id": "st_749"}, {"end": 84568, "text": "nally, Section 4.6 concludes the chapter.", "rank": 750, "start": 84527, "IsComparative": "0", "id": "st_750"}]}, {"paragraph_info": {"end": 84584, "start": 84568, "text": "4.2 Related Work", "rank": 261, "paragraph_comparative_number": 0, "entities": [], "id": "p_261"}, "sentences": [{"end": 84584, "text": "4.2 Related Work", "rank": 751, "start": 84568, "IsComparative": "0", "id": "st_751"}]}, {"paragraph_info": {"end": 84824, "start": 84584, "text": "In this section, we briefly discuss various previous studies from the two main per- spectives: those aiming at efficient interactive visualization and those trying to make computational method user-interactive in visualization applications.", "rank": 262, "paragraph_comparative_number": 0, "entities": [], "id": "p_262"}, "sentences": [{"end": 84824, "text": "In this section, we briefly discuss various previous studies from the two main per- spectives: those aiming at efficient interactive visualization and those trying to make computational method user-interactive in visualization applications.", "rank": 752, "start": 84584, "IsComparative": "0", "id": "st_752"}]}, {"paragraph_info": {"end": 84865, "start": 84824, "text": "4.2.1 Efficient Interactive Visualization", "rank": 263, "paragraph_comparative_number": 0, "entities": [], "id": "p_263"}, "sentences": [{"end": 84865, "text": "4.2.1 Efficient Interactive Visualization", "rank": 753, "start": 84824, "IsComparative": "0", "id": "st_753"}]}, {"paragraph_info": {"end": 85535, "start": 84865, "text": "Not surprisingly, numerous studies have focus on the visualization applications of large-scale data.Among various approaches, one of the straightforward but reason- able approaches is by using a subset of data by sampling.For example, Fisher et al.<57> has proposed an efficient way of dealing with large-scale data visualization by initially using only a small portion of data and then perform an incremental update on the visualization.Ellis et al.<54> has also taken a random sampling-based visual- ization approach mainly for avoiding the visualization clutter due to a large number of visualized objects while considering the efficiency issues during visualization.", "rank": 264, "paragraph_comparative_number": 1, "entities": [], "id": "p_264"}, "sentences": [{"end": 84965, "text": "Not surprisingly, numerous studies have focus on the visualization applications of large-scale data.", "rank": 754, "start": 84865, "IsComparative": "0", "id": "st_754"}, {"end": 85087, "text": "Among various approaches, one of the straightforward but reason- able approaches is by using a subset of data by sampling.", "rank": 755, "start": 84965, "IsComparative": "0", "id": "st_755"}, {"end": 85113, "text": "For example, Fisher et al.", "rank": 756, "start": 85087, "IsComparative": "0", "id": "st_756"}, {"end": 85303, "text": "<57> has proposed an efficient way of dealing with large-scale data visualization by initially using only a small portion of data and then perform an incremental update on the visualization.", "rank": 757, "start": 85113, "IsComparative": "0", "id": "st_757"}, {"end": 85315, "text": "Ellis et al.", "rank": 758, "start": 85303, "IsComparative": "0", "id": "st_758"}, {"end": 85535, "text": "<54> has also taken a random sampling-based visual- ization approach mainly for avoiding the visualization clutter due to a large number of visualized objects while considering the efficiency issues during visualization.", "rank": 759, "start": 85315, "IsComparative": "1", "id": "st_759"}]}, {"paragraph_info": {"end": 86819, "start": 85535, "text": "As another popular approach for improve the efficiency in visualizing large-scale data, numerous studies have been based on multi-threading techniques.In this con- text, the main role of multi-threading is to separate the data processing/computation module and the visualization/rendering modules as multi-threads, allowing their ef- ficient concurrent running.A notable line of research is called in situ visualization <92, 146>.The main idea of it is, given large-scale data, to alleviate some post- processing overheads that had to be taken care of by the visualization module and let these overheads handled in the phase of the data processing/computation in which the powerful computing resource is readily available.In this manner, even though the visualization module does not have a computing power, which is often the case, the visualization can fluidly be performed.Although similar to the in situ visual- ization approach, Tu et at.<128> has utilized the data sharing aspects in a parallel supercomputing environment.On the other hand, there have been approaches that have utilized multi-threading mainly for the purpose of providing a efficient respon- sive user interactions <104> by separating an application and a visualization threads into multiple concurrent threads.", "rank": 265, "paragraph_comparative_number": 3, "entities": [], "id": "p_265"}, "sentences": [{"end": 85686, "text": "As another popular approach for improve the efficiency in visualizing large-scale data, numerous studies have been based on multi-threading techniques.", "rank": 760, "start": 85535, "IsComparative": "0", "id": "st_760"}, {"end": 85896, "text": "In this con- text, the main role of multi-threading is to separate the data processing/computation module and the visualization/rendering modules as multi-threads, allowing their ef- ficient concurrent running.", "rank": 761, "start": 85686, "IsComparative": "1", "id": "st_761"}, {"end": 85965, "text": "A notable line of research is called in situ visualization <92, 146>.", "rank": 762, "start": 85896, "IsComparative": "0", "id": "st_762"}, {"end": 86257, "text": "The main idea of it is, given large-scale data, to alleviate some post- processing overheads that had to be taken care of by the visualization module and let these overheads handled in the phase of the data processing/computation in which the powerful computing resource is readily available.", "rank": 763, "start": 85965, "IsComparative": "1", "id": "st_763"}, {"end": 86411, "text": "In this manner, even though the visualization module does not have a computing power, which is often the case, the visualization can fluidly be performed.", "rank": 764, "start": 86257, "IsComparative": "0", "id": "st_764"}, {"end": 86478, "text": "Although similar to the in situ visual- ization approach, Tu et at.", "rank": 765, "start": 86411, "IsComparative": "0", "id": "st_765"}, {"end": 86563, "text": "<128> has utilized the data sharing aspects in a parallel supercomputing environment.", "rank": 766, "start": 86478, "IsComparative": "0", "id": "st_766"}, {"end": 86819, "text": "On the other hand, there have been approaches that have utilized multi-threading mainly for the purpose of providing a efficient respon- sive user interactions <104> by separating an application and a visualization threads into multiple concurrent threads.", "rank": 767, "start": 86563, "IsComparative": "1", "id": "st_767"}]}, {"paragraph_info": {"end": 87278, "start": 86819, "text": "As will described in detail in Section 4.3, PIVE adopts a similar multi-threading idea in order to reduce the overhead of the visualization module that has to go through a constant updating as the iterations of the computation method go.However, none of these multi-threading-based approaches hardly exploited the nature of the iterative refinement processes found in most computational methods, which makes a clear dis- tinction of PIVE to the previous work.", "rank": 266, "paragraph_comparative_number": 0, "entities": [], "id": "p_266"}, "sentences": [{"end": 87056, "text": "As will described in detail in Section 4.3, PIVE adopts a similar multi-threading idea in order to reduce the overhead of the visualization module that has to go through a constant updating as the iterations of the computation method go.", "rank": 768, "start": 86819, "IsComparative": "0", "id": "st_768"}, {"end": 87278, "text": "However, none of these multi-threading-based approaches hardly exploited the nature of the iterative refinement processes found in most computational methods, which makes a clear dis- tinction of PIVE to the previous work.", "rank": 769, "start": 87056, "IsComparative": "0", "id": "st_769"}]}, {"paragraph_info": {"end": 88019, "start": 87278, "text": "Furthermore, efficient interactive visualization has been a main concern in the context of dynamic/streaming data.When visualizing dynamic/streaming data, the overall theme found in various approaches is to update the visualization efficiently given incremental changes in a data set.In this context, Cottam et al.<44> has recently discussed about a taxonomy for dynamic data visualization.Although the detailed approaches may differ, several prior studies <139, 140> have started from a relatively similar idea that the visualization update is carried out only when significant changes/events have been detected.Additionally, Alsakran et al.<5> has visualized the streaming documents using a GPU-accelerated force-directed layout technique.", "rank": 267, "paragraph_comparative_number": 2, "entities": [], "id": "p_267"}, "sentences": [{"end": 87392, "text": "Furthermore, efficient interactive visualization has been a main concern in the context of dynamic/streaming data.", "rank": 770, "start": 87278, "IsComparative": "1", "id": "st_770"}, {"end": 87562, "text": "When visualizing dynamic/streaming data, the overall theme found in various approaches is to update the visualization efficiently given incremental changes in a data set.", "rank": 771, "start": 87392, "IsComparative": "0", "id": "st_771"}, {"end": 87592, "text": "In this context, Cottam et al.", "rank": 772, "start": 87562, "IsComparative": "0", "id": "st_772"}, {"end": 87668, "text": "<44> has recently discussed about a taxonomy for dynamic data visualization.", "rank": 773, "start": 87592, "IsComparative": "1", "id": "st_773"}, {"end": 87891, "text": "Although the detailed approaches may differ, several prior studies <139, 140> have started from a relatively similar idea that the visualization update is carried out only when significant changes/events have been detected.", "rank": 774, "start": 87668, "IsComparative": "0", "id": "st_774"}, {"end": 87920, "text": "Additionally, Alsakran et al.", "rank": 775, "start": 87891, "IsComparative": "0", "id": "st_775"}, {"end": 88019, "text": "<5> has visualized the streaming documents using a GPU-accelerated force-directed layout technique.", "rank": 776, "start": 87920, "IsComparative": "0", "id": "st_776"}]}, {"paragraph_info": {"end": 88379, "start": 88019, "text": "Various interesting ideas from dynamic data visualization could be applied to fur- ther improve the updating process of visualization in PIVE.Nevertheless, the primary problem that PIVE tackles arises from the intensive amount of computations in the computational methods, and thus an efficient updating of the visualization module is not a concern in general.", "rank": 268, "paragraph_comparative_number": 1, "entities": [], "id": "p_268"}, "sentences": [{"end": 88161, "text": "Various interesting ideas from dynamic data visualization could be applied to fur- ther improve the updating process of visualization in PIVE.", "rank": 777, "start": 88019, "IsComparative": "0", "id": "st_777"}, {"end": 88379, "text": "Nevertheless, the primary problem that PIVE tackles arises from the intensive amount of computations in the computational methods, and thus an efficient updating of the visualization module is not a concern in general.", "rank": 778, "start": 88161, "IsComparative": "1", "id": "st_778"}]}, {"paragraph_info": {"end": 88428, "start": 88379, "text": "4.2.2 User Interaction with Computational Methods", "rank": 269, "paragraph_comparative_number": 1, "entities": [], "id": "p_269"}, "sentences": [{"end": 88428, "text": "4.2.2 User Interaction with Computational Methods", "rank": 779, "start": 88379, "IsComparative": "1", "id": "st_779"}]}, {"paragraph_info": {"end": 88928, "start": 88428, "text": "There have been numerous efforts to make computational methods, which are mostly automated, user-interactive in visualization applications.One of the most represen- tative work is based on MDS <136> that has added MDS a capability of incorporating user feedback based on a user-specified visual region.A more recent work called observation-level interaction <55> has provided a general framework in which the user interaction from a scatter plot is incorporated in a Bayesian probabilistic framework.", "rank": 270, "paragraph_comparative_number": 1, "entities": [], "id": "p_270"}, "sentences": [{"end": 88567, "text": "There have been numerous efforts to make computational methods, which are mostly automated, user-interactive in visualization applications.", "rank": 780, "start": 88428, "IsComparative": "1", "id": "st_780"}, {"end": 88730, "text": "One of the most represen- tative work is based on MDS <136> that has added MDS a capability of incorporating user feedback based on a user-specified visual region.", "rank": 781, "start": 88567, "IsComparative": "0", "id": "st_781"}, {"end": 88928, "text": "A more recent work called observation-level interaction <55> has provided a general framework in which the user interaction from a scatter plot is incorporated in a Bayesian probabilistic framework.", "rank": 782, "start": 88730, "IsComparative": "0", "id": "st_782"}]}, {"paragraph_info": {"end": 89112, "start": 88928, "text": "In this sense, PIVE, which leverages both humans perceptual precision and the iteration-wise behavior of computational methods, bears a potentially great impact in achieving this goal.", "rank": 271, "paragraph_comparative_number": 1, "entities": [], "id": "p_271"}, "sentences": [{"end": 89112, "text": "In this sense, PIVE, which leverages both humans perceptual precision and the iteration-wise behavior of computational methods, bears a potentially great impact in achieving this goal.", "rank": 783, "start": 88928, "IsComparative": "1", "id": "st_783"}]}, {"paragraph_info": {"end": 89162, "start": 89112, "text": "4.3 Per-Iteration Visualization Environment (PIVE)", "rank": 272, "paragraph_comparative_number": 0, "entities": [], "id": "p_272"}, "sentences": [{"end": 89162, "text": "4.3 Per-Iteration Visualization Environment (PIVE)", "rank": 784, "start": 89112, "IsComparative": "0", "id": "st_784"}]}, {"paragraph_info": {"end": 89310, "start": 89162, "text": "First, we describe an overall flow of PIVE (Fig.12(a)), by highlighting its differences from the standard (non-iteration-wise) approach (Fig.12(b)).", "rank": 273, "paragraph_comparative_number": 1, "entities": [], "id": "p_273"}, "sentences": [{"end": 89210, "text": "First, we describe an overall flow of PIVE (Fig.", "rank": 785, "start": 89162, "IsComparative": "0", "id": "st_785"}, {"end": 89303, "text": "12(a)), by highlighting its differences from the standard (non-iteration-wise) approach (Fig.", "rank": 786, "start": 89210, "IsComparative": "1", "id": "st_786"}, {"end": 89310, "text": "12(b)).", "rank": 787, "start": 89303, "IsComparative": "0", "id": "st_787"}]}, {"paragraph_info": {"end": 89825, "start": 89310, "text": "Let us begin with a general procedure when an iterative computational method is integrated in visual analytics.As shown in Fig.12(a), input data, which are usually represented as multidimensional vectors, are given to the computational module along with its required parameter values.The computational module pre-processes the data, if necessary, and runs through iterations, which are usually divided into multiple sub-routines, until it converges.Upon convergence, the output goes through a post- processing step.", "rank": 274, "paragraph_comparative_number": 1, "entities": [], "id": "p_274"}, "sentences": [{"end": 89421, "text": "Let us begin with a general procedure when an iterative computational method is integrated in visual analytics.", "rank": 788, "start": 89310, "IsComparative": "0", "id": "st_788"}, {"end": 89437, "text": "As shown in Fig.", "rank": 789, "start": 89421, "IsComparative": "0", "id": "st_789"}, {"end": 89594, "text": "12(a), input data, which are usually represented as multidimensional vectors, are given to the computational module along with its required parameter values.", "rank": 790, "start": 89437, "IsComparative": "1", "id": "st_790"}, {"end": 89759, "text": "The computational module pre-processes the data, if necessary, and runs through iterations, which are usually divided into multiple sub-routines, until it converges.", "rank": 791, "start": 89594, "IsComparative": "0", "id": "st_791"}, {"end": 89825, "text": "Upon convergence, the output goes through a post- processing step.", "rank": 792, "start": 89759, "IsComparative": "0", "id": "st_792"}]}, {"paragraph_info": {"end": 90209, "start": 89825, "text": "The final output of the computational module is then passed to the visualization module, which encodes it in a visual space and finally delivers its visualization to users.For example, the output of a dimension reduction method, e.g., PCA, can map data items onto the coordinates of the screen space, and the output of clustering can be used to color-code each group of data clusters.", "rank": 275, "paragraph_comparative_number": 1, "entities": [], "id": "p_275"}, "sentences": [{"end": 89997, "text": "The final output of the computational module is then passed to the visualization module, which encodes it in a visual space and finally delivers its visualization to users.", "rank": 793, "start": 89825, "IsComparative": "1", "id": "st_793"}, {"end": 90209, "text": "For example, the output of a dimension reduction method, e.g., PCA, can map data items onto the coordinates of the screen space, and the output of clustering can be used to color-code each group of data clusters.", "rank": 794, "start": 89997, "IsComparative": "0", "id": "st_794"}]}, {"paragraph_info": {"end": 90932, "start": 90209, "text": "Users can then better explore the visually represented data with the help of the information provided by the computational method and often interact with compu- tational methods by adjusting their input data as well as their parameters.These interactions trigger another run of the computational method.For example, given the cluster summary for a set of text documents, if a user finds an interesting cluster, the user may perform another iteration of clustering on the particular subset to obtain more details about the chosen subset.On the other hand, users might want to ad- just the number of clusters, which is usually a user-specified parameter in clustering methods, to find the best clustering result for the data.", "rank": 276, "paragraph_comparative_number": 3, "entities": [], "id": "p_276"}, "sentences": [{"end": 90445, "text": "Users can then better explore the visually represented data with the help of the information provided by the computational method and often interact with compu- tational methods by adjusting their input data as well as their parameters.", "rank": 795, "start": 90209, "IsComparative": "1", "id": "st_795"}, {"end": 90512, "text": "These interactions trigger another run of the computational method.", "rank": 796, "start": 90445, "IsComparative": "0", "id": "st_796"}, {"end": 90745, "text": "For example, given the cluster summary for a set of text documents, if a user finds an interesting cluster, the user may perform another iteration of clustering on the particular subset to obtain more details about the chosen subset.", "rank": 797, "start": 90512, "IsComparative": "1", "id": "st_797"}, {"end": 90932, "text": "On the other hand, users might want to ad- just the number of clusters, which is usually a user-specified parameter in clustering methods, to find the best clustering result for the data.", "rank": 798, "start": 90745, "IsComparative": "1", "id": "st_798"}]}, {"paragraph_info": {"end": 91012, "start": 90932, "text": "In most of the described visualizations and interactions, the standard framework", "rank": 277, "paragraph_comparative_number": 0, "entities": [], "id": "p_277"}, "sentences": [{"end": 91012, "text": "In most of the described visualizations and interactions, the standard framework", "rank": 799, "start": 90932, "IsComparative": "0", "id": "st_799"}]}, {"paragraph_info": {"end": 91341, "start": 91012, "text": "generally treats the computational module as a black box, which the visualization module has no control over, depicted by a gray rectangle in Fig.12(a).In other words, once the computational module has been initiated, visual analytic systems must wait for it to finish its iterations before it outputs the visualization to users.", "rank": 278, "paragraph_comparative_number": 2, "entities": [], "id": "p_278"}, "sentences": [{"end": 91158, "text": "generally treats the computational module as a black box, which the visualization module has no control over, depicted by a gray rectangle in Fig.", "rank": 800, "start": 91012, "IsComparative": "0", "id": "st_800"}, {"end": 91164, "text": "12(a).", "rank": 801, "start": 91158, "IsComparative": "1", "id": "st_801"}, {"end": 91341, "text": "In other words, once the computational module has been initiated, visual analytic systems must wait for it to finish its iterations before it outputs the visualization to users.", "rank": 802, "start": 91164, "IsComparative": "1", "id": "st_802"}]}, {"paragraph_info": {"end": 91930, "start": 91341, "text": "On the contrary, PIVE takes the results of intermediate iterations out of the com- putational module and delivers them to the visualization module whenever they are available.More specifically, as highlighted with the red horizontal line in Fig.12(b), the result from each iteration is always passed to the post-processing step, the out- put of which, in turn, reaches all the way to the visualization module, regardless of whether it has converged or not.Consequently, these intermediate results are visual- ized to users much more quickly than having to wait for the converged solutions.", "rank": 279, "paragraph_comparative_number": 1, "entities": [], "id": "p_279"}, "sentences": [{"end": 91516, "text": "On the contrary, PIVE takes the results of intermediate iterations out of the com- putational module and delivers them to the visualization module whenever they are available.", "rank": 803, "start": 91341, "IsComparative": "0", "id": "st_803"}, {"end": 91586, "text": "More specifically, as highlighted with the red horizontal line in Fig.", "rank": 804, "start": 91516, "IsComparative": "0", "id": "st_804"}, {"end": 91797, "text": "12(b), the result from each iteration is always passed to the post-processing step, the out- put of which, in turn, reaches all the way to the visualization module, regardless of whether it has converged or not.", "rank": 805, "start": 91586, "IsComparative": "1", "id": "st_805"}, {"end": 91930, "text": "Consequently, these intermediate results are visual- ized to users much more quickly than having to wait for the converged solutions.", "rank": 806, "start": 91797, "IsComparative": "0", "id": "st_806"}]}, {"paragraph_info": {"end": 92500, "start": 91930, "text": "In addition, PIVE enables the above-discussed interactions to be instantly re- flected by directly interacting with the process for each iteration of the computational module, as highlighted with the red vertical line in Fig.12(b).For instance, given the result of a particular iteration, one could exclude certain data items from the fol- lowing iterations, which accelerate the later iterations due to the reduced data size.Furthermore, users could change the number of clusters while a clustering method is running, which immediately affects the following iterations.", "rank": 280, "paragraph_comparative_number": 2, "entities": [], "id": "p_280"}, "sentences": [{"end": 92155, "text": "In addition, PIVE enables the above-discussed interactions to be instantly re- flected by directly interacting with the process for each iteration of the computational module, as highlighted with the red vertical line in Fig.", "rank": 807, "start": 91930, "IsComparative": "1", "id": "st_807"}, {"end": 92161, "text": "12(b).", "rank": 808, "start": 92155, "IsComparative": "0", "id": "st_808"}, {"end": 92356, "text": "For instance, given the result of a particular iteration, one could exclude certain data items from the fol- lowing iterations, which accelerate the later iterations due to the reduced data size.", "rank": 809, "start": 92161, "IsComparative": "1", "id": "st_809"}, {"end": 92500, "text": "Furthermore, users could change the number of clusters while a clustering method is running, which immediately affects the following iterations.", "rank": 810, "start": 92356, "IsComparative": "0", "id": "st_810"}]}, {"paragraph_info": {"end": 92526, "start": 92500, "text": "4.3.1 Issues and Solutions", "rank": 281, "paragraph_comparative_number": 1, "entities": [], "id": "p_281"}, "sentences": [{"end": 92526, "text": "4.3.1 Issues and Solutions", "rank": 811, "start": 92500, "IsComparative": "1", "id": "st_811"}]}, {"paragraph_info": {"end": 92576, "start": 92526, "text": "4.3.1.1 Computational Overhead and Multi-threading", "rank": 282, "paragraph_comparative_number": 0, "entities": [], "id": "p_282"}, "sentences": [{"end": 92576, "text": "4.3.1.1 Computational Overhead and Multi-threading", "rank": 812, "start": 92526, "IsComparative": "0", "id": "st_812"}]}, {"paragraph_info": {"end": 93365, "start": 92576, "text": "Computational overheads are one of the issues that can be potentially introduced by this framework.As can be seen in the red-lined stacked rectangle blocks in Fig.12(b), visual analytics systems have to process the output for each iteration repetitively while the standard approach needs to process only the final output once.These additional computations could undermine the effectiveness of the proposed framework.Let us suppose that a particular computational method, which requires 50 iterations to converge, converges in a minute.If the proposed framework runs only 4-5 iterations within the same amount of time, then users might prefer the standard approach instead of being able to check the intermediate results since the results from such early iterations may not be satisfactory.", "rank": 283, "paragraph_comparative_number": 2, "entities": [], "id": "p_283"}, "sentences": [{"end": 92675, "text": "Computational overheads are one of the issues that can be potentially introduced by this framework.", "rank": 813, "start": 92576, "IsComparative": "0", "id": "st_813"}, {"end": 92739, "text": "As can be seen in the red-lined stacked rectangle blocks in Fig.", "rank": 814, "start": 92675, "IsComparative": "0", "id": "st_814"}, {"end": 92902, "text": "12(b), visual analytics systems have to process the output for each iteration repetitively while the standard approach needs to process only the final output once.", "rank": 815, "start": 92739, "IsComparative": "1", "id": "st_815"}, {"end": 92992, "text": "These additional computations could undermine the effectiveness of the proposed framework.", "rank": 816, "start": 92902, "IsComparative": "0", "id": "st_816"}, {"end": 93111, "text": "Let us suppose that a particular computational method, which requires 50 iterations to converge, converges in a minute.", "rank": 817, "start": 92992, "IsComparative": "1", "id": "st_817"}, {"end": 93365, "text": "If the proposed framework runs only 4-5 iterations within the same amount of time, then users might prefer the standard approach instead of being able to check the intermediate results since the results from such early iterations may not be satisfactory.", "rank": 818, "start": 93111, "IsComparative": "0", "id": "st_818"}]}, {"paragraph_info": {"end": 94006, "start": 93365, "text": "However, we claim that this issue can be easily overcome by applying a multi- threaded approach to the proposed framework.As shown by the blue ellipses in Fig.12(b), the entire process can be separated into two concurrent processes/threads.The first thread shown to the left is responsible only for the sub-routines inside the iteration while the second thread on the right handles actions from the post-processing block to the visualization block.These two threads communicate with each other via a message queue, as shown by the blue rectangle on top in Fig.12(b), where the outputs for each iteration for post-processing are to be stored.", "rank": 284, "paragraph_comparative_number": 3, "entities": [], "id": "p_284"}, "sentences": [{"end": 93487, "text": "However, we claim that this issue can be easily overcome by applying a multi- threaded approach to the proposed framework.", "rank": 819, "start": 93365, "IsComparative": "0", "id": "st_819"}, {"end": 93524, "text": "As shown by the blue ellipses in Fig.", "rank": 820, "start": 93487, "IsComparative": "0", "id": "st_820"}, {"end": 93605, "text": "12(b), the entire process can be separated into two concurrent processes/threads.", "rank": 821, "start": 93524, "IsComparative": "0", "id": "st_821"}, {"end": 93813, "text": "The first thread shown to the left is responsible only for the sub-routines inside the iteration while the second thread on the right handles actions from the post-processing block to the visualization block.", "rank": 822, "start": 93605, "IsComparative": "1", "id": "st_822"}, {"end": 93925, "text": "These two threads communicate with each other via a message queue, as shown by the blue rectangle on top in Fig.", "rank": 823, "start": 93813, "IsComparative": "1", "id": "st_823"}, {"end": 94006, "text": "12(b), where the outputs for each iteration for post-processing are to be stored.", "rank": 824, "start": 93925, "IsComparative": "1", "id": "st_824"}]}, {"paragraph_info": {"end": 94489, "start": 94006, "text": "Modern computers are usually equipped with at least two or more cores on the CPU.These two threads can be executed virtually in parallel, which hardly slow down the computational methods compared to the standard approach.Although not included in this chapter, for the computational methods we customized, we compared the total computing time between PIVE and the standard frameworks, but with multi-threading implemented, there were essentially no differences in their running times.", "rank": 285, "paragraph_comparative_number": 2, "entities": [], "id": "p_285"}, "sentences": [{"end": 94087, "text": "Modern computers are usually equipped with at least two or more cores on the CPU.", "rank": 825, "start": 94006, "IsComparative": "0", "id": "st_825"}, {"end": 94227, "text": "These two threads can be executed virtually in parallel, which hardly slow down the computational methods compared to the standard approach.", "rank": 826, "start": 94087, "IsComparative": "1", "id": "st_826"}, {"end": 94489, "text": "Although not included in this chapter, for the computational methods we customized, we compared the total computing time between PIVE and the standard frameworks, but with multi-threading implemented, there were essentially no differences in their running times.", "rank": 827, "start": 94227, "IsComparative": "1", "id": "st_827"}]}, {"paragraph_info": {"end": 95421, "start": 94489, "text": "Even in this multi-threading framework, the following case may still be problem- atic.Suppose the second thread involves more intensive computations than the first thread because, for example, the post-processing block takes more time than the pro- cesses at each iteration.As a result, the second thread would act as a bottleneck in the overall flow of the proposed framework, resulting in the message queue increasing.One way to handle this issue is to store the results of each iteration periodically rather than storing every one of them in the first thread.Alternatively, the second thread could take the most recent iteration-level results and discard the remaining older ones from the message queue.Under this situation, the visualization of the intermediate results may be somewhat discontinuous, but users would always be given the most recent result, which should be the most accurate solution up to the current iteration.", "rank": 286, "paragraph_comparative_number": 1, "entities": [], "id": "p_286"}, "sentences": [{"end": 94575, "text": "Even in this multi-threading framework, the following case may still be problem- atic.", "rank": 828, "start": 94489, "IsComparative": "0", "id": "st_828"}, {"end": 94763, "text": "Suppose the second thread involves more intensive computations than the first thread because, for example, the post-processing block takes more time than the pro- cesses at each iteration.", "rank": 829, "start": 94575, "IsComparative": "0", "id": "st_829"}, {"end": 94909, "text": "As a result, the second thread would act as a bottleneck in the overall flow of the proposed framework, resulting in the message queue increasing.", "rank": 830, "start": 94763, "IsComparative": "0", "id": "st_830"}, {"end": 95051, "text": "One way to handle this issue is to store the results of each iteration periodically rather than storing every one of them in the first thread.", "rank": 831, "start": 94909, "IsComparative": "0", "id": "st_831"}, {"end": 95195, "text": "Alternatively, the second thread could take the most recent iteration-level results and discard the remaining older ones from the message queue.", "rank": 832, "start": 95051, "IsComparative": "1", "id": "st_832"}, {"end": 95421, "text": "Under this situation, the visualization of the intermediate results may be somewhat discontinuous, but users would always be given the most recent result, which should be the most accurate solution up to the current iteration.", "rank": 833, "start": 95195, "IsComparative": "0", "id": "st_833"}]}, {"paragraph_info": {"end": 96358, "start": 95421, "text": "Finally, the other overhead comes from copying results from each iteration to the message queue, which results in a memory write operation.In the standard approach, these results for each iteration are usually written to the same memory space over iterations since the results from previous iterations do not need to be maintained.However, memory write operations are generally very fast.Furthermore, the out- puts from each iteration of computational methods take up a much smaller memory compared to input data.For example, even if the data is a very high-dimensional, say, in the hundreds of thousands of dimensions, such as is the case in text data, the dimension reduction outputs would only be two-dimensional representations assum- ing they are visualized in a 2D space.Since the amount of additional computational time and memory that is required by our approach is minimal, we do not see memory overheads being a critical issue.", "rank": 287, "paragraph_comparative_number": 2, "entities": [], "id": "p_287"}, "sentences": [{"end": 95560, "text": "Finally, the other overhead comes from copying results from each iteration to the message queue, which results in a memory write operation.", "rank": 834, "start": 95421, "IsComparative": "0", "id": "st_834"}, {"end": 95752, "text": "In the standard approach, these results for each iteration are usually written to the same memory space over iterations since the results from previous iterations do not need to be maintained.", "rank": 835, "start": 95560, "IsComparative": "0", "id": "st_835"}, {"end": 95809, "text": "However, memory write operations are generally very fast.", "rank": 836, "start": 95752, "IsComparative": "0", "id": "st_836"}, {"end": 95934, "text": "Furthermore, the out- puts from each iteration of computational methods take up a much smaller memory compared to input data.", "rank": 837, "start": 95809, "IsComparative": "1", "id": "st_837"}, {"end": 96198, "text": "For example, even if the data is a very high-dimensional, say, in the hundreds of thousands of dimensions, such as is the case in text data, the dimension reduction outputs would only be two-dimensional representations assum- ing they are visualized in a 2D space.", "rank": 838, "start": 95934, "IsComparative": "0", "id": "st_838"}, {"end": 96358, "text": "Since the amount of additional computational time and memory that is required by our approach is minimal, we do not see memory overheads being a critical issue.", "rank": 839, "start": 96198, "IsComparative": "1", "id": "st_839"}]}, {"paragraph_info": {"end": 96403, "start": 96358, "text": "4.3.1.2 Visual Inconsistency and User Control", "rank": 288, "paragraph_comparative_number": 0, "entities": [], "id": "p_288"}, "sentences": [{"end": 96403, "text": "4.3.1.2 Visual Inconsistency and User Control", "rank": 840, "start": 96358, "IsComparative": "0", "id": "st_840"}]}, {"paragraph_info": {"end": 96818, "start": 96403, "text": "The second issue in the proposed framework is the visual inconsistency, which occurs during visualization updates, due to dynamic results changing each iteration.The most severe case occurs when the visualization changes too frequently.Although the amount of change generally diminishes as the iterations proceed, frequently changing visualizations may prevent users from obtaining a consistent picture of the data.", "rank": 289, "paragraph_comparative_number": 2, "entities": [], "id": "p_289"}, "sentences": [{"end": 96565, "text": "The second issue in the proposed framework is the visual inconsistency, which occurs during visualization updates, due to dynamic results changing each iteration.", "rank": 841, "start": 96403, "IsComparative": "1", "id": "st_841"}, {"end": 96639, "text": "The most severe case occurs when the visualization changes too frequently.", "rank": 842, "start": 96565, "IsComparative": "0", "id": "st_842"}, {"end": 96818, "text": "Although the amount of change generally diminishes as the iterations proceed, frequently changing visualizations may prevent users from obtaining a consistent picture of the data.", "rank": 843, "start": 96639, "IsComparative": "1", "id": "st_843"}]}, {"paragraph_info": {"end": 97826, "start": 96818, "text": "To address these issues with visual inconsistencies, weve come up with several possible controls.The first most basic option would be a stop and resume control which would stop and resume updates of the visualization.Secondly, a time period control would manage the length of the visualization.Additionally, we could pair this time controller with two choices - the option to visualize the most up-to-date result or to visualize the result of the next item in the queue, which would provide the user with smoother visual transitions.Similar to the stop/resume interaction, since our approach maintains each of the intermediate results, we could simply expand the controls to also add both the play backwards and jump to...options.These interactions would help users understand the overall trajectory of the results through each iteration.Through the use of these controls, it is very possible that the user may uncover an interesting insight into the data at a particular iteration or a series of iterations.", "rank": 290, "paragraph_comparative_number": 3, "entities": [], "id": "p_290"}, "sentences": [{"end": 96915, "text": "To address these issues with visual inconsistencies, weve come up with several possible controls.", "rank": 844, "start": 96818, "IsComparative": "0", "id": "st_844"}, {"end": 97035, "text": "The first most basic option would be a stop and resume control which would stop and resume updates of the visualization.", "rank": 845, "start": 96915, "IsComparative": "0", "id": "st_845"}, {"end": 97112, "text": "Secondly, a time period control would manage the length of the visualization.", "rank": 846, "start": 97035, "IsComparative": "0", "id": "st_846"}, {"end": 97351, "text": "Additionally, we could pair this time controller with two choices - the option to visualize the most up-to-date result or to visualize the result of the next item in the queue, which would provide the user with smoother visual transitions.", "rank": 847, "start": 97112, "IsComparative": "1", "id": "st_847"}, {"end": 97538, "text": "Similar to the stop/resume interaction, since our approach maintains each of the intermediate results, we could simply expand the controls to also add both the play backwards and jump to.", "rank": 848, "start": 97351, "IsComparative": "1", "id": "st_848"}, {"end": 97539, "text": ".", "rank": 849, "start": 97538, "IsComparative": "0", "id": "st_849"}, {"end": 97540, "text": ".", "rank": 850, "start": 97539, "IsComparative": "0", "id": "st_850"}, {"end": 97548, "text": "options.", "rank": 851, "start": 97540, "IsComparative": "0", "id": "st_851"}, {"end": 97656, "text": "These interactions would help users understand the overall trajectory of the results through each iteration.", "rank": 852, "start": 97548, "IsComparative": "0", "id": "st_852"}, {"end": 97826, "text": "Through the use of these controls, it is very possible that the user may uncover an interesting insight into the data at a particular iteration or a series of iterations.", "rank": 853, "start": 97656, "IsComparative": "1", "id": "st_853"}]}, {"paragraph_info": {"end": 97859, "start": 97826, "text": "4.4 Customized Methods under PIVE", "rank": 291, "paragraph_comparative_number": 0, "entities": [], "id": "p_291"}, "sentences": [{"end": 97859, "text": "4.4 Customized Methods under PIVE", "rank": 854, "start": 97826, "IsComparative": "0", "id": "st_854"}]}, {"paragraph_info": {"end": 98141, "start": 97859, "text": "In this section, following the proposed framework, we present several customized com- putational methods in visual analytics systems.To begin with, we have chosen three visual analytics systems, FodavaTestbed,1 Jigsaw,2 and iVisClustering <88>, which involve computational methods.3", "rank": 292, "paragraph_comparative_number": 1, "entities": [], "id": "p_292"}, "sentences": [{"end": 97992, "text": "In this section, following the proposed framework, we present several customized com- putational methods in visual analytics systems.", "rank": 855, "start": 97859, "IsComparative": "0", "id": "st_855"}, {"end": 98141, "text": "To begin with, we have chosen three visual analytics systems, FodavaTestbed,1 Jigsaw,2 and iVisClustering <88>, which involve computational methods.3", "rank": 856, "start": 97992, "IsComparative": "1", "id": "st_856"}]}, {"paragraph_info": {"end": 98563, "start": 98141, "text": "FodavaTestbed is a visual analytics system for high-dimensional data, where users can apply various dimension reduction and clustering methods for exploratory anal- ysis.Among various methods supported, we have chosen three dimension reduction t-SNE.Jigsaw is a well-known system for docu- k-means, which is used to provide a summary in terms of a compact set of clusters.Finally, iVisClustering is an interactive document", "rank": 293, "paragraph_comparative_number": 0, "entities": [], "id": "p_293"}, "sentences": [{"end": 98311, "text": "FodavaTestbed is a visual analytics system for high-dimensional data, where users can apply various dimension reduction and clustering methods for exploratory anal- ysis.", "rank": 857, "start": 98141, "IsComparative": "0", "id": "st_857"}, {"end": 98391, "text": "Among various methods supported, we have chosen three dimension reduction t-SNE.", "rank": 858, "start": 98311, "IsComparative": "0", "id": "st_858"}, {"end": 98513, "text": "Jigsaw is a well-known system for docu- k-means, which is used to provide a summary in terms of a compact set of clusters.", "rank": 859, "start": 98391, "IsComparative": "0", "id": "st_859"}, {"end": 98563, "text": "Finally, iVisClustering is an interactive document", "rank": 860, "start": 98513, "IsComparative": "0", "id": "st_860"}]}, {"paragraph_info": {"end": 98705, "start": 98563, "text": "In the following, we describe how each method is customized along with the ad- ditional interactions we implemented in the proposed framework.", "rank": 294, "paragraph_comparative_number": 0, "entities": [], "id": "p_294"}, "sentences": [{"end": 98705, "text": "In the following, we describe how each method is customized along with the ad- ditional interactions we implemented in the proposed framework.", "rank": 861, "start": 98563, "IsComparative": "0", "id": "st_861"}]}, {"paragraph_info": {"end": 98795, "start": 98705, "text": "1http://fodava.gatech.edu/fodava-testbed-software 2http://www.cc.gatech.edu/gvu/ii/jigsaw/", "rank": 295, "paragraph_comparative_number": 0, "entities": [], "id": "p_295"}, "sentences": [{"end": 98795, "text": "1http://fodava.gatech.edu/fodava-testbed-software 2http://www.cc.gatech.edu/gvu/ii/jigsaw/", "rank": 862, "start": 98705, "IsComparative": "0", "id": "st_862"}]}, {"paragraph_info": {"end": 98895, "start": 98795, "text": "3We obtained the code from the original authors of the systems.LDA, a popular topic modeling method.", "rank": 296, "paragraph_comparative_number": 0, "entities": [], "id": "p_296"}, "sentences": [{"end": 98858, "text": "3We obtained the code from the original authors of the systems.", "rank": 863, "start": 98795, "IsComparative": "0", "id": "st_863"}, {"end": 98895, "text": "LDA, a popular topic modeling method.", "rank": 864, "start": 98858, "IsComparative": "0", "id": "st_864"}]}, {"paragraph_info": {"end": 98935, "start": 98895, "text": "4.4.1 Principal Component Analysis (PCA)", "rank": 297, "paragraph_comparative_number": 0, "entities": [], "id": "p_297"}, "sentences": [{"end": 98935, "text": "4.4.1 Principal Component Analysis (PCA)", "rank": 865, "start": 98895, "IsComparative": "0", "id": "st_865"}]}, {"paragraph_info": {"end": 99235, "start": 98935, "text": "PCA <75> is a well-known dimension reduction method that captures the maximal variance in the data via a linear projection.PCA is mainly based on the method called eigendecomposition, the algorithms of which are categorized into two different methods, the QR algorithm and the Lanczos algorithm <61>.", "rank": 298, "paragraph_comparative_number": 1, "entities": [], "id": "p_298"}, "sentences": [{"end": 99058, "text": "PCA <75> is a well-known dimension reduction method that captures the maximal variance in the data via a linear projection.", "rank": 866, "start": 98935, "IsComparative": "1", "id": "st_866"}, {"end": 99235, "text": "PCA is mainly based on the method called eigendecomposition, the algorithms of which are categorized into two different methods, the QR algorithm and the Lanczos algorithm <61>.", "rank": 867, "start": 99058, "IsComparative": "0", "id": "st_867"}]}, {"paragraph_info": {"end": 99689, "start": 99235, "text": "Basically, the Lanczos algorithm approximates a given data matrix by a much smaller one in the Krylov subspace <61>, the dimension of which iteratively expands, and efficiently solves the eigendecomposition on the latter matrix.Due to the nature that this matrix well-approximates the largest eigenvectors of the original one, the Lanczos algorithm performs much faster than the QR algorithm in visual analytics in which only a few dimensions are needed.", "rank": 299, "paragraph_comparative_number": 0, "entities": [], "id": "p_299"}, "sentences": [{"end": 99463, "text": "Basically, the Lanczos algorithm approximates a given data matrix by a much smaller one in the Krylov subspace <61>, the dimension of which iteratively expands, and efficiently solves the eigendecomposition on the latter matrix.", "rank": 868, "start": 99235, "IsComparative": "0", "id": "st_868"}, {"end": 99689, "text": "Due to the nature that this matrix well-approximates the largest eigenvectors of the original one, the Lanczos algorithm performs much faster than the QR algorithm in visual analytics in which only a few dimensions are needed.", "rank": 869, "start": 99463, "IsComparative": "0", "id": "st_869"}]}, {"paragraph_info": {"end": 99822, "start": 99689, "text": "We customize the Lanczos-based PCA implementation of FodavaTestbed so that the results for each iteration are dynamically visualized.", "rank": 300, "paragraph_comparative_number": 0, "entities": [], "id": "p_300"}, "sentences": [{"end": 99822, "text": "We customize the Lanczos-based PCA implementation of FodavaTestbed so that the results for each iteration are dynamically visualized.", "rank": 870, "start": 99689, "IsComparative": "0", "id": "st_870"}]}, {"paragraph_info": {"end": 99858, "start": 99822, "text": "4.4.2 Multidimensional Scaling (MDS)", "rank": 301, "paragraph_comparative_number": 0, "entities": [], "id": "p_301"}, "sentences": [{"end": 99858, "text": "4.4.2 Multidimensional Scaling (MDS)", "rank": 871, "start": 99822, "IsComparative": "0", "id": "st_871"}]}, {"paragraph_info": {"end": 100067, "start": 99858, "text": "MDS <45> is a traditional dimension reduction method that attempts to preserve given distances/relationships of data items in a lower-dimensional space.Given the ideal distance ij between xi and xj, MDS solves", "rank": 302, "paragraph_comparative_number": 1, "entities": [], "id": "p_302"}, "sentences": [{"end": 100010, "text": "MDS <45> is a traditional dimension reduction method that attempts to preserve given distances/relationships of data items in a lower-dimensional space.", "rank": 872, "start": 99858, "IsComparative": "0", "id": "st_872"}, {"end": 100067, "text": "Given the ideal distance ij between xi and xj, MDS solves", "rank": 873, "start": 100010, "IsComparative": "1", "id": "st_873"}]}, {"paragraph_info": {"end": 100384, "start": 100067, "text": "where dij is the distance between the reduced dimensional vectors xi and xj.A Euclidean distance xi  xj2 is usually used for dij.Solving Eq.(39) iteratively refines xis based on various optimization techniques <46>.We customize MDS in FodavaTestbed by extracting the xis at each iteration from the MDS implementation.", "rank": 303, "paragraph_comparative_number": 0, "entities": [], "id": "p_303"}, "sentences": [{"end": 100143, "text": "where dij is the distance between the reduced dimensional vectors xi and xj.", "rank": 874, "start": 100067, "IsComparative": "0", "id": "st_874"}, {"end": 100196, "text": "A Euclidean distance xi  xj2 is usually used for dij.", "rank": 875, "start": 100143, "IsComparative": "0", "id": "st_875"}, {"end": 100207, "text": "Solving Eq.", "rank": 876, "start": 100196, "IsComparative": "0", "id": "st_876"}, {"end": 100282, "text": "(39) iteratively refines xis based on various optimization techniques <46>.", "rank": 877, "start": 100207, "IsComparative": "0", "id": "st_877"}, {"end": 100384, "text": "We customize MDS in FodavaTestbed by extracting the xis at each iteration from the MDS implementation.", "rank": 878, "start": 100282, "IsComparative": "0", "id": "st_878"}]}, {"paragraph_info": {"end": 100421, "start": 100384, "text": "4.4.2.1 User Interaction Capabilities", "rank": 304, "paragraph_comparative_number": 1, "entities": [], "id": "p_304"}, "sentences": [{"end": 100421, "text": "4.4.2.1 User Interaction Capabilities", "rank": 879, "start": 100384, "IsComparative": "1", "id": "st_879"}]}, {"paragraph_info": {"end": 101217, "start": 100421, "text": "Additionally, while the results for each iteration of MDS are visualized in a scatter plot, we support the interaction capability that enables users to move the data points by mouse via drag-and-drop, similar to the Prefuse force-directed layout.Then, during the MDS iterations, their new positions in the screen space are translated back to the MDS output coordinates, xis.The changes in xis at a particular iteration then affect the following iterations by generating different dijs.In terms of how MDS behaves due to these changes, we provide two different capabilities: soft vs.hard placement.The soft placement continues iterations without any changes in MDS behaviors.It is equivalent to restarting MDS with the intermediate result at the particular iteration as the initial values for xis.", "rank": 305, "paragraph_comparative_number": 2, "entities": [], "id": "p_305"}, "sentences": [{"end": 100667, "text": "Additionally, while the results for each iteration of MDS are visualized in a scatter plot, we support the interaction capability that enables users to move the data points by mouse via drag-and-drop, similar to the Prefuse force-directed layout.", "rank": 880, "start": 100421, "IsComparative": "0", "id": "st_880"}, {"end": 100795, "text": "Then, during the MDS iterations, their new positions in the screen space are translated back to the MDS output coordinates, xis.", "rank": 881, "start": 100667, "IsComparative": "1", "id": "st_881"}, {"end": 100906, "text": "The changes in xis at a particular iteration then affect the following iterations by generating different dijs.", "rank": 882, "start": 100795, "IsComparative": "0", "id": "st_882"}, {"end": 101003, "text": "In terms of how MDS behaves due to these changes, we provide two different capabilities: soft vs.", "rank": 883, "start": 100906, "IsComparative": "0", "id": "st_883"}, {"end": 101018, "text": "hard placement.", "rank": 884, "start": 101003, "IsComparative": "0", "id": "st_884"}, {"end": 101095, "text": "The soft placement continues iterations without any changes in MDS behaviors.", "rank": 885, "start": 101018, "IsComparative": "0", "id": "st_885"}, {"end": 101217, "text": "It is equivalent to restarting MDS with the intermediate result at the particular iteration as the initial values for xis.", "rank": 886, "start": 101095, "IsComparative": "1", "id": "st_886"}]}, {"paragraph_info": {"end": 101609, "start": 101217, "text": "The hard placement capability fixes the values of xis for points moved by the user.This can be easily achieved by skipping the update step of these xis in the following iterations.Note that, however, even though their values do not change, other data points are still influenced by these fixed points, and in this sense, our approach is a semi-supervised MDS that reflects user interventions.", "rank": 306, "paragraph_comparative_number": 1, "entities": [], "id": "p_306"}, "sentences": [{"end": 101300, "text": "The hard placement capability fixes the values of xis for points moved by the user.", "rank": 887, "start": 101217, "IsComparative": "0", "id": "st_887"}, {"end": 101397, "text": "This can be easily achieved by skipping the update step of these xis in the following iterations.", "rank": 888, "start": 101300, "IsComparative": "0", "id": "st_888"}, {"end": 101609, "text": "Note that, however, even though their values do not change, other data points are still influenced by these fixed points, and in this sense, our approach is a semi-supervised MDS that reflects user interventions.", "rank": 889, "start": 101397, "IsComparative": "1", "id": "st_889"}]}, {"paragraph_info": {"end": 101871, "start": 101609, "text": "When using the semi-supervised MDS, an important advantage of the proposed framework is that users can immediately check the effects of these interactions via the iteration-wise visualization.Our modifications in FodavaTestbed support both types of interactions.", "rank": 307, "paragraph_comparative_number": 1, "entities": [], "id": "p_307"}, "sentences": [{"end": 101801, "text": "When using the semi-supervised MDS, an important advantage of the proposed framework is that users can immediately check the effects of these interactions via the iteration-wise visualization.", "rank": 890, "start": 101609, "IsComparative": "0", "id": "st_890"}, {"end": 101871, "text": "Our modifications in FodavaTestbed support both types of interactions.", "rank": 891, "start": 101801, "IsComparative": "1", "id": "st_891"}]}, {"paragraph_info": {"end": 101928, "start": 101871, "text": "4.4.3 t-Distributed Stochastic Neighbor Embedding (t-SNE)", "rank": 308, "paragraph_comparative_number": 0, "entities": [], "id": "p_308"}, "sentences": [{"end": 101928, "text": "4.4.3 t-Distributed Stochastic Neighbor Embedding (t-SNE)", "rank": 892, "start": 101871, "IsComparative": "0", "id": "st_892"}]}, {"paragraph_info": {"end": 102382, "start": 101928, "text": "t-SNE <130> is a relatively new dimension reduction method.It interprets pairwise distances as probabilities both in high-dimensional and lower-dimensional spaces and tries to minimize their KullbackLeibler divergence, a distance measure between prob- ability distributions.Unlike the previous methods discussed, it focuses on preserving neighborhood relationships instead of global ones, and it has shown its outstanding capabilities in visualizations.4", "rank": 309, "paragraph_comparative_number": 2, "entities": [], "id": "p_309"}, "sentences": [{"end": 101987, "text": "t-SNE <130> is a relatively new dimension reduction method.", "rank": 893, "start": 101928, "IsComparative": "0", "id": "st_893"}, {"end": 102202, "text": "It interprets pairwise distances as probabilities both in high-dimensional and lower-dimensional spaces and tries to minimize their KullbackLeibler divergence, a distance measure between prob- ability distributions.", "rank": 894, "start": 101987, "IsComparative": "1", "id": "st_894"}, {"end": 102382, "text": "Unlike the previous methods discussed, it focuses on preserving neighborhood relationships instead of global ones, and it has shown its outstanding capabilities in visualizations.4", "rank": 895, "start": 102202, "IsComparative": "1", "id": "st_895"}]}, {"paragraph_info": {"end": 102896, "start": 102382, "text": "Although we skip the detailed formulations because of the scope of the visual an- alytics community, the algorithm works iteratively by refining the lower-dimensional coordinates based on a gradient descent-based framework.In practice, however, t- SNE does not provide a clear stopping criterion, and thus it typically iterates several hundred times by default for any data set, which usually takes a significant amount of time.We customize the t-SNE in FodavaTestbed in a similar manner to the way we altered MDS.", "rank": 310, "paragraph_comparative_number": 1, "entities": [], "id": "p_310"}, "sentences": [{"end": 102605, "text": "Although we skip the detailed formulations because of the scope of the visual an- alytics community, the algorithm works iteratively by refining the lower-dimensional coordinates based on a gradient descent-based framework.", "rank": 896, "start": 102382, "IsComparative": "0", "id": "st_896"}, {"end": 102810, "text": "In practice, however, t- SNE does not provide a clear stopping criterion, and thus it typically iterates several hundred times by default for any data set, which usually takes a significant amount of time.", "rank": 897, "start": 102605, "IsComparative": "1", "id": "st_897"}, {"end": 102896, "text": "We customize the t-SNE in FodavaTestbed in a similar manner to the way we altered MDS.", "rank": 898, "start": 102810, "IsComparative": "0", "id": "st_898"}]}, {"paragraph_info": {"end": 102933, "start": 102896, "text": "4.4.3.1 User Interaction Capabilities", "rank": 311, "paragraph_comparative_number": 1, "entities": [], "id": "p_311"}, "sentences": [{"end": 102933, "text": "4.4.3.1 User Interaction Capabilities", "rank": 899, "start": 102896, "IsComparative": "1", "id": "st_899"}]}, {"paragraph_info": {"end": 103491, "start": 102933, "text": "Likewise, we provide both the soft and hard placement interactions for t-SNE, as discussed in MDS.Although the algorithm details are different, the overall iterative procedure turns out to be quite similar to MDS.Thus, for the soft placement, we restart t-SNE with the intermediate results immediately during iterations.For the hard one, we skip the update step for data items moved by the user in the following iterations while they still influence other points in the t-SNE iterations.Therefore, our altered method can be viewed as a semi-supervised t-SNE.", "rank": 312, "paragraph_comparative_number": 1, "entities": [], "id": "p_312"}, "sentences": [{"end": 103031, "text": "Likewise, we provide both the soft and hard placement interactions for t-SNE, as discussed in MDS.", "rank": 900, "start": 102933, "IsComparative": "0", "id": "st_900"}, {"end": 103146, "text": "Although the algorithm details are different, the overall iterative procedure turns out to be quite similar to MDS.", "rank": 901, "start": 103031, "IsComparative": "0", "id": "st_901"}, {"end": 103253, "text": "Thus, for the soft placement, we restart t-SNE with the intermediate results immediately during iterations.", "rank": 902, "start": 103146, "IsComparative": "0", "id": "st_902"}, {"end": 103420, "text": "For the hard one, we skip the update step for data items moved by the user in the following iterations while they still influence other points in the t-SNE iterations.", "rank": 903, "start": 103253, "IsComparative": "0", "id": "st_903"}, {"end": 103491, "text": "Therefore, our altered method can be viewed as a semi-supervised t-SNE.", "rank": 904, "start": 103420, "IsComparative": "1", "id": "st_904"}]}, {"paragraph_info": {"end": 103504, "start": 103491, "text": "4.4.4 k-means", "rank": 313, "paragraph_comparative_number": 0, "entities": [], "id": "p_313"}, "sentences": [{"end": 103504, "text": "4.4.4 k-means", "rank": 905, "start": 103491, "IsComparative": "0", "id": "st_905"}]}, {"paragraph_info": {"end": 103865, "start": 103504, "text": "k-means, which is a widely-used clustering method, performs the following steps iter- atively: 1. computing the centroid of each cluster by averaging the data vectors in the corresponding cluster and 2. updating the cluster assignment of each data item based on its closest cluster centroid.The iteration terminates when there are no cluster membership changes.", "rank": 314, "paragraph_comparative_number": 1, "entities": [], "id": "p_314"}, "sentences": [{"end": 103795, "text": "k-means, which is a widely-used clustering method, performs the following steps iter- atively: 1. computing the centroid of each cluster by averaging the data vectors in the corresponding cluster and 2. updating the cluster assignment of each data item based on its closest cluster centroid.", "rank": 906, "start": 103504, "IsComparative": "1", "id": "st_906"}, {"end": 103865, "text": "The iteration terminates when there are no cluster membership changes.", "rank": 907, "start": 103795, "IsComparative": "0", "id": "st_907"}]}, {"paragraph_info": {"end": 104111, "start": 103865, "text": "Although Jigsaw provides a cluster view based on k-means, it currently visualizes only the pre-computed results since k-means is usually very slow to converge.We customize it so that users can run k-means in real-time and the intermediate cluster", "rank": 315, "paragraph_comparative_number": 0, "entities": [], "id": "p_315"}, "sentences": [{"end": 104024, "text": "Although Jigsaw provides a cluster view based on k-means, it currently visualizes only the pre-computed results since k-means is usually very slow to converge.", "rank": 908, "start": 103865, "IsComparative": "0", "id": "st_908"}, {"end": 104111, "text": "We customize it so that users can run k-means in real-time and the intermediate cluster", "rank": 909, "start": 104024, "IsComparative": "0", "id": "st_909"}]}, {"paragraph_info": {"end": 104155, "start": 104111, "text": "(a) PCA criteria values and out- put changes", "rank": 316, "paragraph_comparative_number": 1, "entities": [], "id": "p_316"}, "sentences": [{"end": 104155, "text": "(a) PCA criteria values and out- put changes", "rank": 910, "start": 104111, "IsComparative": "1", "id": "st_910"}]}, {"paragraph_info": {"end": 104283, "start": 104155, "text": "(b) The scatter plot at the first(c) The scatter plot at the third(d) The scatter plot at the 80th iteration iteration iteration", "rank": 317, "paragraph_comparative_number": 0, "entities": [], "id": "p_317"}, "sentences": [{"end": 104283, "text": "(b) The scatter plot at the first(c) The scatter plot at the third(d) The scatter plot at the 80th iteration iteration iteration", "rank": 911, "start": 104155, "IsComparative": "0", "id": "st_911"}]}, {"paragraph_info": {"end": 104850, "start": 104283, "text": "Figure 13: The behavior for each iteration of PCA and its visualization snapshots.In (a), the red lines represent the PCA criteria value, the lower-dimensional variance in PCA.The blue lines are the Euclidean distances of the lower-dimensional outputs between the current and the previous iterations, and the black lines are the Euclidean distances of the lower-dimensional outputs between the current and the final itera- tions.In (a), the black and the blue lines almost coincide.1,420 facial image data representing pixel values in 2,048 dimensions have been used.", "rank": 318, "paragraph_comparative_number": 3, "entities": [], "id": "p_318"}, "sentences": [{"end": 104365, "text": "Figure 13: The behavior for each iteration of PCA and its visualization snapshots.", "rank": 912, "start": 104283, "IsComparative": "1", "id": "st_912"}, {"end": 104459, "text": "In (a), the red lines represent the PCA criteria value, the lower-dimensional variance in PCA.", "rank": 913, "start": 104365, "IsComparative": "1", "id": "st_913"}, {"end": 104712, "text": "The blue lines are the Euclidean distances of the lower-dimensional outputs between the current and the previous iterations, and the black lines are the Euclidean distances of the lower-dimensional outputs between the current and the final itera- tions.", "rank": 914, "start": 104459, "IsComparative": "0", "id": "st_914"}, {"end": 104765, "text": "In (a), the black and the blue lines almost coincide.", "rank": 915, "start": 104712, "IsComparative": "0", "id": "st_915"}, {"end": 104850, "text": "1,420 facial image data representing pixel values in 2,048 dimensions have been used.", "rank": 916, "start": 104765, "IsComparative": "1", "id": "st_916"}]}, {"paragraph_info": {"end": 104889, "start": 104850, "text": "memberships are dynamically visualized.", "rank": 319, "paragraph_comparative_number": 0, "entities": [], "id": "p_319"}, "sentences": [{"end": 104889, "text": "memberships are dynamically visualized.", "rank": 917, "start": 104850, "IsComparative": "0", "id": "st_917"}]}, {"paragraph_info": {"end": 104926, "start": 104889, "text": "4.4.4.1 User Interaction Capabilities", "rank": 320, "paragraph_comparative_number": 1, "entities": [], "id": "p_320"}, "sentences": [{"end": 104926, "text": "4.4.4.1 User Interaction Capabilities", "rank": 918, "start": 104889, "IsComparative": "1", "id": "st_918"}]}, {"paragraph_info": {"end": 105317, "start": 104926, "text": "Additionally, we add several interaction capabilities in the proposed framework.One is to split/merge clusters during iterations.On a split/merge interaction, similar to the soft placement in MDS and t-SNE, k-means restarts with the intermediate cluster memberships that reflect split/merged clusters, involving dynamic changes in a k-means parameter which represents the number of clusters.", "rank": 321, "paragraph_comparative_number": 2, "entities": [], "id": "p_321"}, "sentences": [{"end": 105006, "text": "Additionally, we add several interaction capabilities in the proposed framework.", "rank": 919, "start": 104926, "IsComparative": "1", "id": "st_919"}, {"end": 105055, "text": "One is to split/merge clusters during iterations.", "rank": 920, "start": 105006, "IsComparative": "0", "id": "st_920"}, {"end": 105317, "text": "On a split/merge interaction, similar to the soft placement in MDS and t-SNE, k-means restarts with the intermediate cluster memberships that reflect split/merged clusters, involving dynamic changes in a k-means parameter which represents the number of clusters.", "rank": 921, "start": 105055, "IsComparative": "1", "id": "st_921"}]}, {"paragraph_info": {"end": 105562, "start": 105317, "text": "Similar to the hard placement in MDS and t-SNE, another capability we provide is the option to fix the cluster assignments of the data in a particular cluster.To accomplish this, we skip the updating step of the cluster assignment for these data", "rank": 322, "paragraph_comparative_number": 2, "entities": [], "id": "p_322"}, "sentences": [{"end": 105476, "text": "Similar to the hard placement in MDS and t-SNE, another capability we provide is the option to fix the cluster assignments of the data in a particular cluster.", "rank": 922, "start": 105317, "IsComparative": "1", "id": "st_922"}, {"end": 105562, "text": "To accomplish this, we skip the updating step of the cluster assignment for these data", "rank": 923, "start": 105476, "IsComparative": "1", "id": "st_923"}]}, {"paragraph_info": {"end": 105601, "start": 105562, "text": "4.4.5 Latent Dirichlet Allocation (LDA)", "rank": 323, "paragraph_comparative_number": 0, "entities": [], "id": "p_323"}, "sentences": [{"end": 105601, "text": "4.4.5 Latent Dirichlet Allocation (LDA)", "rank": 924, "start": 105562, "IsComparative": "0", "id": "st_924"}]}, {"paragraph_info": {"end": 106098, "start": 105601, "text": "LDA <20> is a popular topic modeling method for documents based on a generative probabilistic model.Given a number of topics, it gives two outputs: the term-wise distribution of each topic and the topic-wise distribution of each document.The iterations of LDA basically update these two outputs alternately.From a clustering viewpoint, the former corresponds to a centroid vector of each topic cluster, and the latter to a soft-clustering coefficient.By taking the topic index that has the maximum", "rank": 324, "paragraph_comparative_number": 3, "entities": [], "id": "p_324"}, "sentences": [{"end": 105701, "text": "LDA <20> is a popular topic modeling method for documents based on a generative probabilistic model.", "rank": 925, "start": 105601, "IsComparative": "0", "id": "st_925"}, {"end": 105839, "text": "Given a number of topics, it gives two outputs: the term-wise distribution of each topic and the topic-wise distribution of each document.", "rank": 926, "start": 105701, "IsComparative": "1", "id": "st_926"}, {"end": 105908, "text": "The iterations of LDA basically update these two outputs alternately.", "rank": 927, "start": 105839, "IsComparative": "1", "id": "st_927"}, {"end": 106052, "text": "From a clustering viewpoint, the former corresponds to a centroid vector of each topic cluster, and the latter to a soft-clustering coefficient.", "rank": 928, "start": 105908, "IsComparative": "1", "id": "st_928"}, {"end": 106098, "text": "By taking the topic index that has the maximum", "rank": 929, "start": 106052, "IsComparative": "0", "id": "st_929"}]}, {"paragraph_info": {"end": 106156, "start": 106098, "text": "(a) The cluster membership changes (b) The computing times", "rank": 325, "paragraph_comparative_number": 0, "entities": [], "id": "p_325"}, "sentences": [{"end": 106156, "text": "(a) The cluster membership changes (b) The computing times", "rank": 930, "start": 106098, "IsComparative": "0", "id": "st_930"}]}, {"paragraph_info": {"end": 106410, "start": 106156, "text": "Figure 18: The iteration-wise behaviors of LDA.In (a), the black line represents cluster membership changes between the current and the previous iterations while the red line represents the correct cluster memberships with respect to the final solutions.", "rank": 326, "paragraph_comparative_number": 1, "entities": [], "id": "p_326"}, "sentences": [{"end": 106203, "text": "Figure 18: The iteration-wise behaviors of LDA.", "rank": 931, "start": 106156, "IsComparative": "0", "id": "st_931"}, {"end": 106410, "text": "In (a), the black line represents cluster membership changes between the current and the previous iterations while the red line represents the correct cluster memberships with respect to the final solutions.", "rank": 932, "start": 106203, "IsComparative": "1", "id": "st_932"}]}, {"paragraph_info": {"end": 106457, "start": 106410, "text": "iteration takes is almost the same (Fig.17(d)).", "rank": 327, "paragraph_comparative_number": 0, "entities": [], "id": "p_327"}, "sentences": [{"end": 106450, "text": "iteration takes is almost the same (Fig.", "rank": 933, "start": 106410, "IsComparative": "0", "id": "st_933"}, {"end": 106457, "text": "17(d)).", "rank": 934, "start": 106450, "IsComparative": "0", "id": "st_934"}]}, {"paragraph_info": {"end": 106538, "start": 106457, "text": "Finally, LDA, which is a sampling-based approach, shows a significantly different", "rank": 328, "paragraph_comparative_number": 0, "entities": [], "id": "p_328"}, "sentences": [{"end": 106538, "text": "Finally, LDA, which is a sampling-based approach, shows a significantly different", "rank": 935, "start": 106457, "IsComparative": "0", "id": "st_935"}]}, {"paragraph_info": {"end": 107112, "start": 106538, "text": "behavior from the previous methods (Fig.18).Although cluster membership changes between iterations generally decrease and the solution narrows to the final solutions (Fig.18(a)), cluster memberships change significantly even after many iterations, in this case after 1,200 iterations.In iVisClustering, we could see the top keywords of each topic become somewhat stable after several hundreds of iterations (Fig.18(a)), but the randomness of the sampling-based algorithms might make it harder to give consistent visualizations when compared to deterministic methods in PIVE.", "rank": 329, "paragraph_comparative_number": 3, "entities": [], "id": "p_329"}, "sentences": [{"end": 106578, "text": "behavior from the previous methods (Fig.", "rank": 936, "start": 106538, "IsComparative": "0", "id": "st_936"}, {"end": 106582, "text": "18).", "rank": 937, "start": 106578, "IsComparative": "1", "id": "st_937"}, {"end": 106709, "text": "Although cluster membership changes between iterations generally decrease and the solution narrows to the final solutions (Fig.", "rank": 938, "start": 106582, "IsComparative": "0", "id": "st_938"}, {"end": 106822, "text": "18(a)), cluster memberships change significantly even after many iterations, in this case after 1,200 iterations.", "rank": 939, "start": 106709, "IsComparative": "1", "id": "st_939"}, {"end": 106950, "text": "In iVisClustering, we could see the top keywords of each topic become somewhat stable after several hundreds of iterations (Fig.", "rank": 940, "start": 106822, "IsComparative": "0", "id": "st_940"}, {"end": 107112, "text": "18(a)), but the randomness of the sampling-based algorithms might make it harder to give consistent visualizations when compared to deterministic methods in PIVE.", "rank": 941, "start": 106950, "IsComparative": "1", "id": "st_941"}]}, {"paragraph_info": {"end": 107145, "start": 107112, "text": "4.5.2 Real-time User Interactions", "rank": 330, "paragraph_comparative_number": 1, "entities": [], "id": "p_330"}, "sentences": [{"end": 107145, "text": "4.5.2 Real-time User Interactions", "rank": 942, "start": 107112, "IsComparative": "1", "id": "st_942"}]}, {"paragraph_info": {"end": 107380, "start": 107145, "text": "Basically, in all three systems, we provide basic interactions that control the visu- alization for each iteration, as discussed in Section 4.3.1.In the following, we show several use cases of the interactions discussed in Section 4.4.", "rank": 331, "paragraph_comparative_number": 1, "entities": [], "id": "p_331"}, "sentences": [{"end": 107291, "text": "Basically, in all three systems, we provide basic interactions that control the visu- alization for each iteration, as discussed in Section 4.3.1.", "rank": 943, "start": 107145, "IsComparative": "1", "id": "st_943"}, {"end": 107380, "text": "In the following, we show several use cases of the interactions discussed in Section 4.4.", "rank": 944, "start": 107291, "IsComparative": "0", "id": "st_944"}]}, {"paragraph_info": {"end": 107442, "start": 107380, "text": "(a) The second-iteration result (b) The sixth-iteration result", "rank": 332, "paragraph_comparative_number": 0, "entities": [], "id": "p_332"}, "sentences": [{"end": 107442, "text": "(a) The second-iteration result (b) The sixth-iteration result", "rank": 945, "start": 107380, "IsComparative": "0", "id": "st_945"}]}, {"paragraph_info": {"end": 107572, "start": 107442, "text": "(c) The converged (25th-iteration) result after fix-(d) The converged (26th-iteration) result without ing clusters any interaction", "rank": 333, "paragraph_comparative_number": 0, "entities": [], "id": "p_333"}, "sentences": [{"end": 107572, "text": "(c) The converged (25th-iteration) result after fix-(d) The converged (26th-iteration) result without ing clusters any interaction", "rank": 946, "start": 107442, "IsComparative": "0", "id": "st_946"}]}, {"paragraph_info": {"end": 107912, "start": 107572, "text": "Figure 19: The results of the PIVE integration of k-means in Jigsaw.At the sixth iteration, the interaction of fixing the yellow-colored clusters is made (b).The final result with and without this interaction is shown in (c) and (d), respectively.The NSF-awarded abstract data have been used.The detailed keyword summary is shown in Table 7", "rank": 334, "paragraph_comparative_number": 1, "entities": [], "id": "p_334"}, "sentences": [{"end": 107640, "text": "Figure 19: The results of the PIVE integration of k-means in Jigsaw.", "rank": 947, "start": 107572, "IsComparative": "0", "id": "st_947"}, {"end": 107730, "text": "At the sixth iteration, the interaction of fixing the yellow-colored clusters is made (b).", "rank": 948, "start": 107640, "IsComparative": "0", "id": "st_948"}, {"end": 107819, "text": "The final result with and without this interaction is shown in (c) and (d), respectively.", "rank": 949, "start": 107730, "IsComparative": "0", "id": "st_949"}, {"end": 107864, "text": "The NSF-awarded abstract data have been used.", "rank": 950, "start": 107819, "IsComparative": "1", "id": "st_950"}, {"end": 107912, "text": "The detailed keyword summary is shown in Table 7", "rank": 951, "start": 107864, "IsComparative": "0", "id": "st_951"}]}, {"paragraph_info": {"end": 107947, "start": 107912, "text": "4.5.2.1 Moving data points in t-SNE", "rank": 335, "paragraph_comparative_number": 0, "entities": [], "id": "p_335"}, "sentences": [{"end": 107947, "text": "4.5.2.1 Moving data points in t-SNE", "rank": 952, "start": 107912, "IsComparative": "0", "id": "st_952"}]}, {"paragraph_info": {"end": 108450, "start": 107947, "text": "Fig.16 shows an interesting interaction which involves moving a data point in t- SNE.Given some overlapping clusters in a particular visualization generated by t- SNE (Fig.16(a)), users place several points from different clusters far apart (Fig.16(b)), and then t-SNE reflects these changes in the following iterations, resulting in tje separation of most points in two clusters from each other (Figs.16(c)(d)).This simple, yet powerful example clearly illustrates the advantage of providing users with", "rank": 336, "paragraph_comparative_number": 1, "entities": [], "id": "p_336"}, "sentences": [{"end": 107951, "text": "Fig.", "rank": 953, "start": 107947, "IsComparative": "0", "id": "st_953"}, {"end": 108032, "text": "16 shows an interesting interaction which involves moving a data point in t- SNE.", "rank": 954, "start": 107951, "IsComparative": "0", "id": "st_954"}, {"end": 108119, "text": "Given some overlapping clusters in a particular visualization generated by t- SNE (Fig.", "rank": 955, "start": 108032, "IsComparative": "0", "id": "st_955"}, {"end": 108193, "text": "16(a)), users place several points from different clusters far apart (Fig.", "rank": 956, "start": 108119, "IsComparative": "0", "id": "st_956"}, {"end": 108349, "text": "16(b)), and then t-SNE reflects these changes in the following iterations, resulting in tje separation of most points in two clusters from each other (Figs.", "rank": 957, "start": 108193, "IsComparative": "1", "id": "st_957"}, {"end": 108359, "text": "16(c)(d)).", "rank": 958, "start": 108349, "IsComparative": "0", "id": "st_958"}, {"end": 108450, "text": "This simple, yet powerful example clearly illustrates the advantage of providing users with", "rank": 959, "start": 108359, "IsComparative": "0", "id": "st_959"}]}, {"paragraph_info": {"end": 108543, "start": 108450, "text": "(a) The third-iteration result (b) The fourth-iteration result after split/merge interactions", "rank": 337, "paragraph_comparative_number": 0, "entities": [], "id": "p_337"}, "sentences": [{"end": 108543, "text": "(a) The third-iteration result (b) The fourth-iteration result after split/merge interactions", "rank": 960, "start": 108450, "IsComparative": "0", "id": "st_960"}]}, {"paragraph_info": {"end": 108679, "start": 108543, "text": "(c) The final (15th-iteration) result with(d) The final (7th-iteration) result without split/merge interactions split/merge interactions", "rank": 338, "paragraph_comparative_number": 0, "entities": [], "id": "p_338"}, "sentences": [{"end": 108679, "text": "(c) The final (15th-iteration) result with(d) The final (7th-iteration) result without split/merge interactions split/merge interactions", "rank": 961, "start": 108543, "IsComparative": "0", "id": "st_961"}]}, {"paragraph_info": {"end": 108993, "start": 108679, "text": "Figure 20: An example of split/merge interactions.The yellow and green ones in (a) are merged to the same-colored ones, respectively, in (b), and the white one in (a) is split to the-same colored ones in (b).Webpages about autism have been used as an input data set.The detailed keyword summary is shown in Table 8", "rank": 339, "paragraph_comparative_number": 1, "entities": [], "id": "p_339"}, "sentences": [{"end": 108729, "text": "Figure 20: An example of split/merge interactions.", "rank": 962, "start": 108679, "IsComparative": "1", "id": "st_962"}, {"end": 108887, "text": "The yellow and green ones in (a) are merged to the same-colored ones, respectively, in (b), and the white one in (a) is split to the-same colored ones in (b).", "rank": 963, "start": 108729, "IsComparative": "0", "id": "st_963"}, {"end": 108945, "text": "Webpages about autism have been used as an input data set.", "rank": 964, "start": 108887, "IsComparative": "0", "id": "st_964"}, {"end": 108993, "text": "The detailed keyword summary is shown in Table 8", "rank": 965, "start": 108945, "IsComparative": "0", "id": "st_965"}]}, {"paragraph_info": {"end": 109091, "start": 108993, "text": "the ability to interact with computational methods in our framework in real-time visual analytics.", "rank": 340, "paragraph_comparative_number": 1, "entities": [], "id": "p_340"}, "sentences": [{"end": 109091, "text": "the ability to interact with computational methods in our framework in real-time visual analytics.", "rank": 966, "start": 108993, "IsComparative": "1", "id": "st_966"}]}, {"paragraph_info": {"end": 109136, "start": 109091, "text": "4.5.2.2 Fixing cluster assignments in k-means", "rank": 341, "paragraph_comparative_number": 0, "entities": [], "id": "p_341"}, "sentences": [{"end": 109136, "text": "4.5.2.2 Fixing cluster assignments in k-means", "rank": 967, "start": 109091, "IsComparative": "0", "id": "st_967"}]}, {"paragraph_info": {"end": 109476, "start": 109136, "text": "For our k-means method, we provide users with another interaction that allows them to fix cluster assignments for particular data items at a certain iteration.This inter- action becomes especially useful when users feel that particular clusters are adequate and want to prevent them from changing much.In addition, fixing some clusters that", "rank": 342, "paragraph_comparative_number": 2, "entities": [], "id": "p_342"}, "sentences": [{"end": 109295, "text": "For our k-means method, we provide users with another interaction that allows them to fix cluster assignments for particular data items at a certain iteration.", "rank": 968, "start": 109136, "IsComparative": "1", "id": "st_968"}, {"end": 109438, "text": "This inter- action becomes especially useful when users feel that particular clusters are adequate and want to prevent them from changing much.", "rank": 969, "start": 109295, "IsComparative": "1", "id": "st_969"}, {"end": 109476, "text": "In addition, fixing some clusters that", "rank": 970, "start": 109438, "IsComparative": "0", "id": "st_970"}]}, {"paragraph_info": {"end": 109506, "start": 109476, "text": "(a) The third-iteration result", "rank": 343, "paragraph_comparative_number": 0, "entities": [], "id": "p_343"}, "sentences": [{"end": 109506, "text": "(a) The third-iteration result", "rank": 971, "start": 109476, "IsComparative": "0", "id": "st_971"}]}, {"paragraph_info": {"end": 109536, "start": 109506, "text": "(b) The 300th-iteration result", "rank": 344, "paragraph_comparative_number": 0, "entities": [], "id": "p_344"}, "sentences": [{"end": 109536, "text": "(b) The 300th-iteration result", "rank": 972, "start": 109506, "IsComparative": "0", "id": "st_972"}]}, {"paragraph_info": {"end": 109566, "start": 109536, "text": "(c) The 700th-iteration result", "rank": 345, "paragraph_comparative_number": 0, "entities": [], "id": "p_345"}, "sentences": [{"end": 109566, "text": "(c) The 700th-iteration result", "rank": 973, "start": 109536, "IsComparative": "0", "id": "st_973"}]}, {"paragraph_info": {"end": 109786, "start": 109566, "text": "Figure 21: An example of filtering documents whose cluster memberships are unclear.This interaction is done in the 300th iteration, and the topics become clearer in the later iterations.20 newsgroups data have been used.", "rank": 346, "paragraph_comparative_number": 2, "entities": [], "id": "p_346"}, "sentences": [{"end": 109649, "text": "Figure 21: An example of filtering documents whose cluster memberships are unclear.", "rank": 974, "start": 109566, "IsComparative": "1", "id": "st_974"}, {"end": 109752, "text": "This interaction is done in the 300th iteration, and the topics become clearer in the later iterations.", "rank": 975, "start": 109649, "IsComparative": "0", "id": "st_975"}, {"end": 109786, "text": "20 newsgroups data have been used.", "rank": 976, "start": 109752, "IsComparative": "1", "id": "st_976"}]}, {"paragraph_info": {"end": 109912, "start": 109786, "text": "are already stable in early iterations can accelerate the later iterations by excluding them from the cluster assignment step.", "rank": 347, "paragraph_comparative_number": 0, "entities": [], "id": "p_347"}, "sentences": [{"end": 109912, "text": "are already stable in early iterations can accelerate the later iterations by excluding them from the cluster assignment step.", "rank": 977, "start": 109786, "IsComparative": "0", "id": "st_977"}]}, {"paragraph_info": {"end": 110672, "start": 109912, "text": "Fig.17 shows the effects of such interactions.First, we start with the same exam- ple shown in Fig.19, but we fix the clustering assignments of the cluster highlighted in yellow rectangles, which amounts to 44% of the total data items, at the sixth it- eration (Fig.19(b)).Once this interaction is performed, the computing times for the following iterations of k-means drops significantly (Fig.17(b)).However, only less than 10% of the final cluster memberships differ from the final results without this interaction, as shown in the increasing red line in Fig.17(a).The final outputs of the cluster view in Jigsaw of the two cases can also be compared in Figs.19(c) and (d), both of which are similar in terms of cluster sizes as well as keyword descriptions.", "rank": 348, "paragraph_comparative_number": 4, "entities": [], "id": "p_348"}, "sentences": [{"end": 109916, "text": "Fig.", "rank": 978, "start": 109912, "IsComparative": "0", "id": "st_978"}, {"end": 109958, "text": "17 shows the effects of such interactions.", "rank": 979, "start": 109916, "IsComparative": "0", "id": "st_979"}, {"end": 110011, "text": "First, we start with the same exam- ple shown in Fig.", "rank": 980, "start": 109958, "IsComparative": "0", "id": "st_980"}, {"end": 110178, "text": "19, but we fix the clustering assignments of the cluster highlighted in yellow rectangles, which amounts to 44% of the total data items, at the sixth it- eration (Fig.", "rank": 981, "start": 110011, "IsComparative": "1", "id": "st_981"}, {"end": 110185, "text": "19(b)).", "rank": 982, "start": 110178, "IsComparative": "0", "id": "st_982"}, {"end": 110306, "text": "Once this interaction is performed, the computing times for the following iterations of k-means drops significantly (Fig.", "rank": 983, "start": 110185, "IsComparative": "0", "id": "st_983"}, {"end": 110313, "text": "17(b)).", "rank": 984, "start": 110306, "IsComparative": "0", "id": "st_984"}, {"end": 110473, "text": "However, only less than 10% of the final cluster memberships differ from the final results without this interaction, as shown in the increasing red line in Fig.", "rank": 985, "start": 110313, "IsComparative": "1", "id": "st_985"}, {"end": 110479, "text": "17(a).", "rank": 986, "start": 110473, "IsComparative": "1", "id": "st_986"}, {"end": 110573, "text": "The final outputs of the cluster view in Jigsaw of the two cases can also be compared in Figs.", "rank": 987, "start": 110479, "IsComparative": "0", "id": "st_987"}, {"end": 110672, "text": "19(c) and (d), both of which are similar in terms of cluster sizes as well as keyword descriptions.", "rank": 988, "start": 110573, "IsComparative": "1", "id": "st_988"}]}, {"paragraph_info": {"end": 110711, "start": 110672, "text": "4.5.2.3 Split/merge clusters in k-means", "rank": 349, "paragraph_comparative_number": 0, "entities": [], "id": "p_349"}, "sentences": [{"end": 110711, "text": "4.5.2.3 Split/merge clusters in k-means", "rank": 989, "start": 110672, "IsComparative": "0", "id": "st_989"}]}, {"paragraph_info": {"end": 111494, "start": 110711, "text": "Our customization of k-means enables users to merge multiple small or semantically related clusters or split large or unclear clusters.Fig.20 shows its example in Jigsaw.In the third iteration, we merge yellow and green clusters and split a white cluster (Fig.20(a)).The resulting is shown in Fig.20(b).We obtain a much more balanced set of clusters (Fig.20(c)) compared to the final result in which no splitting/merging was performed (Fig.20(d)).Furthermore, after analyzing the documents in two split clusters, we found that one of the clusters primarily contained documents about the causes of autism while the other about the symptoms, as seen in the keyword summary in Fig.20(c).Without the interaction, one will notice in Fig.20(d) that these clusters are not easily separated.", "rank": 350, "paragraph_comparative_number": 6, "entities": [], "id": "p_350"}, "sentences": [{"end": 110846, "text": "Our customization of k-means enables users to merge multiple small or semantically related clusters or split large or unclear clusters.", "rank": 990, "start": 110711, "IsComparative": "1", "id": "st_990"}, {"end": 110850, "text": "Fig.", "rank": 991, "start": 110846, "IsComparative": "0", "id": "st_991"}, {"end": 110881, "text": "20 shows its example in Jigsaw.", "rank": 992, "start": 110850, "IsComparative": "1", "id": "st_992"}, {"end": 110971, "text": "In the third iteration, we merge yellow and green clusters and split a white cluster (Fig.", "rank": 993, "start": 110881, "IsComparative": "0", "id": "st_993"}, {"end": 110978, "text": "20(a)).", "rank": 994, "start": 110971, "IsComparative": "1", "id": "st_994"}, {"end": 111008, "text": "The resulting is shown in Fig.", "rank": 995, "start": 110978, "IsComparative": "0", "id": "st_995"}, {"end": 111014, "text": "20(b).", "rank": 996, "start": 111008, "IsComparative": "0", "id": "st_996"}, {"end": 111066, "text": "We obtain a much more balanced set of clusters (Fig.", "rank": 997, "start": 111014, "IsComparative": "1", "id": "st_997"}, {"end": 111151, "text": "20(c)) compared to the final result in which no splitting/merging was performed (Fig.", "rank": 998, "start": 111066, "IsComparative": "1", "id": "st_998"}, {"end": 111158, "text": "20(d)).", "rank": 999, "start": 111151, "IsComparative": "0", "id": "st_999"}, {"end": 111389, "text": "Furthermore, after analyzing the documents in two split clusters, we found that one of the clusters primarily contained documents about the causes of autism while the other about the symptoms, as seen in the keyword summary in Fig.", "rank": 1000, "start": 111158, "IsComparative": "1", "id": "st_1000"}, {"end": 111395, "text": "20(c).", "rank": 1001, "start": 111389, "IsComparative": "0", "id": "st_1001"}, {"end": 111443, "text": "Without the interaction, one will notice in Fig.", "rank": 1002, "start": 111395, "IsComparative": "0", "id": "st_1002"}, {"end": 111494, "text": "20(d) that these clusters are not easily separated.", "rank": 1003, "start": 111443, "IsComparative": "0", "id": "st_1003"}]}, {"paragraph_info": {"end": 111552, "start": 111494, "text": "4.5.2.4 Filtering noisy documents to improve topics in LDA", "rank": 351, "paragraph_comparative_number": 0, "entities": [], "id": "p_351"}, "sentences": [{"end": 111552, "text": "4.5.2.4 Filtering noisy documents to improve topics in LDA", "rank": 1004, "start": 111494, "IsComparative": "0", "id": "st_1004"}]}, {"paragraph_info": {"end": 112418, "start": 111552, "text": "The ability to filter noisy documents has been an appealing interaction for LDA in iVisClustering.To be specific, given parallel coordinate representations of topic-wise distributions of documents, users can interactively filter out documents that are not strongly related to a single topic, i.e., documents that have a very small maximum value in the topic-wise distribution.By removing them and re-running LDA, iVis- Cluster generally obtains significantly clearer topics.In PIVE, we performed this interaction near the 300th iteration (Fig.21(b)), which is an early iteration when compared to the total number of iterations performed by LDA.However, such an interaction successfully generates clearer topics (Fig.21(c)) over the standard ap- proach where users have to wait for the algorithm to finish its full iterations in order to perform the same interaction.", "rank": 352, "paragraph_comparative_number": 3, "entities": [], "id": "p_352"}, "sentences": [{"end": 111650, "text": "The ability to filter noisy documents has been an appealing interaction for LDA in iVisClustering.", "rank": 1005, "start": 111552, "IsComparative": "1", "id": "st_1005"}, {"end": 111928, "text": "To be specific, given parallel coordinate representations of topic-wise distributions of documents, users can interactively filter out documents that are not strongly related to a single topic, i.e., documents that have a very small maximum value in the topic-wise distribution.", "rank": 1006, "start": 111650, "IsComparative": "0", "id": "st_1006"}, {"end": 112026, "text": "By removing them and re-running LDA, iVis- Cluster generally obtains significantly clearer topics.", "rank": 1007, "start": 111928, "IsComparative": "1", "id": "st_1007"}, {"end": 112095, "text": "In PIVE, we performed this interaction near the 300th iteration (Fig.", "rank": 1008, "start": 112026, "IsComparative": "0", "id": "st_1008"}, {"end": 112196, "text": "21(b)), which is an early iteration when compared to the total number of iterations performed by LDA.", "rank": 1009, "start": 112095, "IsComparative": "0", "id": "st_1009"}, {"end": 112268, "text": "However, such an interaction successfully generates clearer topics (Fig.", "rank": 1010, "start": 112196, "IsComparative": "0", "id": "st_1010"}, {"end": 112418, "text": "21(c)) over the standard ap- proach where users have to wait for the algorithm to finish its full iterations in order to perform the same interaction.", "rank": 1011, "start": 112268, "IsComparative": "1", "id": "st_1011"}]}, {"paragraph_info": {"end": 112433, "start": 112418, "text": "4.6 Conclusions", "rank": 353, "paragraph_comparative_number": 0, "entities": [], "id": "p_353"}, "sentences": [{"end": 112433, "text": "4.6 Conclusions", "rank": 1012, "start": 112418, "IsComparative": "0", "id": "st_1012"}]}, {"paragraph_info": {"end": 113410, "start": 112433, "text": "We have presented PIVE (Per-Iteration Visualization Environment for supporting real-time interactive visualization with computational methods).One of its apparent advantages is its ability to present users with the intermediate results during the inter- actions, which could reveal a significant amount of information immediately in visual analytics.Another important advantage is that it indeed opens up the possibility of performing small multiple interactions, which in the past have been considered to be too inefficient, and allows the real-time control over computational methods in visual analytics.In fact, the interactions we proposed in this chapter are relatively simple, which do not involve any major algorithmic modifications, but after a sequence of interactions, the results reflects the intention of users sufficiently well in real-time.In this sense, PIVE makes them significantly useful by enabling users to perform these interactions easily and efficiently.", "rank": 354, "paragraph_comparative_number": 4, "entities": [], "id": "p_354"}, "sentences": [{"end": 112576, "text": "We have presented PIVE (Per-Iteration Visualization Environment for supporting real-time interactive visualization with computational methods).", "rank": 1013, "start": 112433, "IsComparative": "0", "id": "st_1013"}, {"end": 112783, "text": "One of its apparent advantages is its ability to present users with the intermediate results during the inter- actions, which could reveal a significant amount of information immediately in visual analytics.", "rank": 1014, "start": 112576, "IsComparative": "1", "id": "st_1014"}, {"end": 113039, "text": "Another important advantage is that it indeed opens up the possibility of performing small multiple interactions, which in the past have been considered to be too inefficient, and allows the real-time control over computational methods in visual analytics.", "rank": 1015, "start": 112783, "IsComparative": "1", "id": "st_1015"}, {"end": 113287, "text": "In fact, the interactions we proposed in this chapter are relatively simple, which do not involve any major algorithmic modifications, but after a sequence of interactions, the results reflects the intention of users sufficiently well in real-time.", "rank": 1016, "start": 113039, "IsComparative": "1", "id": "st_1016"}, {"end": 113410, "text": "In this sense, PIVE makes them significantly useful by enabling users to perform these interactions easily and efficiently.", "rank": 1017, "start": 113287, "IsComparative": "1", "id": "st_1017"}]}, {"paragraph_info": {"end": 114117, "start": 113410, "text": "However, the advantage of our framework can be limited when the changes be- tween iterations remain nontrivial, resulting in inconsistent visualizations.We have seen this kinds of limitations when using LDA under PIVE due to the random nature of the used LDA algorithm.As a future work, we plan to tackle this problem more actively by, for example, post-processing the results or even imposing additional con- straints in computational methods so that the results from the following iterations do not change much from the current ones.Finally, another interesting research direction we will pursue is to extend PIVE to various parallelized computational algorithms for the large-scale data visual analytics.", "rank": 355, "paragraph_comparative_number": 2, "entities": [], "id": "p_355"}, "sentences": [{"end": 113563, "text": "However, the advantage of our framework can be limited when the changes be- tween iterations remain nontrivial, resulting in inconsistent visualizations.", "rank": 1018, "start": 113410, "IsComparative": "1", "id": "st_1018"}, {"end": 113679, "text": "We have seen this kinds of limitations when using LDA under PIVE due to the random nature of the used LDA algorithm.", "rank": 1019, "start": 113563, "IsComparative": "0", "id": "st_1019"}, {"end": 113945, "text": "As a future work, we plan to tackle this problem more actively by, for example, post-processing the results or even imposing additional con- straints in computational methods so that the results from the following iterations do not change much from the current ones.", "rank": 1020, "start": 113679, "IsComparative": "0", "id": "st_1020"}, {"end": 114117, "text": "Finally, another interesting research direction we will pursue is to extend PIVE to various parallelized computational algorithms for the large-scale data visual analytics.", "rank": 1021, "start": 113945, "IsComparative": "1", "id": "st_1021"}]}, {"paragraph_info": {"end": 114126, "start": 114117, "text": "CHAPTER V", "rank": 356, "paragraph_comparative_number": 0, "entities": [], "id": "p_356"}, "sentences": [{"end": 114126, "text": "CHAPTER V", "rank": 1022, "start": 114117, "IsComparative": "0", "id": "st_1022"}]}, {"paragraph_info": {"end": 114226, "start": 114126, "text": "TESTBED: AN INTERACTIVE VISUAL TESTBED SYSTEM FOR VARIOUS DIMENSION REDUCTION AND CLUSTERING METHODS", "rank": 357, "paragraph_comparative_number": 1, "entities": [], "id": "p_357"}, "sentences": [{"end": 114226, "text": "TESTBED: AN INTERACTIVE VISUAL TESTBED SYSTEM FOR VARIOUS DIMENSION REDUCTION AND CLUSTERING METHODS", "rank": 1023, "start": 114126, "IsComparative": "1", "id": "st_1023"}]}, {"paragraph_info": {"end": 115818, "start": 114226, "text": "Many of the modern data sets such as text and image data can be represented in high- dimensional vector spaces and have benefited from computational methods that uti- lize advanced computational methods.Visual analytics approaches have contributed greatly to data understanding and analysis due to their capability of leveraging hu- mans ability for quick visual perception.However, visual analytics targeting large- scale data such as text and image data has been challenging due to the limited screen space in terms of both the numbers of data points and features to represent.Among various computational methods supporting visual analytics, dimension reduction and clustering have played essential roles by reducing these numbers in an intelligent way to visually manageable sizes.Given numerous dimension reduction and clustering methods available, however, the decision on the choice of algorithms and their pa- rameters becomes difficult.In this chapter, we present an interactive visual testbed system for dimension reduction and clustering in a large-scale high-dimensional data analysis.The Testbed system enables users to apply various dimension reduction and clustering methods with different settings, visually compare the results from different algorithmic methods to obtain rich knowledge for the data and tasks at hand, and eventually choose the most appropriate path for a collection of algorithms and param- eters.Using various data sets such as documents, images, and others that are already encoded in vectors, we demonstrate how the Testbed system can support these tasks.", "rank": 358, "paragraph_comparative_number": 3, "entities": [], "id": "p_358"}, "sentences": [{"end": 114429, "text": "Many of the modern data sets such as text and image data can be represented in high- dimensional vector spaces and have benefited from computational methods that uti- lize advanced computational methods.", "rank": 1024, "start": 114226, "IsComparative": "0", "id": "st_1024"}, {"end": 114600, "text": "Visual analytics approaches have contributed greatly to data understanding and analysis due to their capability of leveraging hu- mans ability for quick visual perception.", "rank": 1025, "start": 114429, "IsComparative": "1", "id": "st_1025"}, {"end": 114805, "text": "However, visual analytics targeting large- scale data such as text and image data has been challenging due to the limited screen space in terms of both the numbers of data points and features to represent.", "rank": 1026, "start": 114600, "IsComparative": "1", "id": "st_1026"}, {"end": 115010, "text": "Among various computational methods supporting visual analytics, dimension reduction and clustering have played essential roles by reducing these numbers in an intelligent way to visually manageable sizes.", "rank": 1027, "start": 114805, "IsComparative": "0", "id": "st_1027"}, {"end": 115170, "text": "Given numerous dimension reduction and clustering methods available, however, the decision on the choice of algorithms and their pa- rameters becomes difficult.", "rank": 1028, "start": 115010, "IsComparative": "1", "id": "st_1028"}, {"end": 115322, "text": "In this chapter, we present an interactive visual testbed system for dimension reduction and clustering in a large-scale high-dimensional data analysis.", "rank": 1029, "start": 115170, "IsComparative": "0", "id": "st_1029"}, {"end": 115657, "text": "The Testbed system enables users to apply various dimension reduction and clustering methods with different settings, visually compare the results from different algorithmic methods to obtain rich knowledge for the data and tasks at hand, and eventually choose the most appropriate path for a collection of algorithms and param- eters.", "rank": 1030, "start": 115322, "IsComparative": "0", "id": "st_1030"}, {"end": 115818, "text": "Using various data sets such as documents, images, and others that are already encoded in vectors, we demonstrate how the Testbed system can support these tasks.", "rank": 1031, "start": 115657, "IsComparative": "0", "id": "st_1031"}]}, {"paragraph_info": {"end": 115834, "start": 115818, "text": "5.1 Introduction", "rank": 359, "paragraph_comparative_number": 0, "entities": [], "id": "p_359"}, "sentences": [{"end": 115834, "text": "5.1 Introduction", "rank": 1032, "start": 115818, "IsComparative": "0", "id": "st_1032"}]}, {"paragraph_info": {"end": 116329, "start": 115834, "text": "The volume of available data has been increasing at an exponential speed in recent years.Many of the modern data are generated in various forms such as documents and images of which the raw data can be represented in a high-dimensional vector space, allowing various computational methods to be applied.For instance, text documents can be encoded using a bag-of-words model, and images are represented using their feature point descriptors <91>, resulting in hundreds of thousands of dimensions.", "rank": 360, "paragraph_comparative_number": 0, "entities": [], "id": "p_360"}, "sentences": [{"end": 115923, "text": "The volume of available data has been increasing at an exponential speed in recent years.", "rank": 1033, "start": 115834, "IsComparative": "0", "id": "st_1033"}, {"end": 116137, "text": "Many of the modern data are generated in various forms such as documents and images of which the raw data can be represented in a high-dimensional vector space, allowing various computational methods to be applied.", "rank": 1034, "start": 115923, "IsComparative": "0", "id": "st_1034"}, {"end": 116329, "text": "For instance, text documents can be encoded using a bag-of-words model, and images are represented using their feature point descriptors <91>, resulting in hundreds of thousands of dimensions.", "rank": 1035, "start": 116137, "IsComparative": "0", "id": "st_1035"}]}, {"paragraph_info": {"end": 117217, "start": 116329, "text": "Given high-dimensional data, understanding and analyzing these data become more challenging.Visual analytics <78, 127> has gained increasing interest due to its capability of leveraging humans ability of quick visual insight in data analyses and decision processes.However, many state-of-the-art visual analytics techniques or systems are not equipped for high-dimensional large-scale data.One of the reasons is that although humans are good at visually grasping an overall structure, when the number of visualized objects becomes large, it is often difficult to extract meaningful information from visualization.Another factor is the limited dimension of a screen space where high-dimensional data have to be visualized.For instance, parallel co- ordinates, a widely-used visualization technique for multi-dimensional data, do not scale well even when the dimension reaches several tens.", "rank": 361, "paragraph_comparative_number": 3, "entities": [], "id": "p_361"}, "sentences": [{"end": 116421, "text": "Given high-dimensional data, understanding and analyzing these data become more challenging.", "rank": 1036, "start": 116329, "IsComparative": "0", "id": "st_1036"}, {"end": 116594, "text": "Visual analytics <78, 127> has gained increasing interest due to its capability of leveraging humans ability of quick visual insight in data analyses and decision processes.", "rank": 1037, "start": 116421, "IsComparative": "1", "id": "st_1037"}, {"end": 116719, "text": "However, many state-of-the-art visual analytics techniques or systems are not equipped for high-dimensional large-scale data.", "rank": 1038, "start": 116594, "IsComparative": "0", "id": "st_1038"}, {"end": 116942, "text": "One of the reasons is that although humans are good at visually grasping an overall structure, when the number of visualized objects becomes large, it is often difficult to extract meaningful information from visualization.", "rank": 1039, "start": 116719, "IsComparative": "1", "id": "st_1039"}, {"end": 117050, "text": "Another factor is the limited dimension of a screen space where high-dimensional data have to be visualized.", "rank": 1040, "start": 116942, "IsComparative": "1", "id": "st_1040"}, {"end": 117217, "text": "For instance, parallel co- ordinates, a widely-used visualization technique for multi-dimensional data, do not scale well even when the dimension reaches several tens.", "rank": 1041, "start": 117050, "IsComparative": "0", "id": "st_1041"}]}, {"paragraph_info": {"end": 118280, "start": 117217, "text": "To improve this scalability issue, computational methods can support visual ana- lytics by transforming the original data into a more compact and meaningful represen- tation.Among various methods, two main ones, dimension reduction and clustering, play an essential role in visual analytics of large-scale high-dimensional data owing to their nature to reduce the numbers of features and data items into manageable sizes, respectively.Dimension reduction methods can reveal meaningful information by al- lowing the visual representation of high-dimensional data in a much lower-dimensional space.In addition, it allows visualization of high-dimensional data in the form of a 2D/3D scatter plot in which one can obtain insight about data relationships with respect to the geometric locations of data.On the other hand, clustering provides an overview of large-scale data in terms of a small number of groups based on their semantic coherences.Such cluster information can then guide us to a proper data group of interest on which we can further focus our analysis.", "rank": 362, "paragraph_comparative_number": 2, "entities": [], "id": "p_362"}, "sentences": [{"end": 117391, "text": "To improve this scalability issue, computational methods can support visual ana- lytics by transforming the original data into a more compact and meaningful represen- tation.", "rank": 1042, "start": 117217, "IsComparative": "0", "id": "st_1042"}, {"end": 117652, "text": "Among various methods, two main ones, dimension reduction and clustering, play an essential role in visual analytics of large-scale high-dimensional data owing to their nature to reduce the numbers of features and data items into manageable sizes, respectively.", "rank": 1043, "start": 117391, "IsComparative": "1", "id": "st_1043"}, {"end": 117813, "text": "Dimension reduction methods can reveal meaningful information by al- lowing the visual representation of high-dimensional data in a much lower-dimensional space.", "rank": 1044, "start": 117652, "IsComparative": "0", "id": "st_1044"}, {"end": 118016, "text": "In addition, it allows visualization of high-dimensional data in the form of a 2D/3D scatter plot in which one can obtain insight about data relationships with respect to the geometric locations of data.", "rank": 1045, "start": 117813, "IsComparative": "0", "id": "st_1045"}, {"end": 118159, "text": "On the other hand, clustering provides an overview of large-scale data in terms of a small number of groups based on their semantic coherences.", "rank": 1046, "start": 118016, "IsComparative": "0", "id": "st_1046"}, {"end": 118280, "text": "Such cluster information can then guide us to a proper data group of interest on which we can further focus our analysis.", "rank": 1047, "start": 118159, "IsComparative": "1", "id": "st_1047"}]}, {"paragraph_info": {"end": 119500, "start": 118280, "text": "Given a wide variety of computational methods including dimension reduction and clustering methods, it is not easy to determine which method to choose and how to use it properly for a certain data set and a certain task.Sometimes, when a specific method is used for a certain data set, its performance may be dependent on how the data is pre-processed beforehand.In addition, many modern computational methods often require decisions on multiple parameters.Yet there is no theoretical guideline for an optimal set of parameters for a given problem, and we have to go through multiple trials only to obtain some initial understanding of parameter values.As the algorithm gets more complicated, it becomes more difficult for users to understand what these parameters mean and how to select them properly.Consequently, many visual analytics systems choose a certain computational method, which is often basic and/or generic, and treat it as a black box with fixed parameter values while focusing on the subsequent analysis after obtaining the output from it.However, without an appropriate choice of algorithms and their parameters, the performance of these methods may not be satisfactory enough to start an analysis with.", "rank": 363, "paragraph_comparative_number": 4, "entities": [], "id": "p_363"}, "sentences": [{"end": 118500, "text": "Given a wide variety of computational methods including dimension reduction and clustering methods, it is not easy to determine which method to choose and how to use it properly for a certain data set and a certain task.", "rank": 1048, "start": 118280, "IsComparative": "1", "id": "st_1048"}, {"end": 118643, "text": "Sometimes, when a specific method is used for a certain data set, its performance may be dependent on how the data is pre-processed beforehand.", "rank": 1049, "start": 118500, "IsComparative": "0", "id": "st_1049"}, {"end": 118737, "text": "In addition, many modern computational methods often require decisions on multiple parameters.", "rank": 1050, "start": 118643, "IsComparative": "0", "id": "st_1050"}, {"end": 118933, "text": "Yet there is no theoretical guideline for an optimal set of parameters for a given problem, and we have to go through multiple trials only to obtain some initial understanding of parameter values.", "rank": 1051, "start": 118737, "IsComparative": "1", "id": "st_1051"}, {"end": 119082, "text": "As the algorithm gets more complicated, it becomes more difficult for users to understand what these parameters mean and how to select them properly.", "rank": 1052, "start": 118933, "IsComparative": "0", "id": "st_1052"}, {"end": 119335, "text": "Consequently, many visual analytics systems choose a certain computational method, which is often basic and/or generic, and treat it as a black box with fixed parameter values while focusing on the subsequent analysis after obtaining the output from it.", "rank": 1053, "start": 119082, "IsComparative": "1", "id": "st_1053"}, {"end": 119500, "text": "However, without an appropriate choice of algorithms and their parameters, the performance of these methods may not be satisfactory enough to start an analysis with.", "rank": 1054, "start": 119335, "IsComparative": "1", "id": "st_1054"}]}, {"paragraph_info": {"end": 120160, "start": 119500, "text": "Due to these difficulties, the current state of the art in visual analytics has not taken full advantage of the recent advancements of computational methods.To tackle this problem, we claim that users have to be provided with the capability of interac- tively trying out various computational methods and their parameters and reviewing their results at a visual level without having to know the details of algorithms.As a cornerstone to achieve this claim, this chapter presents an interactive visual testbed system for dimension reduction and clustering, the two essential computational meth- ods for the visual analytics of large-scale high-dimensional data.", "rank": 364, "paragraph_comparative_number": 2, "entities": [], "id": "p_364"}, "sentences": [{"end": 119657, "text": "Due to these difficulties, the current state of the art in visual analytics has not taken full advantage of the recent advancements of computational methods.", "rank": 1055, "start": 119500, "IsComparative": "1", "id": "st_1055"}, {"end": 119917, "text": "To tackle this problem, we claim that users have to be provided with the capability of interac- tively trying out various computational methods and their parameters and reviewing their results at a visual level without having to know the details of algorithms.", "rank": 1056, "start": 119657, "IsComparative": "1", "id": "st_1056"}, {"end": 120160, "text": "As a cornerstone to achieve this claim, this chapter presents an interactive visual testbed system for dimension reduction and clustering, the two essential computational meth- ods for the visual analytics of large-scale high-dimensional data.", "rank": 1057, "start": 119917, "IsComparative": "0", "id": "st_1057"}]}, {"paragraph_info": {"end": 121128, "start": 120160, "text": "The main contributions of the proposed Testbed system are as follows.First of all, given various types of input data such as text documents, images, and vector-encoded data, the testbed system provides extensive capabilities to interactively select data pre-processing options and choose a wide variety of clustering and dimension reduc- tion methods along with their parameters.The output of these processes are then visualized in several forms, e.g., parallel coordinates and a scatter plot, equipped with various interaction capabilities, e.g., accessing the original data items and brushing and linking between multiple views.Additionally, the Testbed system facilitates easy comparisons between different dimension reduction and clustering results by com- putationally aligning them.Finally, the Testbed system is implemented in a highly modular way so that new data types and dimension reduction/clustering methods can be easily integrated to the current system.", "rank": 365, "paragraph_comparative_number": 2, "entities": [], "id": "p_365"}, "sentences": [{"end": 120229, "text": "The main contributions of the proposed Testbed system are as follows.", "rank": 1058, "start": 120160, "IsComparative": "0", "id": "st_1058"}, {"end": 120539, "text": "First of all, given various types of input data such as text documents, images, and vector-encoded data, the testbed system provides extensive capabilities to interactively select data pre-processing options and choose a wide variety of clustering and dimension reduc- tion methods along with their parameters.", "rank": 1059, "start": 120229, "IsComparative": "1", "id": "st_1059"}, {"end": 120790, "text": "The output of these processes are then visualized in several forms, e.g., parallel coordinates and a scatter plot, equipped with various interaction capabilities, e.g., accessing the original data items and brushing and linking between multiple views.", "rank": 1060, "start": 120539, "IsComparative": "1", "id": "st_1060"}, {"end": 120948, "text": "Additionally, the Testbed system facilitates easy comparisons between different dimension reduction and clustering results by com- putationally aligning them.", "rank": 1061, "start": 120790, "IsComparative": "0", "id": "st_1061"}, {"end": 121128, "text": "Finally, the Testbed system is implemented in a highly modular way so that new data types and dimension reduction/clustering methods can be easily integrated to the current system.", "rank": 1062, "start": 120948, "IsComparative": "0", "id": "st_1062"}]}, {"paragraph_info": {"end": 121665, "start": 121128, "text": "Note that even though the Testbed system can be used by anyone who wants to apply various methods to their own data, some background knowledge about machine learning and data mining would be of great help in fully utilizing the Testbed system via understanding the data and the applied methods simultaneously.For example, machine learning researchers/developers, who wish to easily plug in and visually evaluate their own methods in practical data analysis scenarios, would be able to receive significant benefit from the Testbed system.", "rank": 366, "paragraph_comparative_number": 2, "entities": [], "id": "p_366"}, "sentences": [{"end": 121437, "text": "Note that even though the Testbed system can be used by anyone who wants to apply various methods to their own data, some background knowledge about machine learning and data mining would be of great help in fully utilizing the Testbed system via understanding the data and the applied methods simultaneously.", "rank": 1063, "start": 121128, "IsComparative": "1", "id": "st_1063"}, {"end": 121665, "text": "For example, machine learning researchers/developers, who wish to easily plug in and visually evaluate their own methods in practical data analysis scenarios, would be able to receive significant benefit from the Testbed system.", "rank": 1064, "start": 121437, "IsComparative": "1", "id": "st_1064"}]}, {"paragraph_info": {"end": 122127, "start": 121665, "text": "The rest of this chapter is organized as follows.In Section 5.2, we review the relevant literature in terms of dimension reduction and clustering methods as well as the visual analytics systems adopting them.Section 5.3 describes the details of the Testbed system as well as several main computational methods used in the system.Section 5.4 shows various usage scenarios of the Testbed system, and finally Section 5.5 presents conclusions along with future work.", "rank": 367, "paragraph_comparative_number": 1, "entities": [], "id": "p_367"}, "sentences": [{"end": 121714, "text": "The rest of this chapter is organized as follows.", "rank": 1065, "start": 121665, "IsComparative": "0", "id": "st_1065"}, {"end": 121873, "text": "In Section 5.2, we review the relevant literature in terms of dimension reduction and clustering methods as well as the visual analytics systems adopting them.", "rank": 1066, "start": 121714, "IsComparative": "0", "id": "st_1066"}, {"end": 121994, "text": "Section 5.3 describes the details of the Testbed system as well as several main computational methods used in the system.", "rank": 1067, "start": 121873, "IsComparative": "0", "id": "st_1067"}, {"end": 122127, "text": "Section 5.4 shows various usage scenarios of the Testbed system, and finally Section 5.5 presents conclusions along with future work.", "rank": 1068, "start": 121994, "IsComparative": "1", "id": "st_1068"}]}, {"paragraph_info": {"end": 122309, "start": 122127, "text": "Figure 22: 2D Scatter plots obtained by two dimension reduction methods, MDS (left) and LDA (right), for a facial image data set.A different color corresponds to a different cluster.", "rank": 368, "paragraph_comparative_number": 1, "entities": [], "id": "p_368"}, "sentences": [{"end": 122256, "text": "Figure 22: 2D Scatter plots obtained by two dimension reduction methods, MDS (left) and LDA (right), for a facial image data set.", "rank": 1069, "start": 122127, "IsComparative": "1", "id": "st_1069"}, {"end": 122309, "text": "A different color corresponds to a different cluster.", "rank": 1070, "start": 122256, "IsComparative": "0", "id": "st_1070"}]}, {"paragraph_info": {"end": 122325, "start": 122309, "text": "5.2 Related Work", "rank": 369, "paragraph_comparative_number": 0, "entities": [], "id": "p_369"}, "sentences": [{"end": 122325, "text": "5.2 Related Work", "rank": 1071, "start": 122309, "IsComparative": "0", "id": "st_1071"}]}, {"paragraph_info": {"end": 122559, "start": 122325, "text": "In this Section, various dimension reduction and clustering methods applicable to visualization are first reviewed.Afterwards, we discuss some of the currently available visual analytics systems that adopt these computational methods.", "rank": 370, "paragraph_comparative_number": 1, "entities": [], "id": "p_370"}, "sentences": [{"end": 122440, "text": "In this Section, various dimension reduction and clustering methods applicable to visualization are first reviewed.", "rank": 1072, "start": 122325, "IsComparative": "1", "id": "st_1072"}, {"end": 122559, "text": "Afterwards, we discuss some of the currently available visual analytics systems that adopt these computational methods.", "rank": 1073, "start": 122440, "IsComparative": "0", "id": "st_1073"}]}, {"paragraph_info": {"end": 122617, "start": 122559, "text": "5.2.1 Dimension Reduction and Clustering for Visualization", "rank": 371, "paragraph_comparative_number": 0, "entities": [], "id": "p_371"}, "sentences": [{"end": 122617, "text": "5.2.1 Dimension Reduction and Clustering for Visualization", "rank": 1074, "start": 122559, "IsComparative": "0", "id": "st_1074"}]}, {"paragraph_info": {"end": 122991, "start": 122617, "text": "Dimension reduction has long been one of the main research topics in data mining and statistical machine learning areas.Numerous dimension reduction methods have been proposed, among which the most commonly used dimension reduction methods include principal component analysis (PCA) <75>, multidimensional scaling (MDS) <45>, and linear discriminant analysis (LDA) <60, 68>.", "rank": 372, "paragraph_comparative_number": 0, "entities": [], "id": "p_372"}, "sentences": [{"end": 122737, "text": "Dimension reduction has long been one of the main research topics in data mining and statistical machine learning areas.", "rank": 1075, "start": 122617, "IsComparative": "0", "id": "st_1075"}, {"end": 122991, "text": "Numerous dimension reduction methods have been proposed, among which the most commonly used dimension reduction methods include principal component analysis (PCA) <75>, multidimensional scaling (MDS) <45>, and linear discriminant analysis (LDA) <60, 68>.", "rank": 1076, "start": 122737, "IsComparative": "0", "id": "st_1076"}]}, {"paragraph_info": {"end": 123461, "start": 122991, "text": "In addition to traditional data analysis problems, they have also been widely utilized in visualization due to their capability of representing high-dimensional data as a form of scatter plots in 2D/3D space.In a scatter plot, each data item is represented as a point and its 2D/3D coordinate is determined from the dimension- reduced representation.In general, the relative locations among data points reflect the pairwise relationships or proximities among data items.", "rank": 373, "paragraph_comparative_number": 2, "entities": [], "id": "p_373"}, "sentences": [{"end": 123199, "text": "In addition to traditional data analysis problems, they have also been widely utilized in visualization due to their capability of representing high-dimensional data as a form of scatter plots in 2D/3D space.", "rank": 1077, "start": 122991, "IsComparative": "1", "id": "st_1077"}, {"end": 123341, "text": "In a scatter plot, each data item is represented as a point and its 2D/3D coordinate is determined from the dimension- reduced representation.", "rank": 1078, "start": 123199, "IsComparative": "1", "id": "st_1078"}, {"end": 123461, "text": "In general, the relative locations among data points reflect the pairwise relationships or proximities among data items.", "rank": 1079, "start": 123341, "IsComparative": "0", "id": "st_1079"}]}, {"paragraph_info": {"end": 124219, "start": 123461, "text": "Each dimension reduction method has its own optimization criteria and behaviors, which result in different visualizations.For instance, the recently proposed man- ifold learning algorithms, e.g., isometric feature mapping (ISOMAP) <125>, locally linear embedding (LLE) <111>, and Laplacian Eigenmaps (LE) <17>, try to preserve the relationships between the local neighborhood rather than global relationships.These methods have been successfully applied to the data that originally have a low- dimensional manifold structure, and often they demonstrated their capability to reveal such a manifold structure in 2D/3D visualizations.However, most of these methods present just several visualization snapshots of limited data sets with no interaction abilities.", "rank": 374, "paragraph_comparative_number": 3, "entities": [], "id": "p_374"}, "sentences": [{"end": 123583, "text": "Each dimension reduction method has its own optimization criteria and behaviors, which result in different visualizations.", "rank": 1080, "start": 123461, "IsComparative": "1", "id": "st_1080"}, {"end": 123870, "text": "For instance, the recently proposed man- ifold learning algorithms, e.g., isometric feature mapping (ISOMAP) <125>, locally linear embedding (LLE) <111>, and Laplacian Eigenmaps (LE) <17>, try to preserve the relationships between the local neighborhood rather than global relationships.", "rank": 1081, "start": 123583, "IsComparative": "0", "id": "st_1081"}, {"end": 124092, "text": "These methods have been successfully applied to the data that originally have a low- dimensional manifold structure, and often they demonstrated their capability to reveal such a manifold structure in 2D/3D visualizations.", "rank": 1082, "start": 123870, "IsComparative": "1", "id": "st_1082"}, {"end": 124219, "text": "However, most of these methods present just several visualization snapshots of limited data sets with no interaction abilities.", "rank": 1083, "start": 124092, "IsComparative": "1", "id": "st_1083"}]}, {"paragraph_info": {"end": 125037, "start": 124219, "text": "Another aspect to consider when applying dimension reduction in visualization applications is the cluster structure of data.A majority of dimension reduction methods take into account only the pairwise relationships between data items.In practice, however, it is not easy to obtain much insight from the 2D/3D scatter plot generated by them for a large number of data items.The left figure in Fig.22 is a visualization example of such a dimension reduction method, MDS, for a facial image data set.Let us, for now, ignore the colors, which indicate the cluster labels.This visualization shows most of the data as a single chunk with a few outliers placed outside.Although these points may give some interesting insight about why they appear to be outliers, one cannot get much more information from this visualization.", "rank": 375, "paragraph_comparative_number": 4, "entities": [], "id": "p_375"}, "sentences": [{"end": 124343, "text": "Another aspect to consider when applying dimension reduction in visualization applications is the cluster structure of data.", "rank": 1084, "start": 124219, "IsComparative": "1", "id": "st_1084"}, {"end": 124454, "text": "A majority of dimension reduction methods take into account only the pairwise relationships between data items.", "rank": 1085, "start": 124343, "IsComparative": "0", "id": "st_1085"}, {"end": 124593, "text": "In practice, however, it is not easy to obtain much insight from the 2D/3D scatter plot generated by them for a large number of data items.", "rank": 1086, "start": 124454, "IsComparative": "0", "id": "st_1086"}, {"end": 124616, "text": "The left figure in Fig.", "rank": 1087, "start": 124593, "IsComparative": "0", "id": "st_1087"}, {"end": 124717, "text": "22 is a visualization example of such a dimension reduction method, MDS, for a facial image data set.", "rank": 1088, "start": 124616, "IsComparative": "1", "id": "st_1088"}, {"end": 124787, "text": "Let us, for now, ignore the colors, which indicate the cluster labels.", "rank": 1089, "start": 124717, "IsComparative": "1", "id": "st_1089"}, {"end": 124882, "text": "This visualization shows most of the data as a single chunk with a few outliers placed outside.", "rank": 1090, "start": 124787, "IsComparative": "0", "id": "st_1090"}, {"end": 125037, "text": "Although these points may give some interesting insight about why they appear to be outliers, one cannot get much more information from this visualization.", "rank": 1091, "start": 124882, "IsComparative": "1", "id": "st_1091"}]}, {"paragraph_info": {"end": 125825, "start": 125037, "text": "Another type of dimension reduction methods incorporates additional information about the cluster structure of data in addition to individual data items.Since these dimension reduction methods require the assigned cluster label associated with each data item as an input, they are called supervised dimension reduction methods while the previous methods are called unsupervised dimension reduction methods.Some representative supervised methods include LDA <60> and orthogonal centroid method (OCM) <102>.The right figure in Fig.22 is an example of LDA visualization.This figure visualizes the data as groups of items computed by LDA based on the given cluster labels, and one can obtain better insight about the overall data structure at the cluster level over the individual data level.", "rank": 376, "paragraph_comparative_number": 3, "entities": [], "id": "p_376"}, "sentences": [{"end": 125190, "text": "Another type of dimension reduction methods incorporates additional information about the cluster structure of data in addition to individual data items.", "rank": 1092, "start": 125037, "IsComparative": "1", "id": "st_1092"}, {"end": 125443, "text": "Since these dimension reduction methods require the assigned cluster label associated with each data item as an input, they are called supervised dimension reduction methods while the previous methods are called unsupervised dimension reduction methods.", "rank": 1093, "start": 125190, "IsComparative": "0", "id": "st_1093"}, {"end": 125542, "text": "Some representative supervised methods include LDA <60> and orthogonal centroid method (OCM) <102>.", "rank": 1094, "start": 125443, "IsComparative": "1", "id": "st_1094"}, {"end": 125566, "text": "The right figure in Fig.", "rank": 1095, "start": 125542, "IsComparative": "0", "id": "st_1095"}, {"end": 125604, "text": "22 is an example of LDA visualization.", "rank": 1096, "start": 125566, "IsComparative": "1", "id": "st_1096"}, {"end": 125825, "text": "This figure visualizes the data as groups of items computed by LDA based on the given cluster labels, and one can obtain better insight about the overall data structure at the cluster level over the individual data level.", "rank": 1097, "start": 125604, "IsComparative": "0", "id": "st_1097"}]}, {"paragraph_info": {"end": 126423, "start": 125825, "text": "Representing the cluster structure has been one of the main concerns in many studies on dimension reduction and 2D/3D scatter plot visualizations even when unsupervised dimension reduction methods are used.Many methods have been eval- uated regarding their ability to visualize cluster structures, which are hidden at the time of computing dimension reduction.For instance, a recently proposed dimension reduction method, t-distributed stochastic neighborhood embedding (t-SNE) <130>, shows its capability of grouping data and revealing the true cluster structure in 2D scatter plot visualizations.", "rank": 377, "paragraph_comparative_number": 2, "entities": [], "id": "p_377"}, "sentences": [{"end": 126031, "text": "Representing the cluster structure has been one of the main concerns in many studies on dimension reduction and 2D/3D scatter plot visualizations even when unsupervised dimension reduction methods are used.", "rank": 1098, "start": 125825, "IsComparative": "0", "id": "st_1098"}, {"end": 126185, "text": "Many methods have been eval- uated regarding their ability to visualize cluster structures, which are hidden at the time of computing dimension reduction.", "rank": 1099, "start": 126031, "IsComparative": "1", "id": "st_1099"}, {"end": 126423, "text": "For instance, a recently proposed dimension reduction method, t-distributed stochastic neighborhood embedding (t-SNE) <130>, shows its capability of grouping data and revealing the true cluster structure in 2D scatter plot visualizations.", "rank": 1100, "start": 126185, "IsComparative": "1", "id": "st_1100"}]}, {"paragraph_info": {"end": 127121, "start": 126423, "text": "Given the importance of the cluster structure in large-scale data visualization, clustering methods can add a significant value to visual analytics approaches by en- abling visual understanding of the overview of data.Clustering partitions the entire data into groups or clusters so that the data items in the same cluster are more similar to each other than to those in different clusters.The resulting grouping information is in a form of cluster labels, which act as an additional categorical variable associ- ated with data items.Such cluster information can be color-coded in visualization, as shown in Fig.22, and help us understand the cluster structure in the data clearly in visualization.", "rank": 378, "paragraph_comparative_number": 1, "entities": [], "id": "p_378"}, "sentences": [{"end": 126641, "text": "Given the importance of the cluster structure in large-scale data visualization, clustering methods can add a significant value to visual analytics approaches by en- abling visual understanding of the overview of data.", "rank": 1101, "start": 126423, "IsComparative": "1", "id": "st_1101"}, {"end": 126813, "text": "Clustering partitions the entire data into groups or clusters so that the data items in the same cluster are more similar to each other than to those in different clusters.", "rank": 1102, "start": 126641, "IsComparative": "0", "id": "st_1102"}, {"end": 126957, "text": "The resulting grouping information is in a form of cluster labels, which act as an additional categorical variable associ- ated with data items.", "rank": 1103, "start": 126813, "IsComparative": "0", "id": "st_1103"}, {"end": 127035, "text": "Such cluster information can be color-coded in visualization, as shown in Fig.", "rank": 1104, "start": 126957, "IsComparative": "0", "id": "st_1104"}, {"end": 127121, "text": "22, and help us understand the cluster structure in the data clearly in visualization.", "rank": 1105, "start": 127035, "IsComparative": "0", "id": "st_1105"}]}, {"paragraph_info": {"end": 128404, "start": 127121, "text": "Clustering, along with dimension reduction, has also been one of the well-studied research topics in data mining and machine learning areas.Widely-used methods include k-means clustering, spectral clustering <96>, and Gaussian mixture models.Recently, more advanced methods such as non-negative matrix factorization (NMF) <79> and latent Dirichlet allocation <20> have shown their successful applications in image segmentation and document topic modeling, etc.These methods are usually evaluated using the data set whose cluster label information is already known and by comparing between the true cluster labels with those obtained by the computational method.However, given the data set that may not have a clear cluster structure, clustering is typically a very challenging task, and thus it is often the case that the resulting cluster quality is unsatisfactory.From a visual analytics perspective, unsatisfactory clustering makes it difficult to understand the coherent meaning of each cluster and how one cluster contrasts with another.For instance, in recent applications of latent Dirichlet allocation for document topic modeling, while several coherent topic clusters have been successfully revealed for the document data, many other topics often seem unclear to understand.", "rank": 379, "paragraph_comparative_number": 3, "entities": [], "id": "p_379"}, "sentences": [{"end": 127261, "text": "Clustering, along with dimension reduction, has also been one of the well-studied research topics in data mining and machine learning areas.", "rank": 1106, "start": 127121, "IsComparative": "0", "id": "st_1106"}, {"end": 127363, "text": "Widely-used methods include k-means clustering, spectral clustering <96>, and Gaussian mixture models.", "rank": 1107, "start": 127261, "IsComparative": "1", "id": "st_1107"}, {"end": 127581, "text": "Recently, more advanced methods such as non-negative matrix factorization (NMF) <79> and latent Dirichlet allocation <20> have shown their successful applications in image segmentation and document topic modeling, etc.", "rank": 1108, "start": 127363, "IsComparative": "1", "id": "st_1108"}, {"end": 127782, "text": "These methods are usually evaluated using the data set whose cluster label information is already known and by comparing between the true cluster labels with those obtained by the computational method.", "rank": 1109, "start": 127581, "IsComparative": "0", "id": "st_1109"}, {"end": 127987, "text": "However, given the data set that may not have a clear cluster structure, clustering is typically a very challenging task, and thus it is often the case that the resulting cluster quality is unsatisfactory.", "rank": 1110, "start": 127782, "IsComparative": "0", "id": "st_1110"}, {"end": 128163, "text": "From a visual analytics perspective, unsatisfactory clustering makes it difficult to understand the coherent meaning of each cluster and how one cluster contrasts with another.", "rank": 1111, "start": 127987, "IsComparative": "1", "id": "st_1111"}, {"end": 128404, "text": "For instance, in recent applications of latent Dirichlet allocation for document topic modeling, while several coherent topic clusters have been successfully revealed for the document data, many other topics often seem unclear to understand.", "rank": 1112, "start": 128163, "IsComparative": "0", "id": "st_1112"}]}, {"paragraph_info": {"end": 128971, "start": 128404, "text": "Even with the obvious needs of computational methods such as dimension reduc- tion and clustering in visual analytics, various issues such as data noise and improper algorithm and parameter choices, as described in Section 1, prevent their initial re- sults from being practically useful enough to support the subsequent visual analysis.Nonetheless, among data mining and machine learning communities, which supply supposedly better computational methods, the efforts of interactively improving these initial results in real-world data analysis seem to be overlooked.", "rank": 380, "paragraph_comparative_number": 0, "entities": [], "id": "p_380"}, "sentences": [{"end": 128741, "text": "Even with the obvious needs of computational methods such as dimension reduc- tion and clustering in visual analytics, various issues such as data noise and improper algorithm and parameter choices, as described in Section 1, prevent their initial re- sults from being practically useful enough to support the subsequent visual analysis.", "rank": 1113, "start": 128404, "IsComparative": "0", "id": "st_1113"}, {"end": 128971, "text": "Nonetheless, among data mining and machine learning communities, which supply supposedly better computational methods, the efforts of interactively improving these initial results in real-world data analysis seem to be overlooked.", "rank": 1114, "start": 128741, "IsComparative": "0", "id": "st_1114"}]}, {"paragraph_info": {"end": 129043, "start": 128971, "text": "5.2.2 Visual Analytic Systems using Dimension Reduction and Cluster- ing", "rank": 381, "paragraph_comparative_number": 0, "entities": [], "id": "p_381"}, "sentences": [{"end": 129043, "text": "5.2.2 Visual Analytic Systems using Dimension Reduction and Cluster- ing", "rank": 1115, "start": 128971, "IsComparative": "0", "id": "st_1115"}]}, {"paragraph_info": {"end": 129408, "start": 129043, "text": "In information visualization and visual analytics communities, various visual analyt- ics systems incorporating computational methods such as dimension reduction and clustering have been proposed to deal with large-scale high-dimensional data.In this section, several systems such as IN-SPIRE <138>, Jigsaw <121>, GGobi <43>, iPCA <72>, and WEKA <65> are discussed.", "rank": 382, "paragraph_comparative_number": 0, "entities": [], "id": "p_382"}, "sentences": [{"end": 129286, "text": "In information visualization and visual analytics communities, various visual analyt- ics systems incorporating computational methods such as dimension reduction and clustering have been proposed to deal with large-scale high-dimensional data.", "rank": 1116, "start": 129043, "IsComparative": "0", "id": "st_1116"}, {"end": 129408, "text": "In this section, several systems such as IN-SPIRE <138>, Jigsaw <121>, GGobi <43>, iPCA <72>, and WEKA <65> are discussed.", "rank": 1117, "start": 129286, "IsComparative": "0", "id": "st_1117"}]}, {"paragraph_info": {"end": 130235, "start": 129408, "text": "IN-SPIRE <138> is one of the well-known visual analytics systems for document data in which dimension reduction and clustering play main roles.Given a set of documents, IN-SPIRE first encodes them as high-dimensional vectors using a bag- of-words model.Then it applies k-means clustering with a pre-defined number of clusters.PCA is computed on cluster centroids and applied to the entire data, which gives 2D coordinates of document data.Based on these 2D coordinates, a galaxy view similar to a scatter plot is shown to users with a keyword summary for each cluster placed at the cluster centroid.Owing to the simple algorithms adopted such as PCA and k-means, IN-SPIRE can deal with a fairly large amount of data, but it provides only a limited number of interaction capabilities to change the algorithms and their settings.", "rank": 383, "paragraph_comparative_number": 3, "entities": [], "id": "p_383"}, "sentences": [{"end": 129551, "text": "IN-SPIRE <138> is one of the well-known visual analytics systems for document data in which dimension reduction and clustering play main roles.", "rank": 1118, "start": 129408, "IsComparative": "0", "id": "st_1118"}, {"end": 129661, "text": "Given a set of documents, IN-SPIRE first encodes them as high-dimensional vectors using a bag- of-words model.", "rank": 1119, "start": 129551, "IsComparative": "0", "id": "st_1119"}, {"end": 129734, "text": "Then it applies k-means clustering with a pre-defined number of clusters.", "rank": 1120, "start": 129661, "IsComparative": "0", "id": "st_1120"}, {"end": 129847, "text": "PCA is computed on cluster centroids and applied to the entire data, which gives 2D coordinates of document data.", "rank": 1121, "start": 129734, "IsComparative": "1", "id": "st_1121"}, {"end": 130007, "text": "Based on these 2D coordinates, a galaxy view similar to a scatter plot is shown to users with a keyword summary for each cluster placed at the cluster centroid.", "rank": 1122, "start": 129847, "IsComparative": "1", "id": "st_1122"}, {"end": 130235, "text": "Owing to the simple algorithms adopted such as PCA and k-means, IN-SPIRE can deal with a fairly large amount of data, but it provides only a limited number of interaction capabilities to change the algorithms and their settings.", "rank": 1123, "start": 130007, "IsComparative": "1", "id": "st_1123"}]}, {"paragraph_info": {"end": 131066, "start": 130235, "text": "Jigsaw <121> is another well-known visual analytics system for document analysis.The main information that Jigsaw utilizes for visualization is named entities such as person name and location and their co-occurrences between documents.Automatic named-entity extraction is one of the key computational components in the analysis in Jigsaw.The named-entity extraction can be viewed as dimension reduction that reduces the number of keywords out of the entire vocabulary.Users can modify the list of named-entities by manually adding/removing them.Jigsaw also provides a cluster view by using the k-means algorithm and visualizes the resulting clusters as groups of documents as well as their keyword summary.Jigsaw also supports basic interactions with clustering such as changing the number of clusters and providing seed documents.", "rank": 384, "paragraph_comparative_number": 4, "entities": [], "id": "p_384"}, "sentences": [{"end": 130316, "text": "Jigsaw <121> is another well-known visual analytics system for document analysis.", "rank": 1124, "start": 130235, "IsComparative": "1", "id": "st_1124"}, {"end": 130470, "text": "The main information that Jigsaw utilizes for visualization is named entities such as person name and location and their co-occurrences between documents.", "rank": 1125, "start": 130316, "IsComparative": "1", "id": "st_1125"}, {"end": 130573, "text": "Automatic named-entity extraction is one of the key computational components in the analysis in Jigsaw.", "rank": 1126, "start": 130470, "IsComparative": "0", "id": "st_1126"}, {"end": 130703, "text": "The named-entity extraction can be viewed as dimension reduction that reduces the number of keywords out of the entire vocabulary.", "rank": 1127, "start": 130573, "IsComparative": "0", "id": "st_1127"}, {"end": 130780, "text": "Users can modify the list of named-entities by manually adding/removing them.", "rank": 1128, "start": 130703, "IsComparative": "0", "id": "st_1128"}, {"end": 130941, "text": "Jigsaw also provides a cluster view by using the k-means algorithm and visualizes the resulting clusters as groups of documents as well as their keyword summary.", "rank": 1129, "start": 130780, "IsComparative": "1", "id": "st_1129"}, {"end": 131066, "text": "Jigsaw also supports basic interactions with clustering such as changing the number of clusters and providing seed documents.", "rank": 1130, "start": 130941, "IsComparative": "1", "id": "st_1130"}]}, {"paragraph_info": {"end": 131758, "start": 131066, "text": "GGobi <43> is an interactive visualization system for high-dimensional data that are already encoded.It mainly uses a 2D scatter plot, where the two dimensions are generated by grand tour <10>.The difference of grand tour from other dimension reduction methods is that it provides an interaction to explore the high-dimensional space by continuously changing the basis vectors that data items are projected into.However, the grand tour method is applicable only when the data dimension is not significantly high, and thus its application is limited when dealing with hundreds or thousands of dimensions, which is often the case in many data types such as text documents, images, and bio data.", "rank": 385, "paragraph_comparative_number": 3, "entities": [], "id": "p_385"}, "sentences": [{"end": 131167, "text": "GGobi <43> is an interactive visualization system for high-dimensional data that are already encoded.", "rank": 1131, "start": 131066, "IsComparative": "1", "id": "st_1131"}, {"end": 131259, "text": "It mainly uses a 2D scatter plot, where the two dimensions are generated by grand tour <10>.", "rank": 1132, "start": 131167, "IsComparative": "1", "id": "st_1132"}, {"end": 131478, "text": "The difference of grand tour from other dimension reduction methods is that it provides an interaction to explore the high-dimensional space by continuously changing the basis vectors that data items are projected into.", "rank": 1133, "start": 131259, "IsComparative": "0", "id": "st_1133"}, {"end": 131758, "text": "However, the grand tour method is applicable only when the data dimension is not significantly high, and thus its application is limited when dealing with hundreds or thousands of dimensions, which is often the case in many data types such as text documents, images, and bio data.", "rank": 1134, "start": 131478, "IsComparative": "1", "id": "st_1134"}]}, {"paragraph_info": {"end": 132832, "start": 131758, "text": "Another system, iPCA <72>, which also takes high-dimensional data as an input, utilizes PCA as the main visualization technique.One of the main advantages of iPCA is that beyond 2D/3D scatter plots, it visualizes the reduced-dimensional data in a higher dimension than 2D or 3D via parallel coordinates.In general, dimension reduction from the original high-dimensional space to 2D/3D space introduces signif- icant information loss.In iPCA, it follows a useful idea to reduce the data dimension to an intermediate one that can be visualized without much clutter via parallel coor- dinates and then to interact with these intermediate dimensions to obtain particular scatter plots.Another aspect of iPCA is that it visualizes the PCA basis vectors in addition to the data items.In doing so, users can understand the role of the reduced dimensions in their visualizations, which leads to a better understanding about the data set as well.Even with these advantages, however, iPCA cannot handle very high-dimensional data since iPCA visualizes each of the original dimensions.", "rank": 386, "paragraph_comparative_number": 5, "entities": [], "id": "p_386"}, "sentences": [{"end": 131886, "text": "Another system, iPCA <72>, which also takes high-dimensional data as an input, utilizes PCA as the main visualization technique.", "rank": 1135, "start": 131758, "IsComparative": "1", "id": "st_1135"}, {"end": 132061, "text": "One of the main advantages of iPCA is that beyond 2D/3D scatter plots, it visualizes the reduced-dimensional data in a higher dimension than 2D or 3D via parallel coordinates.", "rank": 1136, "start": 131886, "IsComparative": "0", "id": "st_1136"}, {"end": 132191, "text": "In general, dimension reduction from the original high-dimensional space to 2D/3D space introduces signif- icant information loss.", "rank": 1137, "start": 132061, "IsComparative": "1", "id": "st_1137"}, {"end": 132439, "text": "In iPCA, it follows a useful idea to reduce the data dimension to an intermediate one that can be visualized without much clutter via parallel coor- dinates and then to interact with these intermediate dimensions to obtain particular scatter plots.", "rank": 1138, "start": 132191, "IsComparative": "1", "id": "st_1138"}, {"end": 132536, "text": "Another aspect of iPCA is that it visualizes the PCA basis vectors in addition to the data items.", "rank": 1139, "start": 132439, "IsComparative": "1", "id": "st_1139"}, {"end": 132695, "text": "In doing so, users can understand the role of the reduced dimensions in their visualizations, which leads to a better understanding about the data set as well.", "rank": 1140, "start": 132536, "IsComparative": "1", "id": "st_1140"}, {"end": 132832, "text": "Even with these advantages, however, iPCA cannot handle very high-dimensional data since iPCA visualizes each of the original dimensions.", "rank": 1141, "start": 132695, "IsComparative": "0", "id": "st_1141"}]}, {"paragraph_info": {"end": 133518, "start": 132832, "text": "Finally, WEKA <65> is mainly a library of various machine learning algorithms for high-dimensional data with several interaction capabilities.Various algorithms can be applied to data, and their performances can be evaluated based on various measures.In addition, WEKA provides simple types of visualizations such as histograms, scatter plots, etc.Although WEKA is similar to our Testbed system in that it provides flexible algorithm choices and settings, most of its visualizations and interactions are focused on the used methods rather than data exploration.For example, WEKA does not support any interactions from its visualizations such as filtering operations and raw data access.", "rank": 387, "paragraph_comparative_number": 4, "entities": [], "id": "p_387"}, "sentences": [{"end": 132974, "text": "Finally, WEKA <65> is mainly a library of various machine learning algorithms for high-dimensional data with several interaction capabilities.", "rank": 1142, "start": 132832, "IsComparative": "1", "id": "st_1142"}, {"end": 133083, "text": "Various algorithms can be applied to data, and their performances can be evaluated based on various measures.", "rank": 1143, "start": 132974, "IsComparative": "1", "id": "st_1143"}, {"end": 133180, "text": "In addition, WEKA provides simple types of visualizations such as histograms, scatter plots, etc.", "rank": 1144, "start": 133083, "IsComparative": "0", "id": "st_1144"}, {"end": 133393, "text": "Although WEKA is similar to our Testbed system in that it provides flexible algorithm choices and settings, most of its visualizations and interactions are focused on the used methods rather than data exploration.", "rank": 1145, "start": 133180, "IsComparative": "1", "id": "st_1145"}, {"end": 133518, "text": "For example, WEKA does not support any interactions from its visualizations such as filtering operations and raw data access.", "rank": 1146, "start": 133393, "IsComparative": "1", "id": "st_1146"}]}, {"paragraph_info": {"end": 134035, "start": 133518, "text": "As discussed above, most of the current visual analytics systems do not fully utilize a wide variety of computational methods.They adopt generic traditional methods for a broad applicability to various data sets and/or treat computational methods as a black box with little options to control them, which would hamper the interactive visual analysis.In this respect, the Testbed system provides the unique capability of bringing a variety of algorithms along with full control to practical visual analytics scenarios.", "rank": 388, "paragraph_comparative_number": 1, "entities": [], "id": "p_388"}, "sentences": [{"end": 133644, "text": "As discussed above, most of the current visual analytics systems do not fully utilize a wide variety of computational methods.", "rank": 1147, "start": 133518, "IsComparative": "0", "id": "st_1147"}, {"end": 133868, "text": "They adopt generic traditional methods for a broad applicability to various data sets and/or treat computational methods as a black box with little options to control them, which would hamper the interactive visual analysis.", "rank": 1148, "start": 133644, "IsComparative": "0", "id": "st_1148"}, {"end": 134035, "text": "In this respect, the Testbed system provides the unique capability of bringing a variety of algorithms along with full control to practical visual analytics scenarios.", "rank": 1149, "start": 133868, "IsComparative": "1", "id": "st_1149"}]}, {"paragraph_info": {"end": 134053, "start": 134035, "text": "5.3 Testbed System", "rank": 389, "paragraph_comparative_number": 0, "entities": [], "id": "p_389"}, "sentences": [{"end": 134053, "text": "5.3 Testbed System", "rank": 1150, "start": 134035, "IsComparative": "0", "id": "st_1150"}]}, {"paragraph_info": {"end": 134568, "start": 134053, "text": "In this Section, we describe the Testbed system1 in detail.First, in Section 5.3.1, we introduce the modules in the system and explain how the overall system works.Next, we describe the details of each module from both the computational and the interactive visualization points of view in Sections 5.3.2 and 5.3.3, respectively.Fi- nally, in Section 5.3.4, we discuss implementation details of the system and how the current system can be extended to adopt new data types and clustering/dimension reduction methods.", "rank": 390, "paragraph_comparative_number": 1, "entities": [], "id": "p_390"}, "sentences": [{"end": 134112, "text": "In this Section, we describe the Testbed system1 in detail.", "rank": 1151, "start": 134053, "IsComparative": "0", "id": "st_1151"}, {"end": 134217, "text": "First, in Section 5.3.1, we introduce the modules in the system and explain how the overall system works.", "rank": 1152, "start": 134112, "IsComparative": "0", "id": "st_1152"}, {"end": 134381, "text": "Next, we describe the details of each module from both the computational and the interactive visualization points of view in Sections 5.3.2 and 5.3.3, respectively.", "rank": 1153, "start": 134217, "IsComparative": "1", "id": "st_1153"}, {"end": 134568, "text": "Fi- nally, in Section 5.3.4, we discuss implementation details of the system and how the current system can be extended to adopt new data types and clustering/dimension reduction methods.", "rank": 1154, "start": 134381, "IsComparative": "0", "id": "st_1154"}]}, {"paragraph_info": {"end": 134588, "start": 134568, "text": "5.3.1 Basic Workflow", "rank": 391, "paragraph_comparative_number": 0, "entities": [], "id": "p_391"}, "sentences": [{"end": 134588, "text": "5.3.1 Basic Workflow", "rank": 1155, "start": 134568, "IsComparative": "0", "id": "st_1155"}]}, {"paragraph_info": {"end": 135063, "start": 134588, "text": "As shown in Fig.23, the Testbed system mainly has two parts: the computational and the interactive visualization parts.At the computational part, the Testbed system is composed of 1. vector encoding, 2. pre-processing, 3. clustering, and 4. dimension reduction.At the interactive visualization part, the Testbed provides the following interactive visualization modules: 1. parallel coordinates, 2. the scatter plot, 3. the cluster label view, and 4. the original data viewer.", "rank": 392, "paragraph_comparative_number": 2, "entities": [], "id": "p_392"}, "sentences": [{"end": 134604, "text": "As shown in Fig.", "rank": 1156, "start": 134588, "IsComparative": "0", "id": "st_1156"}, {"end": 134707, "text": "23, the Testbed system mainly has two parts: the computational and the interactive visualization parts.", "rank": 1157, "start": 134604, "IsComparative": "1", "id": "st_1157"}, {"end": 134849, "text": "At the computational part, the Testbed system is composed of 1. vector encoding, 2. pre-processing, 3. clustering, and 4. dimension reduction.", "rank": 1158, "start": 134707, "IsComparative": "0", "id": "st_1158"}, {"end": 135063, "text": "At the interactive visualization part, the Testbed provides the following interactive visualization modules: 1. parallel coordinates, 2. the scatter plot, 3. the cluster label view, and 4. the original data viewer.", "rank": 1159, "start": 134849, "IsComparative": "1", "id": "st_1159"}]}, {"paragraph_info": {"end": 135562, "start": 135063, "text": "The basic workflow of the Testbed system is as follows.Once a data set is loaded, data items are represented as high-dimensional vectors via a default encoding scheme.Then, users can interactively change the options for pre-processing, clustering, and dimension reduction methods.Each specification of these three components instan- tiates a particular visualization set composed of the parallel coordinates view, the scatter plot view, and the cluster label view.To generate these views, the output", "rank": 393, "paragraph_comparative_number": 1, "entities": [], "id": "p_393"}, "sentences": [{"end": 135118, "text": "The basic workflow of the Testbed system is as follows.", "rank": 1160, "start": 135063, "IsComparative": "0", "id": "st_1160"}, {"end": 135230, "text": "Once a data set is loaded, data items are represented as high-dimensional vectors via a default encoding scheme.", "rank": 1161, "start": 135118, "IsComparative": "0", "id": "st_1161"}, {"end": 135343, "text": "Then, users can interactively change the options for pre-processing, clustering, and dimension reduction methods.", "rank": 1162, "start": 135230, "IsComparative": "0", "id": "st_1162"}, {"end": 135527, "text": "Each specification of these three components instan- tiates a particular visualization set composed of the parallel coordinates view, the scatter plot view, and the cluster label view.", "rank": 1163, "start": 135343, "IsComparative": "1", "id": "st_1163"}, {"end": 135562, "text": "To generate these views, the output", "rank": 1164, "start": 135527, "IsComparative": "0", "id": "st_1164"}]}, {"paragraph_info": {"end": 136185, "start": 135562, "text": "1An introductory video can be downloaded at http://fodava.gatech.edu/files/ testbed-software/testbed.mp4, and the executable files with the used data sets are avail- able at http://fodava.gatech.edu/fodava-testbed-software.of dimension reduction, i.e., reduced-dimensional representation, acts as the coordi- nates of data items in the parallel coordinates view, and two user-selected dimensions of this view are visualized in the scatter plot view.In all three views, the output of clustering, i.e., grouping information of data items, is color-coded along with the cluster name/summary provided in the cluster label view.", "rank": 394, "paragraph_comparative_number": 1, "entities": [], "id": "p_394"}, "sentences": [{"end": 135785, "text": "1An introductory video can be downloaded at http://fodava.gatech.edu/files/ testbed-software/testbed.mp4, and the executable files with the used data sets are avail- able at http://fodava.gatech.edu/fodava-testbed-software.", "rank": 1165, "start": 135562, "IsComparative": "0", "id": "st_1165"}, {"end": 136011, "text": "of dimension reduction, i.e., reduced-dimensional representation, acts as the coordi- nates of data items in the parallel coordinates view, and two user-selected dimensions of this view are visualized in the scatter plot view.", "rank": 1166, "start": 135785, "IsComparative": "0", "id": "st_1166"}, {"end": 136185, "text": "In all three views, the output of clustering, i.e., grouping information of data items, is color-coded along with the cluster name/summary provided in the cluster label view.", "rank": 1167, "start": 136011, "IsComparative": "1", "id": "st_1167"}]}, {"paragraph_info": {"end": 136912, "start": 136185, "text": "The Testbed system can generate as many visualization sets as needed depending on different specifications of dimension reduction and clustering, and users can ex- plore a certain visualization set and compare between different visualization sets.To facilitate an easy comparison between different visualization results, the Testbed sys- tem offers the capability of aligning the different clustering and dimension reduction outputs.In addition, users can highlight and/or filter out certain clusters/data items and look into the details of the selected data items in the original data viewer.Users can also apply another set of clustering and/or dimension reduction to the selected data items to create new visualization sets.", "rank": 395, "paragraph_comparative_number": 3, "entities": [], "id": "p_395"}, "sentences": [{"end": 136432, "text": "The Testbed system can generate as many visualization sets as needed depending on different specifications of dimension reduction and clustering, and users can ex- plore a certain visualization set and compare between different visualization sets.", "rank": 1168, "start": 136185, "IsComparative": "0", "id": "st_1168"}, {"end": 136618, "text": "To facilitate an easy comparison between different visualization results, the Testbed sys- tem offers the capability of aligning the different clustering and dimension reduction outputs.", "rank": 1169, "start": 136432, "IsComparative": "1", "id": "st_1169"}, {"end": 136778, "text": "In addition, users can highlight and/or filter out certain clusters/data items and look into the details of the selected data items in the original data viewer.", "rank": 1170, "start": 136618, "IsComparative": "1", "id": "st_1170"}, {"end": 136912, "text": "Users can also apply another set of clustering and/or dimension reduction to the selected data items to create new visualization sets.", "rank": 1171, "start": 136778, "IsComparative": "1", "id": "st_1171"}]}, {"paragraph_info": {"end": 136939, "start": 136912, "text": "5.3.2 Computational Modules", "rank": 396, "paragraph_comparative_number": 0, "entities": [], "id": "p_396"}, "sentences": [{"end": 136939, "text": "5.3.2 Computational Modules", "rank": 1172, "start": 136912, "IsComparative": "0", "id": "st_1172"}]}, {"paragraph_info": {"end": 136962, "start": 136939, "text": "5.3.2.1 Vector Encoding", "rank": 397, "paragraph_comparative_number": 0, "entities": [], "id": "p_397"}, "sentences": [{"end": 136962, "text": "5.3.2.1 Vector Encoding", "rank": 1173, "start": 136939, "IsComparative": "0", "id": "st_1173"}]}, {"paragraph_info": {"end": 137558, "start": 136962, "text": "The Testbed system can take various types of data such as text documents, images, and pre-encoded vectors in a comma-separated-values (CSV) file format.For docu- ment and image data, the Testbed system provides built-in vector encoding modules.For instance, the Testbed system supports bag-of-words encoding for document data in a sparse matrix form with stop word removal and stemming.Image data are converted into vectors of rasterized gray-scale pixel values.The high-dimensional vectors obtained in this stage act as initial default vectors on which the following pre-processing is performed.", "rank": 398, "paragraph_comparative_number": 1, "entities": [], "id": "p_398"}, "sentences": [{"end": 137114, "text": "The Testbed system can take various types of data such as text documents, images, and pre-encoded vectors in a comma-separated-values (CSV) file format.", "rank": 1174, "start": 136962, "IsComparative": "0", "id": "st_1174"}, {"end": 137206, "text": "For docu- ment and image data, the Testbed system provides built-in vector encoding modules.", "rank": 1175, "start": 137114, "IsComparative": "0", "id": "st_1175"}, {"end": 137348, "text": "For instance, the Testbed system supports bag-of-words encoding for document data in a sparse matrix form with stop word removal and stemming.", "rank": 1176, "start": 137206, "IsComparative": "0", "id": "st_1176"}, {"end": 137424, "text": "Image data are converted into vectors of rasterized gray-scale pixel values.", "rank": 1177, "start": 137348, "IsComparative": "0", "id": "st_1177"}, {"end": 137558, "text": "The high-dimensional vectors obtained in this stage act as initial default vectors on which the following pre-processing is performed.", "rank": 1178, "start": 137424, "IsComparative": "1", "id": "st_1178"}]}, {"paragraph_info": {"end": 137580, "start": 137558, "text": "5.3.2.2 Pre-processing", "rank": 399, "paragraph_comparative_number": 0, "entities": [], "id": "p_399"}, "sentences": [{"end": 137580, "text": "5.3.2.2 Pre-processing", "rank": 1179, "start": 137558, "IsComparative": "0", "id": "st_1179"}]}, {"paragraph_info": {"end": 137927, "start": 137580, "text": "Once the default vectors are generated, the system shows pre-processing options de- pending on the data type (Fig.23A).The following options are provided in common for all data types: 1. normalization, which scales data vectors so that their norms equal to one, and 2. centering, which translates data vectors so that their empirical mean is zero.", "rank": 400, "paragraph_comparative_number": 2, "entities": [], "id": "p_400"}, "sentences": [{"end": 137694, "text": "Once the default vectors are generated, the system shows pre-processing options de- pending on the data type (Fig.", "rank": 1180, "start": 137580, "IsComparative": "0", "id": "st_1180"}, {"end": 137699, "text": "23A).", "rank": 1181, "start": 137694, "IsComparative": "1", "id": "st_1181"}, {"end": 137927, "text": "The following options are provided in common for all data types: 1. normalization, which scales data vectors so that their norms equal to one, and 2. centering, which translates data vectors so that their empirical mean is zero.", "rank": 1182, "start": 137699, "IsComparative": "1", "id": "st_1182"}]}, {"paragraph_info": {"end": 138350, "start": 137927, "text": "In addition, for text documents, we provide options of 1. removing the terms appearing in less than a user-specified number and 2. applying the term-frequency- inverse-document-frequency (TF-IDF) weighting scheme.For images, available are the following options: 1. reducing image sizes to a user-specified ratio to enhance the computational efficiency and 2. applying contrast limited adaptive histogram equalization <107>.", "rank": 401, "paragraph_comparative_number": 1, "entities": [], "id": "p_401"}, "sentences": [{"end": 138140, "text": "In addition, for text documents, we provide options of 1. removing the terms appearing in less than a user-specified number and 2. applying the term-frequency- inverse-document-frequency (TF-IDF) weighting scheme.", "rank": 1183, "start": 137927, "IsComparative": "0", "id": "st_1183"}, {"end": 138350, "text": "For images, available are the following options: 1. reducing image sizes to a user-specified ratio to enhance the computational efficiency and 2. applying contrast limited adaptive histogram equalization <107>.", "rank": 1184, "start": 138140, "IsComparative": "1", "id": "st_1184"}]}, {"paragraph_info": {"end": 138563, "start": 138350, "text": "The Testbed system maintains multiple instances of different pre-processed vector sets, and users can interactively generate and/or choose one of them and proceed to perform its clustering and dimension reduction.", "rank": 402, "paragraph_comparative_number": 1, "entities": [], "id": "p_402"}, "sentences": [{"end": 138563, "text": "The Testbed system maintains multiple instances of different pre-processed vector sets, and users can interactively generate and/or choose one of them and proceed to perform its clustering and dimension reduction.", "rank": 1185, "start": 138350, "IsComparative": "1", "id": "st_1185"}]}, {"paragraph_info": {"end": 138581, "start": 138563, "text": "5.3.2.3 Clustering", "rank": 403, "paragraph_comparative_number": 0, "entities": [], "id": "p_403"}, "sentences": [{"end": 138581, "text": "5.3.2.3 Clustering", "rank": 1186, "start": 138563, "IsComparative": "0", "id": "st_1186"}]}, {"paragraph_info": {"end": 139167, "start": 138581, "text": "Given the default or pre-processed set of high-dimensional vectors, the clustering module performs a user-selected clustering method with specified options (Fig.23B), which assigns each data item a cluster label.The Testbed system currently provides the following clustering methods: 1. k-means, 2. agglomerative hierarchical clustering <66>, 3.Gaussian mixture models, and 4.NMF.Once a specific method is selected in the system, user interfaces to specify the number of clusters as well as method-specific parameters are dynamically shown with their suggested default values (Fig.23B).", "rank": 404, "paragraph_comparative_number": 4, "entities": [], "id": "p_404"}, "sentences": [{"end": 138742, "text": "Given the default or pre-processed set of high-dimensional vectors, the clustering module performs a user-selected clustering method with specified options (Fig.", "rank": 1187, "start": 138581, "IsComparative": "0", "id": "st_1187"}, {"end": 138793, "text": "23B), which assigns each data item a cluster label.", "rank": 1188, "start": 138742, "IsComparative": "0", "id": "st_1188"}, {"end": 138926, "text": "The Testbed system currently provides the following clustering methods: 1. k-means, 2. agglomerative hierarchical clustering <66>, 3.", "rank": 1189, "start": 138793, "IsComparative": "1", "id": "st_1189"}, {"end": 138957, "text": "Gaussian mixture models, and 4.", "rank": 1190, "start": 138926, "IsComparative": "1", "id": "st_1190"}, {"end": 138961, "text": "NMF.", "rank": 1191, "start": 138957, "IsComparative": "0", "id": "st_1191"}, {"end": 139162, "text": "Once a specific method is selected in the system, user interfaces to specify the number of clusters as well as method-specific parameters are dynamically shown with their suggested default values (Fig.", "rank": 1192, "start": 138961, "IsComparative": "1", "id": "st_1192"}, {"end": 139167, "text": "23B).", "rank": 1193, "start": 139162, "IsComparative": "1", "id": "st_1193"}]}, {"paragraph_info": {"end": 139369, "start": 139167, "text": "Additionally, when a data set has pre-given labels, the clustering method list includes an additional item called Use original labels so that users can explore data with respect to the pre-given labels.", "rank": 405, "paragraph_comparative_number": 0, "entities": [], "id": "p_405"}, "sentences": [{"end": 139369, "text": "Additionally, when a data set has pre-given labels, the clustering method list includes an additional item called Use original labels so that users can explore data with respect to the pre-given labels.", "rank": 1194, "start": 139167, "IsComparative": "0", "id": "st_1194"}]}, {"paragraph_info": {"end": 139396, "start": 139369, "text": "5.3.2.4 Dimension Reduction", "rank": 406, "paragraph_comparative_number": 0, "entities": [], "id": "p_406"}, "sentences": [{"end": 139396, "text": "5.3.2.4 Dimension Reduction", "rank": 1195, "start": 139369, "IsComparative": "0", "id": "st_1195"}]}, {"paragraph_info": {"end": 139978, "start": 139396, "text": "Given the high-dimensional vector representations of data items, the dimension re- duction module reduces the data dimension from possibly hundreds of thousands to a visually manageable size, which makes it possible to visualize the data in forms of parallel coordinates and/or a scatter plot.The Testbed system provides both super- vised and unsupervised dimension reduction methods, as discussed in Section 5.2.1.In cases of supervised methods, the cluster label, which is an additional required input to run dimension reduction, is taken from the output of the clustering module.", "rank": 407, "paragraph_comparative_number": 1, "entities": [], "id": "p_407"}, "sentences": [{"end": 139689, "text": "Given the high-dimensional vector representations of data items, the dimension re- duction module reduces the data dimension from possibly hundreds of thousands to a visually manageable size, which makes it possible to visualize the data in forms of parallel coordinates and/or a scatter plot.", "rank": 1196, "start": 139396, "IsComparative": "1", "id": "st_1196"}, {"end": 139811, "text": "The Testbed system provides both super- vised and unsupervised dimension reduction methods, as discussed in Section 5.2.1.", "rank": 1197, "start": 139689, "IsComparative": "0", "id": "st_1197"}, {"end": 139978, "text": "In cases of supervised methods, the cluster label, which is an additional required input to run dimension reduction, is taken from the output of the clustering module.", "rank": 1198, "start": 139811, "IsComparative": "0", "id": "st_1198"}]}, {"paragraph_info": {"end": 140774, "start": 139978, "text": "The currently available dimension reduction methods in the system include su- pervised ones such as 1.LDA, 2.OCM, 3. centroid method (CM) <102>, 4. two-stage methods (TSTG) <35>, 5. discriminative neighborhood metric learning (DNML) <133>, and 6. kernel LDA <101>, and unsupervised ones such as 7.PCA, 8. metric and non- metric MDS, 9.Sammon mapping <113>, 10.ISOMAP, 11.LLE, 12. local tangent space alignment (LTSA) <148>, 13. maximum variance unfolding (MVU) <135>, 14.LE, 15. diffusion maps (DM) <42>, 16. t-SNE, and 17.Kernel PCA <115>.Similar to the clustering module, once a specific method is selected in the system, user interfaces to specify the number of reduced dimensions as well as method-specific parameters are dynamically shown along with their suggested default values (Fig.23C).", "rank": 408, "paragraph_comparative_number": 4, "entities": [], "id": "p_408"}, "sentences": [{"end": 140080, "text": "The currently available dimension reduction methods in the system include su- pervised ones such as 1.", "rank": 1199, "start": 139978, "IsComparative": "0", "id": "st_1199"}, {"end": 140087, "text": "LDA, 2.", "rank": 1200, "start": 140080, "IsComparative": "0", "id": "st_1200"}, {"end": 140275, "text": "OCM, 3. centroid method (CM) <102>, 4. two-stage methods (TSTG) <35>, 5. discriminative neighborhood metric learning (DNML) <133>, and 6. kernel LDA <101>, and unsupervised ones such as 7.", "rank": 1201, "start": 140087, "IsComparative": "1", "id": "st_1201"}, {"end": 140313, "text": "PCA, 8. metric and non- metric MDS, 9.", "rank": 1202, "start": 140275, "IsComparative": "0", "id": "st_1202"}, {"end": 140338, "text": "Sammon mapping <113>, 10.", "rank": 1203, "start": 140313, "IsComparative": "0", "id": "st_1203"}, {"end": 140349, "text": "ISOMAP, 11.", "rank": 1204, "start": 140338, "IsComparative": "0", "id": "st_1204"}, {"end": 140449, "text": "LLE, 12. local tangent space alignment (LTSA) <148>, 13. maximum variance unfolding (MVU) <135>, 14.", "rank": 1205, "start": 140349, "IsComparative": "0", "id": "st_1205"}, {"end": 140501, "text": "LE, 15. diffusion maps (DM) <42>, 16. t-SNE, and 17.", "rank": 1206, "start": 140449, "IsComparative": "1", "id": "st_1206"}, {"end": 140518, "text": "Kernel PCA <115>.", "rank": 1207, "start": 140501, "IsComparative": "0", "id": "st_1207"}, {"end": 140769, "text": "Similar to the clustering module, once a specific method is selected in the system, user interfaces to specify the number of reduced dimensions as well as method-specific parameters are dynamically shown along with their suggested default values (Fig.", "rank": 1208, "start": 140518, "IsComparative": "1", "id": "st_1208"}, {"end": 140774, "text": "23C).", "rank": 1209, "start": 140769, "IsComparative": "1", "id": "st_1209"}]}, {"paragraph_info": {"end": 140813, "start": 140774, "text": "5.3.3 Interactive Visualization Modules", "rank": 409, "paragraph_comparative_number": 1, "entities": [], "id": "p_409"}, "sentences": [{"end": 140813, "text": "5.3.3 Interactive Visualization Modules", "rank": 1210, "start": 140774, "IsComparative": "1", "id": "st_1210"}]}, {"paragraph_info": {"end": 140846, "start": 140813, "text": "5.3.3.1 Parallel Coordinates View", "rank": 410, "paragraph_comparative_number": 0, "entities": [], "id": "p_410"}, "sentences": [{"end": 140846, "text": "5.3.3.1 Parallel Coordinates View", "rank": 1211, "start": 140813, "IsComparative": "0", "id": "st_1211"}]}, {"paragraph_info": {"end": 141253, "start": 140846, "text": "Given an output from the computational part, i.e., lower-dimensional representations of data items and their cluster labels, the Testbed system takes a natural way to visualize the lower-dimensional data in parallel coordinates with a color coding based on the cluster labels (Fig.23E).In this view, the Testbed system supports zoom- in/out via mouse wheel scroll and data selection via mouse drag-and-drop.", "rank": 411, "paragraph_comparative_number": 1, "entities": [], "id": "p_411"}, "sentences": [{"end": 141127, "text": "Given an output from the computational part, i.e., lower-dimensional representations of data items and their cluster labels, the Testbed system takes a natural way to visualize the lower-dimensional data in parallel coordinates with a color coding based on the cluster labels (Fig.", "rank": 1212, "start": 140846, "IsComparative": "0", "id": "st_1212"}, {"end": 141132, "text": "23E).", "rank": 1213, "start": 141127, "IsComparative": "1", "id": "st_1213"}, {"end": 141253, "text": "In this view, the Testbed system supports zoom- in/out via mouse wheel scroll and data selection via mouse drag-and-drop.", "rank": 1214, "start": 141132, "IsComparative": "0", "id": "st_1214"}]}, {"paragraph_info": {"end": 141278, "start": 141253, "text": "5.3.3.2 Scatter Plot View", "rank": 412, "paragraph_comparative_number": 0, "entities": [], "id": "p_412"}, "sentences": [{"end": 141278, "text": "5.3.3.2 Scatter Plot View", "rank": 1215, "start": 141253, "IsComparative": "0", "id": "st_1215"}]}, {"paragraph_info": {"end": 141842, "start": 141278, "text": "Although parallel coordinates can fully visualize an output from the computational part, this view is often ineffective for humans to perceive the relationships between data items, and it does not scale well in terms of the number of data items and dimensions since each line representing a single data item occupies numerous pixels in a screen space.Due to these limitations, the Testbed system visualizes data in a 2D scatter plot (Fig.23F) by selecting two of the parallel coordinates dimensions with the same color encoding as in the parallel coordinates view.", "rank": 413, "paragraph_comparative_number": 0, "entities": [], "id": "p_413"}, "sentences": [{"end": 141629, "text": "Although parallel coordinates can fully visualize an output from the computational part, this view is often ineffective for humans to perceive the relationships between data items, and it does not scale well in terms of the number of data items and dimensions since each line representing a single data item occupies numerous pixels in a screen space.", "rank": 1216, "start": 141278, "IsComparative": "0", "id": "st_1216"}, {"end": 141716, "text": "Due to these limitations, the Testbed system visualizes data in a 2D scatter plot (Fig.", "rank": 1217, "start": 141629, "IsComparative": "0", "id": "st_1217"}, {"end": 141842, "text": "23F) by selecting two of the parallel coordinates dimensions with the same color encoding as in the parallel coordinates view.", "rank": 1218, "start": 141716, "IsComparative": "0", "id": "st_1218"}]}, {"paragraph_info": {"end": 142508, "start": 141842, "text": "In the scatter plot view, users can interactively change these dimensions corre- sponding to horizontal and vertical axes via combo boxes shown in the lower left part of the view.In addition, the Testbed system shows cluster centroids and ellipses, which summarize how the data within each class are distributed, and these features can be turned on/off via check boxes shown in the upper left part of the view.Similar to the parallel coordinates view, supported are zoom-in/out via mouse wheel scroll and data selection via mouse drag-and-drop.Once a subset of data is selected, users can apply another clustering/dimension reduction only on the selected data items.", "rank": 414, "paragraph_comparative_number": 0, "entities": [], "id": "p_414"}, "sentences": [{"end": 142021, "text": "In the scatter plot view, users can interactively change these dimensions corre- sponding to horizontal and vertical axes via combo boxes shown in the lower left part of the view.", "rank": 1219, "start": 141842, "IsComparative": "0", "id": "st_1219"}, {"end": 142252, "text": "In addition, the Testbed system shows cluster centroids and ellipses, which summarize how the data within each class are distributed, and these features can be turned on/off via check boxes shown in the upper left part of the view.", "rank": 1220, "start": 142021, "IsComparative": "0", "id": "st_1220"}, {"end": 142386, "text": "Similar to the parallel coordinates view, supported are zoom-in/out via mouse wheel scroll and data selection via mouse drag-and-drop.", "rank": 1221, "start": 142252, "IsComparative": "0", "id": "st_1221"}, {"end": 142508, "text": "Once a subset of data is selected, users can apply another clustering/dimension reduction only on the selected data items.", "rank": 1222, "start": 142386, "IsComparative": "0", "id": "st_1222"}]}, {"paragraph_info": {"end": 142534, "start": 142508, "text": "5.3.3.3 Cluster Label View", "rank": 415, "paragraph_comparative_number": 0, "entities": [], "id": "p_415"}, "sentences": [{"end": 142534, "text": "5.3.3.3 Cluster Label View", "rank": 1223, "start": 142508, "IsComparative": "0", "id": "st_1223"}]}, {"paragraph_info": {"end": 143062, "start": 142534, "text": "The cluster label view (Fig.23G) shows the cluster index, color, and summary in a simple list form.Currently, the cluster summary is provided only for the text documents type, which is the most frequent keywords in each cluster.Upon clicking a certain cluster index or summary, the corresponding data items are highlighted with thicker lines/points in the parallel coordinates and the scatter plot views.Unchecking the checkboxes, which are shown in the left side of the view, hides the corresponding data items in the two views", "rank": 416, "paragraph_comparative_number": 1, "entities": [], "id": "p_416"}, "sentences": [{"end": 142562, "text": "The cluster label view (Fig.", "rank": 1224, "start": 142534, "IsComparative": "0", "id": "st_1224"}, {"end": 142633, "text": "23G) shows the cluster index, color, and summary in a simple list form.", "rank": 1225, "start": 142562, "IsComparative": "0", "id": "st_1225"}, {"end": 142762, "text": "Currently, the cluster summary is provided only for the text documents type, which is the most frequent keywords in each cluster.", "rank": 1226, "start": 142633, "IsComparative": "0", "id": "st_1226"}, {"end": 142938, "text": "Upon clicking a certain cluster index or summary, the corresponding data items are highlighted with thicker lines/points in the parallel coordinates and the scatter plot views.", "rank": 1227, "start": 142762, "IsComparative": "0", "id": "st_1227"}, {"end": 143062, "text": "Unchecking the checkboxes, which are shown in the left side of the view, hides the corresponding data items in the two views", "rank": 1228, "start": 142938, "IsComparative": "1", "id": "st_1228"}]}, {"paragraph_info": {"end": 143093, "start": 143062, "text": "5.3.3.4 Accessing Original Data", "rank": 417, "paragraph_comparative_number": 0, "entities": [], "id": "p_417"}, "sentences": [{"end": 143093, "text": "5.3.3.4 Accessing Original Data", "rank": 1229, "start": 143062, "IsComparative": "0", "id": "st_1229"}]}, {"paragraph_info": {"end": 143433, "start": 143093, "text": "Both in the parallel coordinates and the scatter plot view, users can access the original form of data for user-selected data items/clusters in the original data viewer.Cur- rently, the Testbed system provides three different original data viewers depending on the data type, e.g., text documents, images, and pre-encoded vectors (Fig.23H).", "rank": 418, "paragraph_comparative_number": 2, "entities": [], "id": "p_418"}, "sentences": [{"end": 143262, "text": "Both in the parallel coordinates and the scatter plot view, users can access the original form of data for user-selected data items/clusters in the original data viewer.", "rank": 1230, "start": 143093, "IsComparative": "0", "id": "st_1230"}, {"end": 143428, "text": "Cur- rently, the Testbed system provides three different original data viewers depending on the data type, e.g., text documents, images, and pre-encoded vectors (Fig.", "rank": 1231, "start": 143262, "IsComparative": "1", "id": "st_1231"}, {"end": 143433, "text": "23H).", "rank": 1232, "start": 143428, "IsComparative": "1", "id": "st_1232"}]}, {"paragraph_info": {"end": 144134, "start": 143433, "text": "In all the original data viewers, selected data items are shown as a list with their cluster colors on the left, and users can multi-select items in the original data viewer to see the original data items.These selected items are then highlighted with dark yellow marks in the scatter plot view (Fig.23H).For text documents, adopting the idea in <88>, we color-code a user-specified number of representative keywords per each cluster with the corresponding cluster color, which helps in understanding why a document belongs to a certain cluster.For pre-encoded vectors, if these vectors are associated with another type of data, these data can also be accessed as shown in the third viewer in Fig.23H.", "rank": 419, "paragraph_comparative_number": 4, "entities": [], "id": "p_419"}, "sentences": [{"end": 143638, "text": "In all the original data viewers, selected data items are shown as a list with their cluster colors on the left, and users can multi-select items in the original data viewer to see the original data items.", "rank": 1233, "start": 143433, "IsComparative": "1", "id": "st_1233"}, {"end": 143733, "text": "These selected items are then highlighted with dark yellow marks in the scatter plot view (Fig.", "rank": 1234, "start": 143638, "IsComparative": "0", "id": "st_1234"}, {"end": 143738, "text": "23H).", "rank": 1235, "start": 143733, "IsComparative": "1", "id": "st_1235"}, {"end": 143978, "text": "For text documents, adopting the idea in <88>, we color-code a user-specified number of representative keywords per each cluster with the corresponding cluster color, which helps in understanding why a document belongs to a certain cluster.", "rank": 1236, "start": 143738, "IsComparative": "1", "id": "st_1236"}, {"end": 144130, "text": "For pre-encoded vectors, if these vectors are associated with another type of data, these data can also be accessed as shown in the third viewer in Fig.", "rank": 1237, "start": 143978, "IsComparative": "0", "id": "st_1237"}, {"end": 144134, "text": "23H.", "rank": 1238, "start": 144130, "IsComparative": "1", "id": "st_1238"}]}, {"paragraph_info": {"end": 144175, "start": 144134, "text": "5.3.3.5 Supporting Multi-view Exploration", "rank": 420, "paragraph_comparative_number": 0, "entities": [], "id": "p_420"}, "sentences": [{"end": 144175, "text": "5.3.3.5 Supporting Multi-view Exploration", "rank": 1239, "start": 144134, "IsComparative": "0", "id": "st_1239"}]}, {"paragraph_info": {"end": 144994, "start": 144175, "text": "Once the visualize button is clicked (Fig.23C) after specifying computational meth- ods, i.e., pre-processing, clustering, and dimension reduction, a new set of the parallel coordinates, the scatter plot, and the cluster label views are instantiated.Each of these three views is created as an individual tab in its corresponding location, and multiple views are maintained flexibly in the Testbed system, as shown in Fig.23(a).For example, any view can be popped out as an independent window and/or split horizontally/vertically in order to make it easy to compare between different sets of views due to different computational methods.When a certain view is activated by a mouse click, all the options of pre-processing, clustering, and dimension reduction used to generate the view are shown in the left in Fig.23(a).", "rank": 421, "paragraph_comparative_number": 4, "entities": [], "id": "p_421"}, "sentences": [{"end": 144217, "text": "Once the visualize button is clicked (Fig.", "rank": 1240, "start": 144175, "IsComparative": "0", "id": "st_1240"}, {"end": 144425, "text": "23C) after specifying computational meth- ods, i.e., pre-processing, clustering, and dimension reduction, a new set of the parallel coordinates, the scatter plot, and the cluster label views are instantiated.", "rank": 1241, "start": 144217, "IsComparative": "1", "id": "st_1241"}, {"end": 144596, "text": "Each of these three views is created as an individual tab in its corresponding location, and multiple views are maintained flexibly in the Testbed system, as shown in Fig.", "rank": 1242, "start": 144425, "IsComparative": "1", "id": "st_1242"}, {"end": 144602, "text": "23(a).", "rank": 1243, "start": 144596, "IsComparative": "1", "id": "st_1243"}, {"end": 144811, "text": "For example, any view can be popped out as an independent window and/or split horizontally/vertically in order to make it easy to compare between different sets of views due to different computational methods.", "rank": 1244, "start": 144602, "IsComparative": "0", "id": "st_1244"}, {"end": 144988, "text": "When a certain view is activated by a mouse click, all the options of pre-processing, clustering, and dimension reduction used to generate the view are shown in the left in Fig.", "rank": 1245, "start": 144811, "IsComparative": "0", "id": "st_1245"}, {"end": 144994, "text": "23(a).", "rank": 1246, "start": 144988, "IsComparative": "1", "id": "st_1246"}]}, {"paragraph_info": {"end": 145484, "start": 144994, "text": "Between different views with such a flexible layout, the Testbed system supports a brushing-and-linking capability.In the current Testbed system, if certain data items/custers are selected in one view, the corresponding data items in all the other views are highlighted as well.We use different colors for highlighting depending on whether the highlighted data items are due to the same view or a different view, which helps identifying the source view in which the data selection was made.", "rank": 422, "paragraph_comparative_number": 0, "entities": [], "id": "p_422"}, "sentences": [{"end": 145109, "text": "Between different views with such a flexible layout, the Testbed system supports a brushing-and-linking capability.", "rank": 1247, "start": 144994, "IsComparative": "0", "id": "st_1247"}, {"end": 145272, "text": "In the current Testbed system, if certain data items/custers are selected in one view, the corresponding data items in all the other views are highlighted as well.", "rank": 1248, "start": 145109, "IsComparative": "0", "id": "st_1248"}, {"end": 145484, "text": "We use different colors for highlighting depending on whether the highlighted data items are due to the same view or a different view, which helps identifying the source view in which the data selection was made.", "rank": 1249, "start": 145272, "IsComparative": "0", "id": "st_1249"}]}, {"paragraph_info": {"end": 145516, "start": 145484, "text": "5.3.3.6 Aligning Different Views", "rank": 423, "paragraph_comparative_number": 0, "entities": [], "id": "p_423"}, "sentences": [{"end": 145516, "text": "5.3.3.6 Aligning Different Views", "rank": 1250, "start": 145484, "IsComparative": "0", "id": "st_1250"}]}, {"paragraph_info": {"end": 145970, "start": 145516, "text": "In addition to the above-described multi-view management and brushing-and-linking capability, the Testbed system provides a more active means to facilitate easy com- parison between visualization sets composed of different clustering and dimension reduction results.To be specific, for a user-selected pair of visualization sets, users can align the clustering and/or dimension reduction outputs (Fig.23D), which are then reflected to visualization sets.", "rank": 424, "paragraph_comparative_number": 2, "entities": [], "id": "p_424"}, "sentences": [{"end": 145782, "text": "In addition to the above-described multi-view management and brushing-and-linking capability, the Testbed system provides a more active means to facilitate easy com- parison between visualization sets composed of different clustering and dimension reduction results.", "rank": 1251, "start": 145516, "IsComparative": "0", "id": "st_1251"}, {"end": 145917, "text": "To be specific, for a user-selected pair of visualization sets, users can align the clustering and/or dimension reduction outputs (Fig.", "rank": 1252, "start": 145782, "IsComparative": "1", "id": "st_1252"}, {"end": 145970, "text": "23D), which are then reflected to visualization sets.", "rank": 1253, "start": 145917, "IsComparative": "1", "id": "st_1253"}]}, {"paragraph_info": {"end": 146624, "start": 145970, "text": "To align the two different clustering results, the Testbed system performs the Hungarian algorithm <85>.Given two different cluster assignments of the same data items, the Hungarian algorithm finds the best pairwise matchings between their clus- ter indices so that the number of common data items within matching cluster pairs is maximized.Once the Hungarian algorithm finishes, the Testbed system changes the cluster indices and colors of the second visualization set according to the matching clusters of the first visualization set.As a result, users can maintain the consistent cluster indices/colors when comparing the two given visualization sets.", "rank": 425, "paragraph_comparative_number": 3, "entities": [], "id": "p_425"}, "sentences": [{"end": 146074, "text": "To align the two different clustering results, the Testbed system performs the Hungarian algorithm <85>.", "rank": 1254, "start": 145970, "IsComparative": "0", "id": "st_1254"}, {"end": 146311, "text": "Given two different cluster assignments of the same data items, the Hungarian algorithm finds the best pairwise matchings between their clus- ter indices so that the number of common data items within matching cluster pairs is maximized.", "rank": 1255, "start": 146074, "IsComparative": "1", "id": "st_1255"}, {"end": 146506, "text": "Once the Hungarian algorithm finishes, the Testbed system changes the cluster indices and colors of the second visualization set according to the matching clusters of the first visualization set.", "rank": 1256, "start": 146311, "IsComparative": "1", "id": "st_1256"}, {"end": 146624, "text": "As a result, users can maintain the consistent cluster indices/colors when comparing the two given visualization sets.", "rank": 1257, "start": 146506, "IsComparative": "1", "id": "st_1257"}]}, {"paragraph_info": {"end": 147496, "start": 146624, "text": "On the other hand, the Testbed system handles the alignment of dimension re- duction results via Procrustes analysis <69, 53>.Although there exist many advanced methods to align the two sets of vectors <30>, we chose Procrustes analysis due to its computational efficiency.Procrustes analysis transforms the second set of vectors via a rigid transformation, which allows only translation, rotation, and reflection, so that their Euclidean distances to the corresponding data vectors in the first set are mini- mized.Currently, instead of aligning the entire dimensions, the Testbed aligns only the two dimensions selected in the scatter plot view so that the alignment between the two scatter plot views are maximized.These alignment functionalities help users understand how differently the corresponding data items/clusters are placed between the two scatter plot views.", "rank": 426, "paragraph_comparative_number": 4, "entities": [], "id": "p_426"}, "sentences": [{"end": 146750, "text": "On the other hand, the Testbed system handles the alignment of dimension re- duction results via Procrustes analysis <69, 53>.", "rank": 1258, "start": 146624, "IsComparative": "1", "id": "st_1258"}, {"end": 146897, "text": "Although there exist many advanced methods to align the two sets of vectors <30>, we chose Procrustes analysis due to its computational efficiency.", "rank": 1259, "start": 146750, "IsComparative": "1", "id": "st_1259"}, {"end": 147140, "text": "Procrustes analysis transforms the second set of vectors via a rigid transformation, which allows only translation, rotation, and reflection, so that their Euclidean distances to the corresponding data vectors in the first set are mini- mized.", "rank": 1260, "start": 146897, "IsComparative": "1", "id": "st_1260"}, {"end": 147342, "text": "Currently, instead of aligning the entire dimensions, the Testbed aligns only the two dimensions selected in the scatter plot view so that the alignment between the two scatter plot views are maximized.", "rank": 1261, "start": 147140, "IsComparative": "1", "id": "st_1261"}, {"end": 147496, "text": "These alignment functionalities help users understand how differently the corresponding data items/clusters are placed between the two scatter plot views.", "rank": 1262, "start": 147342, "IsComparative": "0", "id": "st_1262"}]}, {"paragraph_info": {"end": 147534, "start": 147496, "text": "5.3.4 Implementation and Extensibility", "rank": 427, "paragraph_comparative_number": 0, "entities": [], "id": "p_427"}, "sentences": [{"end": 147534, "text": "5.3.4 Implementation and Extensibility", "rank": 1263, "start": 147496, "IsComparative": "0", "id": "st_1263"}]}, {"paragraph_info": {"end": 147741, "start": 147534, "text": "The current Testbed system is mainly implemented in JAVA to achieve various GUI and interaction capabilities.In order to support flexible window management, Net- Beans Rich Client Platform and IDE2 are used.", "rank": 428, "paragraph_comparative_number": 2, "entities": [], "id": "p_428"}, "sentences": [{"end": 147643, "text": "The current Testbed system is mainly implemented in JAVA to achieve various GUI and interaction capabilities.", "rank": 1264, "start": 147534, "IsComparative": "1", "id": "st_1264"}, {"end": 147741, "text": "In order to support flexible window management, Net- Beans Rich Client Platform and IDE2 are used.", "rank": 1265, "start": 147643, "IsComparative": "1", "id": "st_1265"}]}, {"paragraph_info": {"end": 148299, "start": 147741, "text": "Most of the internal computational methods are, however, written in MATLAB.There are several reasons of using MATLAB codes instead of porting them to JAVA.First of all, in many cases, the source codes of advanced computational methods are readily available in MATLAB due to its simplicity for matrix computations.In this respect, it would be burdensome to re-implement each of these methods in a different programming language in order to make them visual and interactive, and it will eventually become difficult to keep up with the pace of new technologies.", "rank": 429, "paragraph_comparative_number": 3, "entities": [], "id": "p_429"}, "sentences": [{"end": 147816, "text": "Most of the internal computational methods are, however, written in MATLAB.", "rank": 1266, "start": 147741, "IsComparative": "0", "id": "st_1266"}, {"end": 147896, "text": "There are several reasons of using MATLAB codes instead of porting them to JAVA.", "rank": 1267, "start": 147816, "IsComparative": "1", "id": "st_1267"}, {"end": 148054, "text": "First of all, in many cases, the source codes of advanced computational methods are readily available in MATLAB due to its simplicity for matrix computations.", "rank": 1268, "start": 147896, "IsComparative": "1", "id": "st_1268"}, {"end": 148299, "text": "In this respect, it would be burdensome to re-implement each of these methods in a different programming language in order to make them visual and interactive, and it will eventually become difficult to keep up with the pace of new technologies.", "rank": 1269, "start": 148054, "IsComparative": "1", "id": "st_1269"}]}, {"paragraph_info": {"end": 148952, "start": 148299, "text": "Furthermore, MATLAB provides highly optimized matrix computations.For in- stance, MATLAB, by default, auto-identifies the parallelizable subroutines in the code and runs full CPU cores even in a single PC.There also exist many efficient ma- ture core computational methods.For example, the k-means function in MATLAB provides various options for a distance metric to be used (Euclidean, city block, co- sine, and correlation), and a seed initialization (random, uniform, pilot-clustered, and user-selected seeds).Due to these reasons, the current Testbed system interface with the computational methods via a custom JAVA library file created by MATLAB.3", "rank": 430, "paragraph_comparative_number": 1, "entities": [], "id": "p_430"}, "sentences": [{"end": 148365, "text": "Furthermore, MATLAB provides highly optimized matrix computations.", "rank": 1270, "start": 148299, "IsComparative": "0", "id": "st_1270"}, {"end": 148504, "text": "For in- stance, MATLAB, by default, auto-identifies the parallelizable subroutines in the code and runs full CPU cores even in a single PC.", "rank": 1271, "start": 148365, "IsComparative": "1", "id": "st_1271"}, {"end": 148572, "text": "There also exist many efficient ma- ture core computational methods.", "rank": 1272, "start": 148504, "IsComparative": "0", "id": "st_1272"}, {"end": 148812, "text": "For example, the k-means function in MATLAB provides various options for a distance metric to be used (Euclidean, city block, co- sine, and correlation), and a seed initialization (random, uniform, pilot-clustered, and user-selected seeds).", "rank": 1273, "start": 148572, "IsComparative": "0", "id": "st_1273"}, {"end": 148952, "text": "Due to these reasons, the current Testbed system interface with the computational methods via a custom JAVA library file created by MATLAB.3", "rank": 1274, "start": 148812, "IsComparative": "0", "id": "st_1274"}]}, {"paragraph_info": {"end": 149049, "start": 148952, "text": "2http://netbeans.org/features/platform/index.html 3http://www.mathworks.com/products/javabuilder/", "rank": 431, "paragraph_comparative_number": 0, "entities": [], "id": "p_431"}, "sentences": [{"end": 149049, "text": "2http://netbeans.org/features/platform/index.html 3http://www.mathworks.com/products/javabuilder/", "rank": 1275, "start": 148952, "IsComparative": "0", "id": "st_1275"}]}, {"paragraph_info": {"end": 149651, "start": 149049, "text": "In terms of the extensibility of the Testbed system, we designed it in a completely modular way so that it can easily accept new data types and clustering/dimension reduction methods.For instance, if one wants to use the Testbed system for a speech data type, one needs to implement only the encoding module, the possible pre-processing options specific to the speech data type, and the original data viewer that can play audio data.Otherwise, by performing vector encoding separately and putting the encoded vectors as an input to the system, one can easily utilize the full capability of the Testbed.", "rank": 432, "paragraph_comparative_number": 3, "entities": [], "id": "p_432"}, "sentences": [{"end": 149232, "text": "In terms of the extensibility of the Testbed system, we designed it in a completely modular way so that it can easily accept new data types and clustering/dimension reduction methods.", "rank": 1276, "start": 149049, "IsComparative": "1", "id": "st_1276"}, {"end": 149482, "text": "For instance, if one wants to use the Testbed system for a speech data type, one needs to implement only the encoding module, the possible pre-processing options specific to the speech data type, and the original data viewer that can play audio data.", "rank": 1277, "start": 149232, "IsComparative": "1", "id": "st_1277"}, {"end": 149651, "text": "Otherwise, by performing vector encoding separately and putting the encoded vectors as an input to the system, one can easily utilize the full capability of the Testbed.", "rank": 1278, "start": 149482, "IsComparative": "1", "id": "st_1278"}]}, {"paragraph_info": {"end": 150304, "start": 149651, "text": "Adding new dimension reduction/clustering methods is also a simple process.Cur- rently, the implementation of each computational method is composed of two source code files.One file performs the computation by taking an input and generating an output as a primitive two-dimensional double array type, and the other is for user interfaces to change the method-specific options.Therefore, whether the implemen- tation of a new method is written in MATLAB or JAVA, as long as it deals with two-dimensional double array type as an input and an output, it can be easily inte- grated into the current Testbed system without having to modify the entire system.", "rank": 433, "paragraph_comparative_number": 1, "entities": [], "id": "p_433"}, "sentences": [{"end": 149726, "text": "Adding new dimension reduction/clustering methods is also a simple process.", "rank": 1279, "start": 149651, "IsComparative": "0", "id": "st_1279"}, {"end": 149824, "text": "Cur- rently, the implementation of each computational method is composed of two source code files.", "rank": 1280, "start": 149726, "IsComparative": "0", "id": "st_1280"}, {"end": 150027, "text": "One file performs the computation by taking an input and generating an output as a primitive two-dimensional double array type, and the other is for user interfaces to change the method-specific options.", "rank": 1281, "start": 149824, "IsComparative": "1", "id": "st_1281"}, {"end": 150304, "text": "Therefore, whether the implemen- tation of a new method is written in MATLAB or JAVA, as long as it deals with two-dimensional double array type as an input and an output, it can be easily inte- grated into the current Testbed system without having to modify the entire system.", "rank": 1282, "start": 150027, "IsComparative": "0", "id": "st_1282"}]}, {"paragraph_info": {"end": 150323, "start": 150304, "text": "5.4 Usage Scenarios", "rank": 434, "paragraph_comparative_number": 0, "entities": [], "id": "p_434"}, "sentences": [{"end": 150323, "text": "5.4 Usage Scenarios", "rank": 1283, "start": 150304, "IsComparative": "0", "id": "st_1283"}]}, {"paragraph_info": {"end": 150338, "start": 150323, "text": "5.4.1 Data Sets", "rank": 435, "paragraph_comparative_number": 1, "entities": [], "id": "p_435"}, "sentences": [{"end": 150338, "text": "5.4.1 Data Sets", "rank": 1284, "start": 150323, "IsComparative": "1", "id": "st_1284"}]}, {"paragraph_info": {"end": 150549, "start": 150338, "text": "To show how the Testbed system can be utilized in various visual analytics scenarios, we use three different data sets: 1.Pendigits (pre-encoded vectors), 2.Weizmann (images), and 3.InfoVisVAST (text documents).", "rank": 436, "paragraph_comparative_number": 0, "entities": [], "id": "p_436"}, "sentences": [{"end": 150460, "text": "To show how the Testbed system can be utilized in various visual analytics scenarios, we use three different data sets: 1.", "rank": 1285, "start": 150338, "IsComparative": "0", "id": "st_1285"}, {"end": 150495, "text": "Pendigits (pre-encoded vectors), 2.", "rank": 1286, "start": 150460, "IsComparative": "0", "id": "st_1286"}, {"end": 150520, "text": "Weizmann (images), and 3.", "rank": 1287, "start": 150495, "IsComparative": "0", "id": "st_1287"}, {"end": 150549, "text": "InfoVisVAST (text documents).", "rank": 1288, "start": 150520, "IsComparative": "0", "id": "st_1288"}]}, {"paragraph_info": {"end": 150874, "start": 150549, "text": "The Pendigits data set4 is composed of 10,992 handwritten digit data items, each of which is a 16-dimensional vector representing pen trace coordinates <11>.The data set has 10 clusters in terms of which digit each data item corresponds to, i.e., 0, 1, ..., and 9.For our experiments, 50 data items have been chosen from each", "rank": 437, "paragraph_comparative_number": 4, "entities": [], "id": "p_437"}, "sentences": [{"end": 150706, "text": "The Pendigits data set4 is composed of 10,992 handwritten digit data items, each of which is a 16-dimensional vector representing pen trace coordinates <11>.", "rank": 1289, "start": 150549, "IsComparative": "1", "id": "st_1289"}, {"end": 150803, "text": "The data set has 10 clusters in terms of which digit each data item corresponds to, i.e., 0, 1, .", "rank": 1290, "start": 150706, "IsComparative": "1", "id": "st_1290"}, {"end": 150804, "text": ".", "rank": 1291, "start": 150803, "IsComparative": "0", "id": "st_1291"}, {"end": 150805, "text": ".", "rank": 1292, "start": 150804, "IsComparative": "0", "id": "st_1292"}, {"end": 150813, "text": ", and 9.", "rank": 1293, "start": 150805, "IsComparative": "1", "id": "st_1293"}, {"end": 150874, "text": "For our experiments, 50 data items have been chosen from each", "rank": 1294, "start": 150813, "IsComparative": "1", "id": "st_1294"}]}, {"paragraph_info": {"end": 151562, "start": 150874, "text": "4 http://archive.ics.uci.edu/ml/datasets/Pen-Based+Recognition+of+Handwritten+Digits cluster, resulting in 500 items in total.The Weizmann data set5 contains 28 persons facial images with various angles, illuminations, and facial expressions.Excluding unclear images, we have chosen 52 images from each of 15 persons, resulting in 780 data items of 15 clusters.The size of each facial image is 88128, resulting in a 11,264 dimensional vector.The InfoVisVAST data set6 is a document corpus of paper abstracts in IEEE Infovis (1995-2010) and VAST (2006-2010) conferences.It includes 515 documents encoded in 4,185 dimensions via a bag-of-words encoding after stemming and stop word removal.", "rank": 438, "paragraph_comparative_number": 4, "entities": [], "id": "p_438"}, "sentences": [{"end": 151000, "text": "4 http://archive.ics.uci.edu/ml/datasets/Pen-Based+Recognition+of+Handwritten+Digits cluster, resulting in 500 items in total.", "rank": 1295, "start": 150874, "IsComparative": "1", "id": "st_1295"}, {"end": 151116, "text": "The Weizmann data set5 contains 28 persons facial images with various angles, illuminations, and facial expressions.", "rank": 1296, "start": 151000, "IsComparative": "1", "id": "st_1296"}, {"end": 151235, "text": "Excluding unclear images, we have chosen 52 images from each of 15 persons, resulting in 780 data items of 15 clusters.", "rank": 1297, "start": 151116, "IsComparative": "1", "id": "st_1297"}, {"end": 151316, "text": "The size of each facial image is 88128, resulting in a 11,264 dimensional vector.", "rank": 1298, "start": 151235, "IsComparative": "0", "id": "st_1298"}, {"end": 151443, "text": "The InfoVisVAST data set6 is a document corpus of paper abstracts in IEEE Infovis (1995-2010) and VAST (2006-2010) conferences.", "rank": 1299, "start": 151316, "IsComparative": "0", "id": "st_1299"}, {"end": 151562, "text": "It includes 515 documents encoded in 4,185 dimensions via a bag-of-words encoding after stemming and stop word removal.", "rank": 1300, "start": 151443, "IsComparative": "1", "id": "st_1300"}]}, {"paragraph_info": {"end": 151627, "start": 151562, "text": "5.4.2 Parallel Coordinates: Guiding beyond Two Leading Dimensions", "rank": 439, "paragraph_comparative_number": 0, "entities": [], "id": "p_439"}, "sentences": [{"end": 151627, "text": "5.4.2 Parallel Coordinates: Guiding beyond Two Leading Dimensions", "rank": 1301, "start": 151562, "IsComparative": "0", "id": "st_1301"}]}, {"paragraph_info": {"end": 152071, "start": 151627, "text": "When using dimension reduction in high-dimensional data visualization, the leading two dimensions of a dimension reduction method have been usually used to gener- ate a single scatter plot while ignoring the other dimensions.Unlike these previous approaches, the Testbed system first visualizes the reduced-dimensional data in the parallel coordinates view, and then two of these dimensions are interactively selected for the scatter plot view.", "rank": 440, "paragraph_comparative_number": 1, "entities": [], "id": "p_440"}, "sentences": [{"end": 151852, "text": "When using dimension reduction in high-dimensional data visualization, the leading two dimensions of a dimension reduction method have been usually used to gener- ate a single scatter plot while ignoring the other dimensions.", "rank": 1302, "start": 151627, "IsComparative": "1", "id": "st_1302"}, {"end": 152071, "text": "Unlike these previous approaches, the Testbed system first visualizes the reduced-dimensional data in the parallel coordinates view, and then two of these dimensions are interactively selected for the scatter plot view.", "rank": 1303, "start": 151852, "IsComparative": "0", "id": "st_1303"}]}, {"paragraph_info": {"end": 152589, "start": 152071, "text": "Although it is difficult to visually analyze data relationships in the parallel coor- dinates view, it can guide users in various ways.First of all, as shown in Fig.23(a), TSTG, e.g., LDA in this case, tends to separate different clusters into different di- mensions.For instance, although the clusters 2 and 3 are not well separated in the scatter plot view with (1, 2)-dimensions selected, they seem to be separated from the other clusters in dimensions 7 and 4, respectively, based on the parallel coordinates view.", "rank": 441, "paragraph_comparative_number": 0, "entities": [], "id": "p_441"}, "sentences": [{"end": 152206, "text": "Although it is difficult to visually analyze data relationships in the parallel coor- dinates view, it can guide users in various ways.", "rank": 1304, "start": 152071, "IsComparative": "0", "id": "st_1304"}, {"end": 152236, "text": "First of all, as shown in Fig.", "rank": 1305, "start": 152206, "IsComparative": "0", "id": "st_1305"}, {"end": 152338, "text": "23(a), TSTG, e.g., LDA in this case, tends to separate different clusters into different di- mensions.", "rank": 1306, "start": 152236, "IsComparative": "0", "id": "st_1306"}, {"end": 152589, "text": "For instance, although the clusters 2 and 3 are not well separated in the scatter plot view with (1, 2)-dimensions selected, they seem to be separated from the other clusters in dimensions 7 and 4, respectively, based on the parallel coordinates view.", "rank": 1307, "start": 152338, "IsComparative": "0", "id": "st_1307"}]}, {"paragraph_info": {"end": 152832, "start": 152589, "text": "As another example, as shown in Fig.24, given the 10-dimensional results of PCA, the scatter plot view of (1, 2)-dimensions mixes up all the clusters together.However, hinted by the parallel coordinates view showing the peaks of the cluster 12", "rank": 442, "paragraph_comparative_number": 0, "entities": [], "id": "p_442"}, "sentences": [{"end": 152625, "text": "As another example, as shown in Fig.", "rank": 1308, "start": 152589, "IsComparative": "0", "id": "st_1308"}, {"end": 152748, "text": "24, given the 10-dimensional results of PCA, the scatter plot view of (1, 2)-dimensions mixes up all the clusters together.", "rank": 1309, "start": 152625, "IsComparative": "0", "id": "st_1309"}, {"end": 152832, "text": "However, hinted by the parallel coordinates view showing the peaks of the cluster 12", "rank": 1310, "start": 152748, "IsComparative": "0", "id": "st_1310"}]}, {"paragraph_info": {"end": 153314, "start": 152832, "text": "5http://www.wisdom.weizmann.ac.il/ vision/FaceBase 6 http://www.cc.gatech.edu/gvu/ii/jigsaw/datafiles.html and 14 at dimensions 3 and 4, respectively, the scatter plot view of these dimensions turns out to give a well-clustered view.Such an observation is surprising because PCA is an unsupervised method, which does not take into account label information.This indicates that the leading two dimensions may not give enough information for high-dimensional data in visual analytics.", "rank": 443, "paragraph_comparative_number": 1, "entities": [], "id": "p_443"}, "sentences": [{"end": 153065, "text": "5http://www.wisdom.weizmann.ac.il/ vision/FaceBase 6 http://www.cc.gatech.edu/gvu/ii/jigsaw/datafiles.html and 14 at dimensions 3 and 4, respectively, the scatter plot view of these dimensions turns out to give a well-clustered view.", "rank": 1311, "start": 152832, "IsComparative": "0", "id": "st_1311"}, {"end": 153189, "text": "Such an observation is surprising because PCA is an unsupervised method, which does not take into account label information.", "rank": 1312, "start": 153065, "IsComparative": "0", "id": "st_1312"}, {"end": 153314, "text": "This indicates that the leading two dimensions may not give enough information for high-dimensional data in visual analytics.", "rank": 1313, "start": 153189, "IsComparative": "1", "id": "st_1313"}]}, {"paragraph_info": {"end": 153384, "start": 153314, "text": "5.4.3 Effects of Alignment: Helping Comparisons between Visualizations", "rank": 444, "paragraph_comparative_number": 1, "entities": [], "id": "p_444"}, "sentences": [{"end": 153384, "text": "5.4.3 Effects of Alignment: Helping Comparisons between Visualizations", "rank": 1314, "start": 153314, "IsComparative": "1", "id": "st_1314"}]}, {"paragraph_info": {"end": 153671, "start": 153384, "text": "Trying various methods/settings on a given data set and comparing between different visualizations is in the heart of the Testbed system, and the alignment functionality of the system supports this process.Fig.25 shows the effects of the alignment for clustering and dimension reduction.", "rank": 445, "paragraph_comparative_number": 2, "entities": [], "id": "p_445"}, "sentences": [{"end": 153590, "text": "Trying various methods/settings on a given data set and comparing between different visualizations is in the heart of the Testbed system, and the alignment functionality of the system supports this process.", "rank": 1315, "start": 153384, "IsComparative": "1", "id": "st_1315"}, {"end": 153594, "text": "Fig.", "rank": 1316, "start": 153590, "IsComparative": "0", "id": "st_1316"}, {"end": 153671, "text": "25 shows the effects of the alignment for clustering and dimension reduction.", "rank": 1317, "start": 153594, "IsComparative": "1", "id": "st_1317"}]}, {"paragraph_info": {"end": 154531, "start": 153671, "text": "In Fig.25(a), which shows the former, the three scatter plot views have identical coordinates of data items.With the different assignment of cluster labels, it is difficult to compare the cluster membership between the first and the third plots since the clusters have no correspondences in terms of the cluster colors and indices.After aligning the clustering, however, two different clustering results become much easier to compare between the first and the second view.For instance, compared to the original cluster labels shown in the first view, the original cluster 8 is shown to be merged to the original cluster 3.The two subclusters of the original cluster 6, which are shown in the bottom left and the top right in the first figure, are now split into two clusters in k-means clustering, and the former is shown to be merged to the clusters 4 and 10.", "rank": 446, "paragraph_comparative_number": 2, "entities": [], "id": "p_446"}, "sentences": [{"end": 153678, "text": "In Fig.", "rank": 1318, "start": 153671, "IsComparative": "0", "id": "st_1318"}, {"end": 153779, "text": "25(a), which shows the former, the three scatter plot views have identical coordinates of data items.", "rank": 1319, "start": 153678, "IsComparative": "0", "id": "st_1319"}, {"end": 154002, "text": "With the different assignment of cluster labels, it is difficult to compare the cluster membership between the first and the third plots since the clusters have no correspondences in terms of the cluster colors and indices.", "rank": 1320, "start": 153779, "IsComparative": "0", "id": "st_1320"}, {"end": 154143, "text": "After aligning the clustering, however, two different clustering results become much easier to compare between the first and the second view.", "rank": 1321, "start": 154002, "IsComparative": "1", "id": "st_1321"}, {"end": 154293, "text": "For instance, compared to the original cluster labels shown in the first view, the original cluster 8 is shown to be merged to the original cluster 3.", "rank": 1322, "start": 154143, "IsComparative": "1", "id": "st_1322"}, {"end": 154531, "text": "The two subclusters of the original cluster 6, which are shown in the bottom left and the top right in the first figure, are now split into two clusters in k-means clustering, and the former is shown to be merged to the clusters 4 and 10.", "rank": 1323, "start": 154293, "IsComparative": "0", "id": "st_1323"}]}, {"paragraph_info": {"end": 155434, "start": 154531, "text": "On the other hand, Fig.25(b) shows the example of aligning dimension reduction.In this example, the cluster labels are unchanged for all data items in the three fig- ures, but two different dimension reduction methods, TSTG and ISOMAP, are used.When comparing between the first and the third figures, which show the different coordinates generated by these two methods, it is difficult to recognize the correspon- dences between data items/clusters.Between the first and the second figures, whose dimension reduction results are aligned, one can perceive the correspondences in a much easier way.For example, the cluster 4 is shown to be close to the cluster 8 in TSTG, which is not the case in ISOMAP.Any data items in the cluster 6 are not located close to the cluster 7 in ISOMAP, but some data items between the two clusters overlap in TSTG.Such analyses cannot be easily made without the alignment.", "rank": 447, "paragraph_comparative_number": 1, "entities": [], "id": "p_447"}, "sentences": [{"end": 154554, "text": "On the other hand, Fig.", "rank": 1324, "start": 154531, "IsComparative": "0", "id": "st_1324"}, {"end": 154610, "text": "25(b) shows the example of aligning dimension reduction.", "rank": 1325, "start": 154554, "IsComparative": "0", "id": "st_1325"}, {"end": 154776, "text": "In this example, the cluster labels are unchanged for all data items in the three fig- ures, but two different dimension reduction methods, TSTG and ISOMAP, are used.", "rank": 1326, "start": 154610, "IsComparative": "0", "id": "st_1326"}, {"end": 154980, "text": "When comparing between the first and the third figures, which show the different coordinates generated by these two methods, it is difficult to recognize the correspon- dences between data items/clusters.", "rank": 1327, "start": 154776, "IsComparative": "1", "id": "st_1327"}, {"end": 155127, "text": "Between the first and the second figures, whose dimension reduction results are aligned, one can perceive the correspondences in a much easier way.", "rank": 1328, "start": 154980, "IsComparative": "0", "id": "st_1328"}, {"end": 155233, "text": "For example, the cluster 4 is shown to be close to the cluster 8 in TSTG, which is not the case in ISOMAP.", "rank": 1329, "start": 155127, "IsComparative": "0", "id": "st_1329"}, {"end": 155376, "text": "Any data items in the cluster 6 are not located close to the cluster 7 in ISOMAP, but some data items between the two clusters overlap in TSTG.", "rank": 1330, "start": 155233, "IsComparative": "0", "id": "st_1330"}, {"end": 155434, "text": "Such analyses cannot be easily made without the alignment.", "rank": 1331, "start": 155376, "IsComparative": "0", "id": "st_1331"}]}, {"paragraph_info": {"end": 155493, "start": 155434, "text": "5.4.4 Dimension Reduction: Supporting Multiple Perspectives", "rank": 448, "paragraph_comparative_number": 0, "entities": [], "id": "p_448"}, "sentences": [{"end": 155493, "text": "5.4.4 Dimension Reduction: Supporting Multiple Perspectives", "rank": 1332, "start": 155434, "IsComparative": "0", "id": "st_1332"}]}, {"paragraph_info": {"end": 156509, "start": 155493, "text": "Different dimension reduction methods can reveal different aspects of data.To show an example, we now look into the first two figures in Fig.25(b) from the perspective of supervised vs.unsupervised methods.Given a certain assignment of cluster labels, a supervised method, TSTG, gives a clear overview in terms of cluster relationships since most of the clusters are shown relatively compact, as shown in the first figure.On the contrary, an unsupervised method, ISOMAP, may reveal different aspects of data.For example, the second figure indicates that the cluster 6 is composed of two distinct subclusters shown at the top left and the bottom right.However, when the data do not have a clear cluster structure, e.g., most of the text document corpora, unsupervised dimension reduction methods give the results similar to the second figure in Fig.24, which significantly reduces the utility of the scatter plot.In this case, supervised dimension reduction would be the only choice to start with in visual analytics.", "rank": 449, "paragraph_comparative_number": 4, "entities": [], "id": "p_449"}, "sentences": [{"end": 155568, "text": "Different dimension reduction methods can reveal different aspects of data.", "rank": 1333, "start": 155493, "IsComparative": "0", "id": "st_1333"}, {"end": 155634, "text": "To show an example, we now look into the first two figures in Fig.", "rank": 1334, "start": 155568, "IsComparative": "1", "id": "st_1334"}, {"end": 155678, "text": "25(b) from the perspective of supervised vs.", "rank": 1335, "start": 155634, "IsComparative": "1", "id": "st_1335"}, {"end": 155699, "text": "unsupervised methods.", "rank": 1336, "start": 155678, "IsComparative": "0", "id": "st_1336"}, {"end": 155915, "text": "Given a certain assignment of cluster labels, a supervised method, TSTG, gives a clear overview in terms of cluster relationships since most of the clusters are shown relatively compact, as shown in the first figure.", "rank": 1337, "start": 155699, "IsComparative": "0", "id": "st_1337"}, {"end": 156001, "text": "On the contrary, an unsupervised method, ISOMAP, may reveal different aspects of data.", "rank": 1338, "start": 155915, "IsComparative": "0", "id": "st_1338"}, {"end": 156144, "text": "For example, the second figure indicates that the cluster 6 is composed of two distinct subclusters shown at the top left and the bottom right.", "rank": 1339, "start": 156001, "IsComparative": "0", "id": "st_1339"}, {"end": 156341, "text": "However, when the data do not have a clear cluster structure, e.g., most of the text document corpora, unsupervised dimension reduction methods give the results similar to the second figure in Fig.", "rank": 1340, "start": 156144, "IsComparative": "1", "id": "st_1340"}, {"end": 156405, "text": "24, which significantly reduces the utility of the scatter plot.", "rank": 1341, "start": 156341, "IsComparative": "0", "id": "st_1341"}, {"end": 156509, "text": "In this case, supervised dimension reduction would be the only choice to start with in visual analytics.", "rank": 1342, "start": 156405, "IsComparative": "1", "id": "st_1342"}]}, {"paragraph_info": {"end": 157494, "start": 156509, "text": "Even with a single dimension reduction method with different parameter values, different aspects of data can be obtained.In Fig.26(a), one can see that the cluster 5 (the digit 4) of the Pendigits data set moves from the top left near the cluster 10 (the digit 9) towards the cluster 7 (the digit 6) as the ISOMAP parameter k increases.In general, ISOMAP with a smaller k value focuses more on preserving the local neighborhood relationships by making non-neighborhood distances longer.Based on the sample data of each digit shown in 26(b), it can be inferred that the digit 4 is represented much closer to the digit 9, which forms their neighborhood relationships with small k values, than to the digit 6.As the k value increases, the neighborhood relationship between the digits 4 and 6 starts to be formed, which is why they become closer at a bigger k value.In this way, varying the parameter values with the same method can further reveal different interesting insight about data.", "rank": 450, "paragraph_comparative_number": 3, "entities": [], "id": "p_450"}, "sentences": [{"end": 156630, "text": "Even with a single dimension reduction method with different parameter values, different aspects of data can be obtained.", "rank": 1343, "start": 156509, "IsComparative": "0", "id": "st_1343"}, {"end": 156637, "text": "In Fig.", "rank": 1344, "start": 156630, "IsComparative": "0", "id": "st_1344"}, {"end": 156845, "text": "26(a), one can see that the cluster 5 (the digit 4) of the Pendigits data set moves from the top left near the cluster 10 (the digit 9) towards the cluster 7 (the digit 6) as the ISOMAP parameter k increases.", "rank": 1345, "start": 156637, "IsComparative": "0", "id": "st_1345"}, {"end": 156995, "text": "In general, ISOMAP with a smaller k value focuses more on preserving the local neighborhood relationships by making non-neighborhood distances longer.", "rank": 1346, "start": 156845, "IsComparative": "1", "id": "st_1346"}, {"end": 157215, "text": "Based on the sample data of each digit shown in 26(b), it can be inferred that the digit 4 is represented much closer to the digit 9, which forms their neighborhood relationships with small k values, than to the digit 6.", "rank": 1347, "start": 156995, "IsComparative": "1", "id": "st_1347"}, {"end": 157371, "text": "As the k value increases, the neighborhood relationship between the digits 4 and 6 starts to be formed, which is why they become closer at a bigger k value.", "rank": 1348, "start": 157215, "IsComparative": "1", "id": "st_1348"}, {"end": 157494, "text": "In this way, varying the parameter values with the same method can further reveal different interesting insight about data.", "rank": 1349, "start": 157371, "IsComparative": "0", "id": "st_1349"}]}, {"paragraph_info": {"end": 157557, "start": 157494, "text": "5.4.5 Clustering: Combining Knowledge from Different Clustering", "rank": 451, "paragraph_comparative_number": 0, "entities": [], "id": "p_451"}, "sentences": [{"end": 157557, "text": "5.4.5 Clustering: Combining Knowledge from Different Clustering", "rank": 1350, "start": 157494, "IsComparative": "0", "id": "st_1350"}]}, {"paragraph_info": {"end": 158074, "start": 157557, "text": "Clustering is a challenging task, and any single clustering method tends not to give fully satisfactory results.The Testbed system can remedy this problem by enabling users to perform different clustering methods and obtain more meaningful clusters by comparing between them.Fig.27 shows the scatter plot views of TSTG with the cluster labels obtained by two different clustering methods, k-means and NMF.The InfoVisVAST data set are used, and the keyword summaries of clusters for each method are as follows: k-means", "rank": 452, "paragraph_comparative_number": 2, "entities": [], "id": "p_452"}, "sentences": [{"end": 157669, "text": "Clustering is a challenging task, and any single clustering method tends not to give fully satisfactory results.", "rank": 1351, "start": 157557, "IsComparative": "1", "id": "st_1351"}, {"end": 157832, "text": "The Testbed system can remedy this problem by enabling users to perform different clustering methods and obtain more meaningful clusters by comparing between them.", "rank": 1352, "start": 157669, "IsComparative": "1", "id": "st_1352"}, {"end": 157836, "text": "Fig.", "rank": 1353, "start": 157832, "IsComparative": "0", "id": "st_1353"}, {"end": 157962, "text": "27 shows the scatter plot views of TSTG with the cluster labels obtained by two different clustering methods, k-means and NMF.", "rank": 1354, "start": 157836, "IsComparative": "0", "id": "st_1354"}, {"end": 158074, "text": "The InfoVisVAST data set are used, and the keyword summaries of clusters for each method are as follows: k-means", "rank": 1355, "start": 157962, "IsComparative": "0", "id": "st_1355"}]}, {"paragraph_info": {"end": 158125, "start": 158074, "text": "1. graph, trees, node, layout, edge, draw, clusters", "rank": 453, "paragraph_comparative_number": 0, "entities": [], "id": "p_453"}, "sentences": [{"end": 158125, "text": "1. graph, trees, node, layout, edge, draw, clusters", "rank": 1356, "start": 158074, "IsComparative": "0", "id": "st_1356"}]}, {"paragraph_info": {"end": 158278, "start": 158125, "text": "2. querying, interface, multiple, databases, expressive, temporal, magnification 3. document, text, collections, words, sequential, searches, information", "rank": 454, "paragraph_comparative_number": 0, "entities": [], "id": "p_454"}, "sentences": [{"end": 158278, "text": "2. querying, interface, multiple, databases, expressive, temporal, magnification 3. document, text, collections, words, sequential, searches, information", "rank": 1357, "start": 158125, "IsComparative": "0", "id": "st_1357"}]}, {"paragraph_info": {"end": 158430, "start": 158278, "text": "4. multivariate, variable, data, aggregate, coordinates, multidimensional, flow 5.3d, spatial, labelling, animation, map, coloring, display, information", "rank": 455, "paragraph_comparative_number": 0, "entities": [], "id": "p_455"}, "sentences": [{"end": 158360, "text": "4. multivariate, variable, data, aggregate, coordinates, multidimensional, flow 5.", "rank": 1358, "start": 158278, "IsComparative": "0", "id": "st_1358"}, {"end": 158430, "text": "3d, spatial, labelling, animation, map, coloring, display, information", "rank": 1359, "start": 158360, "IsComparative": "0", "id": "st_1359"}]}, {"paragraph_info": {"end": 158589, "start": 158430, "text": "6. treemaps, hierarchy, hierarchical, layout, focuscontext, spacefilling, algorithms 7. clusters, dimensions, image, visualization, measures, number, reduction", "rank": 456, "paragraph_comparative_number": 1, "entities": [], "id": "p_456"}, "sentences": [{"end": 158589, "text": "6. treemaps, hierarchy, hierarchical, layout, focuscontext, spacefilling, algorithms 7. clusters, dimensions, image, visualization, measures, number, reduction", "rank": 1360, "start": 158430, "IsComparative": "1", "id": "st_1360"}]}, {"paragraph_info": {"end": 158659, "start": 158589, "text": "8. analytics, model, systems, video, decisions, information, framework", "rank": 457, "paragraph_comparative_number": 1, "entities": [], "id": "p_457"}, "sentences": [{"end": 158659, "text": "8. analytics, model, systems, video, decisions, information, framework", "rank": 1361, "start": 158589, "IsComparative": "1", "id": "st_1361"}]}, {"paragraph_info": {"end": 158723, "start": 158659, "text": "9. networks, traffic, arcs, diagram, social, internet, duplicate", "rank": 458, "paragraph_comparative_number": 1, "entities": [], "id": "p_458"}, "sentences": [{"end": 158723, "text": "9. networks, traffic, arcs, diagram, social, internet, duplicate", "rank": 1362, "start": 158659, "IsComparative": "1", "id": "st_1362"}]}, {"paragraph_info": {"end": 158806, "start": 158723, "text": "10. collaboration, designed, histories, wikipedia, information, supports, story NMF", "rank": 459, "paragraph_comparative_number": 0, "entities": [], "id": "p_459"}, "sentences": [{"end": 158806, "text": "10. collaboration, designed, histories, wikipedia, information, supports, story NMF", "rank": 1363, "start": 158723, "IsComparative": "0", "id": "st_1363"}]}, {"paragraph_info": {"end": 158874, "start": 158806, "text": "1. graph, clusters, algorithms, methods, data, state, structured 115", "rank": 460, "paragraph_comparative_number": 0, "entities": [], "id": "p_460"}, "sentences": [{"end": 158874, "text": "1. graph, clusters, algorithms, methods, data, state, structured 115", "rank": 1364, "start": 158806, "IsComparative": "0", "id": "st_1364"}]}, {"paragraph_info": {"end": 158943, "start": 158874, "text": "2. querying, interface, databases, searches, temporal, multiple, data", "rank": 461, "paragraph_comparative_number": 0, "entities": [], "id": "p_461"}, "sentences": [{"end": 158943, "text": "2. querying, interface, databases, searches, temporal, multiple, data", "rank": 1365, "start": 158874, "IsComparative": "0", "id": "st_1365"}]}, {"paragraph_info": {"end": 159012, "start": 158943, "text": "3. document, text, image, content, information, collections, searches", "rank": 462, "paragraph_comparative_number": 0, "entities": [], "id": "p_462"}, "sentences": [{"end": 159012, "text": "3. document, text, image, content, information, collections, searches", "rank": 1366, "start": 158943, "IsComparative": "0", "id": "st_1366"}]}, {"paragraph_info": {"end": 159183, "start": 159012, "text": "4. dimensions, parallelize, coordinates, multivariate, multidimensional, datasets, scatterplots 5.3d, spatial, landscapes, information, display, animation, spaces, encoded", "rank": 463, "paragraph_comparative_number": 1, "entities": [], "id": "p_463"}, "sentences": [{"end": 159110, "text": "4. dimensions, parallelize, coordinates, multivariate, multidimensional, datasets, scatterplots 5.", "rank": 1367, "start": 159012, "IsComparative": "1", "id": "st_1367"}, {"end": 159183, "text": "3d, spatial, landscapes, information, display, animation, spaces, encoded", "rank": 1368, "start": 159110, "IsComparative": "0", "id": "st_1368"}]}, {"paragraph_info": {"end": 159258, "start": 159183, "text": "6. treemaps, hierarchical, layout, ratio, algorithms, spacefilling, aspects", "rank": 464, "paragraph_comparative_number": 1, "entities": [], "id": "p_464"}, "sentences": [{"end": 159258, "text": "6. treemaps, hierarchical, layout, ratio, algorithms, spacefilling, aspects", "rank": 1369, "start": 159183, "IsComparative": "1", "id": "st_1369"}]}, {"paragraph_info": {"end": 159322, "start": 159258, "text": "7. trees, hierarchy, node, genealogical, decisions, draw, layout", "rank": 465, "paragraph_comparative_number": 1, "entities": [], "id": "p_465"}, "sentences": [{"end": 159322, "text": "7. trees, hierarchy, node, genealogical, decisions, draw, layout", "rank": 1370, "start": 159258, "IsComparative": "1", "id": "st_1370"}]}, {"paragraph_info": {"end": 159390, "start": 159322, "text": "8. designed, model, information, analytics, framework, systems, data", "rank": 466, "paragraph_comparative_number": 0, "entities": [], "id": "p_466"}, "sentences": [{"end": 159390, "text": "8. designed, model, information, analytics, framework, systems, data", "rank": 1371, "start": 159322, "IsComparative": "0", "id": "st_1371"}]}, {"paragraph_info": {"end": 159450, "start": 159390, "text": "9. networks, traffic, social, querying, analysis, data, flow", "rank": 467, "paragraph_comparative_number": 1, "entities": [], "id": "p_467"}, "sentences": [{"end": 159450, "text": "9. networks, traffic, social, querying, analysis, data, flow", "rank": 1372, "start": 159390, "IsComparative": "1", "id": "st_1372"}]}, {"paragraph_info": {"end": 159528, "start": 159450, "text": "10. collaboration, analytics, wikipedia, analysts, supports, knowledge, shared", "rank": 468, "paragraph_comparative_number": 0, "entities": [], "id": "p_468"}, "sentences": [{"end": 159528, "text": "10. collaboration, analytics, wikipedia, analysts, supports, knowledge, shared", "rank": 1373, "start": 159450, "IsComparative": "0", "id": "st_1373"}]}, {"paragraph_info": {"end": 160226, "start": 159528, "text": "Among these clusters, the cluster 1 of the k-means clustering has a clear meaning of graph-related visualization, e.g., graph drawing, graph layout, and graph clustering.This cluster is also shown to be clearly separated from the other clusters in the left figure.As we perform brushing-and-linking on this cluster, it turns out that this cluster mainly corresponds partially to the clusters 1, 6, and 7 of the NMF clustering.Considering that these clusters contain the keywords, graph and layout and their separations from the other clusters in the right figure are not as clear as that of the cluster 1 in the left figure, one can regard the cluster 1 of k-means as a cluster with better quality.", "rank": 469, "paragraph_comparative_number": 2, "entities": [], "id": "p_469"}, "sentences": [{"end": 159698, "text": "Among these clusters, the cluster 1 of the k-means clustering has a clear meaning of graph-related visualization, e.g., graph drawing, graph layout, and graph clustering.", "rank": 1374, "start": 159528, "IsComparative": "1", "id": "st_1374"}, {"end": 159792, "text": "This cluster is also shown to be clearly separated from the other clusters in the left figure.", "rank": 1375, "start": 159698, "IsComparative": "0", "id": "st_1375"}, {"end": 159954, "text": "As we perform brushing-and-linking on this cluster, it turns out that this cluster mainly corresponds partially to the clusters 1, 6, and 7 of the NMF clustering.", "rank": 1376, "start": 159792, "IsComparative": "0", "id": "st_1376"}, {"end": 160226, "text": "Considering that these clusters contain the keywords, graph and layout and their separations from the other clusters in the right figure are not as clear as that of the cluster 1 in the left figure, one can regard the cluster 1 of k-means as a cluster with better quality.", "rank": 1377, "start": 159954, "IsComparative": "1", "id": "st_1377"}]}, {"paragraph_info": {"end": 160829, "start": 160226, "text": "On the other hand, in the NMF clustering, the cluster 4 seems to be clearly re- lated to multi-variate/multi-dimensional data visualization.By brushing-and-linking on this cluster, we found it corresponds mostly to the clusters 4 and 7 of the k- means clustering, which makes sense based on their keyword summaries although they are relatively more ambiguous than the cluster 4 of the NMF clustering.This observation is also supported by their cluster separations in Fig.27 , which indicates a clearer separation of the cluster 4 in the right figure than that of the clusters 4 and 7 in the left figure.", "rank": 470, "paragraph_comparative_number": 2, "entities": [], "id": "p_470"}, "sentences": [{"end": 160366, "text": "On the other hand, in the NMF clustering, the cluster 4 seems to be clearly re- lated to multi-variate/multi-dimensional data visualization.", "rank": 1378, "start": 160226, "IsComparative": "0", "id": "st_1378"}, {"end": 160626, "text": "By brushing-and-linking on this cluster, we found it corresponds mostly to the clusters 4 and 7 of the k- means clustering, which makes sense based on their keyword summaries although they are relatively more ambiguous than the cluster 4 of the NMF clustering.", "rank": 1379, "start": 160366, "IsComparative": "1", "id": "st_1379"}, {"end": 160697, "text": "This observation is also supported by their cluster separations in Fig.", "rank": 1380, "start": 160626, "IsComparative": "0", "id": "st_1380"}, {"end": 160829, "text": "27 , which indicates a clearer separation of the cluster 4 in the right figure than that of the clusters 4 and 7 in the left figure.", "rank": 1381, "start": 160697, "IsComparative": "1", "id": "st_1381"}]}, {"paragraph_info": {"end": 160978, "start": 160829, "text": "As shown in these cases, one can apply different clustering methods and take full advantage of them by visually analyzing them in the Testbed system.", "rank": 471, "paragraph_comparative_number": 1, "entities": [], "id": "p_471"}, "sentences": [{"end": 160978, "text": "As shown in these cases, one can apply different clustering methods and take full advantage of them by visually analyzing them in the Testbed system.", "rank": 1382, "start": 160829, "IsComparative": "1", "id": "st_1382"}]}, {"paragraph_info": {"end": 160993, "start": 160978, "text": "5.5 Conclusions", "rank": 472, "paragraph_comparative_number": 0, "entities": [], "id": "p_472"}, "sentences": [{"end": 160993, "text": "5.5 Conclusions", "rank": 1383, "start": 160978, "IsComparative": "0", "id": "st_1383"}]}, {"paragraph_info": {"end": 161490, "start": 160993, "text": "In this chapter, we have presented the visual testbed system for dimension reduction and clustering in high-dimensional data visual analytics.The main contribution of our system is to bring a wide variety of traditional and state-of-the-art dimension reduction and clustering methods to visual analytics.The Testbed system provides full control of these methods with interactive visual access to their results.In addition, our system offers a flexible extensibility for new data types and methods.", "rank": 473, "paragraph_comparative_number": 3, "entities": [], "id": "p_473"}, "sentences": [{"end": 161135, "text": "In this chapter, we have presented the visual testbed system for dimension reduction and clustering in high-dimensional data visual analytics.", "rank": 1384, "start": 160993, "IsComparative": "0", "id": "st_1384"}, {"end": 161297, "text": "The main contribution of our system is to bring a wide variety of traditional and state-of-the-art dimension reduction and clustering methods to visual analytics.", "rank": 1385, "start": 161135, "IsComparative": "1", "id": "st_1385"}, {"end": 161403, "text": "The Testbed system provides full control of these methods with interactive visual access to their results.", "rank": 1386, "start": 161297, "IsComparative": "1", "id": "st_1386"}, {"end": 161490, "text": "In addition, our system offers a flexible extensibility for new data types and methods.", "rank": 1387, "start": 161403, "IsComparative": "1", "id": "st_1387"}]}, {"paragraph_info": {"end": 161965, "start": 161490, "text": "As future work, we plan to tackle a scalability issue.As the size of data gets bigger, their computational time takes even longer, which hinders real-time interactive visualizations.Another scalability problem is due to the limited amount of screen space.Even if the computational methods maintain efficiency, a large number of data items cause a clutter in visualization.These issues will be handled using various approaches, e.g., sampling, online learning algorithms, etc.", "rank": 474, "paragraph_comparative_number": 2, "entities": [], "id": "p_474"}, "sentences": [{"end": 161544, "text": "As future work, we plan to tackle a scalability issue.", "rank": 1388, "start": 161490, "IsComparative": "0", "id": "st_1388"}, {"end": 161672, "text": "As the size of data gets bigger, their computational time takes even longer, which hinders real-time interactive visualizations.", "rank": 1389, "start": 161544, "IsComparative": "1", "id": "st_1389"}, {"end": 161745, "text": "Another scalability problem is due to the limited amount of screen space.", "rank": 1390, "start": 161672, "IsComparative": "1", "id": "st_1390"}, {"end": 161862, "text": "Even if the computational methods maintain efficiency, a large number of data items cause a clutter in visualization.", "rank": 1391, "start": 161745, "IsComparative": "0", "id": "st_1391"}, {"end": 161965, "text": "These issues will be handled using various approaches, e.g., sampling, online learning algorithms, etc.", "rank": 1392, "start": 161862, "IsComparative": "0", "id": "st_1392"}]}, {"paragraph_info": {"end": 162514, "start": 161965, "text": "In addition, we plan to enhance the alignment capability by incorporating other advanced algorithms and user interfaces.To be specific, the currently used algorithms do not change anything in the reference view, and the Procrustes analysis does not change internal relationships within each visualization at all.This may limit the performance of alignment for easy comparison between visualizations when they are significantly different.To deal with this problem, we plan to utilize other advanced methods such as graph-embedding-based methods <30>.", "rank": 475, "paragraph_comparative_number": 1, "entities": [], "id": "p_475"}, "sentences": [{"end": 162085, "text": "In addition, we plan to enhance the alignment capability by incorporating other advanced algorithms and user interfaces.", "rank": 1393, "start": 161965, "IsComparative": "0", "id": "st_1393"}, {"end": 162277, "text": "To be specific, the currently used algorithms do not change anything in the reference view, and the Procrustes analysis does not change internal relationships within each visualization at all.", "rank": 1394, "start": 162085, "IsComparative": "0", "id": "st_1394"}, {"end": 162402, "text": "This may limit the performance of alignment for easy comparison between visualizations when they are significantly different.", "rank": 1395, "start": 162277, "IsComparative": "0", "id": "st_1395"}, {"end": 162514, "text": "To deal with this problem, we plan to utilize other advanced methods such as graph-embedding-based methods <30>.", "rank": 1396, "start": 162402, "IsComparative": "1", "id": "st_1396"}]}, {"paragraph_info": {"end": 162538, "start": 162514, "text": "(a) The system overview.", "rank": 476, "paragraph_comparative_number": 0, "entities": [], "id": "p_476"}, "sentences": [{"end": 162538, "text": "(a) The system overview.", "rank": 1397, "start": 162514, "IsComparative": "0", "id": "st_1397"}]}, {"paragraph_info": {"end": 162799, "start": 162538, "text": "(b) The general workflow.Hexagonal blocks correspond to operations/interactions, and rectangular ones to operation inputs/outputs or visualization modules.Stacked rectangles indicate their multiple instantiations, which are dynamically maintained by the system.", "rank": 477, "paragraph_comparative_number": 2, "entities": [], "id": "p_477"}, "sentences": [{"end": 162563, "text": "(b) The general workflow.", "rank": 1398, "start": 162538, "IsComparative": "0", "id": "st_1398"}, {"end": 162693, "text": "Hexagonal blocks correspond to operations/interactions, and rectangular ones to operation inputs/outputs or visualization modules.", "rank": 1399, "start": 162563, "IsComparative": "1", "id": "st_1399"}, {"end": 162799, "text": "Stacked rectangles indicate their multiple instantiations, which are dynamically maintained by the system.", "rank": 1400, "start": 162693, "IsComparative": "1", "id": "st_1400"}]}, {"paragraph_info": {"end": 163211, "start": 162799, "text": "Figure 23: The overview and the workflow of the system.User interfaces for pre- processing (A), clustering (B), dimension reduction (C), and alignment (D) are avail- able.Lower-dimensional data from dimension reduction are visualized as parallel coordinates (E), and the two selected dimensions are shown in the scatter plot (F).Cluster indices/summaries (G) are shown, and the original data can be accessed (H).", "rank": 478, "paragraph_comparative_number": 0, "entities": [], "id": "p_478"}, "sentences": [{"end": 162854, "text": "Figure 23: The overview and the workflow of the system.", "rank": 1401, "start": 162799, "IsComparative": "0", "id": "st_1401"}, {"end": 162970, "text": "User interfaces for pre- processing (A), clustering (B), dimension reduction (C), and alignment (D) are avail- able.", "rank": 1402, "start": 162854, "IsComparative": "0", "id": "st_1402"}, {"end": 163128, "text": "Lower-dimensional data from dimension reduction are visualized as parallel coordinates (E), and the two selected dimensions are shown in the scatter plot (F).", "rank": 1403, "start": 162970, "IsComparative": "0", "id": "st_1403"}, {"end": 163211, "text": "Cluster indices/summaries (G) are shown, and the original data can be accessed (H).", "rank": 1404, "start": 163128, "IsComparative": "0", "id": "st_1404"}]}, {"paragraph_info": {"end": 163534, "start": 163211, "text": "Figure 24: The 10-dimensional results of PCA for the Weizmann facial image data set.The pre-given person ID was used as a color label.The first figure is the parallel coordinates of the entire 10-dimensional representations, and the second and the third are the scatter plots of (1, 2)- and (3, 4)-dimensions, respectively.", "rank": 479, "paragraph_comparative_number": 1, "entities": [], "id": "p_479"}, "sentences": [{"end": 163295, "text": "Figure 24: The 10-dimensional results of PCA for the Weizmann facial image data set.", "rank": 1405, "start": 163211, "IsComparative": "1", "id": "st_1405"}, {"end": 163345, "text": "The pre-given person ID was used as a color label.", "rank": 1406, "start": 163295, "IsComparative": "0", "id": "st_1406"}, {"end": 163534, "text": "The first figure is the parallel coordinates of the entire 10-dimensional representations, and the second and the third are the scatter plots of (1, 2)- and (3, 4)-dimensions, respectively.", "rank": 1407, "start": 163345, "IsComparative": "0", "id": "st_1407"}]}, {"paragraph_info": {"end": 163781, "start": 163534, "text": "(a) The alignment of clustering.For the Pendigits data set, the first figure uses the original cluster labels, and the other two uses the same cluster labels generated by k-means.In all three figures, ISOMAP is used with the same parameter values.", "rank": 480, "paragraph_comparative_number": 1, "entities": [], "id": "p_480"}, "sentences": [{"end": 163566, "text": "(a) The alignment of clustering.", "rank": 1408, "start": 163534, "IsComparative": "0", "id": "st_1408"}, {"end": 163713, "text": "For the Pendigits data set, the first figure uses the original cluster labels, and the other two uses the same cluster labels generated by k-means.", "rank": 1409, "start": 163566, "IsComparative": "0", "id": "st_1409"}, {"end": 163781, "text": "In all three figures, ISOMAP is used with the same parameter values.", "rank": 1410, "start": 163713, "IsComparative": "1", "id": "st_1410"}]}, {"paragraph_info": {"end": 163996, "start": 163781, "text": "(b) The alignment of dimension reduction.For the Pendigits data set, the first figure uses TSTG and the other two use ISOMAP with the same parameter values.In all three figures, the original cluster labels are used.", "rank": 481, "paragraph_comparative_number": 2, "entities": [], "id": "p_481"}, "sentences": [{"end": 163822, "text": "(b) The alignment of dimension reduction.", "rank": 1411, "start": 163781, "IsComparative": "0", "id": "st_1411"}, {"end": 163937, "text": "For the Pendigits data set, the first figure uses TSTG and the other two use ISOMAP with the same parameter values.", "rank": 1412, "start": 163822, "IsComparative": "1", "id": "st_1412"}, {"end": 163996, "text": "In all three figures, the original cluster labels are used.", "rank": 1413, "start": 163937, "IsComparative": "1", "id": "st_1413"}]}, {"paragraph_info": {"end": 164198, "start": 163996, "text": "Figure 25: The effects of alignment.In both (a) and (b), the first is the reference scatter plot view for alignment, and the second is the aligned plot of the third while the third is an un-aligned one.", "rank": 482, "paragraph_comparative_number": 0, "entities": [], "id": "p_482"}, "sentences": [{"end": 164032, "text": "Figure 25: The effects of alignment.", "rank": 1414, "start": 163996, "IsComparative": "0", "id": "st_1414"}, {"end": 164198, "text": "In both (a) and (b), the first is the reference scatter plot view for alignment, and the second is the aligned plot of the third while the third is an un-aligned one.", "rank": 1415, "start": 164032, "IsComparative": "0", "id": "st_1415"}]}, {"paragraph_info": {"end": 164638, "start": 164198, "text": "(a) The scatter plots for the Pendigits data set generated by ISOMAP with different parameter values, k =12, 20, 30, and 50, respectively.The cluster labels represent the digits of data items, as shown on the left.The three figures on the right are aligned plots with respect to the first.As the parameter increases, the cluster 5 (the digit 4), moves from the top left near the cluster 10 (the digit 9) towards the cluster 7 (the digit 6).", "rank": 483, "paragraph_comparative_number": 2, "entities": [], "id": "p_483"}, "sentences": [{"end": 164336, "text": "(a) The scatter plots for the Pendigits data set generated by ISOMAP with different parameter values, k =12, 20, 30, and 50, respectively.", "rank": 1416, "start": 164198, "IsComparative": "0", "id": "st_1416"}, {"end": 164412, "text": "The cluster labels represent the digits of data items, as shown on the left.", "rank": 1417, "start": 164336, "IsComparative": "0", "id": "st_1417"}, {"end": 164487, "text": "The three figures on the right are aligned plots with respect to the first.", "rank": 1418, "start": 164412, "IsComparative": "1", "id": "st_1418"}, {"end": 164638, "text": "As the parameter increases, the cluster 5 (the digit 4), moves from the top left near the cluster 10 (the digit 9) towards the cluster 7 (the digit 6).", "rank": 1419, "start": 164487, "IsComparative": "1", "id": "st_1419"}]}, {"paragraph_info": {"end": 164847, "start": 164638, "text": "(b) The sample data for the digits 4, 9, and 6.Note that the vector representation of the Pendigit data set encodes the pen trace coordinates, which start at the red color and end at the blue in these samples.", "rank": 484, "paragraph_comparative_number": 1, "entities": [], "id": "p_484"}, "sentences": [{"end": 164685, "text": "(b) The sample data for the digits 4, 9, and 6.", "rank": 1420, "start": 164638, "IsComparative": "0", "id": "st_1420"}, {"end": 164847, "text": "Note that the vector representation of the Pendigit data set encodes the pen trace coordinates, which start at the red color and end at the blue in these samples.", "rank": 1421, "start": 164685, "IsComparative": "1", "id": "st_1421"}]}, {"paragraph_info": {"end": 164902, "start": 164847, "text": "Figure 26: The effects of a parameter change in ISOMAP.", "rank": 485, "paragraph_comparative_number": 0, "entities": [], "id": "p_485"}, "sentences": [{"end": 164902, "text": "Figure 26: The effects of a parameter change in ISOMAP.", "rank": 1422, "start": 164847, "IsComparative": "0", "id": "st_1422"}]}, {"paragraph_info": {"end": 165122, "start": 164902, "text": "Figure 27: The scatter plot view of two different clustering, k-means and NMF, using TSTG for the InfoVisVAST data set.The right figure is aligned with respect to the left one for both clustering and dimension reduction.", "rank": 486, "paragraph_comparative_number": 1, "entities": [], "id": "p_486"}, "sentences": [{"end": 165021, "text": "Figure 27: The scatter plot view of two different clustering, k-means and NMF, using TSTG for the InfoVisVAST data set.", "rank": 1423, "start": 164902, "IsComparative": "1", "id": "st_1423"}, {"end": 165122, "text": "The right figure is aligned with respect to the left one for both clustering and dimension reduction.", "rank": 1424, "start": 165021, "IsComparative": "0", "id": "st_1424"}]}, {"paragraph_info": {"end": 165229, "start": 165122, "text": "CHAPTER VI IVISCLASSIFIER: AN INTERACTIVE VISUAL CLASSIFICATION SYSTEM USING SUPERVISED DIMENSION REDUCTION", "rank": 487, "paragraph_comparative_number": 0, "entities": [], "id": "p_487"}, "sentences": [{"end": 165229, "text": "CHAPTER VI IVISCLASSIFIER: AN INTERACTIVE VISUAL CLASSIFICATION SYSTEM USING SUPERVISED DIMENSION REDUCTION", "rank": 1425, "start": 165122, "IsComparative": "0", "id": "st_1425"}]}, {"paragraph_info": {"end": 166396, "start": 165229, "text": "We present an interactive visual analytics system for classification, iVisClassifier, based on a supervised dimension reduction method, linear discriminant analysis (LDA).Given high-dimensional data and associated cluster labels, LDA gives their reduced dimensional representation, which provides a good overview about the cluster struc- ture.Instead of a single two- or three-dimensional scatter plot, iVisClassifier fully interacts with all the reduced dimensions obtained by LDA through parallel coor- dinates and a scatter plot.Furthermore, it significantly improves the interactivity and interpretability of LDA.LDA enables users to understand each of the reduced dimensions and how they influence the data by reconstructing the basis vector into the original data domain.By using heat maps, iVisClassifier gives an overview about the cluster relationship in terms of pairwise distances between cluster centroids both in the original space and in the reduced dimensional space.Equipped with these functionalities, iVisClassifier supports users classification tasks in an efficient way.Using several facial image data, we show how the above analysis is performed.", "rank": 488, "paragraph_comparative_number": 3, "entities": [], "id": "p_488"}, "sentences": [{"end": 165400, "text": "We present an interactive visual analytics system for classification, iVisClassifier, based on a supervised dimension reduction method, linear discriminant analysis (LDA).", "rank": 1426, "start": 165229, "IsComparative": "0", "id": "st_1426"}, {"end": 165572, "text": "Given high-dimensional data and associated cluster labels, LDA gives their reduced dimensional representation, which provides a good overview about the cluster struc- ture.", "rank": 1427, "start": 165400, "IsComparative": "1", "id": "st_1427"}, {"end": 165761, "text": "Instead of a single two- or three-dimensional scatter plot, iVisClassifier fully interacts with all the reduced dimensions obtained by LDA through parallel coor- dinates and a scatter plot.", "rank": 1428, "start": 165572, "IsComparative": "0", "id": "st_1428"}, {"end": 165846, "text": "Furthermore, it significantly improves the interactivity and interpretability of LDA.", "rank": 1429, "start": 165761, "IsComparative": "0", "id": "st_1429"}, {"end": 166006, "text": "LDA enables users to understand each of the reduced dimensions and how they influence the data by reconstructing the basis vector into the original data domain.", "rank": 1430, "start": 165846, "IsComparative": "0", "id": "st_1430"}, {"end": 166211, "text": "By using heat maps, iVisClassifier gives an overview about the cluster relationship in terms of pairwise distances between cluster centroids both in the original space and in the reduced dimensional space.", "rank": 1431, "start": 166006, "IsComparative": "1", "id": "st_1431"}, {"end": 166319, "text": "Equipped with these functionalities, iVisClassifier supports users classification tasks in an efficient way.", "rank": 1432, "start": 166211, "IsComparative": "0", "id": "st_1432"}, {"end": 166396, "text": "Using several facial image data, we show how the above analysis is performed.", "rank": 1433, "start": 166319, "IsComparative": "1", "id": "st_1433"}]}, {"paragraph_info": {"end": 166412, "start": 166396, "text": "6.1 Introduction", "rank": 489, "paragraph_comparative_number": 0, "entities": [], "id": "p_489"}, "sentences": [{"end": 166412, "text": "6.1 Introduction", "rank": 1434, "start": 166396, "IsComparative": "0", "id": "st_1434"}]}, {"paragraph_info": {"end": 166755, "start": 166412, "text": "Classification is a widely-used data analysis technique across many areas such as computer vision, bioinformatics, text mining, etc.Given a set of data with known cluster labels, i.e., under a supervised setting, it builds a classifier (a training phase) to predict the label of new data (a test phase).Examples of classification tasks include", "rank": 490, "paragraph_comparative_number": 1, "entities": [], "id": "p_490"}, "sentences": [{"end": 166544, "text": "Classification is a widely-used data analysis technique across many areas such as computer vision, bioinformatics, text mining, etc.", "rank": 1435, "start": 166412, "IsComparative": "0", "id": "st_1435"}, {"end": 166715, "text": "Given a set of data with known cluster labels, i.e., under a supervised setting, it builds a classifier (a training phase) to predict the label of new data (a test phase).", "rank": 1436, "start": 166544, "IsComparative": "1", "id": "st_1436"}, {"end": 166755, "text": "Examples of classification tasks include", "rank": 1437, "start": 166715, "IsComparative": "0", "id": "st_1437"}]}, {"paragraph_info": {"end": 169611, "start": 166755, "text": "Figure 28: 2D Scatter plots obtained by two dimension reduction methods, LDA and PCA, for artificial Gaussian mixture data with 7 clusters and 1000 original dimen- sions.A different color corresponds to a different cluster.facial recognition, document categorization, spam filtering, and disease detection.Numerous classification algorithms such as an artificial neural network, decision trees, and support vector machines have been developed so far, and each method has advantages and disadvantages making it more suitable in certain domains.Even with its broad applicability, however, most of the classification algorithms are often per- formed in a fully automated manner that prevents users from not only understanding how the algorithm works on their data but also reflecting their domain knowledge into the classification process.Ironically, as classification algorithms become more sophisticated and advanced, they tend to be less interpretable to users due to their complicated internal procedure.These limitations may cause unsatisfactory classifi- cation results in real-world applications such as biometrics in which the reliability of the system is critical <137>.In some cases, there may be no option other than using the manual classification process without being supported by automated techniques.This chapter addresses how visual analytics systems support automated classi- fication for a real-world problem.As in other analytical tasks, the first step is to understand the data.From a classification perspective, users need to gain insight in terms of clusters such as how much the data within each cluster varies, which clusters are close to or distinct from each other, and which data are the most representative ones or outliers for each cluster.The next step is to understand both the charac- teristics of the chosen classifier itself and how they work on the data at hand.For instance, decision trees give a set of rules for classification, which are simple to inter- pret, and users can see which features in the data play an important role.In addition, analysis of misclassified data provides a better understanding of which types of clus- ters and/or data are difficult to classify.Such insight can then be fed back to the classification process in both the training and the test phases.In the training phase, users can refine the training data or modify the automated classification process for better performance in the long run.In the test phase, users can actively participate in determining the label of a new data by verifying each result that the automated process suggests and by performing further classification based on the interaction with a visual analytic system.The latter case ensures nearly perfect classification accuracy while maintaining much better efficiency than in the case of purely manual classification.", "rank": 491, "paragraph_comparative_number": 12, "entities": [], "id": "p_491"}, "sentences": [{"end": 166925, "text": "Figure 28: 2D Scatter plots obtained by two dimension reduction methods, LDA and PCA, for artificial Gaussian mixture data with 7 clusters and 1000 original dimen- sions.", "rank": 1438, "start": 166755, "IsComparative": "1", "id": "st_1438"}, {"end": 166978, "text": "A different color corresponds to a different cluster.", "rank": 1439, "start": 166925, "IsComparative": "0", "id": "st_1439"}, {"end": 167061, "text": "facial recognition, document categorization, spam filtering, and disease detection.", "rank": 1440, "start": 166978, "IsComparative": "0", "id": "st_1440"}, {"end": 167298, "text": "Numerous classification algorithms such as an artificial neural network, decision trees, and support vector machines have been developed so far, and each method has advantages and disadvantages making it more suitable in certain domains.", "rank": 1441, "start": 167061, "IsComparative": "0", "id": "st_1441"}, {"end": 167591, "text": "Even with its broad applicability, however, most of the classification algorithms are often per- formed in a fully automated manner that prevents users from not only understanding how the algorithm works on their data but also reflecting their domain knowledge into the classification process.", "rank": 1442, "start": 167298, "IsComparative": "0", "id": "st_1442"}, {"end": 167760, "text": "Ironically, as classification algorithms become more sophisticated and advanced, they tend to be less interpretable to users due to their complicated internal procedure.", "rank": 1443, "start": 167591, "IsComparative": "1", "id": "st_1443"}, {"end": 167931, "text": "These limitations may cause unsatisfactory classifi- cation results in real-world applications such as biometrics in which the reliability of the system is critical <137>.", "rank": 1444, "start": 167760, "IsComparative": "1", "id": "st_1444"}, {"end": 168068, "text": "In some cases, there may be no option other than using the manual classification process without being supported by automated techniques.", "rank": 1445, "start": 167931, "IsComparative": "0", "id": "st_1445"}, {"end": 168180, "text": "This chapter addresses how visual analytics systems support automated classi- fication for a real-world problem.", "rank": 1446, "start": 168068, "IsComparative": "1", "id": "st_1446"}, {"end": 168251, "text": "As in other analytical tasks, the first step is to understand the data.", "rank": 1447, "start": 168180, "IsComparative": "0", "id": "st_1447"}, {"end": 168522, "text": "From a classification perspective, users need to gain insight in terms of clusters such as how much the data within each cluster varies, which clusters are close to or distinct from each other, and which data are the most representative ones or outliers for each cluster.", "rank": 1448, "start": 168251, "IsComparative": "1", "id": "st_1448"}, {"end": 168650, "text": "The next step is to understand both the charac- teristics of the chosen classifier itself and how they work on the data at hand.", "rank": 1449, "start": 168522, "IsComparative": "1", "id": "st_1449"}, {"end": 168820, "text": "For instance, decision trees give a set of rules for classification, which are simple to inter- pret, and users can see which features in the data play an important role.", "rank": 1450, "start": 168650, "IsComparative": "1", "id": "st_1450"}, {"end": 168963, "text": "In addition, analysis of misclassified data provides a better understanding of which types of clus- ters and/or data are difficult to classify.", "rank": 1451, "start": 168820, "IsComparative": "1", "id": "st_1451"}, {"end": 169068, "text": "Such insight can then be fed back to the classification process in both the training and the test phases.", "rank": 1452, "start": 168963, "IsComparative": "1", "id": "st_1452"}, {"end": 169212, "text": "In the training phase, users can refine the training data or modify the automated classification process for better performance in the long run.", "rank": 1453, "start": 169068, "IsComparative": "1", "id": "st_1453"}, {"end": 169458, "text": "In the test phase, users can actively participate in determining the label of a new data by verifying each result that the automated process suggests and by performing further classification based on the interaction with a visual analytic system.", "rank": 1454, "start": 169212, "IsComparative": "1", "id": "st_1454"}, {"end": 169611, "text": "The latter case ensures nearly perfect classification accuracy while maintaining much better efficiency than in the case of purely manual classification.", "rank": 1455, "start": 169458, "IsComparative": "1", "id": "st_1455"}]}, {"paragraph_info": {"end": 171399, "start": 169611, "text": "Not all classification algorithms are suitable for interactive visualization of how they work.Moreover, when the data is high dimensional such as image, text, and gene expression data, the problem becomes more challenging.To resolve this issue, we choose the classification method based on linear discriminant analysis (LDA) <60>, one of the supervised dimension reduction methods.Unlike other unsupervised methods such as multidimensional scaling (MDS) and principal component analysis (PCA), which only use data, supervised ones also involve additional information such as cluster labels associated in the data.In case of LDA, it maximally discriminates different clusters while keeping the relationship among data within each cluster tight in the reduced dimensional space.This behavior of LDA has two advantages for interactive classification systems.The first advantage is that LDA is able to visualize the data so that their cluster structure can be well exposed.For example, as seen in Fig.28, LDA reveals the cluster structure better than PCA, and through LDA, users can easily find the cluster relationship and explore the data based on it.The other advantage is that the reduced dimensional representation of the data by LDA does not require a sophisticated classification algorithm in general since the data is already transformed to a well-clustered form, and such a transformation would map an unseen data item to a nearby area of its true cluster.Thus, after applying LDA, a simple classification algorithm such as k-nearest neighbors <51> can be performed, which has been successfully applied to many areas <16, 123>.Owing to this simplicity, users can get an idea about how the new data would be classified by looking at a nearby region based on visualization through LDA.", "rank": 492, "paragraph_comparative_number": 5, "entities": [], "id": "p_492"}, "sentences": [{"end": 169705, "text": "Not all classification algorithms are suitable for interactive visualization of how they work.", "rank": 1456, "start": 169611, "IsComparative": "0", "id": "st_1456"}, {"end": 169833, "text": "Moreover, when the data is high dimensional such as image, text, and gene expression data, the problem becomes more challenging.", "rank": 1457, "start": 169705, "IsComparative": "0", "id": "st_1457"}, {"end": 169992, "text": "To resolve this issue, we choose the classification method based on linear discriminant analysis (LDA) <60>, one of the supervised dimension reduction methods.", "rank": 1458, "start": 169833, "IsComparative": "0", "id": "st_1458"}, {"end": 170224, "text": "Unlike other unsupervised methods such as multidimensional scaling (MDS) and principal component analysis (PCA), which only use data, supervised ones also involve additional information such as cluster labels associated in the data.", "rank": 1459, "start": 169992, "IsComparative": "0", "id": "st_1459"}, {"end": 170387, "text": "In case of LDA, it maximally discriminates different clusters while keeping the relationship among data within each cluster tight in the reduced dimensional space.", "rank": 1460, "start": 170224, "IsComparative": "0", "id": "st_1460"}, {"end": 170466, "text": "This behavior of LDA has two advantages for interactive classification systems.", "rank": 1461, "start": 170387, "IsComparative": "1", "id": "st_1461"}, {"end": 170580, "text": "The first advantage is that LDA is able to visualize the data so that their cluster structure can be well exposed.", "rank": 1462, "start": 170466, "IsComparative": "1", "id": "st_1462"}, {"end": 170608, "text": "For example, as seen in Fig.", "rank": 1463, "start": 170580, "IsComparative": "0", "id": "st_1463"}, {"end": 170760, "text": "28, LDA reveals the cluster structure better than PCA, and through LDA, users can easily find the cluster relationship and explore the data based on it.", "rank": 1464, "start": 170608, "IsComparative": "0", "id": "st_1464"}, {"end": 171072, "text": "The other advantage is that the reduced dimensional representation of the data by LDA does not require a sophisticated classification algorithm in general since the data is already transformed to a well-clustered form, and such a transformation would map an unseen data item to a nearby area of its true cluster.", "rank": 1465, "start": 170760, "IsComparative": "1", "id": "st_1465"}, {"end": 171243, "text": "Thus, after applying LDA, a simple classification algorithm such as k-nearest neighbors <51> can be performed, which has been successfully applied to many areas <16, 123>.", "rank": 1466, "start": 171072, "IsComparative": "1", "id": "st_1466"}, {"end": 171399, "text": "Owing to this simplicity, users can get an idea about how the new data would be classified by looking at a nearby region based on visualization through LDA.", "rank": 1467, "start": 171243, "IsComparative": "1", "id": "st_1467"}]}, {"paragraph_info": {"end": 171925, "start": 171399, "text": "Inspired by the above ideas, we have developed a system called iVisClassifier, in which users can visually explore and classify data based on LDA.The first contribu- tion of iVisClassifier lies in its emphasis on interpretation of and interaction with LDA for data understanding.Then, iVisClassifier features the ability to let users cooperate with the LDA visualization for the classification process.To show the usefulness of iVisClassifier, we present facial recognition examples, where LDA-based classification works well.", "rank": 493, "paragraph_comparative_number": 3, "entities": [], "id": "p_493"}, "sentences": [{"end": 171545, "text": "Inspired by the above ideas, we have developed a system called iVisClassifier, in which users can visually explore and classify data based on LDA.", "rank": 1468, "start": 171399, "IsComparative": "0", "id": "st_1468"}, {"end": 171678, "text": "The first contribu- tion of iVisClassifier lies in its emphasis on interpretation of and interaction with LDA for data understanding.", "rank": 1469, "start": 171545, "IsComparative": "1", "id": "st_1469"}, {"end": 171801, "text": "Then, iVisClassifier features the ability to let users cooperate with the LDA visualization for the classification process.", "rank": 1470, "start": 171678, "IsComparative": "1", "id": "st_1470"}, {"end": 171925, "text": "To show the usefulness of iVisClassifier, we present facial recognition examples, where LDA-based classification works well.", "rank": 1471, "start": 171801, "IsComparative": "1", "id": "st_1471"}]}, {"paragraph_info": {"end": 172295, "start": 171925, "text": "The rest of this chapter is organized as follows.Section 6.2 discusses previous work related to interactive data mining systems and dimension reduction methods.Section 6.3 briefly introduces LDA and its use of the regularization in visualization, and Section 6.4 describes the details of iVisClassifier.Section 6.5 shows case studies, and Section 6.6 concludes our work.", "rank": 494, "paragraph_comparative_number": 3, "entities": [], "id": "p_494"}, "sentences": [{"end": 171974, "text": "The rest of this chapter is organized as follows.", "rank": 1472, "start": 171925, "IsComparative": "0", "id": "st_1472"}, {"end": 172085, "text": "Section 6.2 discusses previous work related to interactive data mining systems and dimension reduction methods.", "rank": 1473, "start": 171974, "IsComparative": "1", "id": "st_1473"}, {"end": 172228, "text": "Section 6.3 briefly introduces LDA and its use of the regularization in visualization, and Section 6.4 describes the details of iVisClassifier.", "rank": 1474, "start": 172085, "IsComparative": "1", "id": "st_1474"}, {"end": 172295, "text": "Section 6.5 shows case studies, and Section 6.6 concludes our work.", "rank": 1475, "start": 172228, "IsComparative": "1", "id": "st_1475"}]}, {"paragraph_info": {"end": 172311, "start": 172295, "text": "6.2 Related Work", "rank": 495, "paragraph_comparative_number": 0, "entities": [], "id": "p_495"}, "sentences": [{"end": 172311, "text": "6.2 Related Work", "rank": 1476, "start": 172295, "IsComparative": "0", "id": "st_1476"}]}, {"paragraph_info": {"end": 173131, "start": 172311, "text": "Supporting data mining tasks with interactive systems is an active area of study.As for clustering, an interactive system for hierarchical clustering was presented in <117>, and a visualization-based clustering framework was proposed in <29>, where users can analyze the clustering results and impose their domain knowledge into the next-stage clustering.In addition, various research has been conducted to make the dimension reduction process interactive.Yang et al.<143, 142> proposed a visual hierarchical dimension reduction method, which groups dimensions and visualizes data by using the subset of dimensions obtained from each group.Novel user-defined quality metrics was introduced for effective visualization of high-dimensional data in <73>.A user-driven visualization approach using MDS was proposed in <136>.", "rank": 496, "paragraph_comparative_number": 4, "entities": [], "id": "p_496"}, "sentences": [{"end": 172392, "text": "Supporting data mining tasks with interactive systems is an active area of study.", "rank": 1477, "start": 172311, "IsComparative": "1", "id": "st_1477"}, {"end": 172666, "text": "As for clustering, an interactive system for hierarchical clustering was presented in <117>, and a visualization-based clustering framework was proposed in <29>, where users can analyze the clustering results and impose their domain knowledge into the next-stage clustering.", "rank": 1478, "start": 172392, "IsComparative": "1", "id": "st_1478"}, {"end": 172767, "text": "In addition, various research has been conducted to make the dimension reduction process interactive.", "rank": 1479, "start": 172666, "IsComparative": "0", "id": "st_1479"}, {"end": 172778, "text": "Yang et al.", "rank": 1480, "start": 172767, "IsComparative": "0", "id": "st_1480"}, {"end": 172951, "text": "<143, 142> proposed a visual hierarchical dimension reduction method, which groups dimensions and visualizes data by using the subset of dimensions obtained from each group.", "rank": 1481, "start": 172778, "IsComparative": "1", "id": "st_1481"}, {"end": 173062, "text": "Novel user-defined quality metrics was introduced for effective visualization of high-dimensional data in <73>.", "rank": 1482, "start": 172951, "IsComparative": "1", "id": "st_1482"}, {"end": 173131, "text": "A user-driven visualization approach using MDS was proposed in <136>.", "rank": 1483, "start": 173062, "IsComparative": "0", "id": "st_1483"}]}, {"paragraph_info": {"end": 173553, "start": 173131, "text": "However, in spite of the increasing demand from real-world applications, support- ing classification tasks with an interactive visual system has not been studied exten- sively.Some studies <8, 9, 126> have tried to make a decision tree more interactive through visualization using circle segments <7> and star coordinates <77>.However, other classification methods have not been deeply integrated into interactive systems.", "rank": 497, "paragraph_comparative_number": 2, "entities": [], "id": "p_497"}, "sentences": [{"end": 173307, "text": "However, in spite of the increasing demand from real-world applications, support- ing classification tasks with an interactive visual system has not been studied exten- sively.", "rank": 1484, "start": 173131, "IsComparative": "1", "id": "st_1484"}, {"end": 173458, "text": "Some studies <8, 9, 126> have tried to make a decision tree more interactive through visualization using circle segments <7> and star coordinates <77>.", "rank": 1485, "start": 173307, "IsComparative": "1", "id": "st_1485"}, {"end": 173553, "text": "However, other classification methods have not been deeply integrated into interactive systems.", "rank": 1486, "start": 173458, "IsComparative": "0", "id": "st_1486"}]}, {"paragraph_info": {"end": 174490, "start": 173553, "text": "With respect to dimension reduction methods, a myriad of methods are still being proposed, and some of them claim their advantages on two or three-dimensional vi- sualization.The recently proposed nonlinear manifold learning methods have shown the interesting ability to match the reduced dimensions to some semantic meanings such as the rotation of objects in image data <111, 125>.Another nonlinear method called t-SNE <130> has successfully revealed a hidden cluster structure in the reduced dimensional space for handwritten digit image and facial image data through com- putationally intensive iterations.While all the above-mentioned methods are unsu- pervised dimension reduction methods that do not consider cluster label information, supervised dimension reduction methods <60, 71>, which explicitly utilize them in their computations, typically attempt to preserve the cluster structures by grouping the data with given labels.", "rank": 498, "paragraph_comparative_number": 3, "entities": [], "id": "p_498"}, "sentences": [{"end": 173728, "text": "With respect to dimension reduction methods, a myriad of methods are still being proposed, and some of them claim their advantages on two or three-dimensional vi- sualization.", "rank": 1487, "start": 173553, "IsComparative": "1", "id": "st_1487"}, {"end": 173936, "text": "The recently proposed nonlinear manifold learning methods have shown the interesting ability to match the reduced dimensions to some semantic meanings such as the rotation of objects in image data <111, 125>.", "rank": 1488, "start": 173728, "IsComparative": "0", "id": "st_1488"}, {"end": 174163, "text": "Another nonlinear method called t-SNE <130> has successfully revealed a hidden cluster structure in the reduced dimensional space for handwritten digit image and facial image data through com- putationally intensive iterations.", "rank": 1489, "start": 173936, "IsComparative": "1", "id": "st_1489"}, {"end": 174490, "text": "While all the above-mentioned methods are unsu- pervised dimension reduction methods that do not consider cluster label information, supervised dimension reduction methods <60, 71>, which explicitly utilize them in their computations, typically attempt to preserve the cluster structures by grouping the data with given labels.", "rank": 1490, "start": 174163, "IsComparative": "1", "id": "st_1490"}]}, {"paragraph_info": {"end": 175178, "start": 174490, "text": "Even with such technical advances, people still prefer traditional methods such as PCA, MDS, and self-organizing maps (SOM) because the state-of-the-art methods tend not to work universally for various types of data and they often lack inter- pretability.Motivated by this, a recently proposed system called iPCA <72> enables users to interact with PCA and its visualization results in the form of scatter plots and parallel coordinates.Our system shares a lot in common with iPCA in that users can play with LDA via scatter plots and parallel coordinates.Other than data understanding, our system aims further to support classification tasks utilizing the supervised dimension reduction.", "rank": 499, "paragraph_comparative_number": 3, "entities": [], "id": "p_499"}, "sentences": [{"end": 174745, "text": "Even with such technical advances, people still prefer traditional methods such as PCA, MDS, and self-organizing maps (SOM) because the state-of-the-art methods tend not to work universally for various types of data and they often lack inter- pretability.", "rank": 1491, "start": 174490, "IsComparative": "0", "id": "st_1491"}, {"end": 174927, "text": "Motivated by this, a recently proposed system called iPCA <72> enables users to interact with PCA and its visualization results in the form of scatter plots and parallel coordinates.", "rank": 1492, "start": 174745, "IsComparative": "1", "id": "st_1492"}, {"end": 175046, "text": "Our system shares a lot in common with iPCA in that users can play with LDA via scatter plots and parallel coordinates.", "rank": 1493, "start": 174927, "IsComparative": "1", "id": "st_1493"}, {"end": 175178, "text": "Other than data understanding, our system aims further to support classification tasks utilizing the supervised dimension reduction.", "rank": 1494, "start": 175046, "IsComparative": "1", "id": "st_1494"}]}, {"paragraph_info": {"end": 175210, "start": 175178, "text": "6.3 Linear Discriminant Analysis", "rank": 500, "paragraph_comparative_number": 0, "entities": [], "id": "p_500"}, "sentences": [{"end": 175210, "text": "6.3 Linear Discriminant Analysis", "rank": 1495, "start": 175178, "IsComparative": "0", "id": "st_1495"}]}, {"paragraph_info": {"end": 175414, "start": 175210, "text": "In this section, we briefly introduce LDA and skip rigorous mathematical derivations due to a page limit.For more technical details about LDA and its use in visualization, refer to our previous work <35>.", "rank": 501, "paragraph_comparative_number": 1, "entities": [], "id": "p_501"}, "sentences": [{"end": 175315, "text": "In this section, we briefly introduce LDA and skip rigorous mathematical derivations due to a page limit.", "rank": 1496, "start": 175210, "IsComparative": "0", "id": "st_1496"}, {"end": 175414, "text": "For more technical details about LDA and its use in visualization, refer to our previous work <35>.", "rank": 1497, "start": 175315, "IsComparative": "1", "id": "st_1497"}]}, {"paragraph_info": {"end": 175428, "start": 175414, "text": "6.3.1 Concepts", "rank": 502, "paragraph_comparative_number": 0, "entities": [], "id": "p_502"}, "sentences": [{"end": 175428, "text": "6.3.1 Concepts", "rank": 1498, "start": 175414, "IsComparative": "0", "id": "st_1498"}]}, {"paragraph_info": {"end": 176690, "start": 175428, "text": "LDA is a linear dimension reduction method that represents each of the reduced dimensions as a linear combination of the original dimensions.By projecting the data onto such a linear subspace, LDA puts cluster centroids as remote to each other as possible (by maximizing the weighted sum, B, of squared distances between cluster centroids, as shown in Fig.29(a)), while keeping each cluster as compact as possible (by minimizing the squared sum, W, of the disances between each data item in the cluster and its cluster centroid, as shown in Fig.29(b)), in the reduced dimensional space.Due to this characteristic, LDA can highlight the cluster relationship as shown in Fig.28(a), as opposed to other dimension reduction methods such as PCA.In LDA, this simultaneous optimization is formulated as a generalized eigenvalue problem that maximizes B while keeping its minimum value of W. Theoretically, the objective function value of LDA cannot exceed that in the original space, and such an upper bound is achieved as long as at least k  1 dimensions are allowed in LDA, where k is the number of clusters.Due to this characteristic, LDA usually reduces the data (a) Maximization of distances be-(b) Minimization of approximate tween cluster centroids cluster radii", "rank": 503, "paragraph_comparative_number": 4, "entities": [], "id": "p_503"}, "sentences": [{"end": 175569, "text": "LDA is a linear dimension reduction method that represents each of the reduced dimensions as a linear combination of the original dimensions.", "rank": 1499, "start": 175428, "IsComparative": "0", "id": "st_1499"}, {"end": 175784, "text": "By projecting the data onto such a linear subspace, LDA puts cluster centroids as remote to each other as possible (by maximizing the weighted sum, B, of squared distances between cluster centroids, as shown in Fig.", "rank": 1500, "start": 175569, "IsComparative": "0", "id": "st_1500"}, {"end": 175973, "text": "29(a)), while keeping each cluster as compact as possible (by minimizing the squared sum, W, of the disances between each data item in the cluster and its cluster centroid, as shown in Fig.", "rank": 1501, "start": 175784, "IsComparative": "1", "id": "st_1501"}, {"end": 176014, "text": "29(b)), in the reduced dimensional space.", "rank": 1502, "start": 175973, "IsComparative": "0", "id": "st_1502"}, {"end": 176101, "text": "Due to this characteristic, LDA can highlight the cluster relationship as shown in Fig.", "rank": 1503, "start": 176014, "IsComparative": "0", "id": "st_1503"}, {"end": 176168, "text": "28(a), as opposed to other dimension reduction methods such as PCA.", "rank": 1504, "start": 176101, "IsComparative": "1", "id": "st_1504"}, {"end": 176531, "text": "In LDA, this simultaneous optimization is formulated as a generalized eigenvalue problem that maximizes B while keeping its minimum value of W. Theoretically, the objective function value of LDA cannot exceed that in the original space, and such an upper bound is achieved as long as at least k  1 dimensions are allowed in LDA, where k is the number of clusters.", "rank": 1505, "start": 176168, "IsComparative": "1", "id": "st_1505"}, {"end": 176690, "text": "Due to this characteristic, LDA usually reduces the data (a) Maximization of distances be-(b) Minimization of approximate tween cluster centroids cluster radii", "rank": 1506, "start": 176531, "IsComparative": "1", "id": "st_1506"}]}, {"paragraph_info": {"end": 176825, "start": 176690, "text": "Figure 29: Conceptual description of LDA.A different color corresponds to a different cluster, and c1 and c2 are the cluster centroids.", "rank": 504, "paragraph_comparative_number": 1, "entities": [], "id": "p_504"}, "sentences": [{"end": 176731, "text": "Figure 29: Conceptual description of LDA.", "rank": 1507, "start": 176690, "IsComparative": "0", "id": "st_1507"}, {"end": 176825, "text": "A different color corresponds to a different cluster, and c1 and c2 are the cluster centroids.", "rank": 1508, "start": 176731, "IsComparative": "1", "id": "st_1508"}]}, {"paragraph_info": {"end": 176843, "start": 176825, "text": "dimension to k  1.", "rank": 505, "paragraph_comparative_number": 0, "entities": [], "id": "p_505"}, "sentences": [{"end": 176843, "text": "dimension to k  1.", "rank": 1509, "start": 176825, "IsComparative": "0", "id": "st_1509"}]}, {"paragraph_info": {"end": 176917, "start": 176843, "text": "Although LDA can reduce the data dimension down to k  1 dimensions without", "rank": 506, "paragraph_comparative_number": 1, "entities": [], "id": "p_506"}, "sentences": [{"end": 176917, "text": "Although LDA can reduce the data dimension down to k  1 dimensions without", "rank": 1510, "start": 176843, "IsComparative": "1", "id": "st_1510"}]}, {"paragraph_info": {"end": 177379, "start": 176917, "text": "compromising its maximum objective function value, it is often not enough to use for 2D or 3D visualization purposes.In this case, users can either select a few of the most significant dimensions or perform an additional dimension reduction step to further reduce the dimension to two or three <35>.In iVisClassifier, we adopt the former strategy so that we can easily interpret the dimension reduction step while interacting with all the LDA reduced dimensions.", "rank": 507, "paragraph_comparative_number": 2, "entities": [], "id": "p_507"}, "sentences": [{"end": 177034, "text": "compromising its maximum objective function value, it is often not enough to use for 2D or 3D visualization purposes.", "rank": 1511, "start": 176917, "IsComparative": "1", "id": "st_1511"}, {"end": 177216, "text": "In this case, users can either select a few of the most significant dimensions or perform an additional dimension reduction step to further reduce the dimension to two or three <35>.", "rank": 1512, "start": 177034, "IsComparative": "1", "id": "st_1512"}, {"end": 177379, "text": "In iVisClassifier, we adopt the former strategy so that we can easily interpret the dimension reduction step while interacting with all the LDA reduced dimensions.", "rank": 1513, "start": 177216, "IsComparative": "0", "id": "st_1513"}]}, {"paragraph_info": {"end": 177429, "start": 177379, "text": "6.3.2 Regularization to Control the Cluster Radius", "rank": 508, "paragraph_comparative_number": 0, "entities": [], "id": "p_508"}, "sentences": [{"end": 177429, "text": "6.3.2 Regularization to Control the Cluster Radius", "rank": 1514, "start": 177379, "IsComparative": "0", "id": "st_1514"}]}, {"paragraph_info": {"end": 177839, "start": 177429, "text": "In regularized LDA, a scalar multiple of an identity matrix I is added to the within- scatter matrix Sw, the trace of which represents W.1 It was applied to LDA <58> in order to circumvent a singularity problem when the data matrix has more dimensions than the number of data items, i.e., an undersampled case.In addition, regularization also has an advantage against overfitting in the classification context.", "rank": 509, "paragraph_comparative_number": 1, "entities": [], "id": "p_509"}, "sentences": [{"end": 177739, "text": "In regularized LDA, a scalar multiple of an identity matrix I is added to the within- scatter matrix Sw, the trace of which represents W.1 It was applied to LDA <58> in order to circumvent a singularity problem when the data matrix has more dimensions than the number of data items, i.e., an undersampled case.", "rank": 1515, "start": 177429, "IsComparative": "1", "id": "st_1515"}, {"end": 177839, "text": "In addition, regularization also has an advantage against overfitting in the classification context.", "rank": 1516, "start": 177739, "IsComparative": "0", "id": "st_1516"}]}, {"paragraph_info": {"end": 177918, "start": 177839, "text": "On the other hand, a unified algorithmic framework of LDA using the generalized", "rank": 510, "paragraph_comparative_number": 0, "entities": [], "id": "p_510"}, "sentences": [{"end": 177918, "text": "On the other hand, a unified algorithmic framework of LDA using the generalized", "rank": 1517, "start": 177839, "IsComparative": "0", "id": "st_1517"}]}, {"paragraph_info": {"end": 178049, "start": 177918, "text": "1Instead of W, the LDA formulation uses Sw, which is then replaced with Sw + I by regular- ization.For more details, refer to <35>.", "rank": 511, "paragraph_comparative_number": 1, "entities": [], "id": "p_511"}, "sentences": [{"end": 178017, "text": "1Instead of W, the LDA formulation uses Sw, which is then replaced with Sw + I by regular- ization.", "rank": 1518, "start": 177918, "IsComparative": "0", "id": "st_1518"}, {"end": 178049, "text": "For more details, refer to <35>.", "rank": 1519, "start": 178017, "IsComparative": "1", "id": "st_1519"}]}, {"paragraph_info": {"end": 179262, "start": 178049, "text": "Figure 30: Effects of a regularization parameter  in Sw + I. It can control how scattered each cluster is in the visualization.The data is one of the facial image data called SCface, and we chose the first six persons images.singular value decomposition (LDA/GSVD) was proposed <68>, which broadens the applicability of LDA regardless of the singularity.For undersampled data, e.g., text and image data, LDA/GSVD can fully minimize the cluster radii, making them all equal to zero.However, making the cluster radii zero results in representing all the data points in each cluster as a single point.Although it makes sense in terms of the LDA criteria, it does not keep any information to visualize at an individual data level.Thus, we utilize regularization to control the radius or scatteredness of clusters in the visualization to either focus on the data relationship or the cluster relationship, as shown in Fig.30.In an extreme case, when we sufficiently increase the regularization parameter , Sw is almost ignored in the minimization term, i.e., Sw + I  I, so that LDA focuses only on maximizing B without minimizing W. Mathematically, this case is equivalent to applying PCA on the cluster centroids <35>.", "rank": 512, "paragraph_comparative_number": 7, "entities": [], "id": "p_512"}, "sentences": [{"end": 178176, "text": "Figure 30: Effects of a regularization parameter  in Sw + I. It can control how scattered each cluster is in the visualization.", "rank": 1520, "start": 178049, "IsComparative": "0", "id": "st_1520"}, {"end": 178274, "text": "The data is one of the facial image data called SCface, and we chose the first six persons images.", "rank": 1521, "start": 178176, "IsComparative": "1", "id": "st_1521"}, {"end": 178403, "text": "singular value decomposition (LDA/GSVD) was proposed <68>, which broadens the applicability of LDA regardless of the singularity.", "rank": 1522, "start": 178274, "IsComparative": "1", "id": "st_1522"}, {"end": 178530, "text": "For undersampled data, e.g., text and image data, LDA/GSVD can fully minimize the cluster radii, making them all equal to zero.", "rank": 1523, "start": 178403, "IsComparative": "1", "id": "st_1523"}, {"end": 178647, "text": "However, making the cluster radii zero results in representing all the data points in each cluster as a single point.", "rank": 1524, "start": 178530, "IsComparative": "1", "id": "st_1524"}, {"end": 178775, "text": "Although it makes sense in terms of the LDA criteria, it does not keep any information to visualize at an individual data level.", "rank": 1525, "start": 178647, "IsComparative": "1", "id": "st_1525"}, {"end": 178965, "text": "Thus, we utilize regularization to control the radius or scatteredness of clusters in the visualization to either focus on the data relationship or the cluster relationship, as shown in Fig.", "rank": 1526, "start": 178775, "IsComparative": "0", "id": "st_1526"}, {"end": 178968, "text": "30.", "rank": 1527, "start": 178965, "IsComparative": "1", "id": "st_1527"}, {"end": 179262, "text": "In an extreme case, when we sufficiently increase the regularization parameter , Sw is almost ignored in the minimization term, i.e., Sw + I  I, so that LDA focuses only on maximizing B without minimizing W. Mathematically, this case is equivalent to applying PCA on the cluster centroids <35>.", "rank": 1528, "start": 178968, "IsComparative": "1", "id": "st_1528"}]}, {"paragraph_info": {"end": 179278, "start": 179262, "text": "6.3.3 Algorithms", "rank": 513, "paragraph_comparative_number": 0, "entities": [], "id": "p_513"}, "sentences": [{"end": 179278, "text": "6.3.3 Algorithms", "rank": 1529, "start": 179262, "IsComparative": "0", "id": "st_1529"}]}, {"paragraph_info": {"end": 179811, "start": 179278, "text": "To ensure real-time interactions, it is important to design an efficient algorithm for LDA.Therefore, we reduce the data matrix size by applying either QR decomposition for undersampled cases or Cholesky decomposition for the other cases before running LDA.The main idea here is to transform a rectangular data matrix of size m  n into a square matrix of size min(m, n)  min(m, n) without losing any information.Then, the GSVD-based LDA algorithm is performed on this reduced matrix much efficiently.For more details, refer to <103>.", "rank": 514, "paragraph_comparative_number": 2, "entities": [], "id": "p_514"}, "sentences": [{"end": 179369, "text": "To ensure real-time interactions, it is important to design an efficient algorithm for LDA.", "rank": 1530, "start": 179278, "IsComparative": "1", "id": "st_1530"}, {"end": 179535, "text": "Therefore, we reduce the data matrix size by applying either QR decomposition for undersampled cases or Cholesky decomposition for the other cases before running LDA.", "rank": 1531, "start": 179369, "IsComparative": "0", "id": "st_1531"}, {"end": 179690, "text": "The main idea here is to transform a rectangular data matrix of size m  n into a square matrix of size min(m, n)  min(m, n) without losing any information.", "rank": 1532, "start": 179535, "IsComparative": "0", "id": "st_1532"}, {"end": 179778, "text": "Then, the GSVD-based LDA algorithm is performed on this reduced matrix much efficiently.", "rank": 1533, "start": 179690, "IsComparative": "0", "id": "st_1533"}, {"end": 179811, "text": "For more details, refer to <103>.", "rank": 1534, "start": 179778, "IsComparative": "1", "id": "st_1534"}]}, {"paragraph_info": {"end": 179833, "start": 179811, "text": "6.4 System Description", "rank": 515, "paragraph_comparative_number": 0, "entities": [], "id": "p_515"}, "sentences": [{"end": 179833, "text": "6.4 System Description", "rank": 1535, "start": 179811, "IsComparative": "0", "id": "st_1535"}]}, {"paragraph_info": {"end": 179852, "start": 179833, "text": "6.4.1 Data Encoding", "rank": 516, "paragraph_comparative_number": 1, "entities": [], "id": "p_516"}, "sentences": [{"end": 179852, "text": "6.4.1 Data Encoding", "rank": 1536, "start": 179833, "IsComparative": "1", "id": "st_1536"}]}, {"paragraph_info": {"end": 180480, "start": 179852, "text": "Given a data set along with its labels, iVisClassifier first encodes the data into high- dimensional vectors.In its current implementation, iVisClassifier takes text docu- ments, images, and generic numerical vectors with comma-separated values.When dealing with image data, the pixel values in each image are rasterized to form a single column vector, and text data are encoded using the bag-of-words model.Such encod- ing schemes determine the dimensions of image and text data as the total number of pixels in a single image and the total number of different words, respectively, which can be up to the hundreds of thousands.", "rank": 517, "paragraph_comparative_number": 3, "entities": [], "id": "p_517"}, "sentences": [{"end": 179961, "text": "Given a data set along with its labels, iVisClassifier first encodes the data into high- dimensional vectors.", "rank": 1537, "start": 179852, "IsComparative": "1", "id": "st_1537"}, {"end": 180097, "text": "In its current implementation, iVisClassifier takes text docu- ments, images, and generic numerical vectors with comma-separated values.", "rank": 1538, "start": 179961, "IsComparative": "1", "id": "st_1538"}, {"end": 180260, "text": "When dealing with image data, the pixel values in each image are rasterized to form a single column vector, and text data are encoded using the bag-of-words model.", "rank": 1539, "start": 180097, "IsComparative": "0", "id": "st_1539"}, {"end": 180480, "text": "Such encod- ing schemes determine the dimensions of image and text data as the total number of pixels in a single image and the total number of different words, respectively, which can be up to the hundreds of thousands.", "rank": 1540, "start": 180260, "IsComparative": "1", "id": "st_1540"}]}, {"paragraph_info": {"end": 180853, "start": 180480, "text": "Along with numerical encoding, iVisClassifier has several optional pre-processing steps such as data centering and normalization that makes the norm of every vector equal.In addition, other domain-specific pre-processing steps are also provided, such as contrast limited adaptive histogram equalization <107> for image data and stemming and stop-word removal for text data.", "rank": 518, "paragraph_comparative_number": 1, "entities": [], "id": "p_518"}, "sentences": [{"end": 180651, "text": "Along with numerical encoding, iVisClassifier has several optional pre-processing steps such as data centering and normalization that makes the norm of every vector equal.", "rank": 1541, "start": 180480, "IsComparative": "0", "id": "st_1541"}, {"end": 180853, "text": "In addition, other domain-specific pre-processing steps are also provided, such as contrast limited adaptive histogram equalization <107> for image data and stemming and stop-word removal for text data.", "rank": 1542, "start": 180651, "IsComparative": "1", "id": "st_1542"}]}, {"paragraph_info": {"end": 180880, "start": 180853, "text": "6.4.2 Visualization Modules", "rank": 519, "paragraph_comparative_number": 0, "entities": [], "id": "p_519"}, "sentences": [{"end": 180880, "text": "6.4.2 Visualization Modules", "rank": 1543, "start": 180853, "IsComparative": "0", "id": "st_1543"}]}, {"paragraph_info": {"end": 181753, "start": 180880, "text": "Once the data matrix whose columns represent data items is obtained, LDA is per- formed on this matrix with its associated labels.Users can recompute LDA with different regularization parameter values  through a horizontal slide bar interface until the data within each cluster are adequately scattered.As described in Section 3, LDA reduces the data dimension to k1 where k is the number of clusters.Just as the reduced dimensions in PCA are in an order to preserve the most variance, those in LDA are also in an order of preserving the most value of the LDA criterion.That is, the first reduced dimension represents each cluster most compactly while keeping different clusters most distinctly.With this in mind, we visualize LDA results in four different ways: parallel coordinates (Fig.31A), the basis view (Fig.31B), heat maps (Fig.31C), and 2D scatter plots (Fig.31F).", "rank": 520, "paragraph_comparative_number": 3, "entities": [], "id": "p_520"}, "sentences": [{"end": 181010, "text": "Once the data matrix whose columns represent data items is obtained, LDA is per- formed on this matrix with its associated labels.", "rank": 1544, "start": 180880, "IsComparative": "1", "id": "st_1544"}, {"end": 181183, "text": "Users can recompute LDA with different regularization parameter values  through a horizontal slide bar interface until the data within each cluster are adequately scattered.", "rank": 1545, "start": 181010, "IsComparative": "0", "id": "st_1545"}, {"end": 181281, "text": "As described in Section 3, LDA reduces the data dimension to k1 where k is the number of clusters.", "rank": 1546, "start": 181183, "IsComparative": "0", "id": "st_1546"}, {"end": 181450, "text": "Just as the reduced dimensions in PCA are in an order to preserve the most variance, those in LDA are also in an order of preserving the most value of the LDA criterion.", "rank": 1547, "start": 181281, "IsComparative": "0", "id": "st_1547"}, {"end": 181575, "text": "That is, the first reduced dimension represents each cluster most compactly while keeping different clusters most distinctly.", "rank": 1548, "start": 181450, "IsComparative": "1", "id": "st_1548"}, {"end": 181669, "text": "With this in mind, we visualize LDA results in four different ways: parallel coordinates (Fig.", "rank": 1549, "start": 181575, "IsComparative": "0", "id": "st_1549"}, {"end": 181695, "text": "31A), the basis view (Fig.", "rank": 1550, "start": 181669, "IsComparative": "0", "id": "st_1550"}, {"end": 181716, "text": "31B), heat maps (Fig.", "rank": 1551, "start": 181695, "IsComparative": "0", "id": "st_1551"}, {"end": 181748, "text": "31C), and 2D scatter plots (Fig.", "rank": 1552, "start": 181716, "IsComparative": "0", "id": "st_1552"}, {"end": 181753, "text": "31F).", "rank": 1553, "start": 181748, "IsComparative": "1", "id": "st_1553"}]}, {"paragraph_info": {"end": 181781, "start": 181753, "text": "6.4.2.1 Parallel coordinates", "rank": 521, "paragraph_comparative_number": 0, "entities": [], "id": "p_521"}, "sentences": [{"end": 181781, "text": "6.4.2.1 Parallel coordinates", "rank": 1554, "start": 181753, "IsComparative": "0", "id": "st_1554"}]}, {"paragraph_info": {"end": 183097, "start": 181781, "text": "Parallel coordinates is a common way to visualize multi-dimensional data.In parallel coordinates, the dimension axes are placed side by side as a set of parallel lines, and the data item is represented as a polyline whose vertices on these axes indicate the values in the corresponding dimensions.The main problem of parallel coordinates is that it does not scale well in terms of both the number of data items and the number of dimensions.However, LDA can deal with both problems effectively in the following ways.First, with a manageable number of clusters, k, LDA reduces the number of dimensions to k  1, without losing any information on the cluster structure based on the LDA criterion.In addition, in terms of the number of data items, LDA plays the role of data reduction for undersampled cases since it can represent all the data items within each cluster as a single point by setting  = 0, which in turn visualizes the entire data as k items.The dimension-reduced data by LDA may suffer the same scalability problem when the number of clusters and/or the regularization parameter  increases.Nonetheless, in most cases, LDA significantly alleviates the clutter in parallel coordinates in that dealing with a large number of clusters is not practical and that users can always start their analysis with  = 0.", "rank": 522, "paragraph_comparative_number": 6, "entities": [], "id": "p_522"}, "sentences": [{"end": 181854, "text": "Parallel coordinates is a common way to visualize multi-dimensional data.", "rank": 1555, "start": 181781, "IsComparative": "1", "id": "st_1555"}, {"end": 182078, "text": "In parallel coordinates, the dimension axes are placed side by side as a set of parallel lines, and the data item is represented as a polyline whose vertices on these axes indicate the values in the corresponding dimensions.", "rank": 1556, "start": 181854, "IsComparative": "1", "id": "st_1556"}, {"end": 182221, "text": "The main problem of parallel coordinates is that it does not scale well in terms of both the number of data items and the number of dimensions.", "rank": 1557, "start": 182078, "IsComparative": "1", "id": "st_1557"}, {"end": 182296, "text": "However, LDA can deal with both problems effectively in the following ways.", "rank": 1558, "start": 182221, "IsComparative": "0", "id": "st_1558"}, {"end": 182473, "text": "First, with a manageable number of clusters, k, LDA reduces the number of dimensions to k  1, without losing any information on the cluster structure based on the LDA criterion.", "rank": 1559, "start": 182296, "IsComparative": "1", "id": "st_1559"}, {"end": 182733, "text": "In addition, in terms of the number of data items, LDA plays the role of data reduction for undersampled cases since it can represent all the data items within each cluster as a single point by setting  = 0, which in turn visualizes the entire data as k items.", "rank": 1560, "start": 182473, "IsComparative": "1", "id": "st_1560"}, {"end": 182882, "text": "The dimension-reduced data by LDA may suffer the same scalability problem when the number of clusters and/or the regularization parameter  increases.", "rank": 1561, "start": 182733, "IsComparative": "0", "id": "st_1561"}, {"end": 183097, "text": "Nonetheless, in most cases, LDA significantly alleviates the clutter in parallel coordinates in that dealing with a large number of clusters is not practical and that users can always start their analysis with  = 0.", "rank": 1562, "start": 182882, "IsComparative": "1", "id": "st_1562"}]}, {"paragraph_info": {"end": 183927, "start": 183097, "text": "Our implementation of parallel coordinates has several interactions including a basic zoom-in/out function.First, users can control the transparency of the polylines to see how densely the lines go through a particular region.To this end, users can switch all the colors indicating cluster labels to a single one, e.g.black.In addition, iVisClassifier has several shifting and scaling options.One is to align the minimum value of each dimension at the bottom horizontal line in the view, and the other is to align both the minimum and the maximum values at the top and bottom line, respectively.iVisClassifier is also able to filter the data by selecting particular clusters and/or data points in a certain range specified by a mouse pointer, and brushing and linking is implemented between parallel coordinates and scatter plots.", "rank": 523, "paragraph_comparative_number": 4, "entities": [], "id": "p_523"}, "sentences": [{"end": 183204, "text": "Our implementation of parallel coordinates has several interactions including a basic zoom-in/out function.", "rank": 1563, "start": 183097, "IsComparative": "1", "id": "st_1563"}, {"end": 183323, "text": "First, users can control the transparency of the polylines to see how densely the lines go through a particular region.", "rank": 1564, "start": 183204, "IsComparative": "0", "id": "st_1564"}, {"end": 183415, "text": "To this end, users can switch all the colors indicating cluster labels to a single one, e.g.", "rank": 1565, "start": 183323, "IsComparative": "1", "id": "st_1565"}, {"end": 183421, "text": "black.", "rank": 1566, "start": 183415, "IsComparative": "1", "id": "st_1566"}, {"end": 183490, "text": "In addition, iVisClassifier has several shifting and scaling options.", "rank": 1567, "start": 183421, "IsComparative": "0", "id": "st_1567"}, {"end": 183692, "text": "One is to align the minimum value of each dimension at the bottom horizontal line in the view, and the other is to align both the minimum and the maximum values at the top and bottom line, respectively.", "rank": 1568, "start": 183490, "IsComparative": "1", "id": "st_1568"}, {"end": 183927, "text": "iVisClassifier is also able to filter the data by selecting particular clusters and/or data points in a certain range specified by a mouse pointer, and brushing and linking is implemented between parallel coordinates and scatter plots.", "rank": 1569, "start": 183692, "IsComparative": "0", "id": "st_1569"}]}, {"paragraph_info": {"end": 183945, "start": 183927, "text": "6.4.2.2 Basis view", "rank": 524, "paragraph_comparative_number": 0, "entities": [], "id": "p_524"}, "sentences": [{"end": 183945, "text": "6.4.2.2 Basis view", "rank": 1570, "start": 183927, "IsComparative": "0", "id": "st_1570"}]}, {"paragraph_info": {"end": 184547, "start": 183945, "text": "When data go through any kind of computational algorithms, it is crucial to have a better understanding of what happens in the process.For instance, even though the dimension reduction result is given by LDA, users may need to know the meaning behind each dimension and the reasons why those dimensions maximize the LDA cri- terion.Without such information, users cannot readily understand why certain data points look like outliers or certain clusters are prominent in the LDA result.Follow- ing this motivation, we provide users with the meaning of each reduced dimension of LDA in the following way.", "rank": 525, "paragraph_comparative_number": 2, "entities": [], "id": "p_525"}, "sentences": [{"end": 184080, "text": "When data go through any kind of computational algorithms, it is crucial to have a better understanding of what happens in the process.", "rank": 1571, "start": 183945, "IsComparative": "1", "id": "st_1571"}, {"end": 184277, "text": "For instance, even though the dimension reduction result is given by LDA, users may need to know the meaning behind each dimension and the reasons why those dimensions maximize the LDA cri- terion.", "rank": 1572, "start": 184080, "IsComparative": "0", "id": "st_1572"}, {"end": 184430, "text": "Without such information, users cannot readily understand why certain data points look like outliers or certain clusters are prominent in the LDA result.", "rank": 1573, "start": 184277, "IsComparative": "0", "id": "st_1573"}, {"end": 184547, "text": "Follow- ing this motivation, we provide users with the meaning of each reduced dimension of LDA in the following way.", "rank": 1574, "start": 184430, "IsComparative": "1", "id": "st_1574"}]}, {"paragraph_info": {"end": 185932, "start": 184547, "text": "First of all, LDA is a linear method where each reduced dimension is represented as a linear combination of those in the original space.Thus, we have a linear combination coefficient for each reduced dimension, which we call a basis vector, and the dimension of this basis vector is the same as the original dimension.For image data in which the original dimension is the number of pixels in the image, each coefficient value in this basis vector corresponds to each of the pixels.Based on this idea, we reconstruct the LDA basis in the original data domain, e.g., an image in our case.However, it is not always straightforward to convert the basis back to the original data domain.For example, pixel values in an image have a certain specification that they have to be all integers between 0 and 255 while the LDA basis is real-valued with positive and negative signs mixed.In the past, several heuristics to handle this issue were used in the context of PCA by mapping basis vectors to grayscale images <129, 131> by taking either its absolute value or adding the minimum value.However, these heuristic methods lose or distort the information contained in the basis vectors.Therefore, we map positive and negative numbers in the basis vector into two color channels, red and blue, respectively.In this way, we obtain the reconstructed images of LDA basis vectors as shown in Fig.31B.", "rank": 526, "paragraph_comparative_number": 6, "entities": [], "id": "p_526"}, "sentences": [{"end": 184683, "text": "First of all, LDA is a linear method where each reduced dimension is represented as a linear combination of those in the original space.", "rank": 1575, "start": 184547, "IsComparative": "0", "id": "st_1575"}, {"end": 184865, "text": "Thus, we have a linear combination coefficient for each reduced dimension, which we call a basis vector, and the dimension of this basis vector is the same as the original dimension.", "rank": 1576, "start": 184683, "IsComparative": "0", "id": "st_1576"}, {"end": 185028, "text": "For image data in which the original dimension is the number of pixels in the image, each coefficient value in this basis vector corresponds to each of the pixels.", "rank": 1577, "start": 184865, "IsComparative": "1", "id": "st_1577"}, {"end": 185133, "text": "Based on this idea, we reconstruct the LDA basis in the original data domain, e.g., an image in our case.", "rank": 1578, "start": 185028, "IsComparative": "1", "id": "st_1578"}, {"end": 185229, "text": "However, it is not always straightforward to convert the basis back to the original data domain.", "rank": 1579, "start": 185133, "IsComparative": "0", "id": "st_1579"}, {"end": 185422, "text": "For example, pixel values in an image have a certain specification that they have to be all integers between 0 and 255 while the LDA basis is real-valued with positive and negative signs mixed.", "rank": 1580, "start": 185229, "IsComparative": "1", "id": "st_1580"}, {"end": 185627, "text": "In the past, several heuristics to handle this issue were used in the context of PCA by mapping basis vectors to grayscale images <129, 131> by taking either its absolute value or adding the minimum value.", "rank": 1581, "start": 185422, "IsComparative": "1", "id": "st_1581"}, {"end": 185723, "text": "However, these heuristic methods lose or distort the information contained in the basis vectors.", "rank": 1582, "start": 185627, "IsComparative": "0", "id": "st_1582"}, {"end": 185843, "text": "Therefore, we map positive and negative numbers in the basis vector into two color channels, red and blue, respectively.", "rank": 1583, "start": 185723, "IsComparative": "1", "id": "st_1583"}, {"end": 185928, "text": "In this way, we obtain the reconstructed images of LDA basis vectors as shown in Fig.", "rank": 1584, "start": 185843, "IsComparative": "0", "id": "st_1584"}, {"end": 185932, "text": "31B.", "rank": 1585, "start": 185928, "IsComparative": "1", "id": "st_1585"}]}, {"paragraph_info": {"end": 185949, "start": 185932, "text": "6.4.2.3 Heat maps", "rank": 527, "paragraph_comparative_number": 0, "entities": [], "id": "p_527"}, "sentences": [{"end": 185949, "text": "6.4.2.3 Heat maps", "rank": 1586, "start": 185932, "IsComparative": "0", "id": "st_1586"}]}, {"paragraph_info": {"end": 186595, "start": 185949, "text": "With heat maps, we visualize the pairwise distances between cluster centroids, where each heat map has k  k elements.The leftmost heat map in Fig.31C represents such information in the original high-dimensional space, and the following ones on the right side are computed within each reduced dimension of LDA.Through this visualization, we can get the information about which particular cluster is distinct from the other clusters and which cluster pairs are close or remote in each dimension.Furthermore, comparisons between heat maps of the original space and each of the reduced dimension show which cluster distances are preserved or ignored.", "rank": 528, "paragraph_comparative_number": 0, "entities": [], "id": "p_528"}, "sentences": [{"end": 186066, "text": "With heat maps, we visualize the pairwise distances between cluster centroids, where each heat map has k  k elements.", "rank": 1587, "start": 185949, "IsComparative": "0", "id": "st_1587"}, {"end": 186095, "text": "The leftmost heat map in Fig.", "rank": 1588, "start": 186066, "IsComparative": "0", "id": "st_1588"}, {"end": 186258, "text": "31C represents such information in the original high-dimensional space, and the following ones on the right side are computed within each reduced dimension of LDA.", "rank": 1589, "start": 186095, "IsComparative": "0", "id": "st_1589"}, {"end": 186442, "text": "Through this visualization, we can get the information about which particular cluster is distinct from the other clusters and which cluster pairs are close or remote in each dimension.", "rank": 1590, "start": 186258, "IsComparative": "0", "id": "st_1590"}, {"end": 186595, "text": "Furthermore, comparisons between heat maps of the original space and each of the reduced dimension show which cluster distances are preserved or ignored.", "rank": 1591, "start": 186442, "IsComparative": "0", "id": "st_1591"}]}, {"paragraph_info": {"end": 187081, "start": 186595, "text": "By clicking the (i, j)-th square in the enlarged heat map (Fig.31D), users can compare the data items in the i-th and j-th clusters as shown in Fig.31E.In addition, the slide bar at the bottom in Fig.31E enables users to overlap the data image with its corresponding basis image, which tells us how the pixels in these images are weighted in its corresponding dimension and why the data of the selected two clusters are closely or remotely related in this dimension, as shown in Fig.36.", "rank": 529, "paragraph_comparative_number": 3, "entities": [], "id": "p_529"}, "sentences": [{"end": 186658, "text": "By clicking the (i, j)-th square in the enlarged heat map (Fig.", "rank": 1592, "start": 186595, "IsComparative": "0", "id": "st_1592"}, {"end": 186743, "text": "31D), users can compare the data items in the i-th and j-th clusters as shown in Fig.", "rank": 1593, "start": 186658, "IsComparative": "0", "id": "st_1593"}, {"end": 186747, "text": "31E.", "rank": 1594, "start": 186743, "IsComparative": "1", "id": "st_1594"}, {"end": 186795, "text": "In addition, the slide bar at the bottom in Fig.", "rank": 1595, "start": 186747, "IsComparative": "0", "id": "st_1595"}, {"end": 187078, "text": "31E enables users to overlap the data image with its corresponding basis image, which tells us how the pixels in these images are weighted in its corresponding dimension and why the data of the selected two clusters are closely or remotely related in this dimension, as shown in Fig.", "rank": 1596, "start": 186795, "IsComparative": "1", "id": "st_1596"}, {"end": 187081, "text": "36.", "rank": 1597, "start": 187078, "IsComparative": "1", "id": "st_1597"}]}, {"paragraph_info": {"end": 187102, "start": 187081, "text": "6.4.2.4 Scatter plots", "rank": 530, "paragraph_comparative_number": 0, "entities": [], "id": "p_530"}, "sentences": [{"end": 187102, "text": "6.4.2.4 Scatter plots", "rank": 1598, "start": 187081, "IsComparative": "0", "id": "st_1598"}]}, {"paragraph_info": {"end": 187520, "start": 187102, "text": "The scatter plot visualizes data points in the two user-selected reduced dimensions of LDA with a zoom-in/out functionality.In this view, a data item is represented as a point with an initial letter and a different color of its corresponding cluster label.Additionally, the first and the second order statistics per cluster, which are the mean and the covariance ellipse, give the effective information about clusters.", "rank": 531, "paragraph_comparative_number": 2, "entities": [], "id": "p_531"}, "sentences": [{"end": 187226, "text": "The scatter plot visualizes data points in the two user-selected reduced dimensions of LDA with a zoom-in/out functionality.", "rank": 1599, "start": 187102, "IsComparative": "0", "id": "st_1599"}, {"end": 187358, "text": "In this view, a data item is represented as a point with an initial letter and a different color of its corresponding cluster label.", "rank": 1600, "start": 187226, "IsComparative": "1", "id": "st_1600"}, {"end": 187520, "text": "Additionally, the first and the second order statistics per cluster, which are the mean and the covariance ellipse, give the effective information about clusters.", "rank": 1601, "start": 187358, "IsComparative": "1", "id": "st_1601"}]}, {"paragraph_info": {"end": 188035, "start": 187520, "text": "Our scatter plot view given by LDA allows users to interactively explore the data in view of the overall cluster structure in the following senses: 1. which data points are outliers or representative points in their corresponding clusters, 2. which data points are outliers or representative points in their corresponding clusters, 3. how widely the data points within a cluster are distributed and accordingly, which clusters have potential subclusters, and 4. which data points overlap between different clusters.", "rank": 532, "paragraph_comparative_number": 0, "entities": [], "id": "p_532"}, "sentences": [{"end": 188035, "text": "Our scatter plot view given by LDA allows users to interactively explore the data in view of the overall cluster structure in the following senses: 1. which data points are outliers or representative points in their corresponding clusters, 2. which data points are outliers or representative points in their corresponding clusters, 3. how widely the data points within a cluster are distributed and accordingly, which clusters have potential subclusters, and 4. which data points overlap between different clusters.", "rank": 1602, "start": 187520, "IsComparative": "0", "id": "st_1602"}]}, {"paragraph_info": {"end": 188301, "start": 188035, "text": "In addition, brushing and linking with parallel coordinates overcome the limitation that the scatter plot can only show two or three dimensions at a time.In this way, users can see how the selected data or clusters in the scatter plot behave in the other dimensions.", "rank": 533, "paragraph_comparative_number": 1, "entities": [], "id": "p_533"}, "sentences": [{"end": 188189, "text": "In addition, brushing and linking with parallel coordinates overcome the limitation that the scatter plot can only show two or three dimensions at a time.", "rank": 1603, "start": 188035, "IsComparative": "1", "id": "st_1603"}, {"end": 188301, "text": "In this way, users can see how the selected data or clusters in the scatter plot behave in the other dimensions.", "rank": 1604, "start": 188189, "IsComparative": "0", "id": "st_1604"}]}, {"paragraph_info": {"end": 188329, "start": 188301, "text": "6.4.3 Classification Modules", "rank": 534, "paragraph_comparative_number": 0, "entities": [], "id": "p_534"}, "sentences": [{"end": 188329, "text": "6.4.3 Classification Modules", "rank": 1605, "start": 188301, "IsComparative": "0", "id": "st_1605"}]}, {"paragraph_info": {"end": 189563, "start": 188329, "text": "After obtaining insight from exploring the data with known cluster labels, users can now interactively perform classification on the new data whose labels are to be de- termined.This process works as follows.First, a new data item is mapped onto the reduced dimensional space formed by the previous data.It is then visualized in parallel coordinates and in the scatter plot view.Such visualization significantly increases the efficiency of users classification tasks by visually reducing the search space.Within this reduced visual search space, users can easily compare the new data item with the existing data or clusters nearby.When the new data point falls into a cluttered region where many different clusters overlap, users can select or filter out some data or clusters and recompute LDA with this subset of data including the new point, which we call a computational zoom-in process.In other words, LDA takes into account the selected clusters and/or those corresponding to the selected data, which requires a much smaller number of dimensions than k  1 for LDA to fully discriminate the selected clusters.Based on the new visualization generated in this way, users can better identify which clusters the new point belongs to.", "rank": 535, "paragraph_comparative_number": 5, "entities": [], "id": "p_535"}, "sentences": [{"end": 188507, "text": "After obtaining insight from exploring the data with known cluster labels, users can now interactively perform classification on the new data whose labels are to be de- termined.", "rank": 1606, "start": 188329, "IsComparative": "1", "id": "st_1606"}, {"end": 188537, "text": "This process works as follows.", "rank": 1607, "start": 188507, "IsComparative": "0", "id": "st_1607"}, {"end": 188633, "text": "First, a new data item is mapped onto the reduced dimensional space formed by the previous data.", "rank": 1608, "start": 188537, "IsComparative": "0", "id": "st_1608"}, {"end": 188708, "text": "It is then visualized in parallel coordinates and in the scatter plot view.", "rank": 1609, "start": 188633, "IsComparative": "0", "id": "st_1609"}, {"end": 188834, "text": "Such visualization significantly increases the efficiency of users classification tasks by visually reducing the search space.", "rank": 1610, "start": 188708, "IsComparative": "1", "id": "st_1610"}, {"end": 188960, "text": "Within this reduced visual search space, users can easily compare the new data item with the existing data or clusters nearby.", "rank": 1611, "start": 188834, "IsComparative": "1", "id": "st_1611"}, {"end": 189220, "text": "When the new data point falls into a cluttered region where many different clusters overlap, users can select or filter out some data or clusters and recompute LDA with this subset of data including the new point, which we call a computational zoom-in process.", "rank": 1612, "start": 188960, "IsComparative": "0", "id": "st_1612"}, {"end": 189443, "text": "In other words, LDA takes into account the selected clusters and/or those corresponding to the selected data, which requires a much smaller number of dimensions than k  1 for LDA to fully discriminate the selected clusters.", "rank": 1613, "start": 189220, "IsComparative": "1", "id": "st_1613"}, {"end": 189563, "text": "Based on the new visualization generated in this way, users can better identify which clusters the new point belongs to.", "rank": 1614, "start": 189443, "IsComparative": "1", "id": "st_1614"}]}, {"paragraph_info": {"end": 189987, "start": 189563, "text": "On completing the visually-supported classification process, users can assign a label to the new data item and optionally include the newly labeled data in future LDA computations, which is initiated only when users want to recompute them.The reason we do not force users to include every new data in LDA computations is that users confidence level of the assigned label may not be high enough for some reason such as noise.", "rank": 536, "paragraph_comparative_number": 1, "entities": [], "id": "p_536"}, "sentences": [{"end": 189802, "text": "On completing the visually-supported classification process, users can assign a label to the new data item and optionally include the newly labeled data in future LDA computations, which is initiated only when users want to recompute them.", "rank": 1615, "start": 189563, "IsComparative": "1", "id": "st_1615"}, {"end": 189987, "text": "The reason we do not force users to include every new data in LDA computations is that users confidence level of the assigned label may not be high enough for some reason such as noise.", "rank": 1616, "start": 189802, "IsComparative": "0", "id": "st_1616"}]}, {"paragraph_info": {"end": 190003, "start": 189987, "text": "6.5 Case Studies", "rank": 537, "paragraph_comparative_number": 0, "entities": [], "id": "p_537"}, "sentences": [{"end": 190003, "text": "6.5 Case Studies", "rank": 1617, "start": 189987, "IsComparative": "0", "id": "st_1617"}]}, {"paragraph_info": {"end": 191086, "start": 190003, "text": "In this section, we present an interactive analysis using two sets of facial image data, Weizmann database2 and SCface database <64>, for facial recognition.Weizmann is composed of 28 persons frontal images in a constant background, in which each person has 52 images.The variations within each persons images exist regarding viewing angles, illuminations, and facial expressions.We resized the original 512  352 pixel images to 64  44 pixel images, resulting in 2816 dimensional vectors.SCface is an image collection taken in an uncontrolled indoor environment using multiple video surveillance cameras with various image qualities.It is composed of 4160 static images of 130 subjects, of which we randomly selected 30 persons images for our study, where each person has 32 images.Since the images in SCface generally contain parts other than a face, such as the upper body of a person and a different background, we have cropped a facial part using an affine transformation that aligns the images based on the eye coordinates.The image samples of two data sets are shown in Fig.32.", "rank": 538, "paragraph_comparative_number": 5, "entities": [], "id": "p_538"}, "sentences": [{"end": 190160, "text": "In this section, we present an interactive analysis using two sets of facial image data, Weizmann database2 and SCface database <64>, for facial recognition.", "rank": 1618, "start": 190003, "IsComparative": "1", "id": "st_1618"}, {"end": 190271, "text": "Weizmann is composed of 28 persons frontal images in a constant background, in which each person has 52 images.", "rank": 1619, "start": 190160, "IsComparative": "0", "id": "st_1619"}, {"end": 190383, "text": "The variations within each persons images exist regarding viewing angles, illuminations, and facial expressions.", "rank": 1620, "start": 190271, "IsComparative": "0", "id": "st_1620"}, {"end": 190491, "text": "We resized the original 512  352 pixel images to 64  44 pixel images, resulting in 2816 dimensional vectors.", "rank": 1621, "start": 190383, "IsComparative": "1", "id": "st_1621"}, {"end": 190636, "text": "SCface is an image collection taken in an uncontrolled indoor environment using multiple video surveillance cameras with various image qualities.", "rank": 1622, "start": 190491, "IsComparative": "0", "id": "st_1622"}, {"end": 190785, "text": "It is composed of 4160 static images of 130 subjects, of which we randomly selected 30 persons images for our study, where each person has 32 images.", "rank": 1623, "start": 190636, "IsComparative": "1", "id": "st_1623"}, {"end": 191031, "text": "Since the images in SCface generally contain parts other than a face, such as the upper body of a person and a different background, we have cropped a facial part using an affine transformation that aligns the images based on the eye coordinates.", "rank": 1624, "start": 190785, "IsComparative": "0", "id": "st_1624"}, {"end": 191083, "text": "The image samples of two data sets are shown in Fig.", "rank": 1625, "start": 191031, "IsComparative": "1", "id": "st_1625"}, {"end": 191086, "text": "32.", "rank": 1626, "start": 191083, "IsComparative": "1", "id": "st_1626"}]}, {"paragraph_info": {"end": 191331, "start": 191086, "text": "In the following, we present an exploratory analysis towards better understanding of both the data and the computational method we have used, i.e., LDA.Next, we describe how users interactively perform classification supported by iVisClassifier.", "rank": 539, "paragraph_comparative_number": 0, "entities": [], "id": "p_539"}, "sentences": [{"end": 191238, "text": "In the following, we present an exploratory analysis towards better understanding of both the data and the computational method we have used, i.e., LDA.", "rank": 1627, "start": 191086, "IsComparative": "0", "id": "st_1627"}, {"end": 191331, "text": "Next, we describe how users interactively perform classification supported by iVisClassifier.", "rank": 1628, "start": 191238, "IsComparative": "0", "id": "st_1628"}]}, {"paragraph_info": {"end": 191388, "start": 191331, "text": "2 http://www.wisdom.weizmann.ac.il/ vision/databases.html", "rank": 540, "paragraph_comparative_number": 0, "entities": [], "id": "p_540"}, "sentences": [{"end": 191388, "text": "2 http://www.wisdom.weizmann.ac.il/ vision/databases.html", "rank": 1629, "start": 191331, "IsComparative": "0", "id": "st_1629"}]}, {"paragraph_info": {"end": 191419, "start": 191388, "text": "6.5.1 Exploratory Data Analysis", "rank": 541, "paragraph_comparative_number": 0, "entities": [], "id": "p_541"}, "sentences": [{"end": 191419, "text": "6.5.1 Exploratory Data Analysis", "rank": 1630, "start": 191388, "IsComparative": "0", "id": "st_1630"}]}, {"paragraph_info": {"end": 192244, "start": 191419, "text": "In general, understanding the data at the cluster level is essential to deriving an initial idea about the overall structure in a large-scale data set.In this sense, we can begin with the heat map view of the pairwise distances in the original space to look at how the clusters are related.From the heat maps shown in Fig.33(a) and 34(a), we can see that pairwise cluster distances vary more in Weizmann than in SCface.This view also reveals the clusters that look distinct from the other clusters, e.g., person 14 in Weizmann and person 7 in SCface.Element-wise comparisons reveal that persons 11 and 14 look quite distinct, which makes sense due to baldness and shirt colors, but persons 2 and 10 look similar in Fig.33(a).Similarly, persons 1 and 7 look different while persons 2 and 26 are indistinguishable in Fig.34(a).", "rank": 542, "paragraph_comparative_number": 5, "entities": [], "id": "p_542"}, "sentences": [{"end": 191570, "text": "In general, understanding the data at the cluster level is essential to deriving an initial idea about the overall structure in a large-scale data set.", "rank": 1631, "start": 191419, "IsComparative": "1", "id": "st_1631"}, {"end": 191709, "text": "In this sense, we can begin with the heat map view of the pairwise distances in the original space to look at how the clusters are related.", "rank": 1632, "start": 191570, "IsComparative": "1", "id": "st_1632"}, {"end": 191741, "text": "From the heat maps shown in Fig.", "rank": 1633, "start": 191709, "IsComparative": "0", "id": "st_1633"}, {"end": 191838, "text": "33(a) and 34(a), we can see that pairwise cluster distances vary more in Weizmann than in SCface.", "rank": 1634, "start": 191741, "IsComparative": "0", "id": "st_1634"}, {"end": 191969, "text": "This view also reveals the clusters that look distinct from the other clusters, e.g., person 14 in Weizmann and person 7 in SCface.", "rank": 1635, "start": 191838, "IsComparative": "0", "id": "st_1635"}, {"end": 192138, "text": "Element-wise comparisons reveal that persons 11 and 14 look quite distinct, which makes sense due to baldness and shirt colors, but persons 2 and 10 look similar in Fig.", "rank": 1636, "start": 191969, "IsComparative": "1", "id": "st_1636"}, {"end": 192144, "text": "33(a).", "rank": 1637, "start": 192138, "IsComparative": "1", "id": "st_1637"}, {"end": 192238, "text": "Similarly, persons 1 and 7 look different while persons 2 and 26 are indistinguishable in Fig.", "rank": 1638, "start": 192144, "IsComparative": "0", "id": "st_1638"}, {"end": 192244, "text": "34(a).", "rank": 1639, "start": 192238, "IsComparative": "1", "id": "st_1639"}]}, {"paragraph_info": {"end": 192639, "start": 192244, "text": "Next, let us look at the heat maps of the LDA dimensions shown in Figs.33-34.The first dimension turns out to reflect the most distinct clusters in the original space.In addition, the heat maps in the LDA dimensions have mostly blue-colored elements, i.e., almost zero, except for a few rows and columns, which indicates that each of the LDA dimensions tends to discriminate only a few clusters.", "rank": 543, "paragraph_comparative_number": 1, "entities": [], "id": "p_543"}, "sentences": [{"end": 192315, "text": "Next, let us look at the heat maps of the LDA dimensions shown in Figs.", "rank": 1640, "start": 192244, "IsComparative": "0", "id": "st_1640"}, {"end": 192321, "text": "33-34.", "rank": 1641, "start": 192315, "IsComparative": "1", "id": "st_1641"}, {"end": 192411, "text": "The first dimension turns out to reflect the most distinct clusters in the original space.", "rank": 1642, "start": 192321, "IsComparative": "0", "id": "st_1642"}, {"end": 192639, "text": "In addition, the heat maps in the LDA dimensions have mostly blue-colored elements, i.e., almost zero, except for a few rows and columns, which indicates that each of the LDA dimensions tends to discriminate only a few clusters.", "rank": 1643, "start": 192411, "IsComparative": "0", "id": "st_1643"}]}, {"paragraph_info": {"end": 193049, "start": 192639, "text": "Next, Fig.35 shows the image reconstruction of the first six LDA bases for both data sets.It is interesting to see that in both cases, the forehead part is heavily weighted in the first dimension,3 and then in the second dimension, the forehead part is differentiated into upper and lower parts.This indicates that the forehead part is the most prominent factor for facial recognition based on LDA in our data.", "rank": 544, "paragraph_comparative_number": 1, "entities": [], "id": "p_544"}, "sentences": [{"end": 192649, "text": "Next, Fig.", "rank": 1644, "start": 192639, "IsComparative": "0", "id": "st_1644"}, {"end": 192729, "text": "35 shows the image reconstruction of the first six LDA bases for both data sets.", "rank": 1645, "start": 192649, "IsComparative": "0", "id": "st_1645"}, {"end": 192934, "text": "It is interesting to see that in both cases, the forehead part is heavily weighted in the first dimension,3 and then in the second dimension, the forehead part is differentiated into upper and lower parts.", "rank": 1646, "start": 192729, "IsComparative": "0", "id": "st_1646"}, {"end": 193049, "text": "This indicates that the forehead part is the most prominent factor for facial recognition based on LDA in our data.", "rank": 1647, "start": 192934, "IsComparative": "1", "id": "st_1647"}]}, {"paragraph_info": {"end": 193384, "start": 193049, "text": "Basis images can be overlapped with the original images to highlight the region in the images that is heavily weighted in a specific reduced dimension.The example shown in Fig.36 was obtained by selecting one of the most remote cluster pairs (red- colored one in Fig.33(b)) in the first dimension.In the region covered by a blue color,", "rank": 545, "paragraph_comparative_number": 1, "entities": [], "id": "p_545"}, "sentences": [{"end": 193200, "text": "Basis images can be overlapped with the original images to highlight the region in the images that is heavily weighted in a specific reduced dimension.", "rank": 1648, "start": 193049, "IsComparative": "0", "id": "st_1648"}, {"end": 193225, "text": "The example shown in Fig.", "rank": 1649, "start": 193200, "IsComparative": "0", "id": "st_1649"}, {"end": 193316, "text": "36 was obtained by selecting one of the most remote cluster pairs (red- colored one in Fig.", "rank": 1650, "start": 193225, "IsComparative": "1", "id": "st_1650"}, {"end": 193346, "text": "33(b)) in the first dimension.", "rank": 1651, "start": 193316, "IsComparative": "0", "id": "st_1651"}, {"end": 193384, "text": "In the region covered by a blue color,", "rank": 1652, "start": 193346, "IsComparative": "0", "id": "st_1652"}]}, {"paragraph_info": {"end": 193734, "start": 193384, "text": "3Negative weighting coefficients represented as blue colors are equivalent to positive ones by negating the basis and the corresponding coordinate values of the data.we can see that the pixel values are quite different, i.e., light in the first cluster and dark in the second cluster, which puts them far apart in the corresponding reduced dimension.", "rank": 546, "paragraph_comparative_number": 1, "entities": [], "id": "p_546"}, "sentences": [{"end": 193550, "text": "3Negative weighting coefficients represented as blue colors are equivalent to positive ones by negating the basis and the corresponding coordinate values of the data.", "rank": 1653, "start": 193384, "IsComparative": "1", "id": "st_1653"}, {"end": 193734, "text": "we can see that the pixel values are quite different, i.e., light in the first cluster and dark in the second cluster, which puts them far apart in the corresponding reduced dimension.", "rank": 1654, "start": 193550, "IsComparative": "0", "id": "st_1654"}]}, {"paragraph_info": {"end": 193766, "start": 193734, "text": "6.5.2 Interactive Classification", "rank": 547, "paragraph_comparative_number": 0, "entities": [], "id": "p_547"}, "sentences": [{"end": 193766, "text": "6.5.2 Interactive Classification", "rank": 1655, "start": 193734, "IsComparative": "0", "id": "st_1655"}]}, {"paragraph_info": {"end": 194857, "start": 193766, "text": "As described in Section 4.3, the main benefit of iVisClassifier for classification is that it visually guides users to the correct clusters for unseen data while allowing users to have control over the classification process.In general, most of the new data would be closely placed to their corresponding clusters in the scatter plot.If only a few clusters are found nearby, e.g., when a point to classify is placed near the cluster 7, which is almost isolated from the other clusters at the leftmost part in Fig.37(a), then by checking some of the nearby data in the cluster 7, users can quickly classify them into their corresponding clusters.However, a problem arises when the new point is visualized near a cluttered region as shown in Fig.37(a).With this visualization, we have a less clear idea as to which clusters to look at because numerous clusters exist near the point of interest.In this case, we can select a subset of data points around it and then recompute the dimension reduction only with this subset.Fig.37 shows that this process guides the new point to its true cluster.", "rank": 548, "paragraph_comparative_number": 5, "entities": [], "id": "p_548"}, "sentences": [{"end": 193991, "text": "As described in Section 4.3, the main benefit of iVisClassifier for classification is that it visually guides users to the correct clusters for unseen data while allowing users to have control over the classification process.", "rank": 1656, "start": 193766, "IsComparative": "0", "id": "st_1656"}, {"end": 194100, "text": "In general, most of the new data would be closely placed to their corresponding clusters in the scatter plot.", "rank": 1657, "start": 193991, "IsComparative": "1", "id": "st_1657"}, {"end": 194279, "text": "If only a few clusters are found nearby, e.g., when a point to classify is placed near the cluster 7, which is almost isolated from the other clusters at the leftmost part in Fig.", "rank": 1658, "start": 194100, "IsComparative": "0", "id": "st_1658"}, {"end": 194411, "text": "37(a), then by checking some of the nearby data in the cluster 7, users can quickly classify them into their corresponding clusters.", "rank": 1659, "start": 194279, "IsComparative": "1", "id": "st_1659"}, {"end": 194510, "text": "However, a problem arises when the new point is visualized near a cluttered region as shown in Fig.", "rank": 1660, "start": 194411, "IsComparative": "0", "id": "st_1660"}, {"end": 194516, "text": "37(a).", "rank": 1661, "start": 194510, "IsComparative": "1", "id": "st_1661"}, {"end": 194658, "text": "With this visualization, we have a less clear idea as to which clusters to look at because numerous clusters exist near the point of interest.", "rank": 1662, "start": 194516, "IsComparative": "1", "id": "st_1662"}, {"end": 194785, "text": "In this case, we can select a subset of data points around it and then recompute the dimension reduction only with this subset.", "rank": 1663, "start": 194658, "IsComparative": "0", "id": "st_1663"}, {"end": 194789, "text": "Fig.", "rank": 1664, "start": 194785, "IsComparative": "0", "id": "st_1664"}, {"end": 194857, "text": "37 shows that this process guides the new point to its true cluster.", "rank": 1665, "start": 194789, "IsComparative": "1", "id": "st_1665"}]}, {"paragraph_info": {"end": 195524, "start": 194857, "text": "Another scenario for interactive classification in iVisClassifier is cooperative fil- tering between parallel coordinates and the scatter plot.Fig.38(a) shows a case where the new point is placed in an ambiguous region to classify.As we find that the new point (shown in a gray color in parallel coordinates) goes through the top region in dimension 7, we can filter the data in this dimension, and accordingly, the selected data are also highlighted in the scatter plot with a black circle, as shown in Fig.38(b).Additional filtering in the scatter plot by selecting either nearby clusters or data items ends up with only one possible cluster, as shown in Fig.38(c).", "rank": 549, "paragraph_comparative_number": 1, "entities": [], "id": "p_549"}, "sentences": [{"end": 195000, "text": "Another scenario for interactive classification in iVisClassifier is cooperative fil- tering between parallel coordinates and the scatter plot.", "rank": 1666, "start": 194857, "IsComparative": "1", "id": "st_1666"}, {"end": 195004, "text": "Fig.", "rank": 1667, "start": 195000, "IsComparative": "0", "id": "st_1667"}, {"end": 195088, "text": "38(a) shows a case where the new point is placed in an ambiguous region to classify.", "rank": 1668, "start": 195004, "IsComparative": "0", "id": "st_1668"}, {"end": 195365, "text": "As we find that the new point (shown in a gray color in parallel coordinates) goes through the top region in dimension 7, we can filter the data in this dimension, and accordingly, the selected data are also highlighted in the scatter plot with a black circle, as shown in Fig.", "rank": 1669, "start": 195088, "IsComparative": "0", "id": "st_1669"}, {"end": 195371, "text": "38(b).", "rank": 1670, "start": 195365, "IsComparative": "0", "id": "st_1670"}, {"end": 195518, "text": "Additional filtering in the scatter plot by selecting either nearby clusters or data items ends up with only one possible cluster, as shown in Fig.", "rank": 1671, "start": 195371, "IsComparative": "0", "id": "st_1671"}, {"end": 195524, "text": "38(c).", "rank": 1672, "start": 195518, "IsComparative": "0", "id": "st_1672"}]}, {"paragraph_info": {"end": 195986, "start": 195524, "text": "Once some of the new data are assigned their labels, users can recompute LDA by taking into account the newly labeled data.Fig.39 shows the distributions of the new data whose label is 0 before and after LDA recomputation with a newly labeled data item.As we can see, the rest of the unseen data in cluster 0 becomes closer to its centroid after LDA recomputation, which indicates that the updated LDA dimensions potentially better discriminates the unseen data.", "rank": 550, "paragraph_comparative_number": 3, "entities": [], "id": "p_550"}, "sentences": [{"end": 195647, "text": "Once some of the new data are assigned their labels, users can recompute LDA by taking into account the newly labeled data.", "rank": 1673, "start": 195524, "IsComparative": "1", "id": "st_1673"}, {"end": 195651, "text": "Fig.", "rank": 1674, "start": 195647, "IsComparative": "0", "id": "st_1674"}, {"end": 195777, "text": "39 shows the distributions of the new data whose label is 0 before and after LDA recomputation with a newly labeled data item.", "rank": 1675, "start": 195651, "IsComparative": "1", "id": "st_1675"}, {"end": 195986, "text": "As we can see, the rest of the unseen data in cluster 0 becomes closer to its centroid after LDA recomputation, which indicates that the updated LDA dimensions potentially better discriminates the unseen data.", "rank": 1676, "start": 195777, "IsComparative": "1", "id": "st_1676"}]}, {"paragraph_info": {"end": 196001, "start": 195986, "text": "6.6 Conclusions", "rank": 551, "paragraph_comparative_number": 0, "entities": [], "id": "p_551"}, "sentences": [{"end": 196001, "text": "6.6 Conclusions", "rank": 1677, "start": 195986, "IsComparative": "0", "id": "st_1677"}]}, {"paragraph_info": {"end": 196945, "start": 196001, "text": "In this study, we have presented iVisClassifier, a visual analytics system for clustered data and classification.Our system enables users to explore high-dimensional data through LDA, which is a supervised dimension reduction method.We interpret the effect of regularization in visualization and provide an effective user-interface in which users can control the cluster radii depending on whether they focus on the cluster- or the data-level relationships.In addition, iVisClassifier facilitates the interpretability of the computational model applied to their data.Various views such as parallel coordinates, the scatter plot, and heat maps interactively show rich aspects of the data.Finally, we showed that iVisClassifier can efficiently support a user-driven classification process by reducing humans search space, e.g., recomputing LDA with a user-selected subset of data and mutual filtering in parallel coordinates and the scatter plot.", "rank": 552, "paragraph_comparative_number": 3, "entities": [], "id": "p_552"}, "sentences": [{"end": 196114, "text": "In this study, we have presented iVisClassifier, a visual analytics system for clustered data and classification.", "rank": 1678, "start": 196001, "IsComparative": "0", "id": "st_1678"}, {"end": 196234, "text": "Our system enables users to explore high-dimensional data through LDA, which is a supervised dimension reduction method.", "rank": 1679, "start": 196114, "IsComparative": "1", "id": "st_1679"}, {"end": 196458, "text": "We interpret the effect of regularization in visualization and provide an effective user-interface in which users can control the cluster radii depending on whether they focus on the cluster- or the data-level relationships.", "rank": 1680, "start": 196234, "IsComparative": "0", "id": "st_1680"}, {"end": 196568, "text": "In addition, iVisClassifier facilitates the interpretability of the computational model applied to their data.", "rank": 1681, "start": 196458, "IsComparative": "1", "id": "st_1681"}, {"end": 196688, "text": "Various views such as parallel coordinates, the scatter plot, and heat maps interactively show rich aspects of the data.", "rank": 1682, "start": 196568, "IsComparative": "0", "id": "st_1682"}, {"end": 196945, "text": "Finally, we showed that iVisClassifier can efficiently support a user-driven classification process by reducing humans search space, e.g., recomputing LDA with a user-selected subset of data and mutual filtering in parallel coordinates and the scatter plot.", "rank": 1683, "start": 196688, "IsComparative": "1", "id": "st_1683"}]}, {"paragraph_info": {"end": 197336, "start": 196945, "text": "As our future work, we plan to improve our system to better handle other types of high-dimensional data and their classification tasks.Although our system can currently load and visualize other types of high-dimensional data such as text data, how we accommodate the basis view and blend the data item with the basis in the original data domain, as shown in Fig.36, would be the main issues.", "rank": 553, "paragraph_comparative_number": 2, "entities": [], "id": "p_553"}, "sentences": [{"end": 197080, "text": "As our future work, we plan to improve our system to better handle other types of high-dimensional data and their classification tasks.", "rank": 1684, "start": 196945, "IsComparative": "1", "id": "st_1684"}, {"end": 197307, "text": "Although our system can currently load and visualize other types of high-dimensional data such as text data, how we accommodate the basis view and blend the data item with the basis in the original data domain, as shown in Fig.", "rank": 1685, "start": 197080, "IsComparative": "1", "id": "st_1685"}, {"end": 197336, "text": "36, would be the main issues.", "rank": 1686, "start": 197307, "IsComparative": "0", "id": "st_1686"}]}, {"paragraph_info": {"end": 197719, "start": 197336, "text": "In addition, although our tool works well when there is a reasonable number of clusters, it may not scale well when we have many clusters, e.g., hundreds of people in facial recognition.To handle this problem, we are considering the hierarchical approaches that group the clusters based on their relative similarities to keep the number of clusters manageable in an initial analysis.", "rank": 554, "paragraph_comparative_number": 1, "entities": [], "id": "p_554"}, "sentences": [{"end": 197522, "text": "In addition, although our tool works well when there is a reasonable number of clusters, it may not scale well when we have many clusters, e.g., hundreds of people in facial recognition.", "rank": 1687, "start": 197336, "IsComparative": "1", "id": "st_1687"}, {"end": 197719, "text": "To handle this problem, we are considering the hierarchical approaches that group the clusters based on their relative similarities to keep the number of clusters manageable in an initial analysis.", "rank": 1688, "start": 197522, "IsComparative": "0", "id": "st_1688"}]}, {"paragraph_info": {"end": 198279, "start": 197719, "text": "Finally, the computation of LDA can be burdensome for user interactions when we have a large-scale data.Novel interactions with LDA provided by iVisClassifier motivate the new types of dynamic updating algorithms based on the previous LDA results in various situations.For instance, updating the LDA results when changing the regularization parameter value has not been studied before.Thus, we are currently exploring for various situations and their corresponding updating algorithms when computational algorithms are integrated into user-interactive systems.", "rank": 555, "paragraph_comparative_number": 0, "entities": [], "id": "p_555"}, "sentences": [{"end": 197823, "text": "Finally, the computation of LDA can be burdensome for user interactions when we have a large-scale data.", "rank": 1689, "start": 197719, "IsComparative": "0", "id": "st_1689"}, {"end": 197988, "text": "Novel interactions with LDA provided by iVisClassifier motivate the new types of dynamic updating algorithms based on the previous LDA results in various situations.", "rank": 1690, "start": 197823, "IsComparative": "0", "id": "st_1690"}, {"end": 198104, "text": "For instance, updating the LDA results when changing the regularization parameter value has not been studied before.", "rank": 1691, "start": 197988, "IsComparative": "0", "id": "st_1691"}, {"end": 198279, "text": "Thus, we are currently exploring for various situations and their corresponding updating algorithms when computational algorithms are integrated into user-interactive systems.", "rank": 1692, "start": 198104, "IsComparative": "0", "id": "st_1692"}]}, {"paragraph_info": {"end": 199665, "start": 198279, "text": "Figure 31: The overview of the system.SCface data with 30 randomly chosen persons images were used, and different colors correspond to different clusters, e.g., persons.The arrow indicates a clicking operation.(A) Parallel coordinates view.The LDA results are represented in 29 dimensions.(B) Basis view.The LDA basis vectors are reconstructed in the original data domain, which in this case is an image.(C) Heat map view.The pairwise distances between cluster centroids are visualized.The leftmost one is computed from the original space, and the rest from each of the LDA dimensions.Upon clicking, the full-size of a heat map is shown (D), and clicking each square shows the existing data in the corresponding pair of clusters (E).(F) Scatter plot view.A 2D scatter plot is visualized using two user-selected dimensions.When clicking a particular data point, its original data item is shown (G).(H) Control interfaces.Users can change the transparency and the colors in parallel coordinates.Data can be filtered at the data level as well as at the cluster level.The interfaces for unseen data visualize them one by one, interactively classify them, and finally update the LDA model.A horizontal slide bar for the regularization parameter value in LDA controls the scattering of each cluster.(I) shows the legend about cluster labels in terms of their assigned colors and enumerations.", "rank": 556, "paragraph_comparative_number": 4, "entities": [], "id": "p_556"}, "sentences": [{"end": 198317, "text": "Figure 31: The overview of the system.", "rank": 1693, "start": 198279, "IsComparative": "0", "id": "st_1693"}, {"end": 198448, "text": "SCface data with 30 randomly chosen persons images were used, and different colors correspond to different clusters, e.g., persons.", "rank": 1694, "start": 198317, "IsComparative": "1", "id": "st_1694"}, {"end": 198489, "text": "The arrow indicates a clicking operation.", "rank": 1695, "start": 198448, "IsComparative": "0", "id": "st_1695"}, {"end": 198519, "text": "(A) Parallel coordinates view.", "rank": 1696, "start": 198489, "IsComparative": "0", "id": "st_1696"}, {"end": 198568, "text": "The LDA results are represented in 29 dimensions.", "rank": 1697, "start": 198519, "IsComparative": "1", "id": "st_1697"}, {"end": 198583, "text": "(B) Basis view.", "rank": 1698, "start": 198568, "IsComparative": "0", "id": "st_1698"}, {"end": 198683, "text": "The LDA basis vectors are reconstructed in the original data domain, which in this case is an image.", "rank": 1699, "start": 198583, "IsComparative": "0", "id": "st_1699"}, {"end": 198701, "text": "(C) Heat map view.", "rank": 1700, "start": 198683, "IsComparative": "0", "id": "st_1700"}, {"end": 198765, "text": "The pairwise distances between cluster centroids are visualized.", "rank": 1701, "start": 198701, "IsComparative": "0", "id": "st_1701"}, {"end": 198864, "text": "The leftmost one is computed from the original space, and the rest from each of the LDA dimensions.", "rank": 1702, "start": 198765, "IsComparative": "0", "id": "st_1702"}, {"end": 199012, "text": "Upon clicking, the full-size of a heat map is shown (D), and clicking each square shows the existing data in the corresponding pair of clusters (E).", "rank": 1703, "start": 198864, "IsComparative": "0", "id": "st_1703"}, {"end": 199034, "text": "(F) Scatter plot view.", "rank": 1704, "start": 199012, "IsComparative": "0", "id": "st_1704"}, {"end": 199101, "text": "A 2D scatter plot is visualized using two user-selected dimensions.", "rank": 1705, "start": 199034, "IsComparative": "0", "id": "st_1705"}, {"end": 199176, "text": "When clicking a particular data point, its original data item is shown (G).", "rank": 1706, "start": 199101, "IsComparative": "1", "id": "st_1706"}, {"end": 199199, "text": "(H) Control interfaces.", "rank": 1707, "start": 199176, "IsComparative": "0", "id": "st_1707"}, {"end": 199272, "text": "Users can change the transparency and the colors in parallel coordinates.", "rank": 1708, "start": 199199, "IsComparative": "0", "id": "st_1708"}, {"end": 199343, "text": "Data can be filtered at the data level as well as at the cluster level.", "rank": 1709, "start": 199272, "IsComparative": "0", "id": "st_1709"}, {"end": 199463, "text": "The interfaces for unseen data visualize them one by one, interactively classify them, and finally update the LDA model.", "rank": 1710, "start": 199343, "IsComparative": "0", "id": "st_1710"}, {"end": 199572, "text": "A horizontal slide bar for the regularization parameter value in LDA controls the scattering of each cluster.", "rank": 1711, "start": 199463, "IsComparative": "0", "id": "st_1711"}, {"end": 199665, "text": "(I) shows the legend about cluster labels in terms of their assigned colors and enumerations.", "rank": 1712, "start": 199572, "IsComparative": "1", "id": "st_1712"}]}, {"paragraph_info": {"end": 199677, "start": 199665, "text": "(a) Weizmann", "rank": 557, "paragraph_comparative_number": 0, "entities": [], "id": "p_557"}, "sentences": [{"end": 199677, "text": "(a) Weizmann", "rank": 1713, "start": 199665, "IsComparative": "0", "id": "st_1713"}]}, {"paragraph_info": {"end": 199687, "start": 199677, "text": "(b) SCface", "rank": 558, "paragraph_comparative_number": 0, "entities": [], "id": "p_558"}, "sentences": [{"end": 199687, "text": "(b) SCface", "rank": 1714, "start": 199677, "IsComparative": "0", "id": "st_1714"}]}, {"paragraph_info": {"end": 199746, "start": 199687, "text": "Figure 32: A single persons image samples in two data sets.", "rank": 559, "paragraph_comparative_number": 1, "entities": [], "id": "p_559"}, "sentences": [{"end": 199746, "text": "Figure 32: A single persons image samples in two data sets.", "rank": 1715, "start": 199687, "IsComparative": "1", "id": "st_1715"}]}, {"paragraph_info": {"end": 199816, "start": 199746, "text": "(a) The original space (b) The first dimension (c) The fifth dimension", "rank": 560, "paragraph_comparative_number": 0, "entities": [], "id": "p_560"}, "sentences": [{"end": 199816, "text": "(a) The original space (b) The first dimension (c) The fifth dimension", "rank": 1716, "start": 199746, "IsComparative": "0", "id": "st_1716"}]}, {"paragraph_info": {"end": 199900, "start": 199816, "text": "Figure 33: Heat map view of the pairwise cluster distances of the Weizmann data set.", "rank": 561, "paragraph_comparative_number": 0, "entities": [], "id": "p_561"}, "sentences": [{"end": 199900, "text": "Figure 33: Heat map view of the pairwise cluster distances of the Weizmann data set.", "rank": 1717, "start": 199816, "IsComparative": "0", "id": "st_1717"}]}, {"paragraph_info": {"end": 199970, "start": 199900, "text": "(a) The original space (b) The first dimension (c) The sixth dimension", "rank": 562, "paragraph_comparative_number": 0, "entities": [], "id": "p_562"}, "sentences": [{"end": 199970, "text": "(a) The original space (b) The first dimension (c) The sixth dimension", "rank": 1718, "start": 199900, "IsComparative": "0", "id": "st_1718"}]}, {"paragraph_info": {"end": 200052, "start": 199970, "text": "Figure 34: Heat map view of the pairwise cluster distances of the SCface data set.", "rank": 563, "paragraph_comparative_number": 0, "entities": [], "id": "p_563"}, "sentences": [{"end": 200052, "text": "Figure 34: Heat map view of the pairwise cluster distances of the SCface data set.", "rank": 1719, "start": 199970, "IsComparative": "0", "id": "st_1719"}]}, {"paragraph_info": {"end": 200064, "start": 200052, "text": "(a) Weizmann", "rank": 564, "paragraph_comparative_number": 0, "entities": [], "id": "p_564"}, "sentences": [{"end": 200064, "text": "(a) Weizmann", "rank": 1720, "start": 200052, "IsComparative": "0", "id": "st_1720"}]}, {"paragraph_info": {"end": 200074, "start": 200064, "text": "(b) SCface", "rank": 565, "paragraph_comparative_number": 0, "entities": [], "id": "p_565"}, "sentences": [{"end": 200074, "text": "(b) SCface", "rank": 1721, "start": 200064, "IsComparative": "0", "id": "st_1721"}]}, {"paragraph_info": {"end": 200133, "start": 200074, "text": "Figure 35: Reconstructed images of the first six LDA bases.", "rank": 566, "paragraph_comparative_number": 0, "entities": [], "id": "p_566"}, "sentences": [{"end": 200133, "text": "Figure 35: Reconstructed images of the first six LDA bases.", "rank": 1722, "start": 200074, "IsComparative": "0", "id": "st_1722"}]}, {"paragraph_info": {"end": 200273, "start": 200133, "text": "Figure 36: The effect of overlapping a basis image over the original data.Users can see which part of images are weighted by a basis vector.", "rank": 567, "paragraph_comparative_number": 0, "entities": [], "id": "p_567"}, "sentences": [{"end": 200207, "text": "Figure 36: The effect of overlapping a basis image over the original data.", "rank": 1723, "start": 200133, "IsComparative": "0", "id": "st_1723"}, {"end": 200273, "text": "Users can see which part of images are weighted by a basis vector.", "rank": 1724, "start": 200207, "IsComparative": "0", "id": "st_1724"}]}, {"paragraph_info": {"end": 200358, "start": 200273, "text": "(a) The initial filtering (b) The second filtering (c) The final visualization result", "rank": 568, "paragraph_comparative_number": 0, "entities": [], "id": "p_568"}, "sentences": [{"end": 200358, "text": "(a) The initial filtering (b) The second filtering (c) The final visualization result", "rank": 1725, "start": 200273, "IsComparative": "0", "id": "st_1725"}]}, {"paragraph_info": {"end": 200608, "start": 200358, "text": "Figure 37: Interactive classification by computational zoom-in.Recursive visualiza- tion by recomputing LDA for interactively selected subsets of data guides a new point into its corresponding cluster.The thick arrow indicates the new point position.", "rank": 569, "paragraph_comparative_number": 1, "entities": [], "id": "p_569"}, "sentences": [{"end": 200421, "text": "Figure 37: Interactive classification by computational zoom-in.", "rank": 1726, "start": 200358, "IsComparative": "0", "id": "st_1726"}, {"end": 200559, "text": "Recursive visualiza- tion by recomputing LDA for interactively selected subsets of data guides a new point into its corresponding cluster.", "rank": 1727, "start": 200421, "IsComparative": "1", "id": "st_1727"}, {"end": 200608, "text": "The thick arrow indicates the new point position.", "rank": 1728, "start": 200559, "IsComparative": "0", "id": "st_1728"}]}, {"paragraph_info": {"end": 200718, "start": 200608, "text": "(a) The initial visualization (b) The filtering in parallel coor-(c) The filtering in the scatter plot dinates", "rank": 570, "paragraph_comparative_number": 1, "entities": [], "id": "p_570"}, "sentences": [{"end": 200718, "text": "(a) The initial visualization (b) The filtering in parallel coor-(c) The filtering in the scatter plot dinates", "rank": 1729, "start": 200608, "IsComparative": "1", "id": "st_1729"}]}, {"paragraph_info": {"end": 200911, "start": 200718, "text": "Figure 38: Interactive classification by mutual filtering.Filtering both in parallel coordinates and the scatter plot leads to a single cluster.The thick arrow indicates the new point position.", "rank": 571, "paragraph_comparative_number": 1, "entities": [], "id": "p_571"}, "sentences": [{"end": 200776, "text": "Figure 38: Interactive classification by mutual filtering.", "rank": 1730, "start": 200718, "IsComparative": "0", "id": "st_1730"}, {"end": 200862, "text": "Filtering both in parallel coordinates and the scatter plot leads to a single cluster.", "rank": 1731, "start": 200776, "IsComparative": "1", "id": "st_1731"}, {"end": 200911, "text": "The thick arrow indicates the new point position.", "rank": 1732, "start": 200862, "IsComparative": "0", "id": "st_1732"}]}, {"paragraph_info": {"end": 200980, "start": 200911, "text": "(a) Before labelling the test point (b) After labelling the new point", "rank": 572, "paragraph_comparative_number": 0, "entities": [], "id": "p_572"}, "sentences": [{"end": 200980, "text": "(a) Before labelling the test point (b) After labelling the new point", "rank": 1733, "start": 200911, "IsComparative": "0", "id": "st_1733"}]}, {"paragraph_info": {"end": 201212, "start": 200980, "text": "Figure 39: Effects of LDA recomputation when including a newly labeled point in the existing data.The arrow indicates the newly labeled point, and the red circles represent the distribution of the remaining unseen data in cluster 0.", "rank": 573, "paragraph_comparative_number": 0, "entities": [], "id": "p_573"}, "sentences": [{"end": 201078, "text": "Figure 39: Effects of LDA recomputation when including a newly labeled point in the existing data.", "rank": 1734, "start": 200980, "IsComparative": "0", "id": "st_1734"}, {"end": 201212, "text": "The arrow indicates the newly labeled point, and the red circles represent the distribution of the remaining unseen data in cluster 0.", "rank": 1735, "start": 201078, "IsComparative": "0", "id": "st_1735"}]}, {"paragraph_info": {"end": 201328, "start": 201212, "text": "CHAPTER VII VISIRR: AN INTERACTIVE VISUAL INFORMATION RETRIEVAL AND RECOMMENDER SYSTEM FOR LARGE-SCALE DOCUMENT DATA", "rank": 574, "paragraph_comparative_number": 1, "entities": [], "id": "p_574"}, "sentences": [{"end": 201328, "text": "CHAPTER VII VISIRR: AN INTERACTIVE VISUAL INFORMATION RETRIEVAL AND RECOMMENDER SYSTEM FOR LARGE-SCALE DOCUMENT DATA", "rank": 1736, "start": 201212, "IsComparative": "1", "id": "st_1736"}]}, {"paragraph_info": {"end": 202574, "start": 201328, "text": "We present a visual analytics system called VisIRR, which is an interactive visual information retrieval and recommendation system for document discovery.VisIRR effectively combines both the paradigms of passive pull through a query processes for retrieval and active push that recommends the items of potential interest based on the user preferences.Equipped with efficient dynamic query interfaces for a large corpus of document data, VisIRR visualizes the retrieved documents in a scatter plot form with their overall topic clusters.At the same time, based on interactive person- alized preference feedback on documents, VisIRR provides recommended documents reaching out to the entire corpus beyond the retrieved sets.Such recommended doc- uments are represented in the same scatter space of the retrieved documents so that users can perform integrated analyses of both retrieved and recommended documents seamlessly.We describe the state-of-the-art computational methods that make these integrated and informative representations as well as real time interaction possible.We illustrate the way the system works by using detailed usage scenarios.In addition, we present a preliminary user study that evaluates the effectiveness of the system.", "rank": 575, "paragraph_comparative_number": 1, "entities": [], "id": "p_575"}, "sentences": [{"end": 201482, "text": "We present a visual analytics system called VisIRR, which is an interactive visual information retrieval and recommendation system for document discovery.", "rank": 1737, "start": 201328, "IsComparative": "0", "id": "st_1737"}, {"end": 201679, "text": "VisIRR effectively combines both the paradigms of passive pull through a query processes for retrieval and active push that recommends the items of potential interest based on the user preferences.", "rank": 1738, "start": 201482, "IsComparative": "0", "id": "st_1738"}, {"end": 201864, "text": "Equipped with efficient dynamic query interfaces for a large corpus of document data, VisIRR visualizes the retrieved documents in a scatter plot form with their overall topic clusters.", "rank": 1739, "start": 201679, "IsComparative": "1", "id": "st_1739"}, {"end": 202050, "text": "At the same time, based on interactive person- alized preference feedback on documents, VisIRR provides recommended documents reaching out to the entire corpus beyond the retrieved sets.", "rank": 1740, "start": 201864, "IsComparative": "0", "id": "st_1740"}, {"end": 202249, "text": "Such recommended doc- uments are represented in the same scatter space of the retrieved documents so that users can perform integrated analyses of both retrieved and recommended documents seamlessly.", "rank": 1741, "start": 202050, "IsComparative": "0", "id": "st_1741"}, {"end": 202405, "text": "We describe the state-of-the-art computational methods that make these integrated and informative representations as well as real time interaction possible.", "rank": 1742, "start": 202249, "IsComparative": "0", "id": "st_1742"}, {"end": 202478, "text": "We illustrate the way the system works by using detailed usage scenarios.", "rank": 1743, "start": 202405, "IsComparative": "0", "id": "st_1743"}, {"end": 202574, "text": "In addition, we present a preliminary user study that evaluates the effectiveness of the system.", "rank": 1744, "start": 202478, "IsComparative": "0", "id": "st_1744"}]}, {"paragraph_info": {"end": 202590, "start": 202574, "text": "7.1 Introduction", "rank": 576, "paragraph_comparative_number": 0, "entities": [], "id": "p_576"}, "sentences": [{"end": 202590, "text": "7.1 Introduction", "rank": 1745, "start": 202574, "IsComparative": "0", "id": "st_1745"}]}, {"paragraph_info": {"end": 202839, "start": 202590, "text": "These days, researchers are faced with a deluge of new papers appearing each day, any of which might potentially contain a new development which could be critical to one of the questions he or she is investigating.The challenge is similar to that of", "rank": 577, "paragraph_comparative_number": 1, "entities": [], "id": "p_577"}, "sentences": [{"end": 202804, "text": "These days, researchers are faced with a deluge of new papers appearing each day, any of which might potentially contain a new development which could be critical to one of the questions he or she is investigating.", "rank": 1746, "start": 202590, "IsComparative": "1", "id": "st_1746"}, {"end": 202839, "text": "The challenge is similar to that of", "rank": 1747, "start": 202804, "IsComparative": "0", "id": "st_1747"}]}, {"paragraph_info": {"end": 204208, "start": 202839, "text": "Figure 40: An overview of the VisIRR system.Given about half a million academic papers in the system, the user can start by issuing a query (A), which in this case is a keyword disease.By performing clustering and dimension reduction, VisIRR visualizes the retrieved documents in a scatter plot and a table view (B) along with a topic cluster summary (B)(E).In the scatter plot view, a circular node represents a query-retrieved item, and a rectangular one denotes a recommended item.Their node size encodes the number of citations.After identifying a few documents of interest, the user can assign them his/her preference in a 5-star rating scale both in a scatter plot and in a table view.Based on this preference feedback, the system now provides a list of recommended items in another table view (C), and furthermore they are projected back to the existing scatter plot view (B) so that the consistent topical perspective can be maintained.To better understand the recommended items, the user can apply computational zoom-in on this set, which gives a clearer scatter plot with a more semantically meaningful summary (D).Finally, the system provides the option to choose different recommendation schemes based on contents, a citation network, and a co-authorship network.finding an available needle in a haystack each day, with limited attention and time resources.", "rank": 578, "paragraph_comparative_number": 5, "entities": [], "id": "p_578"}, "sentences": [{"end": 202883, "text": "Figure 40: An overview of the VisIRR system.", "rank": 1748, "start": 202839, "IsComparative": "0", "id": "st_1748"}, {"end": 203024, "text": "Given about half a million academic papers in the system, the user can start by issuing a query (A), which in this case is a keyword disease.", "rank": 1749, "start": 202883, "IsComparative": "0", "id": "st_1749"}, {"end": 203197, "text": "By performing clustering and dimension reduction, VisIRR visualizes the retrieved documents in a scatter plot and a table view (B) along with a topic cluster summary (B)(E).", "rank": 1750, "start": 203024, "IsComparative": "0", "id": "st_1750"}, {"end": 203323, "text": "In the scatter plot view, a circular node represents a query-retrieved item, and a rectangular one denotes a recommended item.", "rank": 1751, "start": 203197, "IsComparative": "0", "id": "st_1751"}, {"end": 203371, "text": "Their node size encodes the number of citations.", "rank": 1752, "start": 203323, "IsComparative": "1", "id": "st_1752"}, {"end": 203530, "text": "After identifying a few documents of interest, the user can assign them his/her preference in a 5-star rating scale both in a scatter plot and in a table view.", "rank": 1753, "start": 203371, "IsComparative": "1", "id": "st_1753"}, {"end": 203783, "text": "Based on this preference feedback, the system now provides a list of recommended items in another table view (C), and furthermore they are projected back to the existing scatter plot view (B) so that the consistent topical perspective can be maintained.", "rank": 1754, "start": 203530, "IsComparative": "0", "id": "st_1754"}, {"end": 203964, "text": "To better understand the recommended items, the user can apply computational zoom-in on this set, which gives a clearer scatter plot with a more semantically meaningful summary (D).", "rank": 1755, "start": 203783, "IsComparative": "1", "id": "st_1755"}, {"end": 204114, "text": "Finally, the system provides the option to choose different recommendation schemes based on contents, a citation network, and a co-authorship network.", "rank": 1756, "start": 203964, "IsComparative": "1", "id": "st_1756"}, {"end": 204208, "text": "finding an available needle in a haystack each day, with limited attention and time resources.", "rank": 1757, "start": 204114, "IsComparative": "1", "id": "st_1757"}]}, {"paragraph_info": {"end": 204797, "start": 204208, "text": "This problem regime is highly under-explored, compared to the billions that have been invested in the related paradigm of web search.Instead, the researcher or analyst is solving a subtle investigative problem for which each of several documents provides clues.By seeing this as an information retrieval (IR) problem, the focus in this chapter is on the long tail, or recall (making sure that few relevant documents are missed), while in web search the focus is generally on the quicker gratification of precision (making sure the first page of hits or so contain very relevant documents).", "rank": 579, "paragraph_comparative_number": 0, "entities": [], "id": "p_579"}, "sentences": [{"end": 204341, "text": "This problem regime is highly under-explored, compared to the billions that have been invested in the related paradigm of web search.", "rank": 1758, "start": 204208, "IsComparative": "0", "id": "st_1758"}, {"end": 204469, "text": "Instead, the researcher or analyst is solving a subtle investigative problem for which each of several documents provides clues.", "rank": 1759, "start": 204341, "IsComparative": "0", "id": "st_1759"}, {"end": 204797, "text": "By seeing this as an information retrieval (IR) problem, the focus in this chapter is on the long tail, or recall (making sure that few relevant documents are missed), while in web search the focus is generally on the quicker gratification of precision (making sure the first page of hits or so contain very relevant documents).", "rank": 1760, "start": 204469, "IsComparative": "0", "id": "st_1760"}]}, {"paragraph_info": {"end": 205506, "start": 204797, "text": "In general, search is a form of pull technology, in which the user takes actions by forming and issuing queries.However, in the former case where a high recall is concerned, what queries to issue, e.g., proper keywords, becomes crucial in order for users to obtain the documents of their interest.As a way to compensate this issue, a recommendation, or a push technology, with which the system finds things of interest to suggest to the individual user, has recently been popular in various domains.Whereas a search engine is more or less stateless and the same for all users, a recommendation system involves personalization, remembering aspects of the state of the users interests and investigations so far.", "rank": 580, "paragraph_comparative_number": 2, "entities": [], "id": "p_580"}, "sentences": [{"end": 204909, "text": "In general, search is a form of pull technology, in which the user takes actions by forming and issuing queries.", "rank": 1761, "start": 204797, "IsComparative": "1", "id": "st_1761"}, {"end": 205094, "text": "However, in the former case where a high recall is concerned, what queries to issue, e.g., proper keywords, becomes crucial in order for users to obtain the documents of their interest.", "rank": 1762, "start": 204909, "IsComparative": "1", "id": "st_1762"}, {"end": 205296, "text": "As a way to compensate this issue, a recommendation, or a push technology, with which the system finds things of interest to suggest to the individual user, has recently been popular in various domains.", "rank": 1763, "start": 205094, "IsComparative": "0", "id": "st_1763"}, {"end": 205506, "text": "Whereas a search engine is more or less stateless and the same for all users, a recommendation system involves personalization, remembering aspects of the state of the users interests and investigations so far.", "rank": 1764, "start": 205296, "IsComparative": "0", "id": "st_1764"}]}, {"paragraph_info": {"end": 206153, "start": 205506, "text": "In the context of visual analytics, document analyses have long been one of the main areas studied.Visual analytics systems for document data, such as IN-SPIRE <138> and JIGSAW <121>, can help to give an overall understanding about a set of documents as well as revealing their intra-set relationships that would have been difficult and time-consuming without the help of interactive visualization.However, despite the fact that personalized recommendations seem to be a natural fit with interactive visualization in that it directly utilizes the history of user interactions, there are few instances of such work in the visual analytic community.", "rank": 581, "paragraph_comparative_number": 3, "entities": [], "id": "p_581"}, "sentences": [{"end": 205605, "text": "In the context of visual analytics, document analyses have long been one of the main areas studied.", "rank": 1765, "start": 205506, "IsComparative": "1", "id": "st_1765"}, {"end": 205904, "text": "Visual analytics systems for document data, such as IN-SPIRE <138> and JIGSAW <121>, can help to give an overall understanding about a set of documents as well as revealing their intra-set relationships that would have been difficult and time-consuming without the help of interactive visualization.", "rank": 1766, "start": 205605, "IsComparative": "1", "id": "st_1766"}, {"end": 206153, "text": "However, despite the fact that personalized recommendations seem to be a natural fit with interactive visualization in that it directly utilizes the history of user interactions, there are few instances of such work in the visual analytic community.", "rank": 1767, "start": 205904, "IsComparative": "1", "id": "st_1767"}]}, {"paragraph_info": {"end": 206833, "start": 206153, "text": "As one of the milestones to fill this gap, we present a novel document visual an- alytics system called VisIRR, an interactive Visual Information Retrieval and Recommendation for document data, which effectively combines traditional query- based information retrieval and personalized recommendation.Basically, as seen in Fig.40, VisIRR adopts a scatter plot as a main visualization form similar to IN- SPIRE.In other words, the documents to be visualized are first clustered into several groups via a clustering algorithm and then projected to a 2D space via a dimension reduction algorithm.However, VisIRR features various novel aspects compared to existing systems, as follows.", "rank": 582, "paragraph_comparative_number": 0, "entities": [], "id": "p_582"}, "sentences": [{"end": 206453, "text": "As one of the milestones to fill this gap, we present a novel document visual an- alytics system called VisIRR, an interactive Visual Information Retrieval and Recommendation for document data, which effectively combines traditional query- based information retrieval and personalized recommendation.", "rank": 1768, "start": 206153, "IsComparative": "0", "id": "st_1768"}, {"end": 206479, "text": "Basically, as seen in Fig.", "rank": 1769, "start": 206453, "IsComparative": "0", "id": "st_1769"}, {"end": 206562, "text": "40, VisIRR adopts a scatter plot as a main visualization form similar to IN- SPIRE.", "rank": 1770, "start": 206479, "IsComparative": "0", "id": "st_1770"}, {"end": 206745, "text": "In other words, the documents to be visualized are first clustered into several groups via a clustering algorithm and then projected to a 2D space via a dimension reduction algorithm.", "rank": 1771, "start": 206562, "IsComparative": "0", "id": "st_1771"}, {"end": 206833, "text": "However, VisIRR features various novel aspects compared to existing systems, as follows.", "rank": 1772, "start": 206745, "IsComparative": "0", "id": "st_1772"}]}, {"paragraph_info": {"end": 207053, "start": 206833, "text": "Efficient large scale data processing: VisIRR currently handles about half a million documents and scales linearly with respect to newly added documents in terms of the amount of the required computation and memory size.", "rank": 583, "paragraph_comparative_number": 1, "entities": [], "id": "p_583"}, "sentences": [{"end": 207053, "text": "Efficient large scale data processing: VisIRR currently handles about half a million documents and scales linearly with respect to newly added documents in terms of the amount of the required computation and memory size.", "rank": 1773, "start": 206833, "IsComparative": "1", "id": "st_1773"}]}, {"paragraph_info": {"end": 207694, "start": 207053, "text": "Advanced clustering and dimension reduction techniques: As core computational modules, VisIRR adopts state-of-the-art techniques such as nonnegative matrix factorization (NMF) for clustering and linear discriminant analysis (LDA) for dimension reduction.These techniques give the results with a much better quality as well as with a faster computational time than traditional methods including k-means, principal component analysis (PCA), and multidimensional scaling.Additionally, VisIRR provides an alignment capability for both clus- tering and dimension reduction to facilitate easy comparisons between different visualization snapshots.", "rank": 584, "paragraph_comparative_number": 1, "entities": [], "id": "p_584"}, "sentences": [{"end": 207307, "text": "Advanced clustering and dimension reduction techniques: As core computational modules, VisIRR adopts state-of-the-art techniques such as nonnegative matrix factorization (NMF) for clustering and linear discriminant analysis (LDA) for dimension reduction.", "rank": 1774, "start": 207053, "IsComparative": "0", "id": "st_1774"}, {"end": 207521, "text": "These techniques give the results with a much better quality as well as with a faster computational time than traditional methods including k-means, principal component analysis (PCA), and multidimensional scaling.", "rank": 1775, "start": 207307, "IsComparative": "0", "id": "st_1775"}, {"end": 207694, "text": "Additionally, VisIRR provides an alignment capability for both clus- tering and dimension reduction to facilitate easy comparisons between different visualization snapshots.", "rank": 1776, "start": 207521, "IsComparative": "1", "id": "st_1776"}]}, {"paragraph_info": {"end": 208187, "start": 207694, "text": "Preference-based personalized recommendation: In addition to exploratory anal- ysis of query-retrieved results, VisIRR supports recommendation of potentially interesting documents to users based on the preferences users assign to docu- ments.This recommendation enables users to discover those documents users query processes cannot reveal easily.The back-end recommendation module, which is based on PageRank-style graph diffusion algorithm <98>, performs effi- ciently with large-scale data.", "rank": 585, "paragraph_comparative_number": 1, "entities": [], "id": "p_585"}, "sentences": [{"end": 207936, "text": "Preference-based personalized recommendation: In addition to exploratory anal- ysis of query-retrieved results, VisIRR supports recommendation of potentially interesting documents to users based on the preferences users assign to docu- ments.", "rank": 1777, "start": 207694, "IsComparative": "0", "id": "st_1777"}, {"end": 208041, "text": "This recommendation enables users to discover those documents users query processes cannot reveal easily.", "rank": 1778, "start": 207936, "IsComparative": "0", "id": "st_1778"}, {"end": 208187, "text": "The back-end recommendation module, which is based on PageRank-style graph diffusion algorithm <98>, performs effi- ciently with large-scale data.", "rank": 1779, "start": 208041, "IsComparative": "1", "id": "st_1779"}]}, {"paragraph_info": {"end": 209072, "start": 208187, "text": "To integrate all these capabilities into a mature visual analytics system, we incor- porate various building blocks for front-end GUIs and back-end computational al- gorithms.This chapter mainly presents these building blocks in more detail with detailed usage scenarios.The rest of this chapter is organized as follows.Section 7.2 discusses related work.Section 7.3 explains the front-end GUI modules and comprehensive usage scenarios that highlight the key capabilities of the system.Af- terwards, Section 7.4 mainly discusses how we efficiently handle all the necessary information from a large-scale data corpus with a scalable expansion, and Section 7.5 describes computational methods used in the back-end of the system.Section 7.6 briefly presents the user study we conducted to evaluate the system.Finally, Section 7.7 concludes the chapter and discusses about the future work.", "rank": 586, "paragraph_comparative_number": 4, "entities": [], "id": "p_586"}, "sentences": [{"end": 208362, "text": "To integrate all these capabilities into a mature visual analytics system, we incor- porate various building blocks for front-end GUIs and back-end computational al- gorithms.", "rank": 1780, "start": 208187, "IsComparative": "1", "id": "st_1780"}, {"end": 208458, "text": "This chapter mainly presents these building blocks in more detail with detailed usage scenarios.", "rank": 1781, "start": 208362, "IsComparative": "1", "id": "st_1781"}, {"end": 208507, "text": "The rest of this chapter is organized as follows.", "rank": 1782, "start": 208458, "IsComparative": "0", "id": "st_1782"}, {"end": 208542, "text": "Section 7.2 discusses related work.", "rank": 1783, "start": 208507, "IsComparative": "1", "id": "st_1783"}, {"end": 208673, "text": "Section 7.3 explains the front-end GUI modules and comprehensive usage scenarios that highlight the key capabilities of the system.", "rank": 1784, "start": 208542, "IsComparative": "1", "id": "st_1784"}, {"end": 208913, "text": "Af- terwards, Section 7.4 mainly discusses how we efficiently handle all the necessary information from a large-scale data corpus with a scalable expansion, and Section 7.5 describes computational methods used in the back-end of the system.", "rank": 1785, "start": 208673, "IsComparative": "0", "id": "st_1785"}, {"end": 208993, "text": "Section 7.6 briefly presents the user study we conducted to evaluate the system.", "rank": 1786, "start": 208913, "IsComparative": "0", "id": "st_1786"}, {"end": 209072, "text": "Finally, Section 7.7 concludes the chapter and discusses about the future work.", "rank": 1787, "start": 208993, "IsComparative": "0", "id": "st_1787"}]}, {"paragraph_info": {"end": 209088, "start": 209072, "text": "7.2 Related Work", "rank": 587, "paragraph_comparative_number": 0, "entities": [], "id": "p_587"}, "sentences": [{"end": 209088, "text": "7.2 Related Work", "rank": 1788, "start": 209072, "IsComparative": "0", "id": "st_1788"}]}, {"paragraph_info": {"end": 209545, "start": 209088, "text": "Information seeking behavior is a complex human activity, and one that varies dramat- ically with system capabilities and users model of those capabilities <93>.Ill-defined document search tasks such as literature searches are often termed exploratory search tasks, in contrast with well-defined tasks such as finding a known, specific item from among a set.In the past, traditional information retrieval has focused much more on the latter than the former.", "rank": 588, "paragraph_comparative_number": 2, "entities": [], "id": "p_588"}, "sentences": [{"end": 209249, "text": "Information seeking behavior is a complex human activity, and one that varies dramat- ically with system capabilities and users model of those capabilities <93>.", "rank": 1789, "start": 209088, "IsComparative": "1", "id": "st_1789"}, {"end": 209446, "text": "Ill-defined document search tasks such as literature searches are often termed exploratory search tasks, in contrast with well-defined tasks such as finding a known, specific item from among a set.", "rank": 1790, "start": 209249, "IsComparative": "1", "id": "st_1790"}, {"end": 209545, "text": "In the past, traditional information retrieval has focused much more on the latter than the former.", "rank": 1791, "start": 209446, "IsComparative": "0", "id": "st_1791"}]}, {"paragraph_info": {"end": 210897, "start": 209545, "text": "In the context of exploratory interfaces, information foraging <106> and scent the- ory <105> suggest making clusters of related data clear and facilitating the process of finding new clusters of interest.To that end, many search result visualization sys- tems also work in concert with automated clustering algorithms, especially when the information space is extremely large or unstructured.The Pacific Northwest National Labs SPIRE system (and IN-SPIRE follow-on) uses clustering to extract common themes, and includes several visualization components <138>.Its Themescape compo- nent is an abstract 3D landscape depiction of a document space, with arrangements of hills and valleys representing the relatively strength of various themes in the docu- ment corpus and how those themes interrelate.Other systems have used this general clusters-in-landscapes (both 2D and 3D) as well <116, 21, 6>.iVisClustering <88> is an interactive document clustering system focused on the user interactions to improve cluster quality based on an advanced technique called latent Dirichlet allocation <20>.On the other hand, rather than providing user interactions customized to a partic- ular clustering technique, the Testbed system <32> offers a wide variety of clustering algorithms and easy comparisons between them via an alignment process VisIRR has adopted.", "rank": 589, "paragraph_comparative_number": 5, "entities": [], "id": "p_589"}, "sentences": [{"end": 209750, "text": "In the context of exploratory interfaces, information foraging <106> and scent the- ory <105> suggest making clusters of related data clear and facilitating the process of finding new clusters of interest.", "rank": 1792, "start": 209545, "IsComparative": "1", "id": "st_1792"}, {"end": 209938, "text": "To that end, many search result visualization sys- tems also work in concert with automated clustering algorithms, especially when the information space is extremely large or unstructured.", "rank": 1793, "start": 209750, "IsComparative": "1", "id": "st_1793"}, {"end": 210106, "text": "The Pacific Northwest National Labs SPIRE system (and IN-SPIRE follow-on) uses clustering to extract common themes, and includes several visualization components <138>.", "rank": 1794, "start": 209938, "IsComparative": "1", "id": "st_1794"}, {"end": 210344, "text": "Its Themescape compo- nent is an abstract 3D landscape depiction of a document space, with arrangements of hills and valleys representing the relatively strength of various themes in the docu- ment corpus and how those themes interrelate.", "rank": 1795, "start": 210106, "IsComparative": "1", "id": "st_1795"}, {"end": 210442, "text": "Other systems have used this general clusters-in-landscapes (both 2D and 3D) as well <116, 21, 6>.", "rank": 1796, "start": 210344, "IsComparative": "0", "id": "st_1796"}, {"end": 210638, "text": "iVisClustering <88> is an interactive document clustering system focused on the user interactions to improve cluster quality based on an advanced technique called latent Dirichlet allocation <20>.", "rank": 1797, "start": 210442, "IsComparative": "1", "id": "st_1797"}, {"end": 210897, "text": "On the other hand, rather than providing user interactions customized to a partic- ular clustering technique, the Testbed system <32> offers a wide variety of clustering algorithms and easy comparisons between them via an alignment process VisIRR has adopted.", "rank": 1798, "start": 210638, "IsComparative": "0", "id": "st_1798"}]}, {"paragraph_info": {"end": 211172, "start": 210897, "text": "Using visualization for exploring text data is an active research area within and among many fields.Here, we highlight only a sample of relevant work from different areas and refer the reader to a recent survey of visual text analytics <4> for a more comprehensive treatment.", "rank": 590, "paragraph_comparative_number": 2, "entities": [], "id": "p_590"}, "sentences": [{"end": 210997, "text": "Using visualization for exploring text data is an active research area within and among many fields.", "rank": 1799, "start": 210897, "IsComparative": "1", "id": "st_1799"}, {"end": 211172, "text": "Here, we highlight only a sample of relevant work from different areas and refer the reader to a recent survey of visual text analytics <4> for a more comprehensive treatment.", "rank": 1800, "start": 210997, "IsComparative": "1", "id": "st_1800"}]}, {"paragraph_info": {"end": 211808, "start": 211172, "text": "Unsurprisingly, visualization of document collections has been explored for some time in library science.A relatively early example is the Envision digital library, which includes a visualization system that places documents in a 2D grid according to user-selectable attributes <97>.Systems have used various information visualization techniques such as hyperbolic trees <76, 119> and treemaps <62, 41> to visualize results.Curated collections such as those found in digital libraries more often have pre-formed hierarchies to leverage in visual analytics applications, but simple clustering methods have been implemented as well <119>.", "rank": 591, "paragraph_comparative_number": 2, "entities": [], "id": "p_591"}, "sentences": [{"end": 211277, "text": "Unsurprisingly, visualization of document collections has been explored for some time in library science.", "rank": 1801, "start": 211172, "IsComparative": "1", "id": "st_1801"}, {"end": 211455, "text": "A relatively early example is the Envision digital library, which includes a visualization system that places documents in a 2D grid according to user-selectable attributes <97>.", "rank": 1802, "start": 211277, "IsComparative": "0", "id": "st_1802"}, {"end": 211596, "text": "Systems have used various information visualization techniques such as hyperbolic trees <76, 119> and treemaps <62, 41> to visualize results.", "rank": 1803, "start": 211455, "IsComparative": "1", "id": "st_1803"}, {"end": 211808, "text": "Curated collections such as those found in digital libraries more often have pre-formed hierarchies to leverage in visual analytics applications, but simple clustering methods have been implemented as well <119>.", "rank": 1804, "start": 211596, "IsComparative": "0", "id": "st_1804"}]}, {"paragraph_info": {"end": 212574, "start": 211808, "text": "When document categories and groupings are not already extant, automated methods of clustering and classifying collections are key to exploratory tools, includ- ing those supporting visual analysis.A recent survey <4, 63> distinguishes between the visualization of a single document (e.g., tag clouds) and a document collection and between time- (e.g., TIARA <134>) and network-oriented collection systems.Because VisIRRs clustering system implicitly creates relationships among members (and its graph diffusion-based recommendation system explicitly uses such data), examples of the last category are most relevant.Jigsaw <121> visualizes network relationships between documents and various entities, e.g., actors, events, etc., automatically ex- tracted from them.", "rank": 592, "paragraph_comparative_number": 2, "entities": [], "id": "p_592"}, "sentences": [{"end": 212006, "text": "When document categories and groupings are not already extant, automated methods of clustering and classifying collections are key to exploratory tools, includ- ing those supporting visual analysis.", "rank": 1805, "start": 211808, "IsComparative": "1", "id": "st_1805"}, {"end": 212214, "text": "A recent survey <4, 63> distinguishes between the visualization of a single document (e.g., tag clouds) and a document collection and between time- (e.g., TIARA <134>) and network-oriented collection systems.", "rank": 1806, "start": 212006, "IsComparative": "1", "id": "st_1806"}, {"end": 212424, "text": "Because VisIRRs clustering system implicitly creates relationships among members (and its graph diffusion-based recommendation system explicitly uses such data), examples of the last category are most relevant.", "rank": 1807, "start": 212214, "IsComparative": "0", "id": "st_1807"}, {"end": 212574, "text": "Jigsaw <121> visualizes network relationships between documents and various entities, e.g., actors, events, etc., automatically ex- tracted from them.", "rank": 1808, "start": 212424, "IsComparative": "0", "id": "st_1808"}]}, {"paragraph_info": {"end": 213444, "start": 212574, "text": "A recently proposed Apolo system <27> uses a mixed-initiative approach that boot- straps initial user-specified categories and classifications into more comprehensive system-suggested categorization of new documents.However, Apolo is exemplar- based method where the user is assumed to clearly have a few of documents of their interest.In this sense, Apolo mainly supports a bottom-up style of analyses.On the contrary, VisIRR initially takes a top-down approach in that it initially starts from an overview visualization of a potentially fairly large amount of documents retrieved by user queries.Once the documents of the users interest is identified, however, VisIRR also supports a bottom-up style approach via recommendation processes based on the user preferences on particular documents, thereby gradually expanding the users scope beyond the query-retrieved set.", "rank": 593, "paragraph_comparative_number": 1, "entities": [], "id": "p_593"}, "sentences": [{"end": 212790, "text": "A recently proposed Apolo system <27> uses a mixed-initiative approach that boot- straps initial user-specified categories and classifications into more comprehensive system-suggested categorization of new documents.", "rank": 1809, "start": 212574, "IsComparative": "0", "id": "st_1809"}, {"end": 212910, "text": "However, Apolo is exemplar- based method where the user is assumed to clearly have a few of documents of their interest.", "rank": 1810, "start": 212790, "IsComparative": "1", "id": "st_1810"}, {"end": 212977, "text": "In this sense, Apolo mainly supports a bottom-up style of analyses.", "rank": 1811, "start": 212910, "IsComparative": "0", "id": "st_1811"}, {"end": 213172, "text": "On the contrary, VisIRR initially takes a top-down approach in that it initially starts from an overview visualization of a potentially fairly large amount of documents retrieved by user queries.", "rank": 1812, "start": 212977, "IsComparative": "0", "id": "st_1812"}, {"end": 213444, "text": "Once the documents of the users interest is identified, however, VisIRR also supports a bottom-up style approach via recommendation processes based on the user preferences on particular documents, thereby gradually expanding the users scope beyond the query-retrieved set.", "rank": 1813, "start": 213172, "IsComparative": "0", "id": "st_1813"}]}, {"paragraph_info": {"end": 214082, "start": 213444, "text": "There has been significant commercial and academic interest in the topic of ex- ploratory search for scientific literature for some time.Several commercial tools are targeted to this problem, with a variety of automated and visual features.Google Scholar <1> automatically extracts research works and their citation networks, but has few visual or recommendation features.The Microsoft Academic Search system from Microsoft Research <2> is a similar offering that also includes more advanced network- style visualization of authorship connections as well as various ways of examining topical, institutional, and venue trends and rankings.", "rank": 594, "paragraph_comparative_number": 2, "entities": [], "id": "p_594"}, "sentences": [{"end": 213581, "text": "There has been significant commercial and academic interest in the topic of ex- ploratory search for scientific literature for some time.", "rank": 1814, "start": 213444, "IsComparative": "0", "id": "st_1814"}, {"end": 213684, "text": "Several commercial tools are targeted to this problem, with a variety of automated and visual features.", "rank": 1815, "start": 213581, "IsComparative": "1", "id": "st_1815"}, {"end": 213816, "text": "Google Scholar <1> automatically extracts research works and their citation networks, but has few visual or recommendation features.", "rank": 1816, "start": 213684, "IsComparative": "1", "id": "st_1816"}, {"end": 214082, "text": "The Microsoft Academic Search system from Microsoft Research <2> is a similar offering that also includes more advanced network- style visualization of authorship connections as well as various ways of examining topical, institutional, and venue trends and rankings.", "rank": 1817, "start": 213816, "IsComparative": "0", "id": "st_1817"}]}, {"paragraph_info": {"end": 215116, "start": 214082, "text": "Direct introspection of the academic research process has been a common topic in academia as well.One variation is automated recommender/matching systems, often applied to the problem of matching individual papers from a corpus to indi- viduals from a slate of candidate reviewers <14, 132>.More relevant to VisIRR are those systems that are more exploratory or analytical in nature.The Action Science Explorer (ASE) <52> focuses on co-citation network visualization, with document clus- ters created manually or by heuristics <95>.It also includes full-text citation context features not available on VisIRR.The FacetAtlas system <24> automatically clusters document collections using a Kernel density estimation algorithm and provides multi- faceted links between document nodes (rather than just keyword or author searches as in VisIRR).CiteSpace II <28> is a visual tool for identifying new or old research trends in a given set of documents (assumed to be a relatively coherent set produced by a keyword query on a large corpus).", "rank": 595, "paragraph_comparative_number": 1, "entities": [], "id": "p_595"}, "sentences": [{"end": 214180, "text": "Direct introspection of the academic research process has been a common topic in academia as well.", "rank": 1818, "start": 214082, "IsComparative": "0", "id": "st_1818"}, {"end": 214373, "text": "One variation is automated recommender/matching systems, often applied to the problem of matching individual papers from a corpus to indi- viduals from a slate of candidate reviewers <14, 132>.", "rank": 1819, "start": 214180, "IsComparative": "1", "id": "st_1819"}, {"end": 214465, "text": "More relevant to VisIRR are those systems that are more exploratory or analytical in nature.", "rank": 1820, "start": 214373, "IsComparative": "0", "id": "st_1820"}, {"end": 214614, "text": "The Action Science Explorer (ASE) <52> focuses on co-citation network visualization, with document clus- ters created manually or by heuristics <95>.", "rank": 1821, "start": 214465, "IsComparative": "0", "id": "st_1821"}, {"end": 214691, "text": "It also includes full-text citation context features not available on VisIRR.", "rank": 1822, "start": 214614, "IsComparative": "0", "id": "st_1822"}, {"end": 214922, "text": "The FacetAtlas system <24> automatically clusters document collections using a Kernel density estimation algorithm and provides multi- faceted links between document nodes (rather than just keyword or author searches as in VisIRR).", "rank": 1823, "start": 214691, "IsComparative": "0", "id": "st_1823"}, {"end": 215116, "text": "CiteSpace II <28> is a visual tool for identifying new or old research trends in a given set of documents (assumed to be a relatively coherent set produced by a keyword query on a large corpus).", "rank": 1824, "start": 214922, "IsComparative": "0", "id": "st_1824"}]}, {"paragraph_info": {"end": 215357, "start": 215116, "text": "However, none of these systems include one of VisIRRs key contributions: a user-driven recommendation system that explicitly includes relevant documents from the larger search space vs.a dramatically reduced one from an initial search query.", "rank": 596, "paragraph_comparative_number": 0, "entities": [], "id": "p_596"}, "sentences": [{"end": 215301, "text": "However, none of these systems include one of VisIRRs key contributions: a user-driven recommendation system that explicitly includes relevant documents from the larger search space vs.", "rank": 1825, "start": 215116, "IsComparative": "0", "id": "st_1825"}, {"end": 215357, "text": "a dramatically reduced one from an initial search query.", "rank": 1826, "start": 215301, "IsComparative": "0", "id": "st_1826"}]}, {"paragraph_info": {"end": 215387, "start": 215357, "text": "7.3 VisIRR Design and Function", "rank": 597, "paragraph_comparative_number": 0, "entities": [], "id": "p_597"}, "sentences": [{"end": 215387, "text": "7.3 VisIRR Design and Function", "rank": 1827, "start": 215357, "IsComparative": "0", "id": "st_1827"}]}, {"paragraph_info": {"end": 215538, "start": 215387, "text": "In this section, we briefly introduce the user interfaces of VisIRR and describe example analysis scenarios to demonstrate how VisIRR works in detail.1", "rank": 598, "paragraph_comparative_number": 0, "entities": [], "id": "p_598"}, "sentences": [{"end": 215538, "text": "In this section, we briefly introduce the user interfaces of VisIRR and describe example analysis scenarios to demonstrate how VisIRR works in detail.1", "rank": 1828, "start": 215387, "IsComparative": "0", "id": "st_1828"}]}, {"paragraph_info": {"end": 215558, "start": 215538, "text": "7.3.1 User Interface", "rank": 599, "paragraph_comparative_number": 0, "entities": [], "id": "p_599"}, "sentences": [{"end": 215558, "text": "7.3.1 User Interface", "rank": 1829, "start": 215538, "IsComparative": "0", "id": "st_1829"}]}, {"paragraph_info": {"end": 216055, "start": 215558, "text": "The user interface of VisIRR is mainly composed of four parts.The Query Bar at the top (Fig.40(A)) enables users to issue queries dynamically using various fields such as a keyword, an author name, a publication year, and a citation count.The Scatter Plot view (with document details shown in the lower table) (Fig.40(B)) visualizes the retrieved documents (as well as any recommended documents) with their cluster summary labels.The color and the size of each node in a scatter plot represent the", "rank": 600, "paragraph_comparative_number": 3, "entities": [], "id": "p_600"}, "sentences": [{"end": 215620, "text": "The user interface of VisIRR is mainly composed of four parts.", "rank": 1830, "start": 215558, "IsComparative": "1", "id": "st_1830"}, {"end": 215650, "text": "The Query Bar at the top (Fig.", "rank": 1831, "start": 215620, "IsComparative": "0", "id": "st_1831"}, {"end": 215797, "text": "40(A)) enables users to issue queries dynamically using various fields such as a keyword, an author name, a publication year, and a citation count.", "rank": 1832, "start": 215650, "IsComparative": "1", "id": "st_1832"}, {"end": 215873, "text": "The Scatter Plot view (with document details shown in the lower table) (Fig.", "rank": 1833, "start": 215797, "IsComparative": "0", "id": "st_1833"}, {"end": 215988, "text": "40(B)) visualizes the retrieved documents (as well as any recommended documents) with their cluster summary labels.", "rank": 1834, "start": 215873, "IsComparative": "1", "id": "st_1834"}, {"end": 216055, "text": "The color and the size of each node in a scatter plot represent the", "rank": 1835, "start": 215988, "IsComparative": "0", "id": "st_1835"}]}, {"paragraph_info": {"end": 216982, "start": 216055, "text": "1A high-quality video introducing VisIRR is available at http://www.cc.gatech.edu/~joyfull/ vast13/visirr/visirr_final.html cluster it belongs to and its citation count, respectively.Such a view can also be gen- erated from any user-selected subset of data (Fig.40(D)).The Recommendation view on the top left (Fig.40(C)) provides tabular representations of the documents whose ratings have been assigned by users (Fig.40(C) upper table) as well as the resulting recommended documents (Fig.40(C) lower table).These recommended documents are also visualized in the Scatter Plot view as rectangles while the query-retrieved documents are shown as circles.Finally, the Label panel provides additional controls such as highlighting and/or hiding particular clusters, changing how cluster summary labels are chosen, and showing direct edge relationships from rated documents to their system-derived recommended documents (Fig.40(E)).", "rank": 601, "paragraph_comparative_number": 3, "entities": [], "id": "p_601"}, "sentences": [{"end": 216238, "text": "1A high-quality video introducing VisIRR is available at http://www.cc.gatech.edu/~joyfull/ vast13/visirr/visirr_final.html cluster it belongs to and its citation count, respectively.", "rank": 1836, "start": 216055, "IsComparative": "1", "id": "st_1836"}, {"end": 216317, "text": "Such a view can also be gen- erated from any user-selected subset of data (Fig.", "rank": 1837, "start": 216238, "IsComparative": "0", "id": "st_1837"}, {"end": 216324, "text": "40(D)).", "rank": 1838, "start": 216317, "IsComparative": "0", "id": "st_1838"}, {"end": 216369, "text": "The Recommendation view on the top left (Fig.", "rank": 1839, "start": 216324, "IsComparative": "0", "id": "st_1839"}, {"end": 216473, "text": "40(C)) provides tabular representations of the documents whose ratings have been assigned by users (Fig.", "rank": 1840, "start": 216369, "IsComparative": "0", "id": "st_1840"}, {"end": 216544, "text": "40(C) upper table) as well as the resulting recommended documents (Fig.", "rank": 1841, "start": 216473, "IsComparative": "0", "id": "st_1841"}, {"end": 216563, "text": "40(C) lower table).", "rank": 1842, "start": 216544, "IsComparative": "1", "id": "st_1842"}, {"end": 216707, "text": "These recommended documents are also visualized in the Scatter Plot view as rectangles while the query-retrieved documents are shown as circles.", "rank": 1843, "start": 216563, "IsComparative": "0", "id": "st_1843"}, {"end": 216975, "text": "Finally, the Label panel provides additional controls such as highlighting and/or hiding particular clusters, changing how cluster summary labels are chosen, and showing direct edge relationships from rated documents to their system-derived recommended documents (Fig.", "rank": 1844, "start": 216707, "IsComparative": "1", "id": "st_1844"}, {"end": 216982, "text": "40(E)).", "rank": 1845, "start": 216975, "IsComparative": "0", "id": "st_1845"}]}, {"paragraph_info": {"end": 217003, "start": 216982, "text": "7.3.2 Usage Scenarios", "rank": 602, "paragraph_comparative_number": 0, "entities": [], "id": "p_602"}, "sentences": [{"end": 217003, "text": "7.3.2 Usage Scenarios", "rank": 1846, "start": 216982, "IsComparative": "0", "id": "st_1846"}]}, {"paragraph_info": {"end": 217370, "start": 217003, "text": "VisIRR has been implemented using a modified version of the ArnetMiner dataset, which contains approximately 430,000 academic research articles from a variety of disciplines and venues (primarily conferences, journals and books), as will be described in detail in Section 7.4.The following scenarios illustrate the utility of VisIRR for tasks related to this dataset.", "rank": 603, "paragraph_comparative_number": 1, "entities": [], "id": "p_603"}, "sentences": [{"end": 217279, "text": "VisIRR has been implemented using a modified version of the ArnetMiner dataset, which contains approximately 430,000 academic research articles from a variety of disciplines and venues (primarily conferences, journals and books), as will be described in detail in Section 7.4.", "rank": 1847, "start": 217003, "IsComparative": "1", "id": "st_1847"}, {"end": 217370, "text": "The following scenarios illustrate the utility of VisIRR for tasks related to this dataset.", "rank": 1848, "start": 217279, "IsComparative": "0", "id": "st_1848"}]}, {"paragraph_info": {"end": 217424, "start": 217370, "text": "7.3.2.1 A Visual Overview of Query-Retrieved Documents", "rank": 604, "paragraph_comparative_number": 1, "entities": [], "id": "p_604"}, "sentences": [{"end": 217424, "text": "7.3.2.1 A Visual Overview of Query-Retrieved Documents", "rank": 1849, "start": 217370, "IsComparative": "1", "id": "st_1849"}]}, {"paragraph_info": {"end": 218174, "start": 217424, "text": "The user starts by issuing queries from the Query Toolbar.Suppose the user issues a query of keyword disease from a title field.Once documents are retrieved due to this query, the clustering and dimension reduction steps are performed to generate the Scatter plot view (Fig.40(A)).Since most clusters contain the keyword disease, the user can adjust a slider in the Label panel in order to obtain more distinctive cluster summaries, as shown in Fig.41.From the Scatter plot view, the user can drill down to a cluster of interest, e.g., the clusters about gene expression data (the top right), and image analysis (the top left).By moving a mouse pointer to a data point, the user can check the document details via a tooltip text and also skim through", "rank": 605, "paragraph_comparative_number": 4, "entities": [], "id": "p_605"}, "sentences": [{"end": 217482, "text": "The user starts by issuing queries from the Query Toolbar.", "rank": 1850, "start": 217424, "IsComparative": "0", "id": "st_1850"}, {"end": 217552, "text": "Suppose the user issues a query of keyword disease from a title field.", "rank": 1851, "start": 217482, "IsComparative": "0", "id": "st_1851"}, {"end": 217698, "text": "Once documents are retrieved due to this query, the clustering and dimension reduction steps are performed to generate the Scatter plot view (Fig.", "rank": 1852, "start": 217552, "IsComparative": "0", "id": "st_1852"}, {"end": 217705, "text": "40(A)).", "rank": 1853, "start": 217698, "IsComparative": "1", "id": "st_1853"}, {"end": 217873, "text": "Since most clusters contain the keyword disease, the user can adjust a slider in the Label panel in order to obtain more distinctive cluster summaries, as shown in Fig.", "rank": 1854, "start": 217705, "IsComparative": "1", "id": "st_1854"}, {"end": 217876, "text": "41.", "rank": 1855, "start": 217873, "IsComparative": "1", "id": "st_1855"}, {"end": 218051, "text": "From the Scatter plot view, the user can drill down to a cluster of interest, e.g., the clusters about gene expression data (the top right), and image analysis (the top left).", "rank": 1856, "start": 217876, "IsComparative": "1", "id": "st_1856"}, {"end": 218174, "text": "By moving a mouse pointer to a data point, the user can check the document details via a tooltip text and also skim through", "rank": 1857, "start": 218051, "IsComparative": "0", "id": "st_1857"}]}, {"paragraph_info": {"end": 218230, "start": 218174, "text": "(a) Default cluster summary (b) Distinct cluster summary", "rank": 606, "paragraph_comparative_number": 0, "entities": [], "id": "p_606"}, "sentences": [{"end": 218230, "text": "(a) Default cluster summary (b) Distinct cluster summary", "rank": 1858, "start": 218174, "IsComparative": "0", "id": "st_1858"}]}, {"paragraph_info": {"end": 218552, "start": 218230, "text": "Figure 41: A Comparison between default and distinct cluster summaries.Since all the documents include the query word disease, most clusters contain this word as one of the most frequent keywords (a).By adjusting the slider of common-vs-unique words in the Label panel, the cluster summary shows much clearer meanings (b).", "rank": 607, "paragraph_comparative_number": 2, "entities": [], "id": "p_607"}, "sentences": [{"end": 218301, "text": "Figure 41: A Comparison between default and distinct cluster summaries.", "rank": 1859, "start": 218230, "IsComparative": "1", "id": "st_1859"}, {"end": 218430, "text": "Since all the documents include the query word disease, most clusters contain this word as one of the most frequent keywords (a).", "rank": 1860, "start": 218301, "IsComparative": "0", "id": "st_1860"}, {"end": 218552, "text": "By adjusting the slider of common-vs-unique words in the Label panel, the cluster summary shows much clearer meanings (b).", "rank": 1861, "start": 218430, "IsComparative": "1", "id": "st_1861"}]}, {"paragraph_info": {"end": 218727, "start": 218552, "text": "the document list in the lower table, which is by default sorted by the number of citations.The user can also pan and zoom to enlarge a particular cluster or area of interest.", "rank": 608, "paragraph_comparative_number": 0, "entities": [], "id": "p_608"}, "sentences": [{"end": 218644, "text": "the document list in the lower table, which is by default sorted by the number of citations.", "rank": 1862, "start": 218552, "IsComparative": "0", "id": "st_1862"}, {"end": 218727, "text": "The user can also pan and zoom to enlarge a particular cluster or area of interest.", "rank": 1863, "start": 218644, "IsComparative": "0", "id": "st_1863"}]}, {"paragraph_info": {"end": 218774, "start": 218727, "text": "7.3.2.2 Drilling Down via Computational Zoom-in", "rank": 609, "paragraph_comparative_number": 0, "entities": [], "id": "p_609"}, "sentences": [{"end": 218774, "text": "7.3.2.2 Drilling Down via Computational Zoom-in", "rank": 1864, "start": 218727, "IsComparative": "0", "id": "st_1864"}]}, {"paragraph_info": {"end": 219279, "start": 218774, "text": "Now, the user can drill down a particular cluster via an interaction we call compu- tational zoom-in.The computational zoom-in enables the user to select an arbitrary subset of documents by visualizing them as a separate view with their own cluster- ing and dimension reduction results.These subsets can be, for example, particular clusters when their semantic meanings are not clear involving multiple topics.On the other hand, the user can select a cluttered region where many points are mixed together.", "rank": 610, "paragraph_comparative_number": 2, "entities": [], "id": "p_610"}, "sentences": [{"end": 218875, "text": "Now, the user can drill down a particular cluster via an interaction we call compu- tational zoom-in.", "rank": 1865, "start": 218774, "IsComparative": "0", "id": "st_1865"}, {"end": 219060, "text": "The computational zoom-in enables the user to select an arbitrary subset of documents by visualizing them as a separate view with their own cluster- ing and dimension reduction results.", "rank": 1866, "start": 218875, "IsComparative": "1", "id": "st_1866"}, {"end": 219184, "text": "These subsets can be, for example, particular clusters when their semantic meanings are not clear involving multiple topics.", "rank": 1867, "start": 219060, "IsComparative": "1", "id": "st_1867"}, {"end": 219279, "text": "On the other hand, the user can select a cluttered region where many points are mixed together.", "rank": 1868, "start": 219184, "IsComparative": "0", "id": "st_1868"}]}, {"paragraph_info": {"end": 219438, "start": 219279, "text": "Fig.42 shows an example of the computational zoom-in interaction.After per- forming computational zoom-in on a highly cluttered area in an original view (black", "rank": 611, "paragraph_comparative_number": 0, "entities": [], "id": "p_611"}, "sentences": [{"end": 219283, "text": "Fig.", "rank": 1869, "start": 219279, "IsComparative": "0", "id": "st_1869"}, {"end": 219344, "text": "42 shows an example of the computational zoom-in interaction.", "rank": 1870, "start": 219283, "IsComparative": "0", "id": "st_1870"}, {"end": 219438, "text": "After per- forming computational zoom-in on a highly cluttered area in an original view (black", "rank": 1871, "start": 219344, "IsComparative": "0", "id": "st_1871"}]}, {"paragraph_info": {"end": 219949, "start": 219438, "text": "Figure 42: An example of computational zoom-in interaction.For a user-selected region (black rectangle on the top left), this interaction provides a separate view by involving only these points to compute their own cluster summary and dimension reduction coordinates.The resulting view now shows a clear overview about these cluttered data, revealing detailed clusters about support vector machines and de- cision trees that are typically applied in medical image analyses (black rectangle on the bottom right).", "rank": 612, "paragraph_comparative_number": 1, "entities": [], "id": "p_612"}, "sentences": [{"end": 219497, "text": "Figure 42: An example of computational zoom-in interaction.", "rank": 1872, "start": 219438, "IsComparative": "0", "id": "st_1872"}, {"end": 219705, "text": "For a user-selected region (black rectangle on the top left), this interaction provides a separate view by involving only these points to compute their own cluster summary and dimension reduction coordinates.", "rank": 1873, "start": 219497, "IsComparative": "1", "id": "st_1873"}, {"end": 219949, "text": "The resulting view now shows a clear overview about these cluttered data, revealing detailed clusters about support vector machines and de- cision trees that are typically applied in medical image analyses (black rectangle on the bottom right).", "rank": 1874, "start": 219705, "IsComparative": "0", "id": "st_1874"}]}, {"paragraph_info": {"end": 220201, "start": 219949, "text": "rectangle on the top left), the resulting view successfully reveals several clear clus- ters e.g., the one about support vector machines and another about decision trees typically applied in medical image analyses (black rectangle on the bottom right).", "rank": 613, "paragraph_comparative_number": 0, "entities": [], "id": "p_613"}, "sentences": [{"end": 220201, "text": "rectangle on the top left), the resulting view successfully reveals several clear clus- ters e.g., the one about support vector machines and another about decision trees typically applied in medical image analyses (black rectangle on the bottom right).", "rank": 1875, "start": 219949, "IsComparative": "0", "id": "st_1875"}]}, {"paragraph_info": {"end": 220249, "start": 220201, "text": "7.3.2.3 Dynamic Queries and Multi-view Alignment", "rank": 614, "paragraph_comparative_number": 0, "entities": [], "id": "p_614"}, "sentences": [{"end": 220249, "text": "7.3.2.3 Dynamic Queries and Multi-view Alignment", "rank": 1876, "start": 220201, "IsComparative": "0", "id": "st_1876"}]}, {"paragraph_info": {"end": 220924, "start": 220249, "text": "In addition to exploring visualized clusters, the user can apply additional queries to further narrow down the retrieved document set.Suppose the user wanted to focus on those recently published in 2008 or later and thus created another filter from the Query Toolbar in conjunction with the previous keyword query disease.Given such a new set of documents, VisIRR creates another visualization with its own clustering and dimension reduction.The user could then compare between the new and the previous visualization results, as shown in Figs.43(a) and (b), respectively, by brushing-and- linking in order to identify, for example, which topic clusters were more/less popular", "rank": 615, "paragraph_comparative_number": 4, "entities": [], "id": "p_615"}, "sentences": [{"end": 220383, "text": "In addition to exploring visualized clusters, the user can apply additional queries to further narrow down the retrieved document set.", "rank": 1877, "start": 220249, "IsComparative": "1", "id": "st_1877"}, {"end": 220571, "text": "Suppose the user wanted to focus on those recently published in 2008 or later and thus created another filter from the Query Toolbar in conjunction with the previous keyword query disease.", "rank": 1878, "start": 220383, "IsComparative": "0", "id": "st_1878"}, {"end": 220691, "text": "Given such a new set of documents, VisIRR creates another visualization with its own clustering and dimension reduction.", "rank": 1879, "start": 220571, "IsComparative": "1", "id": "st_1879"}, {"end": 220792, "text": "The user could then compare between the new and the previous visualization results, as shown in Figs.", "rank": 1880, "start": 220691, "IsComparative": "1", "id": "st_1880"}, {"end": 220924, "text": "43(a) and (b), respectively, by brushing-and- linking in order to identify, for example, which topic clusters were more/less popular", "rank": 1881, "start": 220792, "IsComparative": "1", "id": "st_1881"}]}, {"paragraph_info": {"end": 220986, "start": 220924, "text": "(a) An unaligned view (b) A reference view (c) An aligned view", "rank": 616, "paragraph_comparative_number": 0, "entities": [], "id": "p_616"}, "sentences": [{"end": 220986, "text": "(a) An unaligned view (b) A reference view (c) An aligned view", "rank": 1882, "start": 220924, "IsComparative": "0", "id": "st_1882"}]}, {"paragraph_info": {"end": 221796, "start": 220986, "text": "Figure 43: Effects of clustering and dimension reduction alignments.A reference view (b) shows the documents with a query word disease while the other two views (a)(c) contain the subset of them published from year 2008 with their own clustering and dimension reduction steps applied.For an unaligned view (a), it is difficult to compare against the reference view since there is no correspondence in terms of the coordinates of data points and clusters.However, in an aligned view (c), the clusters match those in the reference, and their spatial correspondences in the scatter plot are maintained.from 2008.However, since the cluster colors and the dimension reduction results have been computed independently, it is not straightforward to easily compare these differences based on the visualization results.", "rank": 617, "paragraph_comparative_number": 3, "entities": [], "id": "p_617"}, "sentences": [{"end": 221054, "text": "Figure 43: Effects of clustering and dimension reduction alignments.", "rank": 1883, "start": 220986, "IsComparative": "0", "id": "st_1883"}, {"end": 221270, "text": "A reference view (b) shows the documents with a query word disease while the other two views (a)(c) contain the subset of them published from year 2008 with their own clustering and dimension reduction steps applied.", "rank": 1884, "start": 221054, "IsComparative": "1", "id": "st_1884"}, {"end": 221440, "text": "For an unaligned view (a), it is difficult to compare against the reference view since there is no correspondence in terms of the coordinates of data points and clusters.", "rank": 1885, "start": 221270, "IsComparative": "1", "id": "st_1885"}, {"end": 221585, "text": "However, in an aligned view (c), the clusters match those in the reference, and their spatial correspondences in the scatter plot are maintained.", "rank": 1886, "start": 221440, "IsComparative": "0", "id": "st_1886"}, {"end": 221595, "text": "from 2008.", "rank": 1887, "start": 221585, "IsComparative": "1", "id": "st_1887"}, {"end": 221796, "text": "However, since the cluster colors and the dimension reduction results have been computed independently, it is not straightforward to easily compare these differences based on the visualization results.", "rank": 1888, "start": 221595, "IsComparative": "0", "id": "st_1888"}]}, {"paragraph_info": {"end": 222550, "start": 221796, "text": "To solve this problem, once a new visualization is created, VisIRR performs an alignment step on the new clustering and dimension reduction results with respect to the previous visualization result so that the visual coherences in terms of the cluster colors and the spatial coordinates of data points can be maintained.The algorithm details are discussed in Section 7.5.3.For instance, as opposed to an unaligned visualization in Fig 43(a), an aligned one in Fig 43(c) is shown to be much easier to compare against the previous visualization shown in (Fig 43(b).From the aligned visualization, the user can easily see that the cluster about outbreak detection, shown as a green cluster in the middle of Figs.43(b)(c), was not actively studied from 2008.", "rank": 618, "paragraph_comparative_number": 2, "entities": [], "id": "p_618"}, "sentences": [{"end": 222116, "text": "To solve this problem, once a new visualization is created, VisIRR performs an alignment step on the new clustering and dimension reduction results with respect to the previous visualization result so that the visual coherences in terms of the cluster colors and the spatial coordinates of data points can be maintained.", "rank": 1889, "start": 221796, "IsComparative": "0", "id": "st_1889"}, {"end": 222169, "text": "The algorithm details are discussed in Section 7.5.3.", "rank": 1890, "start": 222116, "IsComparative": "0", "id": "st_1890"}, {"end": 222359, "text": "For instance, as opposed to an unaligned visualization in Fig 43(a), an aligned one in Fig 43(c) is shown to be much easier to compare against the previous visualization shown in (Fig 43(b).", "rank": 1891, "start": 222169, "IsComparative": "1", "id": "st_1891"}, {"end": 222505, "text": "From the aligned visualization, the user can easily see that the cluster about outbreak detection, shown as a green cluster in the middle of Figs.", "rank": 1892, "start": 222359, "IsComparative": "1", "id": "st_1892"}, {"end": 222550, "text": "43(b)(c), was not actively studied from 2008.", "rank": 1893, "start": 222505, "IsComparative": "0", "id": "st_1893"}]}, {"paragraph_info": {"end": 222586, "start": 222550, "text": "7.3.2.4 Content-based Recommendation", "rank": 619, "paragraph_comparative_number": 0, "entities": [], "id": "p_619"}, "sentences": [{"end": 222586, "text": "7.3.2.4 Content-based Recommendation", "rank": 1894, "start": 222550, "IsComparative": "0", "id": "st_1894"}]}, {"paragraph_info": {"end": 223208, "start": 222586, "text": "Throughout analyses, the user can assign ratings to the documents he/she likes or dislikes.Among the retrieved documents, suppose the user found a document Au- tomatic tool for Alzheimers disease diagnosis using PCA and Bayesian classification rules interesting and assigned the document a 5-star rating(highly-like) by right- clicking the corresponding data point in the Scatter Plot View.Based on this user preference information, VisIRR identifies the recommended documents based on the content similarity.These rated and the recommended documents are displayed in a tabular form in the Recommendation view (Fig.40(C)).", "rank": 620, "paragraph_comparative_number": 1, "entities": [], "id": "p_620"}, "sentences": [{"end": 222677, "text": "Throughout analyses, the user can assign ratings to the documents he/she likes or dislikes.", "rank": 1895, "start": 222586, "IsComparative": "0", "id": "st_1895"}, {"end": 222976, "text": "Among the retrieved documents, suppose the user found a document Au- tomatic tool for Alzheimers disease diagnosis using PCA and Bayesian classification rules interesting and assigned the document a 5-star rating(highly-like) by right- clicking the corresponding data point in the Scatter Plot View.", "rank": 1896, "start": 222677, "IsComparative": "1", "id": "st_1896"}, {"end": 223095, "text": "Based on this user preference information, VisIRR identifies the recommended documents based on the content similarity.", "rank": 1897, "start": 222976, "IsComparative": "0", "id": "st_1897"}, {"end": 223201, "text": "These rated and the recommended documents are displayed in a tabular form in the Recommendation view (Fig.", "rank": 1898, "start": 223095, "IsComparative": "0", "id": "st_1898"}, {"end": 223208, "text": "40(C)).", "rank": 1899, "start": 223201, "IsComparative": "0", "id": "st_1899"}]}, {"paragraph_info": {"end": 224248, "start": 223208, "text": "From the list of recommended documents shown in the lower table, the user could obtain an idea that the research about Alzheimers disease mainly involves an image analysis, clustering, classification, etc.Notice that without such a recommendation capability of VisIRR, the user would not be able to obtain these documents since these documents were not included in the retrieved set by user queries.In the Scatter Plot view, the user can see these recommended documents at the upper left corner around the rated document and its nearby clusters.To obtain a better idea about the recommended documents, the user can create another visualization by only using this subset with a new clustering and dimension reduction (Fig.40(D)).From its own cluster summary and visualization, the user could see that the documents directly related to Alzheimers disease are mainly shown in the bottom half while the upper half in the Scatter Plot view, shows documents mainly related to image analysis such as content-based image retrieval, clustering, etc.", "rank": 621, "paragraph_comparative_number": 3, "entities": [], "id": "p_621"}, "sentences": [{"end": 223413, "text": "From the list of recommended documents shown in the lower table, the user could obtain an idea that the research about Alzheimers disease mainly involves an image analysis, clustering, classification, etc.", "rank": 1900, "start": 223208, "IsComparative": "0", "id": "st_1900"}, {"end": 223607, "text": "Notice that without such a recommendation capability of VisIRR, the user would not be able to obtain these documents since these documents were not included in the retrieved set by user queries.", "rank": 1901, "start": 223413, "IsComparative": "0", "id": "st_1901"}, {"end": 223753, "text": "In the Scatter Plot view, the user can see these recommended documents at the upper left corner around the rated document and its nearby clusters.", "rank": 1902, "start": 223607, "IsComparative": "1", "id": "st_1902"}, {"end": 223929, "text": "To obtain a better idea about the recommended documents, the user can create another visualization by only using this subset with a new clustering and dimension reduction (Fig.", "rank": 1903, "start": 223753, "IsComparative": "1", "id": "st_1903"}, {"end": 223936, "text": "40(D)).", "rank": 1904, "start": 223929, "IsComparative": "0", "id": "st_1904"}, {"end": 224248, "text": "From its own cluster summary and visualization, the user could see that the documents directly related to Alzheimers disease are mainly shown in the bottom half while the upper half in the Scatter Plot view, shows documents mainly related to image analysis such as content-based image retrieval, clustering, etc.", "rank": 1905, "start": 223936, "IsComparative": "1", "id": "st_1905"}]}, {"paragraph_info": {"end": 224304, "start": 224248, "text": "7.3.2.5 Citation- and Co-authorship-based Recommendation", "rank": 622, "paragraph_comparative_number": 0, "entities": [], "id": "p_622"}, "sentences": [{"end": 224304, "text": "7.3.2.5 Citation- and Co-authorship-based Recommendation", "rank": 1906, "start": 224248, "IsComparative": "0", "id": "st_1906"}]}, {"paragraph_info": {"end": 224546, "start": 224304, "text": "Now, among the recommended documents, the user chose another document Auto- matic Classification System for the Diagnosis of Alzheimer Disease Using Component- Based SVM Aggregations and assigned it a 5-star rating.This time, the user changes", "rank": 623, "paragraph_comparative_number": 1, "entities": [], "id": "p_623"}, "sentences": [{"end": 224519, "text": "Now, among the recommended documents, the user chose another document Auto- matic Classification System for the Diagnosis of Alzheimer Disease Using Component- Based SVM Aggregations and assigned it a 5-star rating.", "rank": 1907, "start": 224304, "IsComparative": "1", "id": "st_1907"}, {"end": 224546, "text": "This time, the user changes", "rank": 1908, "start": 224519, "IsComparative": "0", "id": "st_1908"}]}, {"paragraph_info": {"end": 224590, "start": 224546, "text": "(a) The top-ranked recommended document list", "rank": 624, "paragraph_comparative_number": 0, "entities": [], "id": "p_624"}, "sentences": [{"end": 224590, "text": "(a) The top-ranked recommended document list", "rank": 1909, "start": 224546, "IsComparative": "0", "id": "st_1909"}]}, {"paragraph_info": {"end": 224636, "start": 224590, "text": "(b) A visualization of recommended doc- uments", "rank": 625, "paragraph_comparative_number": 1, "entities": [], "id": "p_625"}, "sentences": [{"end": 224636, "text": "(b) A visualization of recommended doc- uments", "rank": 1910, "start": 224590, "IsComparative": "1", "id": "st_1910"}]}, {"paragraph_info": {"end": 224952, "start": 224636, "text": "Figure 44: Citation-based recommendation results obtained by assigning a 5-star rating to the paper, Automatic Classification System for the Diagnosis of Alzheimer Disease Using Component-Based SVM Aggregations.VisIRR recommends various papers mostly with high citation counts, which are relevant to the rated paper.", "rank": 626, "paragraph_comparative_number": 2, "entities": [], "id": "p_626"}, "sentences": [{"end": 224847, "text": "Figure 44: Citation-based recommendation results obtained by assigning a 5-star rating to the paper, Automatic Classification System for the Diagnosis of Alzheimer Disease Using Component-Based SVM Aggregations.", "rank": 1911, "start": 224636, "IsComparative": "1", "id": "st_1911"}, {"end": 224952, "text": "VisIRR recommends various papers mostly with high citation counts, which are relevant to the rated paper.", "rank": 1912, "start": 224847, "IsComparative": "1", "id": "st_1912"}]}, {"paragraph_info": {"end": 225067, "start": 224952, "text": "(a) A visualization of retrieved and rec-(b) A visualization of only the recom- ommended documents mended documents", "rank": 627, "paragraph_comparative_number": 1, "entities": [], "id": "p_627"}, "sentences": [{"end": 225067, "text": "(a) A visualization of retrieved and rec-(b) A visualization of only the recom- ommended documents mended documents", "rank": 1913, "start": 224952, "IsComparative": "1", "id": "st_1913"}]}, {"paragraph_info": {"end": 226150, "start": 225067, "text": "Figure 45: Co-authorship-based recommendation results based on the paper, Auto- matic Classification System for the Diagnosis of Alzheimer Disease Using Component- Based SVM Aggregations.Edges show direct co-authorship relations from the rated document.its recommendation type to a citation-based one from the Recommendation panel in order to obtain highly-cited documents relevant to this document.As expected, VisIRRs top-ranked recommended documents are relatively highly cited papers, as shown in Fig.44(a).After generating another visualization only using these recom- mended items, the user can obtain a summary about them, the clusters of which are composed of image retrieval, object detection/recognition, face recognition, and texture analyses (Fig.44(b)).Notice that these types of recommendation results would not be easily obtained by a simple keyword search since these recommended documents do not contain specific keywords in common.Instead, they are only im- plicitly related with each other via a citation network on which VisIRR can perform a recommendation based.", "rank": 628, "paragraph_comparative_number": 3, "entities": [], "id": "p_628"}, "sentences": [{"end": 225254, "text": "Figure 45: Co-authorship-based recommendation results based on the paper, Auto- matic Classification System for the Diagnosis of Alzheimer Disease Using Component- Based SVM Aggregations.", "rank": 1914, "start": 225067, "IsComparative": "1", "id": "st_1914"}, {"end": 225320, "text": "Edges show direct co-authorship relations from the rated document.", "rank": 1915, "start": 225254, "IsComparative": "0", "id": "st_1915"}, {"end": 225466, "text": "its recommendation type to a citation-based one from the Recommendation panel in order to obtain highly-cited documents relevant to this document.", "rank": 1916, "start": 225320, "IsComparative": "1", "id": "st_1916"}, {"end": 225572, "text": "As expected, VisIRRs top-ranked recommended documents are relatively highly cited papers, as shown in Fig.", "rank": 1917, "start": 225466, "IsComparative": "0", "id": "st_1917"}, {"end": 225578, "text": "44(a).", "rank": 1918, "start": 225572, "IsComparative": "1", "id": "st_1918"}, {"end": 225826, "text": "After generating another visualization only using these recom- mended items, the user can obtain a summary about them, the clusters of which are composed of image retrieval, object detection/recognition, face recognition, and texture analyses (Fig.", "rank": 1919, "start": 225578, "IsComparative": "0", "id": "st_1919"}, {"end": 225833, "text": "44(b)).", "rank": 1920, "start": 225826, "IsComparative": "0", "id": "st_1920"}, {"end": 226016, "text": "Notice that these types of recommendation results would not be easily obtained by a simple keyword search since these recommended documents do not contain specific keywords in common.", "rank": 1921, "start": 225833, "IsComparative": "0", "id": "st_1921"}, {"end": 226150, "text": "Instead, they are only im- plicitly related with each other via a citation network on which VisIRR can perform a recommendation based.", "rank": 1922, "start": 226016, "IsComparative": "0", "id": "st_1922"}]}, {"paragraph_info": {"end": 227536, "start": 226150, "text": "In addition, the user also wanted to know what other topics or areas the authors of this paper are involved in.To this end, the user changed the recommendation type to a co-authorship-based one from the Recommendation view.In addition, to better show the direct co-authorship relationships from the rated paper, the user turned on the Edges checkbox by selecting the edge type as Co-authorship in the Label panel.The existing visualization of the retrieved documents now includes the recommended documents as well as the direct co-authorship relations from the rated document, as shown in Fig.45(a).Similar to the previous case, the user can generate another visualization of only the recommended items to have a better idea about the recommended documents.After varying the number of clusters, the user obtains a new visualization as shown in Fig.45(b).From this visualization, the user could gain an insight that the authors of the rated paper have written the papers, other than Alzheimers disease-related papers (the green cluster on the right), in the four areas corresponding to blind source separation, gene expression, speech processing, and neural networks.This potentially indicates that the user, who was originally interested in Alzheimers disease diagnosis, could expand his/her research by following the ways the authors of the rated paper have published in other domains.", "rank": 629, "paragraph_comparative_number": 4, "entities": [], "id": "p_629"}, "sentences": [{"end": 226261, "text": "In addition, the user also wanted to know what other topics or areas the authors of this paper are involved in.", "rank": 1923, "start": 226150, "IsComparative": "0", "id": "st_1923"}, {"end": 226373, "text": "To this end, the user changed the recommendation type to a co-authorship-based one from the Recommendation view.", "rank": 1924, "start": 226261, "IsComparative": "0", "id": "st_1924"}, {"end": 226563, "text": "In addition, to better show the direct co-authorship relationships from the rated paper, the user turned on the Edges checkbox by selecting the edge type as Co-authorship in the Label panel.", "rank": 1925, "start": 226373, "IsComparative": "0", "id": "st_1925"}, {"end": 226743, "text": "The existing visualization of the retrieved documents now includes the recommended documents as well as the direct co-authorship relations from the rated document, as shown in Fig.", "rank": 1926, "start": 226563, "IsComparative": "0", "id": "st_1926"}, {"end": 226749, "text": "45(a).", "rank": 1927, "start": 226743, "IsComparative": "1", "id": "st_1927"}, {"end": 226907, "text": "Similar to the previous case, the user can generate another visualization of only the recommended items to have a better idea about the recommended documents.", "rank": 1928, "start": 226749, "IsComparative": "1", "id": "st_1928"}, {"end": 226998, "text": "After varying the number of clusters, the user obtains a new visualization as shown in Fig.", "rank": 1929, "start": 226907, "IsComparative": "1", "id": "st_1929"}, {"end": 227004, "text": "45(b).", "rank": 1930, "start": 226998, "IsComparative": "0", "id": "st_1930"}, {"end": 227316, "text": "From this visualization, the user could gain an insight that the authors of the rated paper have written the papers, other than Alzheimers disease-related papers (the green cluster on the right), in the four areas corresponding to blind source separation, gene expression, speech processing, and neural networks.", "rank": 1931, "start": 227004, "IsComparative": "1", "id": "st_1931"}, {"end": 227536, "text": "This potentially indicates that the user, who was originally interested in Alzheimers disease diagnosis, could expand his/her research by following the ways the authors of the rated paper have published in other domains.", "rank": 1932, "start": 227316, "IsComparative": "0", "id": "st_1932"}]}, {"paragraph_info": {"end": 227567, "start": 227536, "text": "7.4 Data Collection / Ingestion", "rank": 630, "paragraph_comparative_number": 1, "entities": [], "id": "p_630"}, "sentences": [{"end": 227567, "text": "7.4 Data Collection / Ingestion", "rank": 1933, "start": 227536, "IsComparative": "1", "id": "st_1933"}]}, {"paragraph_info": {"end": 227596, "start": 227567, "text": "7.4.1 Initial Data Collection", "rank": 631, "paragraph_comparative_number": 0, "entities": [], "id": "p_631"}, "sentences": [{"end": 227596, "text": "7.4.1 Initial Data Collection", "rank": 1934, "start": 227567, "IsComparative": "0", "id": "st_1934"}]}, {"paragraph_info": {"end": 228205, "start": 227596, "text": "VisIRR is intended to efficiently handle a large-scale document corpus with a rich set of features.To this end, VisIRR begins with the ArnetMiner data set, which is composed of about half a million academic papers, books, etc.<124>.2 Although the data set is mainly used in citation network analyses, it includes a variety of both structured and unstructured information such as a title, keywords, an abstract, authors, a publication year, a venue, a document type such as a book, a paper, etc., papers in the reference list, papers citing this document, the number of references, and the number of citations.", "rank": 632, "paragraph_comparative_number": 0, "entities": [], "id": "p_632"}, "sentences": [{"end": 227695, "text": "VisIRR is intended to efficiently handle a large-scale document corpus with a rich set of features.", "rank": 1935, "start": 227596, "IsComparative": "0", "id": "st_1935"}, {"end": 227822, "text": "To this end, VisIRR begins with the ArnetMiner data set, which is composed of about half a million academic papers, books, etc.", "rank": 1936, "start": 227695, "IsComparative": "0", "id": "st_1936"}, {"end": 228205, "text": "<124>.2 Although the data set is mainly used in citation network analyses, it includes a variety of both structured and unstructured information such as a title, keywords, an abstract, authors, a publication year, a venue, a document type such as a book, a paper, etc., papers in the reference list, papers citing this document, the number of references, and the number of citations.", "rank": 1937, "start": 227822, "IsComparative": "0", "id": "st_1937"}]}, {"paragraph_info": {"end": 228730, "start": 228205, "text": "However, the original data set has numerous missing values and inconsistencies such as different expressions of an authors name, a publication venue, etc.To clean up the data, we utilized the Microsoft Academic Search APIs.3 Specifically, we used a title of each document as a query in order to obtain the full information about the document from the Microsoft Academic Search API, which fills the missing values and rectifies the inconsistencies.Finally, VisIRR builds upon 432,605 documents spanning from year 1825 to 2011.", "rank": 633, "paragraph_comparative_number": 2, "entities": [], "id": "p_633"}, "sentences": [{"end": 228359, "text": "However, the original data set has numerous missing values and inconsistencies such as different expressions of an authors name, a publication venue, etc.", "rank": 1938, "start": 228205, "IsComparative": "0", "id": "st_1938"}, {"end": 228652, "text": "To clean up the data, we utilized the Microsoft Academic Search APIs.3 Specifically, we used a title of each document as a query in order to obtain the full information about the document from the Microsoft Academic Search API, which fills the missing values and rectifies the inconsistencies.", "rank": 1939, "start": 228359, "IsComparative": "1", "id": "st_1939"}, {"end": 228730, "text": "Finally, VisIRR builds upon 432,605 documents spanning from year 1825 to 2011.", "rank": 1940, "start": 228652, "IsComparative": "1", "id": "st_1940"}]}, {"paragraph_info": {"end": 228750, "start": 228730, "text": "7.4.2 Data Ingestion", "rank": 634, "paragraph_comparative_number": 1, "entities": [], "id": "p_634"}, "sentences": [{"end": 228750, "text": "7.4.2 Data Ingestion", "rank": 1941, "start": 228730, "IsComparative": "1", "id": "st_1941"}]}, {"paragraph_info": {"end": 229090, "start": 228750, "text": "Now we describe how we make these large-scale data readily available for real-time interactive analyses in VisIRR.Basically, VisIRR maintains the information about data in three different forms, (1) original fields of data, (2) a vector representation, and (3) graph representations, in an efficient and scalable way.In order to efficiently", "rank": 635, "paragraph_comparative_number": 1, "entities": [], "id": "p_635"}, "sentences": [{"end": 228864, "text": "Now we describe how we make these large-scale data readily available for real-time interactive analyses in VisIRR.", "rank": 1942, "start": 228750, "IsComparative": "0", "id": "st_1942"}, {"end": 229067, "text": "Basically, VisIRR maintains the information about data in three different forms, (1) original fields of data, (2) a vector representation, and (3) graph representations, in an efficient and scalable way.", "rank": 1943, "start": 228864, "IsComparative": "1", "id": "st_1943"}, {"end": 229090, "text": "In order to efficiently", "rank": 1944, "start": 229067, "IsComparative": "0", "id": "st_1944"}]}, {"paragraph_info": {"end": 229563, "start": 229090, "text": "2The used data is available as DBLP-Citation-network V5 at http://arnetminer.org/ citation.manage the large-scale data in all these various forms, we carefully optimized vari- ous data processing/storage techniques via database construction, pre-computation of frequently used information, and balanced storage between disk and memory.Even- tually, the system is easily and widely deployable in typical commodity PCs instead of requiring high-performance parallel machines.", "rank": 636, "paragraph_comparative_number": 2, "entities": [], "id": "p_636"}, "sentences": [{"end": 229181, "text": "2The used data is available as DBLP-Citation-network V5 at http://arnetminer.org/ citation.", "rank": 1945, "start": 229090, "IsComparative": "0", "id": "st_1945"}, {"end": 229425, "text": "manage the large-scale data in all these various forms, we carefully optimized vari- ous data processing/storage techniques via database construction, pre-computation of frequently used information, and balanced storage between disk and memory.", "rank": 1946, "start": 229181, "IsComparative": "1", "id": "st_1946"}, {"end": 229563, "text": "Even- tually, the system is easily and widely deployable in typical commodity PCs instead of requiring high-performance parallel machines.", "rank": 1947, "start": 229425, "IsComparative": "1", "id": "st_1947"}]}, {"paragraph_info": {"end": 229593, "start": 229563, "text": "7.4.2.1 Original Field of Data", "rank": 637, "paragraph_comparative_number": 0, "entities": [], "id": "p_637"}, "sentences": [{"end": 229593, "text": "7.4.2.1 Original Field of Data", "rank": 1948, "start": 229563, "IsComparative": "0", "id": "st_1948"}]}, {"paragraph_info": {"end": 230237, "start": 229593, "text": "For efficient and flexible query support, we have encoded the original data as a SQL database including full-text search capabilities on a title, keywords, an abstract, and a venue fields.For clustering and dimension reduction steps, we have pre-computed the sparse vector representations of individual documents based on a title, keywords, and an abstract fields together via a bag-of-words encoding scheme.Each vector representation is stored as a single file in a disk, the file name of which is the document ID.In this way, VisIRR can retrieve the vector representations of documents using their document IDs in the time complexity of O(1).", "rank": 638, "paragraph_comparative_number": 1, "entities": [], "id": "p_638"}, "sentences": [{"end": 229781, "text": "For efficient and flexible query support, we have encoded the original data as a SQL database including full-text search capabilities on a title, keywords, an abstract, and a venue fields.", "rank": 1949, "start": 229593, "IsComparative": "0", "id": "st_1949"}, {"end": 230001, "text": "For clustering and dimension reduction steps, we have pre-computed the sparse vector representations of individual documents based on a title, keywords, and an abstract fields together via a bag-of-words encoding scheme.", "rank": 1950, "start": 229781, "IsComparative": "0", "id": "st_1950"}, {"end": 230108, "text": "Each vector representation is stored as a single file in a disk, the file name of which is the document ID.", "rank": 1951, "start": 230001, "IsComparative": "0", "id": "st_1951"}, {"end": 230237, "text": "In this way, VisIRR can retrieve the vector representations of documents using their document IDs in the time complexity of O(1).", "rank": 1952, "start": 230108, "IsComparative": "1", "id": "st_1952"}]}, {"paragraph_info": {"end": 230266, "start": 230237, "text": "7.4.2.2 Vector Representation", "rank": 639, "paragraph_comparative_number": 0, "entities": [], "id": "p_639"}, "sentences": [{"end": 230266, "text": "7.4.2.2 Vector Representation", "rank": 1953, "start": 230237, "IsComparative": "0", "id": "st_1953"}]}, {"paragraph_info": {"end": 231005, "start": 230266, "text": "Once the vector representations of documents are loaded into a memory, VisIRR manages them in a similar way to cache replacement algorithms.That is, the vector representations already loaded into the memory is referenced from the memory when- ever needed.When the total memory-loaded vectors exceed a pre-defined maximum memory size, the least recently used vectors are removed from the memory.When needed later, they are loaded from a disk once again.This way, VisIRR does not need to load the vector representations of all the documents from the beginning, which will take significant time and memory at the system startup.At the same time, VisIRR prevents the required memory size from blowing up due to a long-term usage of the system.", "rank": 640, "paragraph_comparative_number": 2, "entities": [], "id": "p_640"}, "sentences": [{"end": 230406, "text": "Once the vector representations of documents are loaded into a memory, VisIRR manages them in a similar way to cache replacement algorithms.", "rank": 1954, "start": 230266, "IsComparative": "0", "id": "st_1954"}, {"end": 230521, "text": "That is, the vector representations already loaded into the memory is referenced from the memory when- ever needed.", "rank": 1955, "start": 230406, "IsComparative": "0", "id": "st_1955"}, {"end": 230660, "text": "When the total memory-loaded vectors exceed a pre-defined maximum memory size, the least recently used vectors are removed from the memory.", "rank": 1956, "start": 230521, "IsComparative": "0", "id": "st_1956"}, {"end": 230718, "text": "When needed later, they are loaded from a disk once again.", "rank": 1957, "start": 230660, "IsComparative": "0", "id": "st_1957"}, {"end": 230891, "text": "This way, VisIRR does not need to load the vector representations of all the documents from the beginning, which will take significant time and memory at the system startup.", "rank": 1958, "start": 230718, "IsComparative": "1", "id": "st_1958"}, {"end": 231005, "text": "At the same time, VisIRR prevents the required memory size from blowing up due to a long-term usage of the system.", "rank": 1959, "start": 230891, "IsComparative": "1", "id": "st_1959"}]}, {"paragraph_info": {"end": 231033, "start": 231005, "text": "7.4.2.3 Graph Representation", "rank": 641, "paragraph_comparative_number": 0, "entities": [], "id": "p_641"}, "sentences": [{"end": 231033, "text": "7.4.2.3 Graph Representation", "rank": 1960, "start": 231005, "IsComparative": "0", "id": "st_1960"}]}, {"paragraph_info": {"end": 232348, "start": 231033, "text": "The recommendation module, which will be described in Section 7.5, requires an input graph where the nodes correspond to documents and the edges represent their pairwise similarities/relationships.We have pre-computed three such graphs for the entire data set using contents, a citation network, and co-authorship, respectively, in order to support various recommendation capabilities.For content-based graph, we initially computed the pairwise cosine similarities between all the pairs of documents using their vector representations.Since maintaining all the pairwise information requires O(n2) storage where n is the total number of documents, we identified the fixed number (10 in our case) of the most similar documents for each document and kept only the edges between them.For the citation graph, we formed edges between a pair of documents if either cites the other.For the co-authorship graph, edges are created if two documents share the common author(s).Since citation and co- authorship graphs are typically sparse, we stored all these edge information.For each graph, VisIRR maintains the mappings from an individual document to a list of edges in terms of the destination document and its edge value so that it can retrieve the edge information for particular documents in the time complexity of O(1).", "rank": 642, "paragraph_comparative_number": 8, "entities": [], "id": "p_642"}, "sentences": [{"end": 231230, "text": "The recommendation module, which will be described in Section 7.5, requires an input graph where the nodes correspond to documents and the edges represent their pairwise similarities/relationships.", "rank": 1961, "start": 231033, "IsComparative": "1", "id": "st_1961"}, {"end": 231418, "text": "We have pre-computed three such graphs for the entire data set using contents, a citation network, and co-authorship, respectively, in order to support various recommendation capabilities.", "rank": 1962, "start": 231230, "IsComparative": "1", "id": "st_1962"}, {"end": 231568, "text": "For content-based graph, we initially computed the pairwise cosine similarities between all the pairs of documents using their vector representations.", "rank": 1963, "start": 231418, "IsComparative": "1", "id": "st_1963"}, {"end": 231813, "text": "Since maintaining all the pairwise information requires O(n2) storage where n is the total number of documents, we identified the fixed number (10 in our case) of the most similar documents for each document and kept only the edges between them.", "rank": 1964, "start": 231568, "IsComparative": "1", "id": "st_1964"}, {"end": 231907, "text": "For the citation graph, we formed edges between a pair of documents if either cites the other.", "rank": 1965, "start": 231813, "IsComparative": "1", "id": "st_1965"}, {"end": 231998, "text": "For the co-authorship graph, edges are created if two documents share the common author(s).", "rank": 1966, "start": 231907, "IsComparative": "1", "id": "st_1966"}, {"end": 232098, "text": "Since citation and co- authorship graphs are typically sparse, we stored all these edge information.", "rank": 1967, "start": 231998, "IsComparative": "1", "id": "st_1967"}, {"end": 232348, "text": "For each graph, VisIRR maintains the mappings from an individual document to a list of edges in terms of the destination document and its edge value so that it can retrieve the edge information for particular documents in the time complexity of O(1).", "rank": 1968, "start": 232098, "IsComparative": "1", "id": "st_1968"}]}, {"paragraph_info": {"end": 232382, "start": 232348, "text": "7.4.3 Scalable Update for New Data", "rank": 643, "paragraph_comparative_number": 1, "entities": [], "id": "p_643"}, "sentences": [{"end": 232382, "text": "7.4.3 Scalable Update for New Data", "rank": 1969, "start": 232348, "IsComparative": "1", "id": "st_1969"}]}, {"paragraph_info": {"end": 233033, "start": 232382, "text": "Even though VisIRR already contains a large-scale data of about half a million doc- uments, it is crucial to have a capability to efficiently update the above-described information including newly added documents.An updating process is composed of two parts: updating the information about existing documents and obtaining the representations of new documents.First, in the case of the original fields of data, the information about new documents can be easily added to the database with- out affecting the existing data.Second, In the case of updating bag-of-words vector representations, new documents generally causes newly appearing keywords to be", "rank": 644, "paragraph_comparative_number": 3, "entities": [], "id": "p_644"}, "sentences": [{"end": 232595, "text": "Even though VisIRR already contains a large-scale data of about half a million doc- uments, it is crucial to have a capability to efficiently update the above-described information including newly added documents.", "rank": 1970, "start": 232382, "IsComparative": "1", "id": "st_1970"}, {"end": 232742, "text": "An updating process is composed of two parts: updating the information about existing documents and obtaining the representations of new documents.", "rank": 1971, "start": 232595, "IsComparative": "1", "id": "st_1971"}, {"end": 232903, "text": "First, in the case of the original fields of data, the information about new documents can be easily added to the database with- out affecting the existing data.", "rank": 1972, "start": 232742, "IsComparative": "0", "id": "st_1972"}, {"end": 233033, "text": "Second, In the case of updating bag-of-words vector representations, new documents generally causes newly appearing keywords to be", "rank": 1973, "start": 232903, "IsComparative": "1", "id": "st_1973"}]}, {"paragraph_info": {"end": 233152, "start": 233033, "text": "(a) Maximization of dis-(b) Minimization of ap- (c) LDA (d) PCA tances between clusterproximate cluster radii centroids", "rank": 645, "paragraph_comparative_number": 0, "entities": [], "id": "p_645"}, "sentences": [{"end": 233152, "text": "(a) Maximization of dis-(b) Minimization of ap- (c) LDA (d) PCA tances between clusterproximate cluster radii centroids", "rank": 1974, "start": 233033, "IsComparative": "0", "id": "st_1974"}]}, {"paragraph_info": {"end": 234002, "start": 233152, "text": "Figure 46: A high-level idea of LDA and a comparison example betwee LDA and PCA.A different color corresponds to a different cluster, and c1 and c2 are the cluster centroids.LDA tries to find a reduced-dimensional representation of data by putting different clusters as far as possible (a) and representing each cluster as compact as possible (b).(c) and (d) show an example 2D scatter plots obtained by PCA and LDA, respectively, for artificial Gaussian mixture data with 7 clusters and 1,000 original dimensions.From a comparison between them, LDA is shown to reveal a much clearer cluster structure than PCA in a 2D space.indexed as additional dimensions.However, sparse vector representations of exist- ing documents would still remain the same, and thus we only need to compute the representation of new documents, which can also be easily done.", "rank": 646, "paragraph_comparative_number": 4, "entities": [], "id": "p_646"}, "sentences": [{"end": 233232, "text": "Figure 46: A high-level idea of LDA and a comparison example betwee LDA and PCA.", "rank": 1975, "start": 233152, "IsComparative": "0", "id": "st_1975"}, {"end": 233326, "text": "A different color corresponds to a different cluster, and c1 and c2 are the cluster centroids.", "rank": 1976, "start": 233232, "IsComparative": "1", "id": "st_1976"}, {"end": 233499, "text": "LDA tries to find a reduced-dimensional representation of data by putting different clusters as far as possible (a) and representing each cluster as compact as possible (b).", "rank": 1977, "start": 233326, "IsComparative": "1", "id": "st_1977"}, {"end": 233666, "text": "(c) and (d) show an example 2D scatter plots obtained by PCA and LDA, respectively, for artificial Gaussian mixture data with 7 clusters and 1,000 original dimensions.", "rank": 1978, "start": 233499, "IsComparative": "1", "id": "st_1978"}, {"end": 233777, "text": "From a comparison between them, LDA is shown to reveal a much clearer cluster structure than PCA in a 2D space.", "rank": 1979, "start": 233666, "IsComparative": "0", "id": "st_1979"}, {"end": 233810, "text": "indexed as additional dimensions.", "rank": 1980, "start": 233777, "IsComparative": "0", "id": "st_1980"}, {"end": 234002, "text": "However, sparse vector representations of exist- ing documents would still remain the same, and thus we only need to compute the representation of new documents, which can also be easily done.", "rank": 1981, "start": 233810, "IsComparative": "1", "id": "st_1981"}]}, {"paragraph_info": {"end": 234718, "start": 234002, "text": "Finally, in the case of updating graph representations, the only tricky part is to update the content similarity graph, where the top 10 most similar documents and their cosine similarity values are maintained.Specifically, we have to compute the pairwise similarity between all the existing documents and all the new documents, and then compare these similarity values against the current top 10 similarity values.If any of the former similarity values are greater than the latter similarity values, the corresponding edges are replaced with those to the new documents.The computa- tional complexity of this process is O(n  nnew) where n and nnew are the numbers of the existing and the new documents, respectively.", "rank": 647, "paragraph_comparative_number": 2, "entities": [], "id": "p_647"}, "sentences": [{"end": 234212, "text": "Finally, in the case of updating graph representations, the only tricky part is to update the content similarity graph, where the top 10 most similar documents and their cosine similarity values are maintained.", "rank": 1982, "start": 234002, "IsComparative": "1", "id": "st_1982"}, {"end": 234417, "text": "Specifically, we have to compute the pairwise similarity between all the existing documents and all the new documents, and then compare these similarity values against the current top 10 similarity values.", "rank": 1983, "start": 234212, "IsComparative": "0", "id": "st_1983"}, {"end": 234572, "text": "If any of the former similarity values are greater than the latter similarity values, the corresponding edges are replaced with those to the new documents.", "rank": 1984, "start": 234417, "IsComparative": "1", "id": "st_1984"}, {"end": 234718, "text": "The computa- tional complexity of this process is O(n  nnew) where n and nnew are the numbers of the existing and the new documents, respectively.", "rank": 1985, "start": 234572, "IsComparative": "0", "id": "st_1985"}]}, {"paragraph_info": {"end": 234743, "start": 234718, "text": "7.5 Computational Methods", "rank": 648, "paragraph_comparative_number": 0, "entities": [], "id": "p_648"}, "sentences": [{"end": 234743, "text": "7.5 Computational Methods", "rank": 1986, "start": 234718, "IsComparative": "0", "id": "st_1986"}]}, {"paragraph_info": {"end": 234915, "start": 234743, "text": "The key computational methods in VisIRR are clustering, dimension reduction, align- ment, and graph-based recommendation.In this section, we describe each module in detail.", "rank": 649, "paragraph_comparative_number": 0, "entities": [], "id": "p_649"}, "sentences": [{"end": 234864, "text": "The key computational methods in VisIRR are clustering, dimension reduction, align- ment, and graph-based recommendation.", "rank": 1987, "start": 234743, "IsComparative": "0", "id": "st_1987"}, {"end": 234915, "text": "In this section, we describe each module in detail.", "rank": 1988, "start": 234864, "IsComparative": "0", "id": "st_1988"}]}, {"paragraph_info": {"end": 234931, "start": 234915, "text": "7.5.1 Clustering", "rank": 650, "paragraph_comparative_number": 0, "entities": [], "id": "p_650"}, "sentences": [{"end": 234931, "text": "7.5.1 Clustering", "rank": 1989, "start": 234915, "IsComparative": "0", "id": "st_1989"}]}, {"paragraph_info": {"end": 235460, "start": 234931, "text": "Clustering plays a crucial role in providing a summary of a given set of documents as a manageable number of groups based on their semantic meanings.The resulting cluster indices are used to color-code documents in a scatter plot with their cluster summaries in terms of the most frequently shown keywords (Fig.40(B)(E)).VisIRR adopts a state-of-the-art technique called nonnegative matrix factorization (NMF) <80>, which have shown superior performances in document clustering over traditional methods such as k-means <81, 141>.", "rank": 651, "paragraph_comparative_number": 2, "entities": [], "id": "p_651"}, "sentences": [{"end": 235080, "text": "Clustering plays a crucial role in providing a summary of a given set of documents as a manageable number of groups based on their semantic meanings.", "rank": 1990, "start": 234931, "IsComparative": "1", "id": "st_1990"}, {"end": 235242, "text": "The resulting cluster indices are used to color-code documents in a scatter plot with their cluster summaries in terms of the most frequently shown keywords (Fig.", "rank": 1991, "start": 235080, "IsComparative": "1", "id": "st_1991"}, {"end": 235252, "text": "40(B)(E)).", "rank": 1992, "start": 235242, "IsComparative": "0", "id": "st_1992"}, {"end": 235460, "text": "VisIRR adopts a state-of-the-art technique called nonnegative matrix factorization (NMF) <80>, which have shown superior performances in document clustering over traditional methods such as k-means <81, 141>.", "rank": 1993, "start": 235252, "IsComparative": "0", "id": "st_1993"}]}, {"paragraph_info": {"end": 235569, "start": 235460, "text": "Given a nonnegative matrix X  Rmn, and an integer k  min(m, n), NMF finds a lower-rank approximation given by", "rank": 652, "paragraph_comparative_number": 0, "entities": [], "id": "p_652"}, "sentences": [{"end": 235569, "text": "Given a nonnegative matrix X  Rmn, and an integer k  min(m, n), NMF finds a lower-rank approximation given by", "rank": 1994, "start": 235460, "IsComparative": "0", "id": "st_1994"}]}, {"paragraph_info": {"end": 235811, "start": 235569, "text": "X  WH, (40) where W  Rmk and H  Rkn are nonnegative factors.NMF can be formulated using the Frobenius norm as for the i-th document, and by taking the index the value of which is the largest, the cluster index of the document can be obtained.", "rank": 653, "paragraph_comparative_number": 1, "entities": [], "id": "p_653"}, "sentences": [{"end": 235629, "text": "X  WH, (40) where W  Rmk and H  Rkn are nonnegative factors.", "rank": 1995, "start": 235569, "IsComparative": "0", "id": "st_1995"}, {"end": 235811, "text": "NMF can be formulated using the Frobenius norm as for the i-th document, and by taking the index the value of which is the largest, the cluster index of the document can be obtained.", "rank": 1996, "start": 235629, "IsComparative": "1", "id": "st_1996"}]}, {"paragraph_info": {"end": 236286, "start": 235811, "text": "The specific NMF algorithm we have used is based on a recently proposed block principal pivoting algorithm <82>,4 which is found to be one of the fastest and reliable algorithms.Although not reported, we have conducted an extensive amount of com- parison of NMF against traditional clustering techniques such as k-means, and we found that NMF mostly gives semantically more meaningful clusters than any other methods while requiring a significantly faster computational time.", "rank": 654, "paragraph_comparative_number": 0, "entities": [], "id": "p_654"}, "sentences": [{"end": 235989, "text": "The specific NMF algorithm we have used is based on a recently proposed block principal pivoting algorithm <82>,4 which is found to be one of the fastest and reliable algorithms.", "rank": 1997, "start": 235811, "IsComparative": "0", "id": "st_1997"}, {"end": 236286, "text": "Although not reported, we have conducted an extensive amount of com- parison of NMF against traditional clustering techniques such as k-means, and we found that NMF mostly gives semantically more meaningful clusters than any other methods while requiring a significantly faster computational time.", "rank": 1998, "start": 235989, "IsComparative": "0", "id": "st_1998"}]}, {"paragraph_info": {"end": 236311, "start": 236286, "text": "7.5.2 Dimension Reduction", "rank": 655, "paragraph_comparative_number": 0, "entities": [], "id": "p_655"}, "sentences": [{"end": 236311, "text": "7.5.2 Dimension Reduction", "rank": 1999, "start": 236286, "IsComparative": "0", "id": "st_1999"}]}, {"paragraph_info": {"end": 236688, "start": 236311, "text": "Given high-dimensional vector representations of documents, dimension reduction computes their 2D representations so that they can be visualized in a scatter plot (Fig.40(B)).From the scatter plot, users can get an idea about how clusters/documents are related with each other.VisIRR adopts an advanced dimension reduction method called linear discriminant analysis (LDA) <68>.", "rank": 656, "paragraph_comparative_number": 1, "entities": [], "id": "p_656"}, "sentences": [{"end": 236479, "text": "Given high-dimensional vector representations of documents, dimension reduction computes their 2D representations so that they can be visualized in a scatter plot (Fig.", "rank": 2000, "start": 236311, "IsComparative": "1", "id": "st_2000"}, {"end": 236486, "text": "40(B)).", "rank": 2001, "start": 236479, "IsComparative": "0", "id": "st_2001"}, {"end": 236588, "text": "From the scatter plot, users can get an idea about how clusters/documents are related with each other.", "rank": 2002, "start": 236486, "IsComparative": "0", "id": "st_2002"}, {"end": 236688, "text": "VisIRR adopts an advanced dimension reduction method called linear discriminant analysis (LDA) <68>.", "rank": 2003, "start": 236588, "IsComparative": "0", "id": "st_2003"}]}, {"paragraph_info": {"end": 237359, "start": 236688, "text": "Unlike traditional methods such as principal component analysis and multidi- mensional scaling, LDA explicitly utilizes additional cluster label information, which are taken from the clustering module, associated with the input high-dimensional vectors.Using this information, LDA tries to preserve the cluster structure in the low-dimensional space such that the dimension-reduced result can clearly reveal the underlying cluster structure in the input data.In this manner, as shown in Fig.46, LDA has an advantage over most traditional methods such as PCA and MDS in that it can provide a clear cluster structure in the data when the cluster label information is given.", "rank": 657, "paragraph_comparative_number": 1, "entities": [], "id": "p_657"}, "sentences": [{"end": 236941, "text": "Unlike traditional methods such as principal component analysis and multidi- mensional scaling, LDA explicitly utilizes additional cluster label information, which are taken from the clustering module, associated with the input high-dimensional vectors.", "rank": 2004, "start": 236688, "IsComparative": "0", "id": "st_2004"}, {"end": 237147, "text": "Using this information, LDA tries to preserve the cluster structure in the low-dimensional space such that the dimension-reduced result can clearly reveal the underlying cluster structure in the input data.", "rank": 2005, "start": 236941, "IsComparative": "0", "id": "st_2005"}, {"end": 237179, "text": "In this manner, as shown in Fig.", "rank": 2006, "start": 237147, "IsComparative": "0", "id": "st_2006"}, {"end": 237359, "text": "46, LDA has an advantage over most traditional methods such as PCA and MDS in that it can provide a clear cluster structure in the data when the cluster label information is given.", "rank": 2007, "start": 237179, "IsComparative": "1", "id": "st_2007"}]}, {"paragraph_info": {"end": 237519, "start": 237359, "text": "Furthermore, VisIRR provides a slider interface for controlling how compactly each cluster is represented by using regularization on LDA, which enables users to", "rank": 658, "paragraph_comparative_number": 0, "entities": [], "id": "p_658"}, "sentences": [{"end": 237519, "text": "Furthermore, VisIRR provides a slider interface for controlling how compactly each cluster is represented by using regularization on LDA, which enables users to", "rank": 2008, "start": 237359, "IsComparative": "0", "id": "st_2008"}]}, {"paragraph_info": {"end": 237715, "start": 237519, "text": "4The source code is available at http://www.cc.gatech.edu/~hpark/nmfsoftware.php.focus their analyses at either a cluster level or an individual document level.For more details, refer to <35, 36>.", "rank": 659, "paragraph_comparative_number": 2, "entities": [], "id": "p_659"}, "sentences": [{"end": 237600, "text": "4The source code is available at http://www.cc.gatech.edu/~hpark/nmfsoftware.php.", "rank": 2009, "start": 237519, "IsComparative": "0", "id": "st_2009"}, {"end": 237679, "text": "focus their analyses at either a cluster level or an individual document level.", "rank": 2010, "start": 237600, "IsComparative": "1", "id": "st_2010"}, {"end": 237715, "text": "For more details, refer to <35, 36>.", "rank": 2011, "start": 237679, "IsComparative": "1", "id": "st_2011"}]}, {"paragraph_info": {"end": 237730, "start": 237715, "text": "7.5.3 Alignment", "rank": 660, "paragraph_comparative_number": 0, "entities": [], "id": "p_660"}, "sentences": [{"end": 237730, "text": "7.5.3 Alignment", "rank": 2012, "start": 237715, "IsComparative": "0", "id": "st_2012"}]}, {"paragraph_info": {"end": 238472, "start": 237730, "text": "In VisIRR, users can create multiple scatter plots for (1) new parameter values, e.g., the number of clusters in NMF, a regularization value in LDA, and (2) a new set of data from different queries or arbitrary selection by users.In order to maintain con- sistency between different scatter plots and facilitate their easy comparisons, VisIRR provides alignment capabilities on different clustering and dimension reduction re- sults.By aligning clustering results, users can expect that the same cluster index and color indicate semantically similar meanings.On the other hand, by aligning dimension reduction results, users can expect that the same data point is located in a similar position in the 2D space between different scatter plots.", "rank": 661, "paragraph_comparative_number": 3, "entities": [], "id": "p_661"}, "sentences": [{"end": 237960, "text": "In VisIRR, users can create multiple scatter plots for (1) new parameter values, e.g., the number of clusters in NMF, a regularization value in LDA, and (2) a new set of data from different queries or arbitrary selection by users.", "rank": 2013, "start": 237730, "IsComparative": "1", "id": "st_2013"}, {"end": 238163, "text": "In order to maintain con- sistency between different scatter plots and facilitate their easy comparisons, VisIRR provides alignment capabilities on different clustering and dimension reduction re- sults.", "rank": 2014, "start": 237960, "IsComparative": "1", "id": "st_2014"}, {"end": 238289, "text": "By aligning clustering results, users can expect that the same cluster index and color indicate semantically similar meanings.", "rank": 2015, "start": 238163, "IsComparative": "0", "id": "st_2015"}, {"end": 238472, "text": "On the other hand, by aligning dimension reduction results, users can expect that the same data point is located in a similar position in the 2D space between different scatter plots.", "rank": 2016, "start": 238289, "IsComparative": "1", "id": "st_2016"}]}, {"paragraph_info": {"end": 239128, "start": 238472, "text": "To align different clustering results, VisIRR utilizes the Hungarian algorithm <85>.Given two sets of cluster assignments for the same set of documents, the Hungarian algorithm finds the optimal pairwise matching of cluster indices between the two sets so that the number of common data items within matching cluster pairs can be maximized.Based on the resulting matching, VisIRR changes the cluster indices and the colors of the newly created scatter plot with respect to those of the used reference scatter plot.In this manner, VisIRR maintains the cluster indices/colors with their consistent semantic meanings throughout multiple visualization results.", "rank": 662, "paragraph_comparative_number": 2, "entities": [], "id": "p_662"}, "sentences": [{"end": 238556, "text": "To align different clustering results, VisIRR utilizes the Hungarian algorithm <85>.", "rank": 2017, "start": 238472, "IsComparative": "0", "id": "st_2017"}, {"end": 238812, "text": "Given two sets of cluster assignments for the same set of documents, the Hungarian algorithm finds the optimal pairwise matching of cluster indices between the two sets so that the number of common data items within matching cluster pairs can be maximized.", "rank": 2018, "start": 238556, "IsComparative": "1", "id": "st_2018"}, {"end": 238986, "text": "Based on the resulting matching, VisIRR changes the cluster indices and the colors of the newly created scatter plot with respect to those of the used reference scatter plot.", "rank": 2019, "start": 238812, "IsComparative": "0", "id": "st_2019"}, {"end": 239128, "text": "In this manner, VisIRR maintains the cluster indices/colors with their consistent semantic meanings throughout multiple visualization results.", "rank": 2020, "start": 238986, "IsComparative": "1", "id": "st_2020"}]}, {"paragraph_info": {"end": 239566, "start": 239128, "text": "The alignment of different dimension reduction results is based on Procrustes analysis <69, 53>, which best maps one result to the other with only a rotation matrix.In addition, VisIRR extends the original Procrustes analysis by incorporating trans- lation and isotropic scaling factors as well.That is, given two reduced-dimensional matrices X, Y Rmn, where m is the number of dimensions and n is the number of data points, VisIRR solves", "rank": 663, "paragraph_comparative_number": 2, "entities": [], "id": "p_663"}, "sentences": [{"end": 239293, "text": "The alignment of different dimension reduction results is based on Procrustes analysis <69, 53>, which best maps one result to the other with only a rotation matrix.", "rank": 2021, "start": 239128, "IsComparative": "0", "id": "st_2021"}, {"end": 239423, "text": "In addition, VisIRR extends the original Procrustes analysis by incorporating trans- lation and isotropic scaling factors as well.", "rank": 2022, "start": 239293, "IsComparative": "1", "id": "st_2022"}, {"end": 239566, "text": "That is, given two reduced-dimensional matrices X, Y Rmn, where m is the number of dimensions and n is the number of data points, VisIRR solves", "rank": 2023, "start": 239423, "IsComparative": "1", "id": "st_2023"}]}, {"paragraph_info": {"end": 239997, "start": 239566, "text": "where Q  Rmm is an orthogonal matrix (for rotation), X and Y are m-dimensional column vectors (for translation), k is a scalar (for isotropic scaling), and 1n is an n- dimensional column vector whose elements are all 1s.Eq.(42) is efficiently solved by using eigendecomposition.These alignment functionalities help users understand how similarly/differently the corresponding data items/clusters are placed between different views.", "rank": 664, "paragraph_comparative_number": 1, "entities": [], "id": "p_664"}, "sentences": [{"end": 239786, "text": "where Q  Rmm is an orthogonal matrix (for rotation), X and Y are m-dimensional column vectors (for translation), k is a scalar (for isotropic scaling), and 1n is an n- dimensional column vector whose elements are all 1s.", "rank": 2024, "start": 239566, "IsComparative": "1", "id": "st_2024"}, {"end": 239789, "text": "Eq.", "rank": 2025, "start": 239786, "IsComparative": "0", "id": "st_2025"}, {"end": 239844, "text": "(42) is efficiently solved by using eigendecomposition.", "rank": 2026, "start": 239789, "IsComparative": "0", "id": "st_2026"}, {"end": 239997, "text": "These alignment functionalities help users understand how similarly/differently the corresponding data items/clusters are placed between different views.", "rank": 2027, "start": 239844, "IsComparative": "0", "id": "st_2027"}]}, {"paragraph_info": {"end": 240017, "start": 239997, "text": "7.5.4 Recommendation", "rank": 665, "paragraph_comparative_number": 0, "entities": [], "id": "p_665"}, "sentences": [{"end": 240017, "text": "7.5.4 Recommendation", "rank": 2028, "start": 239997, "IsComparative": "0", "id": "st_2028"}]}, {"paragraph_info": {"end": 240490, "start": 240017, "text": "The main input to the recommendation algorithm is the personalized preference to particular documents, which are interactively assigned by users in a 5-star rating scale, as shown in the bottom-right in Fig.40(B).By default, all the documents are assumed to have a 3-star rating, which is converted to a zero preference value, but users can interactively assign ratings to particular documents, where a 1-star corresponds to a preference value of -2, and 5-star to +2, etc.", "rank": 666, "paragraph_comparative_number": 0, "entities": [], "id": "p_666"}, "sentences": [{"end": 240224, "text": "The main input to the recommendation algorithm is the personalized preference to particular documents, which are interactively assigned by users in a 5-star rating scale, as shown in the bottom-right in Fig.", "rank": 2029, "start": 240017, "IsComparative": "0", "id": "st_2029"}, {"end": 240230, "text": "40(B).", "rank": 2030, "start": 240224, "IsComparative": "0", "id": "st_2030"}, {"end": 240490, "text": "By default, all the documents are assumed to have a 3-star rating, which is converted to a zero preference value, but users can interactively assign ratings to particular documents, where a 1-star corresponds to a preference value of -2, and 5-star to +2, etc.", "rank": 2031, "start": 240230, "IsComparative": "0", "id": "st_2031"}]}, {"paragraph_info": {"end": 241781, "start": 240490, "text": "Given these user preference information, VisIRR identifies the recommended doc- uments by performing a PageRank-style graph diffusion algorithm on a weighted graph of the entire document set.As briefly discussed in Section 7.4, such a graph can be based on either contents, a citation network, or co-authorship depending on users choice.Particularly, VisIRR has adopted a heat-kernel-based algorithm <40>, which gives a much faster convergence than the other traditional algorithms.In de- tail, given an input graph W  RNN between N documents, where each column of W is normalized such that its sum is equal to one, and a user preference vector p  RN1, where the i-th component pi is the preference value, VisIRR computes the An intuitive explanation of this formulation is that the preference value piof node i is propagated to its neighbor nodes with the corresponding weights specified in the graph W at the first iteration, and then the resulting values are then propagated again with the same graph W with the scale factor (1  ) at the next iteration, and so on.Finally, those values computed from each iteration is added up, forming a final recommendation score vector r. Once the computation is done, VisIRR presents the documents with the biggest scores in r as the recommended ones.", "rank": 667, "paragraph_comparative_number": 2, "entities": [], "id": "p_667"}, "sentences": [{"end": 240681, "text": "Given these user preference information, VisIRR identifies the recommended doc- uments by performing a PageRank-style graph diffusion algorithm on a weighted graph of the entire document set.", "rank": 2032, "start": 240490, "IsComparative": "1", "id": "st_2032"}, {"end": 240827, "text": "As briefly discussed in Section 7.4, such a graph can be based on either contents, a citation network, or co-authorship depending on users choice.", "rank": 2033, "start": 240681, "IsComparative": "0", "id": "st_2033"}, {"end": 240972, "text": "Particularly, VisIRR has adopted a heat-kernel-based algorithm <40>, which gives a much faster convergence than the other traditional algorithms.", "rank": 2034, "start": 240827, "IsComparative": "0", "id": "st_2034"}, {"end": 241557, "text": "In de- tail, given an input graph W  RNN between N documents, where each column of W is normalized such that its sum is equal to one, and a user preference vector p  RN1, where the i-th component pi is the preference value, VisIRR computes the An intuitive explanation of this formulation is that the preference value piof node i is propagated to its neighbor nodes with the corresponding weights specified in the graph W at the first iteration, and then the resulting values are then propagated again with the same graph W with the scale factor (1  ) at the next iteration, and so on.", "rank": 2035, "start": 240972, "IsComparative": "1", "id": "st_2035"}, {"end": 241781, "text": "Finally, those values computed from each iteration is added up, forming a final recommendation score vector r. Once the computation is done, VisIRR presents the documents with the biggest scores in r as the recommended ones.", "rank": 2036, "start": 241557, "IsComparative": "0", "id": "st_2036"}]}, {"paragraph_info": {"end": 242507, "start": 241781, "text": "One may think that Eq.(43) is computationally intensive because our input graph W is very large-scale.However, all the computations, which are basically matrix- vector multiplications, are performed based on sparse representations.Therefore, as long as W and p have few non-zero entries, the computation is typically done fast.Furthermore, VisIRR supports the capabilities of interactively adding/removing the rated documents as well as changing the ratings of the existing documents.Such computations are performed dynamically per their individual interactions, which es- sentially makes p have only one non-zero entry.In this way, VisIRR maintains the real-time efficiency of computations during users frequent interactions.", "rank": 668, "paragraph_comparative_number": 2, "entities": [], "id": "p_668"}, "sentences": [{"end": 241803, "text": "One may think that Eq.", "rank": 2037, "start": 241781, "IsComparative": "0", "id": "st_2037"}, {"end": 241883, "text": "(43) is computationally intensive because our input graph W is very large-scale.", "rank": 2038, "start": 241803, "IsComparative": "1", "id": "st_2038"}, {"end": 242012, "text": "However, all the computations, which are basically matrix- vector multiplications, are performed based on sparse representations.", "rank": 2039, "start": 241883, "IsComparative": "1", "id": "st_2039"}, {"end": 242108, "text": "Therefore, as long as W and p have few non-zero entries, the computation is typically done fast.", "rank": 2040, "start": 242012, "IsComparative": "0", "id": "st_2040"}, {"end": 242265, "text": "Furthermore, VisIRR supports the capabilities of interactively adding/removing the rated documents as well as changing the ratings of the existing documents.", "rank": 2041, "start": 242108, "IsComparative": "0", "id": "st_2041"}, {"end": 242401, "text": "Such computations are performed dynamically per their individual interactions, which es- sentially makes p have only one non-zero entry.", "rank": 2042, "start": 242265, "IsComparative": "0", "id": "st_2042"}, {"end": 242507, "text": "In this way, VisIRR maintains the real-time efficiency of computations during users frequent interactions.", "rank": 2043, "start": 242401, "IsComparative": "0", "id": "st_2043"}]}, {"paragraph_info": {"end": 242527, "start": 242507, "text": "7.5.5 Implementation", "rank": 669, "paragraph_comparative_number": 0, "entities": [], "id": "p_669"}, "sentences": [{"end": 242527, "text": "7.5.5 Implementation", "rank": 2044, "start": 242507, "IsComparative": "0", "id": "st_2044"}]}, {"paragraph_info": {"end": 243122, "start": 242527, "text": "The system is mainly implemented in JAVA for front-end UI and rendering modules, which are partly based on the FODAVA Testbed system <32>.NetBeans Rich Client Platform and IDE5 have been used for flexible window management.The back-end computational modules NMF and LDA are originally written in MATLAB but we have wrapped them into a JAVA library by using a Matlab built-in functionality called Javabuilder.6 Since the library made in this manner is self-contained, VisIRR does not require an actual Matlab to be installed.For querying and accessing with the database, we have used H2 library.7", "rank": 670, "paragraph_comparative_number": 0, "entities": [], "id": "p_670"}, "sentences": [{"end": 242665, "text": "The system is mainly implemented in JAVA for front-end UI and rendering modules, which are partly based on the FODAVA Testbed system <32>.", "rank": 2045, "start": 242527, "IsComparative": "0", "id": "st_2045"}, {"end": 242750, "text": "NetBeans Rich Client Platform and IDE5 have been used for flexible window management.", "rank": 2046, "start": 242665, "IsComparative": "0", "id": "st_2046"}, {"end": 243051, "text": "The back-end computational modules NMF and LDA are originally written in MATLAB but we have wrapped them into a JAVA library by using a Matlab built-in functionality called Javabuilder.6 Since the library made in this manner is self-contained, VisIRR does not require an actual Matlab to be installed.", "rank": 2047, "start": 242750, "IsComparative": "0", "id": "st_2047"}, {"end": 243122, "text": "For querying and accessing with the database, we have used H2 library.7", "rank": 2048, "start": 243051, "IsComparative": "0", "id": "st_2048"}]}, {"paragraph_info": {"end": 243149, "start": 243122, "text": "7.6 Confirmatory User Study", "rank": 671, "paragraph_comparative_number": 0, "entities": [], "id": "p_671"}, "sentences": [{"end": 243149, "text": "7.6 Confirmatory User Study", "rank": 2049, "start": 243122, "IsComparative": "0", "id": "st_2049"}]}, {"paragraph_info": {"end": 243603, "start": 243149, "text": "The evaluation of information visualization and visual analytic systems has been an acknowledged challenge <108>.Insight-based evaluation <114, 109> has gained pop- ularity recently as an alternative to traditional time-and-accuracy measures.As a preliminary gauge of how well our usage scenarios match real user behaviors, we have conducted an evaluation of VisIRR with end users, which consisted of an informal, non-experimental insight-based protocol.", "rank": 672, "paragraph_comparative_number": 1, "entities": [], "id": "p_672"}, "sentences": [{"end": 243262, "text": "The evaluation of information visualization and visual analytic systems has been an acknowledged challenge <108>.", "rank": 2050, "start": 243149, "IsComparative": "0", "id": "st_2050"}, {"end": 243391, "text": "Insight-based evaluation <114, 109> has gained pop- ularity recently as an alternative to traditional time-and-accuracy measures.", "rank": 2051, "start": 243262, "IsComparative": "0", "id": "st_2051"}, {"end": 243603, "text": "As a preliminary gauge of how well our usage scenarios match real user behaviors, we have conducted an evaluation of VisIRR with end users, which consisted of an informal, non-experimental insight-based protocol.", "rank": 2052, "start": 243391, "IsComparative": "1", "id": "st_2052"}]}, {"paragraph_info": {"end": 244371, "start": 243603, "text": "The design of this study is evidence-by-existence.That is, our goal is to provide some support of our implicit VisIRR design claims.For example, we seek to show that recommendations outside the initial query set are useful to some people and they can find useful documents with VisIRR.It is not an experimental design as it includes no control condition, so we cannot and do not make any relative claims about VisIRRs effectiveness compared to other research or commercial alternatives (e.g., Google Scholar).Instead, our purpose is modest: demonstrate VisIRR can meet its intended purpose for real users (providing evidence that our imagined user scenarios above are valid), and provide direction for a future, comprehensive experimental or quasi-experimental design.", "rank": 673, "paragraph_comparative_number": 2, "entities": [], "id": "p_673"}, "sentences": [{"end": 243653, "text": "The design of this study is evidence-by-existence.", "rank": 2053, "start": 243603, "IsComparative": "0", "id": "st_2053"}, {"end": 243735, "text": "That is, our goal is to provide some support of our implicit VisIRR design claims.", "rank": 2054, "start": 243653, "IsComparative": "1", "id": "st_2054"}, {"end": 243888, "text": "For example, we seek to show that recommendations outside the initial query set are useful to some people and they can find useful documents with VisIRR.", "rank": 2055, "start": 243735, "IsComparative": "0", "id": "st_2055"}, {"end": 244112, "text": "It is not an experimental design as it includes no control condition, so we cannot and do not make any relative claims about VisIRRs effectiveness compared to other research or commercial alternatives (e.g., Google Scholar).", "rank": 2056, "start": 243888, "IsComparative": "0", "id": "st_2056"}, {"end": 244371, "text": "Instead, our purpose is modest: demonstrate VisIRR can meet its intended purpose for real users (providing evidence that our imagined user scenarios above are valid), and provide direction for a future, comprehensive experimental or quasi-experimental design.", "rank": 2057, "start": 244112, "IsComparative": "1", "id": "st_2057"}]}, {"paragraph_info": {"end": 244399, "start": 244371, "text": "7.6.1 Method and Limitations", "rank": 674, "paragraph_comparative_number": 0, "entities": [], "id": "p_674"}, "sentences": [{"end": 244399, "text": "7.6.1 Method and Limitations", "rank": 2058, "start": 244371, "IsComparative": "0", "id": "st_2058"}]}, {"paragraph_info": {"end": 245175, "start": 244399, "text": "Participants in the study used VisIRR implemented with the same ArnetMiner-based set of academic articles described in the usage scenarios above.After completing a consent form and a brief demographics questionnaire, they were provided a live demo of the system usage scenario (lasting 5-10 minutes, depending on questions).Partici- pants then used the system to conduct searches of their own choosing and to complete a set of pre-defined tasks concerning either ubiquitous computing or information visu- alization (e.g., Describe any apparent subfields or application areas of information visualization.).Finally, we deployed a version of the IBM Computer System Usabil- ity Questionnaire (CSUQ) <90> along with a few other subjective assessment questions specific to VisIRR.", "rank": 675, "paragraph_comparative_number": 3, "entities": [], "id": "p_675"}, "sentences": [{"end": 244544, "text": "Participants in the study used VisIRR implemented with the same ArnetMiner-based set of academic articles described in the usage scenarios above.", "rank": 2059, "start": 244399, "IsComparative": "0", "id": "st_2059"}, {"end": 244723, "text": "After completing a consent form and a brief demographics questionnaire, they were provided a live demo of the system usage scenario (lasting 5-10 minutes, depending on questions).", "rank": 2060, "start": 244544, "IsComparative": "1", "id": "st_2060"}, {"end": 245005, "text": "Partici- pants then used the system to conduct searches of their own choosing and to complete a set of pre-defined tasks concerning either ubiquitous computing or information visu- alization (e.g., Describe any apparent subfields or application areas of information visualization.).", "rank": 2061, "start": 244723, "IsComparative": "1", "id": "st_2061"}, {"end": 245175, "text": "Finally, we deployed a version of the IBM Computer System Usabil- ity Questionnaire (CSUQ) <90> along with a few other subjective assessment questions specific to VisIRR.", "rank": 2062, "start": 245005, "IsComparative": "1", "id": "st_2062"}]}, {"paragraph_info": {"end": 245617, "start": 245175, "text": "The system was installed on a workstation with dual 2.5GHz Intel Xeon processors and 128GB RAM running 64-bit Windows 7, though the Java VM memory limit was set to only 8 GB.It was connected to both a 30 monitor (1920x1200) and a 19 monitor (1280x1024); users were free to arrange windows on either monitor, but most chose to use the majority of the 30 screen for the VisIRR windows and dialogs with the task response window on the 19 screen.", "rank": 676, "paragraph_comparative_number": 1, "entities": [], "id": "p_676"}, "sentences": [{"end": 245349, "text": "The system was installed on a workstation with dual 2.5GHz Intel Xeon processors and 128GB RAM running 64-bit Windows 7, though the Java VM memory limit was set to only 8 GB.", "rank": 2063, "start": 245175, "IsComparative": "1", "id": "st_2063"}, {"end": 245617, "text": "It was connected to both a 30 monitor (1920x1200) and a 19 monitor (1280x1024); users were free to arrange windows on either monitor, but most chose to use the majority of the 30 screen for the VisIRR windows and dialogs with the task response window on the 19 screen.", "rank": 2064, "start": 245349, "IsComparative": "0", "id": "st_2064"}]}, {"paragraph_info": {"end": 246389, "start": 245617, "text": "We recruited 7 male Ph.D.students between the ages of 24-40 enrolled in various technical degree programs (engineering, computer science, robotics).As such, they all had experience doing academic literature searches using online resources such as Google, Google Scholar, the IEEE/ACM digital libraries, etc.We asked participants to self-rate their familiarity with information visualization and ubiquitous computing literature; all self-rated 4 or less on a 7-point Likert scale for information visualization and 6 of the 7 did so for ubiquitous computing.Participants completed tasks for the area with which they were less familiar.The VisIRR system was instrumented to log the UI actions shown in Table 9.We non-intrusively observed users while they completed the tasks.", "rank": 677, "paragraph_comparative_number": 3, "entities": [], "id": "p_677"}, "sentences": [{"end": 245642, "text": "We recruited 7 male Ph.D.", "rank": 2065, "start": 245617, "IsComparative": "0", "id": "st_2065"}, {"end": 245765, "text": "students between the ages of 24-40 enrolled in various technical degree programs (engineering, computer science, robotics).", "rank": 2066, "start": 245642, "IsComparative": "0", "id": "st_2066"}, {"end": 245924, "text": "As such, they all had experience doing academic literature searches using online resources such as Google, Google Scholar, the IEEE/ACM digital libraries, etc.", "rank": 2067, "start": 245765, "IsComparative": "1", "id": "st_2067"}, {"end": 246173, "text": "We asked participants to self-rate their familiarity with information visualization and ubiquitous computing literature; all self-rated 4 or less on a 7-point Likert scale for information visualization and 6 of the 7 did so for ubiquitous computing.", "rank": 2068, "start": 245924, "IsComparative": "1", "id": "st_2068"}, {"end": 246250, "text": "Participants completed tasks for the area with which they were less familiar.", "rank": 2069, "start": 246173, "IsComparative": "1", "id": "st_2069"}, {"end": 246324, "text": "The VisIRR system was instrumented to log the UI actions shown in Table 9.", "rank": 2070, "start": 246250, "IsComparative": "0", "id": "st_2070"}, {"end": 246389, "text": "We non-intrusively observed users while they completed the tasks.", "rank": 2071, "start": 246324, "IsComparative": "0", "id": "st_2071"}]}, {"paragraph_info": {"end": 246812, "start": 246389, "text": "We present only a few quantitative measures in our results and no mean values as the limited sample and non-experimental nature of the study would render them specious.The tooltip counts in Table 9 are somewhat exaggerated because the VisIRR tooltips have a very short timeout triggering their appearance, meaning many tooltips could be triggered just from panning over one of the document lists or through the scatterplot.", "rank": 678, "paragraph_comparative_number": 1, "entities": [], "id": "p_678"}, "sentences": [{"end": 246557, "text": "We present only a few quantitative measures in our results and no mean values as the limited sample and non-experimental nature of the study would render them specious.", "rank": 2072, "start": 246389, "IsComparative": "0", "id": "st_2072"}, {"end": 246812, "text": "The tooltip counts in Table 9 are somewhat exaggerated because the VisIRR tooltips have a very short timeout triggering their appearance, meaning many tooltips could be triggered just from panning over one of the document lists or through the scatterplot.", "rank": 2073, "start": 246557, "IsComparative": "1", "id": "st_2073"}]}, {"paragraph_info": {"end": 246840, "start": 246812, "text": "7.6.2 Results and Discussion", "rank": 679, "paragraph_comparative_number": 1, "entities": [], "id": "p_679"}, "sentences": [{"end": 246840, "text": "7.6.2 Results and Discussion", "rank": 2074, "start": 246812, "IsComparative": "1", "id": "st_2074"}]}, {"paragraph_info": {"end": 247293, "start": 246840, "text": "Table 9 shows the raw action counts across all 7 users and all tasks.Those counts match our subjective impressions of watching users complete tasks: they consistently made use of the major VisIRR features (visualization, ratings and recommendations and details-on-demand).Since one of our most basic questions was whether users would actually make use of the more novel features like ratings and recommendations, this preliminary result was encouraging.", "rank": 680, "paragraph_comparative_number": 3, "entities": [], "id": "p_680"}, "sentences": [{"end": 246909, "text": "Table 9 shows the raw action counts across all 7 users and all tasks.", "rank": 2075, "start": 246840, "IsComparative": "1", "id": "st_2075"}, {"end": 247112, "text": "Those counts match our subjective impressions of watching users complete tasks: they consistently made use of the major VisIRR features (visualization, ratings and recommendations and details-on-demand).", "rank": 2076, "start": 246909, "IsComparative": "1", "id": "st_2076"}, {"end": 247293, "text": "Since one of our most basic questions was whether users would actually make use of the more novel features like ratings and recommendations, this preliminary result was encouraging.", "rank": 2077, "start": 247112, "IsComparative": "1", "id": "st_2077"}]}, {"paragraph_info": {"end": 248017, "start": 247293, "text": "All users made at least 9 distinct document ratings (again, across all tasks), and interestingly did so relatively evenly from different portions of the UI (the recom- mended, rating and query lists, and the scatterplot).Document details were dispro- portionately triggered from the visualization (112/146), indicating both that partici- pants interacted with the visualization and drilled down into document details from there.This matches both our subjective observations and post-test user comments like Its good to have that first clustering result ... Its easy to go deeper down from one or two clusters.Unfortunately, the logging does not distinguish between regular and recommended document nodes in the scatter plot.", "rank": 681, "paragraph_comparative_number": 1, "entities": [], "id": "p_681"}, "sentences": [{"end": 247514, "text": "All users made at least 9 distinct document ratings (again, across all tasks), and interestingly did so relatively evenly from different portions of the UI (the recom- mended, rating and query lists, and the scatterplot).", "rank": 2078, "start": 247293, "IsComparative": "0", "id": "st_2078"}, {"end": 247721, "text": "Document details were dispro- portionately triggered from the visualization (112/146), indicating both that partici- pants interacted with the visualization and drilled down into document details from there.", "rank": 2079, "start": 247514, "IsComparative": "0", "id": "st_2079"}, {"end": 247902, "text": "This matches both our subjective observations and post-test user comments like Its good to have that first clustering result ... Its easy to go deeper down from one or two clusters.", "rank": 2080, "start": 247721, "IsComparative": "1", "id": "st_2080"}, {"end": 248017, "text": "Unfortunately, the logging does not distinguish between regular and recommended document nodes in the scatter plot.", "rank": 2081, "start": 247902, "IsComparative": "0", "id": "st_2081"}]}, {"paragraph_info": {"end": 248475, "start": 248017, "text": "On the subjective CSUQ, scores were generally 5 or higher, with the lowest rated scores coming on the questions The system has all the functions and capabilities I expect it to have; The system gives error messages that clearly tell me how to fix problems; and Whenever I make a mistake using the system, I recover easily and quickly.We suspect these ratings reflect occasional software bugs and crashes that occurred during some of the participant sessions.", "rank": 682, "paragraph_comparative_number": 2, "entities": [], "id": "p_682"}, "sentences": [{"end": 248351, "text": "On the subjective CSUQ, scores were generally 5 or higher, with the lowest rated scores coming on the questions The system has all the functions and capabilities I expect it to have; The system gives error messages that clearly tell me how to fix problems; and Whenever I make a mistake using the system, I recover easily and quickly.", "rank": 2082, "start": 248017, "IsComparative": "1", "id": "st_2082"}, {"end": 248475, "text": "We suspect these ratings reflect occasional software bugs and crashes that occurred during some of the participant sessions.", "rank": 2083, "start": 248351, "IsComparative": "1", "id": "st_2083"}]}, {"paragraph_info": {"end": 249218, "start": 248475, "text": "Our results also suggest a potential interesting contrast in user behavior with more traditional keyword-based search algorithms: one might expect in exploratory tasks with keyword engines to see multiple iterations of keyword refinement and result inspection for a given task or user.However, our users performed relatively few filter actions (all keyword refinements rather than by author, time or citation).However, because VisIRR recommendations expand the search query outside its original bounds (and highlight those nodes which are outside those bounds), iterating keyword terms is less necessary, though future work is necessary to confirm this idea, or to gauge whether this approach is more or less effective than keyword refinement.", "rank": 683, "paragraph_comparative_number": 3, "entities": [], "id": "p_683"}, "sentences": [{"end": 248760, "text": "Our results also suggest a potential interesting contrast in user behavior with more traditional keyword-based search algorithms: one might expect in exploratory tasks with keyword engines to see multiple iterations of keyword refinement and result inspection for a given task or user.", "rank": 2084, "start": 248475, "IsComparative": "1", "id": "st_2084"}, {"end": 248885, "text": "However, our users performed relatively few filter actions (all keyword refinements rather than by author, time or citation).", "rank": 2085, "start": 248760, "IsComparative": "1", "id": "st_2085"}, {"end": 249218, "text": "However, because VisIRR recommendations expand the search query outside its original bounds (and highlight those nodes which are outside those bounds), iterating keyword terms is less necessary, though future work is necessary to confirm this idea, or to gauge whether this approach is more or less effective than keyword refinement.", "rank": 2086, "start": 248885, "IsComparative": "1", "id": "st_2086"}]}, {"paragraph_info": {"end": 249543, "start": 249218, "text": "Of course, we would hypothesize that rating-based refinement is more productive since it does require less user expertise at generating useful keyword sequences; at least one user agreed, saying that VisIRR ... is definitely much better than blindly searching Google Scholar or basic search engines using just a few keywords.", "rank": 684, "paragraph_comparative_number": 1, "entities": [], "id": "p_684"}, "sentences": [{"end": 249543, "text": "Of course, we would hypothesize that rating-based refinement is more productive since it does require less user expertise at generating useful keyword sequences; at least one user agreed, saying that VisIRR ... is definitely much better than blindly searching Google Scholar or basic search engines using just a few keywords.", "rank": 2087, "start": 249218, "IsComparative": "1", "id": "st_2087"}]}, {"paragraph_info": {"end": 249558, "start": 249543, "text": "7.7 Conclusions", "rank": 685, "paragraph_comparative_number": 0, "entities": [], "id": "p_685"}, "sentences": [{"end": 249558, "text": "7.7 Conclusions", "rank": 2088, "start": 249543, "IsComparative": "0", "id": "st_2088"}]}, {"paragraph_info": {"end": 250161, "start": 249558, "text": "In this chapter, we have presented a visual analytics system called VisIRR, an inter- active visual information retrieval and recommendation system for document discov- ery.One of the primary contributions of VisIRR is that it has effectively combined both paradigms of passive query process and active recommendation by reflecting the user preference feedback.In addition, VisIRR directly tackles a large-scale docu- ment corpus via efficient data management and new data updating as well as a suite of state-of-the-art computational methods such as NMF, LDA, and graph diffusion- based recommendation.", "rank": 686, "paragraph_comparative_number": 0, "entities": [], "id": "p_686"}, "sentences": [{"end": 249731, "text": "In this chapter, we have presented a visual analytics system called VisIRR, an inter- active visual information retrieval and recommendation system for document discov- ery.", "rank": 2089, "start": 249558, "IsComparative": "0", "id": "st_2089"}, {"end": 249919, "text": "One of the primary contributions of VisIRR is that it has effectively combined both paradigms of passive query process and active recommendation by reflecting the user preference feedback.", "rank": 2090, "start": 249731, "IsComparative": "0", "id": "st_2090"}, {"end": 250161, "text": "In addition, VisIRR directly tackles a large-scale docu- ment corpus via efficient data management and new data updating as well as a suite of state-of-the-art computational methods such as NMF, LDA, and graph diffusion- based recommendation.", "rank": 2091, "start": 249919, "IsComparative": "0", "id": "st_2091"}]}, {"paragraph_info": {"end": 250200, "start": 250161, "text": "Our future work includes the following.", "rank": 687, "paragraph_comparative_number": 1, "entities": [], "id": "p_687"}, "sentences": [{"end": 250200, "text": "Our future work includes the following.", "rank": 2092, "start": 250161, "IsComparative": "1", "id": "st_2092"}]}, {"paragraph_info": {"end": 250792, "start": 250200, "text": "Collaborative filtering-based recommendation: In addition to the preference- based recommendation we have taken, it would be more effective if VisIRR could support collaborative filtering-based approach <22> by using multiple other users preference information.However, collecting this preference information from various users is not easy.In this respect, VisIRR could conversely be used as an easy visual interactive tool to collect these preference information after deployed to many users, just as we have collected various information about the user interaction history in Section (7.6).", "rank": 688, "paragraph_comparative_number": 1, "entities": [], "id": "p_688"}, "sentences": [{"end": 250461, "text": "Collaborative filtering-based recommendation: In addition to the preference- based recommendation we have taken, it would be more effective if VisIRR could support collaborative filtering-based approach <22> by using multiple other users preference information.", "rank": 2093, "start": 250200, "IsComparative": "1", "id": "st_2093"}, {"end": 250540, "text": "However, collecting this preference information from various users is not easy.", "rank": 2094, "start": 250461, "IsComparative": "0", "id": "st_2094"}, {"end": 250792, "text": "In this respect, VisIRR could conversely be used as an easy visual interactive tool to collect these preference information after deployed to many users, just as we have collected various information about the user interaction history in Section (7.6).", "rank": 2095, "start": 250540, "IsComparative": "0", "id": "st_2095"}]}, {"paragraph_info": {"end": 251366, "start": 250792, "text": "Fast interactive clustering and layout: We found that many users often com- plained about visualization not coming up immediately, which is due to high computation time.When hundreds or thousands of documents are involved, the clustering and the dimension reduction computation typically takes from a few seconds to a minute.In addition, the user sometimes wanted to move doc- uments/clusters to see what other documents/clusters move correspondingly.The fast and interactive clustering and layout algorithms incorporating this user feedback would help VisIRR substantially.", "rank": 689, "paragraph_comparative_number": 0, "entities": [], "id": "p_689"}, "sentences": [{"end": 250961, "text": "Fast interactive clustering and layout: We found that many users often com- plained about visualization not coming up immediately, which is due to high computation time.", "rank": 2096, "start": 250792, "IsComparative": "0", "id": "st_2096"}, {"end": 251117, "text": "When hundreds or thousands of documents are involved, the clustering and the dimension reduction computation typically takes from a few seconds to a minute.", "rank": 2097, "start": 250961, "IsComparative": "0", "id": "st_2097"}, {"end": 251243, "text": "In addition, the user sometimes wanted to move doc- uments/clusters to see what other documents/clusters move correspondingly.", "rank": 2098, "start": 251117, "IsComparative": "0", "id": "st_2098"}, {"end": 251366, "text": "The fast and interactive clustering and layout algorithms incorporating this user feedback would help VisIRR substantially.", "rank": 2099, "start": 251243, "IsComparative": "0", "id": "st_2099"}]}, {"paragraph_info": {"end": 251406, "start": 251366, "text": "CHAPTER VIII CONCLUSIONS AND FUTURE WORK", "rank": 690, "paragraph_comparative_number": 0, "entities": [], "id": "p_690"}, "sentences": [{"end": 251406, "text": "CHAPTER VIII CONCLUSIONS AND FUTURE WORK", "rank": 2100, "start": 251366, "IsComparative": "0", "id": "st_2100"}]}, {"paragraph_info": {"end": 251434, "start": 251406, "text": "8.1 Summary of Contributions", "rank": 691, "paragraph_comparative_number": 0, "entities": [], "id": "p_691"}, "sentences": [{"end": 251434, "text": "8.1 Summary of Contributions", "rank": 2101, "start": 251406, "IsComparative": "0", "id": "st_2101"}]}, {"paragraph_info": {"end": 252384, "start": 251434, "text": "In this thesis, we have discussed how to tightly integrate automated computational approaches and interactive visualization approaches for large-scale high-dimensional data analysis such as images and text documents.Even with a clear motivation of the integration between them, there exist several hurdles such as significant computational time and interpretation difficulties.To handle these problems, the thesis presents several ways to customize the computational techniques in terms of the theoretical re-formulation and algorithmic re-design.Such improved algorithms make it possible for complicated and computationally intensive techniques to be easily integrated into visual analytics scenarios.Based on the redesigned techniques, the thesis includes development of an actual visual analytics system that tightly integrates the advanced computational techniques and enables users to take advantage of them in practical data analysis scenarios.", "rank": 692, "paragraph_comparative_number": 3, "entities": [], "id": "p_692"}, "sentences": [{"end": 251650, "text": "In this thesis, we have discussed how to tightly integrate automated computational approaches and interactive visualization approaches for large-scale high-dimensional data analysis such as images and text documents.", "rank": 2102, "start": 251434, "IsComparative": "1", "id": "st_2102"}, {"end": 251811, "text": "Even with a clear motivation of the integration between them, there exist several hurdles such as significant computational time and interpretation difficulties.", "rank": 2103, "start": 251650, "IsComparative": "1", "id": "st_2103"}, {"end": 251981, "text": "To handle these problems, the thesis presents several ways to customize the computational techniques in terms of the theoretical re-formulation and algorithmic re-design.", "rank": 2104, "start": 251811, "IsComparative": "0", "id": "st_2104"}, {"end": 252136, "text": "Such improved algorithms make it possible for complicated and computationally intensive techniques to be easily integrated into visual analytics scenarios.", "rank": 2105, "start": 251981, "IsComparative": "0", "id": "st_2105"}, {"end": 252384, "text": "Based on the redesigned techniques, the thesis includes development of an actual visual analytics system that tightly integrates the advanced computational techniques and enables users to take advantage of them in practical data analysis scenarios.", "rank": 2106, "start": 252136, "IsComparative": "1", "id": "st_2106"}]}, {"paragraph_info": {"end": 252454, "start": 252384, "text": "In summary, the contributions of the thesis are summarized as follows:", "rank": 693, "paragraph_comparative_number": 0, "entities": [], "id": "p_693"}, "sentences": [{"end": 252454, "text": "In summary, the contributions of the thesis are summarized as follows:", "rank": 2107, "start": 252384, "IsComparative": "0", "id": "st_2107"}]}, {"paragraph_info": {"end": 252799, "start": 252454, "text": "1.A theoretical framework of visualizing clustered high-dimensional data via di- mension reduction.The proposed two-stage framework enables various com- binations and their interpretations of several well-known supervised and unsu- pervised dimension reduction methods to obtain appropriate 2D/3D represen- tations of high-dimensional data.<35>.", "rank": 694, "paragraph_comparative_number": 3, "entities": [], "id": "p_694"}, "sentences": [{"end": 252456, "text": "1.", "rank": 2108, "start": 252454, "IsComparative": "1", "id": "st_2108"}, {"end": 252553, "text": "A theoretical framework of visualizing clustered high-dimensional data via di- mension reduction.", "rank": 2109, "start": 252456, "IsComparative": "0", "id": "st_2109"}, {"end": 252794, "text": "The proposed two-stage framework enables various com- binations and their interpretations of several well-known supervised and unsu- pervised dimension reduction methods to obtain appropriate 2D/3D represen- tations of high-dimensional data.", "rank": 2110, "start": 252553, "IsComparative": "1", "id": "st_2110"}, {"end": 252799, "text": "<35>.", "rank": 2111, "start": 252794, "IsComparative": "1", "id": "st_2111"}]}, {"paragraph_info": {"end": 253240, "start": 252799, "text": "2.An algorithmic redesign of computational modules to enable real-time visualiza- tion and interaction with computationally intensive algorithms and large-scale data.The presented parametric updating algorithms will support one of the most essential interactions, changing the parameters, and the iteration-level in- tegration of computational modules with interactive visualization will help users quickly explore the results visually <34>.", "rank": 695, "paragraph_comparative_number": 2, "entities": [], "id": "p_695"}, "sentences": [{"end": 252801, "text": "2.", "rank": 2112, "start": 252799, "IsComparative": "1", "id": "st_2112"}, {"end": 252965, "text": "An algorithmic redesign of computational modules to enable real-time visualiza- tion and interaction with computationally intensive algorithms and large-scale data.", "rank": 2113, "start": 252801, "IsComparative": "1", "id": "st_2113"}, {"end": 253240, "text": "The presented parametric updating algorithms will support one of the most essential interactions, changing the parameters, and the iteration-level in- tegration of computational modules with interactive visualization will help users quickly explore the results visually <34>.", "rank": 2114, "start": 252965, "IsComparative": "0", "id": "st_2114"}]}, {"paragraph_info": {"end": 253665, "start": 253240, "text": "3.Iteration-wise integration framework of computational methods for real-time visualization and interaction.The presented framework and several applications of this idea in existing visual analytics systems, such as Jigsaw, iVisClustering, and the Testbed system, shows the effectiveness of the proposed framework using widely-used computational methods such as PCA, MDS, t-SNE, k-means, and latent Dirichlet allocation <31>.", "rank": 696, "paragraph_comparative_number": 1, "entities": [], "id": "p_696"}, "sentences": [{"end": 253242, "text": "3.", "rank": 2115, "start": 253240, "IsComparative": "1", "id": "st_2115"}, {"end": 253348, "text": "Iteration-wise integration framework of computational methods for real-time visualization and interaction.", "rank": 2116, "start": 253242, "IsComparative": "0", "id": "st_2116"}, {"end": 253665, "text": "The presented framework and several applications of this idea in existing visual analytics systems, such as Jigsaw, iVisClustering, and the Testbed system, shows the effectiveness of the proposed framework using widely-used computational methods such as PCA, MDS, t-SNE, k-means, and latent Dirichlet allocation <31>.", "rank": 2117, "start": 253348, "IsComparative": "0", "id": "st_2117"}]}, {"paragraph_info": {"end": 253741, "start": 253665, "text": "In terms of the developed visual analytics systems, the thesis has presented", "rank": 697, "paragraph_comparative_number": 0, "entities": [], "id": "p_697"}, "sentences": [{"end": 253741, "text": "In terms of the developed visual analytics systems, the thesis has presented", "rank": 2118, "start": 253665, "IsComparative": "0", "id": "st_2118"}]}, {"paragraph_info": {"end": 254186, "start": 253741, "text": "1.Testbed: an interactive visual testbed system for various dimension reduction and clustering methods.The Testbed system brings a wide variety of tradi- tional and state-of-the-art dimension reduction and clustering methods to vi- sual analytics.The Testbed system provides full control of these methods with interactive visual access to their results.In addition, our system offers a flexible extensibility for new data types and methods <32>.", "rank": 698, "paragraph_comparative_number": 4, "entities": [], "id": "p_698"}, "sentences": [{"end": 253743, "text": "1.", "rank": 2119, "start": 253741, "IsComparative": "1", "id": "st_2119"}, {"end": 253844, "text": "Testbed: an interactive visual testbed system for various dimension reduction and clustering methods.", "rank": 2120, "start": 253743, "IsComparative": "0", "id": "st_2120"}, {"end": 253988, "text": "The Testbed system brings a wide variety of tradi- tional and state-of-the-art dimension reduction and clustering methods to vi- sual analytics.", "rank": 2121, "start": 253844, "IsComparative": "1", "id": "st_2121"}, {"end": 254094, "text": "The Testbed system provides full control of these methods with interactive visual access to their results.", "rank": 2122, "start": 253988, "IsComparative": "1", "id": "st_2122"}, {"end": 254186, "text": "In addition, our system offers a flexible extensibility for new data types and methods <32>.", "rank": 2123, "start": 254094, "IsComparative": "1", "id": "st_2123"}]}, {"paragraph_info": {"end": 254988, "start": 254186, "text": "2. iVisClassifier: an interactive visual classification system that uses supervised dimension reduction.iVisClassifier enables users to explore high-dimensional data through a supervised dimension reduction method, LDA.We interpret the effect of regularization in visualization and provide an effective user-interface in which users can control the cluster radii depending on whether they focus on the cluster- or the data-level relationships.In addition, iVisClassifier facilitates the interpretability of the computational model applied to their data.Various views such as parallel coordinates, scatter plots, and heat maps interactively show rich aspects of the data.Finally, we showed that iVisClassifier can efficiently support a user-driven classification process by reducing humans search space,", "rank": 699, "paragraph_comparative_number": 2, "entities": [], "id": "p_699"}, "sentences": [{"end": 254290, "text": "2. iVisClassifier: an interactive visual classification system that uses supervised dimension reduction.", "rank": 2124, "start": 254186, "IsComparative": "0", "id": "st_2124"}, {"end": 254405, "text": "iVisClassifier enables users to explore high-dimensional data through a supervised dimension reduction method, LDA.", "rank": 2125, "start": 254290, "IsComparative": "0", "id": "st_2125"}, {"end": 254629, "text": "We interpret the effect of regularization in visualization and provide an effective user-interface in which users can control the cluster radii depending on whether they focus on the cluster- or the data-level relationships.", "rank": 2126, "start": 254405, "IsComparative": "0", "id": "st_2126"}, {"end": 254739, "text": "In addition, iVisClassifier facilitates the interpretability of the computational model applied to their data.", "rank": 2127, "start": 254629, "IsComparative": "1", "id": "st_2127"}, {"end": 254856, "text": "Various views such as parallel coordinates, scatter plots, and heat maps interactively show rich aspects of the data.", "rank": 2128, "start": 254739, "IsComparative": "0", "id": "st_2128"}, {"end": 254988, "text": "Finally, we showed that iVisClassifier can efficiently support a user-driven classification process by reducing humans search space,", "rank": 2129, "start": 254856, "IsComparative": "1", "id": "st_2129"}]}, {"paragraph_info": {"end": 255162, "start": 254988, "text": "(a) The scatter plot in the screen(b) The scatter plot in the screen(c) The scatter plot in the screen of a resolution 16  12. of a resolution 48  36. of a resolution 80  60.", "rank": 700, "paragraph_comparative_number": 0, "entities": [], "id": "p_700"}, "sentences": [{"end": 255162, "text": "(a) The scatter plot in the screen(b) The scatter plot in the screen(c) The scatter plot in the screen of a resolution 16  12. of a resolution 48  36. of a resolution 80  60.", "rank": 2130, "start": 254988, "IsComparative": "0", "id": "st_2130"}]}, {"paragraph_info": {"end": 255356, "start": 255162, "text": "Figure 47: Hierarchical precision refinement of PCA computational results.1,420 facial image data represented as 11,264-dimensional vectors have been visualized with their person ID color-coded.", "rank": 701, "paragraph_comparative_number": 1, "entities": [], "id": "p_701"}, "sentences": [{"end": 255236, "text": "Figure 47: Hierarchical precision refinement of PCA computational results.", "rank": 2131, "start": 255162, "IsComparative": "0", "id": "st_2131"}, {"end": 255356, "text": "1,420 facial image data represented as 11,264-dimensional vectors have been visualized with their person ID color-coded.", "rank": 2132, "start": 255236, "IsComparative": "1", "id": "st_2132"}]}, {"paragraph_info": {"end": 255484, "start": 255356, "text": "e.g., recomputing LDA with a user-selected subset of data and mutual filtering in parallel coordinates and the scatter plot<36>.", "rank": 702, "paragraph_comparative_number": 1, "entities": [], "id": "p_702"}, "sentences": [{"end": 255484, "text": "e.g., recomputing LDA with a user-selected subset of data and mutual filtering in parallel coordinates and the scatter plot<36>.", "rank": 2133, "start": 255356, "IsComparative": "1", "id": "st_2133"}]}, {"paragraph_info": {"end": 256078, "start": 255484, "text": "3.VisIRR: an interactive visual information retrieval and recommender system for large-scale document data.VisIRR integrates two main notions of information retrieval and personalized recommendation into a single visual analytics system.VisIRR is well-engineered to handle large-scale data and streaming data and utilizes the state-of-the-art clustering and dimension reduction methods such as NMF and LDA.The recommendation module works on an efficient graph diffusion algorithm on large-scale sparse graphs based on various criteria such as content, co-authorhship, and citation network.<33>.", "rank": 703, "paragraph_comparative_number": 4, "entities": [], "id": "p_703"}, "sentences": [{"end": 255486, "text": "3.", "rank": 2134, "start": 255484, "IsComparative": "1", "id": "st_2134"}, {"end": 255591, "text": "VisIRR: an interactive visual information retrieval and recommender system for large-scale document data.", "rank": 2135, "start": 255486, "IsComparative": "1", "id": "st_2135"}, {"end": 255721, "text": "VisIRR integrates two main notions of information retrieval and personalized recommendation into a single visual analytics system.", "rank": 2136, "start": 255591, "IsComparative": "0", "id": "st_2136"}, {"end": 255890, "text": "VisIRR is well-engineered to handle large-scale data and streaming data and utilizes the state-of-the-art clustering and dimension reduction methods such as NMF and LDA.", "rank": 2137, "start": 255721, "IsComparative": "1", "id": "st_2137"}, {"end": 256073, "text": "The recommendation module works on an efficient graph diffusion algorithm on large-scale sparse graphs based on various criteria such as content, co-authorhship, and citation network.", "rank": 2138, "start": 255890, "IsComparative": "0", "id": "st_2138"}, {"end": 256078, "text": "<33>.", "rank": 2139, "start": 256073, "IsComparative": "1", "id": "st_2139"}]}, {"paragraph_info": {"end": 256099, "start": 256078, "text": "8.2 Future Directions", "rank": 704, "paragraph_comparative_number": 0, "entities": [], "id": "p_704"}, "sentences": [{"end": 256099, "text": "8.2 Future Directions", "rank": 2140, "start": 256078, "IsComparative": "0", "id": "st_2140"}]}, {"paragraph_info": {"end": 256809, "start": 256099, "text": "This thesis opens up various future research directions when truly integrating com- putational methods with human-in-the-loop visual analytics approaches.In order for computational methods to be fully utilized in visual analytics, computational meth- ods have to provide users with real-time interactivity and output trustworthiness for users own tasks as detailed in the following subsections.Based on such improve- ments in various ways, users should be able to focus more on their own tasks and goals instead of worrying much about computational methods themselves.In this manner, the visual analytics with computational methods would be able to find more real-world values in practical application domains.", "rank": 705, "paragraph_comparative_number": 1, "entities": [], "id": "p_705"}, "sentences": [{"end": 256253, "text": "This thesis opens up various future research directions when truly integrating com- putational methods with human-in-the-loop visual analytics approaches.", "rank": 2141, "start": 256099, "IsComparative": "0", "id": "st_2141"}, {"end": 256493, "text": "In order for computational methods to be fully utilized in visual analytics, computational meth- ods have to provide users with real-time interactivity and output trustworthiness for users own tasks as detailed in the following subsections.", "rank": 2142, "start": 256253, "IsComparative": "0", "id": "st_2142"}, {"end": 256667, "text": "Based on such improve- ments in various ways, users should be able to focus more on their own tasks and goals instead of worrying much about computational methods themselves.", "rank": 2143, "start": 256493, "IsComparative": "0", "id": "st_2143"}, {"end": 256809, "text": "In this manner, the visual analytics with computational methods would be able to find more real-world values in practical application domains.", "rank": 2144, "start": 256667, "IsComparative": "1", "id": "st_2144"}]}, {"paragraph_info": {"end": 256838, "start": 256809, "text": "8.2.1 Real-time Interactivity", "rank": 706, "paragraph_comparative_number": 0, "entities": [], "id": "p_706"}, "sentences": [{"end": 256838, "text": "8.2.1 Real-time Interactivity", "rank": 2145, "start": 256809, "IsComparative": "0", "id": "st_2145"}]}, {"paragraph_info": {"end": 258284, "start": 256838, "text": "For the former, this thesis already presented one approach called PIVE, described in Chapter 4, where we have exploited the existing characteristics of most modern algorithms that they are mainly based on algorithmic iterations.However, rather revolutionary paradigm changes of computational methods could be considered when designing the algorithms of computational methods for visual analytics applications.One such approach would be to re-design the algorithms by hierarchically refining the precision of the solutions of computational methods.Fig.47 shows an example in which the precision of the computational results are iteratively refined with respect to the increasing screen resolution.During this precision refinement process, the next step of refinement could utilize the results of the previous step, which makes each refinement step efficient.More specifically, to find out the new position of a specific data item in a higher resolution, the refinement process may only need to examine the nearby areas from the previous position.Although we do not provide detailed algorithms on how to realize this approach, one could find relevant literature from other domains.Such literature includes adaptive mesh refinement <18>1 in numerical analysis and wavelet transform <39>2 in image coding/compression.Applying these ideas to the context of integrating computational methods in visual analytics would be a promising research direction.", "rank": 707, "paragraph_comparative_number": 2, "entities": [], "id": "p_707"}, "sentences": [{"end": 257066, "text": "For the former, this thesis already presented one approach called PIVE, described in Chapter 4, where we have exploited the existing characteristics of most modern algorithms that they are mainly based on algorithmic iterations.", "rank": 2146, "start": 256838, "IsComparative": "0", "id": "st_2146"}, {"end": 257247, "text": "However, rather revolutionary paradigm changes of computational methods could be considered when designing the algorithms of computational methods for visual analytics applications.", "rank": 2147, "start": 257066, "IsComparative": "1", "id": "st_2147"}, {"end": 257385, "text": "One such approach would be to re-design the algorithms by hierarchically refining the precision of the solutions of computational methods.", "rank": 2148, "start": 257247, "IsComparative": "0", "id": "st_2148"}, {"end": 257389, "text": "Fig.", "rank": 2149, "start": 257385, "IsComparative": "0", "id": "st_2149"}, {"end": 257534, "text": "47 shows an example in which the precision of the computational results are iteratively refined with respect to the increasing screen resolution.", "rank": 2150, "start": 257389, "IsComparative": "0", "id": "st_2150"}, {"end": 257695, "text": "During this precision refinement process, the next step of refinement could utilize the results of the previous step, which makes each refinement step efficient.", "rank": 2151, "start": 257534, "IsComparative": "0", "id": "st_2151"}, {"end": 257883, "text": "More specifically, to find out the new position of a specific data item in a higher resolution, the refinement process may only need to examine the nearby areas from the previous position.", "rank": 2152, "start": 257695, "IsComparative": "0", "id": "st_2152"}, {"end": 258017, "text": "Although we do not provide detailed algorithms on how to realize this approach, one could find relevant literature from other domains.", "rank": 2153, "start": 257883, "IsComparative": "1", "id": "st_2153"}, {"end": 258151, "text": "Such literature includes adaptive mesh refinement <18>1 in numerical analysis and wavelet transform <39>2 in image coding/compression.", "rank": 2154, "start": 258017, "IsComparative": "0", "id": "st_2154"}, {"end": 258284, "text": "Applying these ideas to the context of integrating computational methods in visual analytics would be a promising research direction.", "rank": 2155, "start": 258151, "IsComparative": "0", "id": "st_2155"}]}, {"paragraph_info": {"end": 259491, "start": 258284, "text": "Another potential idea to achieve real-time interactivity is to confine the data scale.As clearly seen in Fig.47(a), the finite resolution in the screen space introduces the limitation in the number of data items that can be visualized.Suppose there are much more data items than the total number of pixels available.In this case, it does not make sense to compute algorithms on the entire set of data items even though there is no possible way to visualize all of them.This approach is particularly useful when it comes to the computational complexity of algorithms.In principle, as the number of data items increases, the algorithm complexity cannot be more efficient than O(n), which assumes that every data item is processed at least once.Even with such an ideal complexity, a computational bottleneck can exist in real-time visual analytics.The notion of a fixed number of available pixels can turn the algorithm complexity into O(1) in the sense that we can visualize only a specific number of data items at most.One of the easiest ways to select this subset of data is random sampling, although one could adopt other more carefully designed sampling methods that better represent the entire data set.", "rank": 708, "paragraph_comparative_number": 5, "entities": [], "id": "p_708"}, "sentences": [{"end": 258371, "text": "Another potential idea to achieve real-time interactivity is to confine the data scale.", "rank": 2156, "start": 258284, "IsComparative": "0", "id": "st_2156"}, {"end": 258394, "text": "As clearly seen in Fig.", "rank": 2157, "start": 258371, "IsComparative": "0", "id": "st_2157"}, {"end": 258520, "text": "47(a), the finite resolution in the screen space introduces the limitation in the number of data items that can be visualized.", "rank": 2158, "start": 258394, "IsComparative": "1", "id": "st_2158"}, {"end": 258601, "text": "Suppose there are much more data items than the total number of pixels available.", "rank": 2159, "start": 258520, "IsComparative": "0", "id": "st_2159"}, {"end": 258754, "text": "In this case, it does not make sense to compute algorithms on the entire set of data items even though there is no possible way to visualize all of them.", "rank": 2160, "start": 258601, "IsComparative": "1", "id": "st_2160"}, {"end": 258851, "text": "This approach is particularly useful when it comes to the computational complexity of algorithms.", "rank": 2161, "start": 258754, "IsComparative": "1", "id": "st_2161"}, {"end": 259027, "text": "In principle, as the number of data items increases, the algorithm complexity cannot be more efficient than O(n), which assumes that every data item is processed at least once.", "rank": 2162, "start": 258851, "IsComparative": "0", "id": "st_2162"}, {"end": 259130, "text": "Even with such an ideal complexity, a computational bottleneck can exist in real-time visual analytics.", "rank": 2163, "start": 259027, "IsComparative": "0", "id": "st_2163"}, {"end": 259303, "text": "The notion of a fixed number of available pixels can turn the algorithm complexity into O(1) in the sense that we can visualize only a specific number of data items at most.", "rank": 2164, "start": 259130, "IsComparative": "1", "id": "st_2164"}, {"end": 259491, "text": "One of the easiest ways to select this subset of data is random sampling, although one could adopt other more carefully designed sampling methods that better represent the entire data set.", "rank": 2165, "start": 259303, "IsComparative": "1", "id": "st_2165"}]}, {"paragraph_info": {"end": 260553, "start": 259491, "text": "However, some user interactions such as zoom-in/out may require the computa- tional results on the rest of the data items whose results have yet to be computed.However in this case, one can handle the situation via different efficient computa- tions.For example, suppose one wants to perform clustering on a large-scale data set, and the computations have been performed only on a certain subset of data.Then, to obtain the cluster labels of the other data items, one could apply a simple classification method based on already computed clusters.In addition, in the case of dimension reduction, suppose PCA has been computed on a subset of data.Then, the rest of the data can be projected onto the same space via a linear transformation matrix given by PCA, which is a much more efficient process than computing PCA on the entire data set.Although these approximated approaches cannot give the exact same results as the ones generated by using the entire data from the beginning, it is a viable approach to ensure real-time visual analytics for large-scale data.", "rank": 709, "paragraph_comparative_number": 1, "entities": [], "id": "p_709"}, "sentences": [{"end": 259651, "text": "However, some user interactions such as zoom-in/out may require the computa- tional results on the rest of the data items whose results have yet to be computed.", "rank": 2166, "start": 259491, "IsComparative": "0", "id": "st_2166"}, {"end": 259741, "text": "However in this case, one can handle the situation via different efficient computa- tions.", "rank": 2167, "start": 259651, "IsComparative": "1", "id": "st_2167"}, {"end": 259895, "text": "For example, suppose one wants to perform clustering on a large-scale data set, and the computations have been performed only on a certain subset of data.", "rank": 2168, "start": 259741, "IsComparative": "0", "id": "st_2168"}, {"end": 260037, "text": "Then, to obtain the cluster labels of the other data items, one could apply a simple classification method based on already computed clusters.", "rank": 2169, "start": 259895, "IsComparative": "0", "id": "st_2169"}, {"end": 260136, "text": "In addition, in the case of dimension reduction, suppose PCA has been computed on a subset of data.", "rank": 2170, "start": 260037, "IsComparative": "0", "id": "st_2170"}, {"end": 260330, "text": "Then, the rest of the data can be projected onto the same space via a linear transformation matrix given by PCA, which is a much more efficient process than computing PCA on the entire data set.", "rank": 2171, "start": 260136, "IsComparative": "0", "id": "st_2171"}, {"end": 260553, "text": "Although these approximated approaches cannot give the exact same results as the ones generated by using the entire data from the beginning, it is a viable approach to ensure real-time visual analytics for large-scale data.", "rank": 2172, "start": 260330, "IsComparative": "0", "id": "st_2172"}]}, {"paragraph_info": {"end": 260581, "start": 260553, "text": "8.2.2 Output Trustworthiness", "rank": 710, "paragraph_comparative_number": 0, "entities": [], "id": "p_710"}, "sentences": [{"end": 260581, "text": "8.2.2 Output Trustworthiness", "rank": 2173, "start": 260553, "IsComparative": "0", "id": "st_2173"}]}, {"paragraph_info": {"end": 260917, "start": 260581, "text": "Other than the real-time interactive capability, the overall quality of computational results should be reasonably trustworthy enough in practice for users own tasks.Even though a particular computational method gives the best result from the perspective of its own criteria, it may not be a faithfully good quality of results to users.", "rank": 711, "paragraph_comparative_number": 1, "entities": [], "id": "p_711"}, "sentences": [{"end": 260747, "text": "Other than the real-time interactive capability, the overall quality of computational results should be reasonably trustworthy enough in practice for users own tasks.", "rank": 2174, "start": 260581, "IsComparative": "0", "id": "st_2174"}, {"end": 260917, "text": "Even though a particular computational method gives the best result from the perspective of its own criteria, it may not be a faithfully good quality of results to users.", "rank": 2175, "start": 260747, "IsComparative": "1", "id": "st_2175"}]}, {"paragraph_info": {"end": 261796, "start": 260917, "text": "For example in dimension reduction, MDS gives the reduced-dimensional re- sult that best preserves the original pairwise dimension reduction under the low- dimensional space within a given dimension.Fig.48 shows how these pairwise distances in the lower-dimensional space are degraded compared to those in the original high-dimensional space as the target dimension decreases.Each depicted line in Fig.48 represents the pairwise distances as their original values in the high- dimensional space decrease along with a horizontal axis.Fig.48 indicates that MDS significantly distorts the original data relationships by severely decreasing particular pairwise distances while some others are almost preserved.Even though MDS sup- posedly gives the best result in preserving the pairwise distances, one would not be able to trust the result considering such a significant distortion.", "rank": 712, "paragraph_comparative_number": 2, "entities": [], "id": "p_712"}, "sentences": [{"end": 261116, "text": "For example in dimension reduction, MDS gives the reduced-dimensional re- sult that best preserves the original pairwise dimension reduction under the low- dimensional space within a given dimension.", "rank": 2176, "start": 260917, "IsComparative": "0", "id": "st_2176"}, {"end": 261120, "text": "Fig.", "rank": 2177, "start": 261116, "IsComparative": "0", "id": "st_2177"}, {"end": 261293, "text": "48 shows how these pairwise distances in the lower-dimensional space are degraded compared to those in the original high-dimensional space as the target dimension decreases.", "rank": 2178, "start": 261120, "IsComparative": "0", "id": "st_2178"}, {"end": 261319, "text": "Each depicted line in Fig.", "rank": 2179, "start": 261293, "IsComparative": "0", "id": "st_2179"}, {"end": 261450, "text": "48 represents the pairwise distances as their original values in the high- dimensional space decrease along with a horizontal axis.", "rank": 2180, "start": 261319, "IsComparative": "1", "id": "st_2180"}, {"end": 261454, "text": "Fig.", "rank": 2181, "start": 261450, "IsComparative": "0", "id": "st_2181"}, {"end": 261623, "text": "48 indicates that MDS significantly distorts the original data relationships by severely decreasing particular pairwise distances while some others are almost preserved.", "rank": 2182, "start": 261454, "IsComparative": "0", "id": "st_2182"}, {"end": 261796, "text": "Even though MDS sup- posedly gives the best result in preserving the pairwise distances, one would not be able to trust the result considering such a significant distortion.", "rank": 2183, "start": 261623, "IsComparative": "1", "id": "st_2183"}]}, {"paragraph_info": {"end": 262597, "start": 261796, "text": "There are two ways to tackle these problems.The first one would be to de- sign a new computational method based on improved criteria that perceptually make more sense.For instance, one could come up with a new criterion for an alternative method to MDS so that distance losses can be evenly distributed throughout all the pairwise distances.However, such perception-friendly criteria may cause additional computational complexities.Therefore, as another way to tackle the trustworthiness problem, visual analytics could at least provide users with the information about how trustworthy the computational results are by showing the perceptual quality measures.Studying these new computational methods as well as corresponding per- ceptual quality measures would be another promising research direction.", "rank": 713, "paragraph_comparative_number": 4, "entities": [], "id": "p_713"}, "sentences": [{"end": 261840, "text": "There are two ways to tackle these problems.", "rank": 2184, "start": 261796, "IsComparative": "1", "id": "st_2184"}, {"end": 261963, "text": "The first one would be to de- sign a new computational method based on improved criteria that perceptually make more sense.", "rank": 2185, "start": 261840, "IsComparative": "0", "id": "st_2185"}, {"end": 262137, "text": "For instance, one could come up with a new criterion for an alternative method to MDS so that distance losses can be evenly distributed throughout all the pairwise distances.", "rank": 2186, "start": 261963, "IsComparative": "1", "id": "st_2186"}, {"end": 262228, "text": "However, such perception-friendly criteria may cause additional computational complexities.", "rank": 2187, "start": 262137, "IsComparative": "0", "id": "st_2187"}, {"end": 262455, "text": "Therefore, as another way to tackle the trustworthiness problem, visual analytics could at least provide users with the information about how trustworthy the computational results are by showing the perceptual quality measures.", "rank": 2188, "start": 262228, "IsComparative": "1", "id": "st_2188"}, {"end": 262597, "text": "Studying these new computational methods as well as corresponding per- ceptual quality measures would be another promising research direction.", "rank": 2189, "start": 262455, "IsComparative": "1", "id": "st_2189"}]}], "sentence_scenes_info": [{"x": 10, "text": "The latter parts of the thesis focus on the development of visual analytics systems involving the presented compu- tational methods, such as (1) Testbed: an interactive visual testbed system for various dimension reduction and clustering methods, (2) iVisClassifier: an interactive visual classification system using supervised dimension reduction, and (3) VisIRR: an inter- active visual information retrieval and recommender system for large-scale document data."}, {"x": 21, "text": "As an- other example, even though the recent nonlinear kernel-based methods sound appealing, how to determine the optimal kernel parameters, e.g., a bandwidth parameter in Gaussian kernels, is also dependent on the data."}, {"x": 34, "text": "3."}, {"x": 41, "text": "In contrast to the fully automated computational approaches that lack deep under- standing and careful treatment of the data, the area of visual analytics <127, 78>, which is defined as the science of analytical reasoning facilitated by interactive visual inter- faces, has gained increasing interest."}, {"x": 54, "text": "A recently proposed method, latent Dirichlet allocation, has been utilized in visual analytics tools for text documents <134, 50, 38, 37>."}, {"x": 57, "text": "As a result, people still tend to only use a few of the basic computational methods, e.g., principal component analysis (PCA) <74> for dimension reduction and k-means<19> for clustering in many real-world analysis tasks."}, {"x": 71, "text": "I claim that to this end, the computational methods have to be customized and even be re-invented for use in visual analytics, and at the same time, the visual analytics systems have to expose them to users out of a black box via interactive capabilities of choosing the right methods and the best parameters."}, {"x": 75, "text": "The theoretical and algorithmic customization of computational methods will enable their appropriate integration with visual analytics for analyses of complex large-scale high-dimensional data."}, {"x": 78, "text": "More specifically, the thesis mainly focuses on two key categories of computational methods: dimension reduction and clustering."}, {"x": 80, "text": "In other words, dimension reduction can reveal meaningful dimensions or features out of numerous original dimensions as well as provide a means of visualizing high- dimensional data in visual 2D/3D spaces so that analysts can obtain the insight about data relationships with respect to geometric locations of data."}, {"x": 85, "text": "Which characteristics in terms of data, algorithms, and humans, should be exploited in order to make computational methods better support visual ana- lytics?"}, {"x": 107, "text": "It is inevitable that significant information will be lost when reducing the original high dimension of data into 2D/3D in visualization."}, {"x": 115, "text": "Chapter 4 presents another novel approach called PIVE (Per-Iteration Visualization Environment for supporting real-time interactive visualization with computational methods) to make computational methods significantly efficient in visual analytics."}, {"x": 119, "text": "Chapter 5 describes the fundamental visual analytics system called the Testbed system, which makes various traditional and advanced dimension reduction and clus- tering algorithms readily available in visual analytics scenarios."}, {"x": 122, "text": "Chapter 6 presents iVisClassifier, another visual analytics system developed based on the Testbed system."}, {"x": 123, "text": "iVisClassifier is a customized system for classification appli- cations, which mainly utilizes a specific dimension reduction among those available in the Testbed system."}, {"x": 126, "text": "Chapter 6 presents VisIRR, an interactive visual information retrieval and recom- mender system based on the Testbed system."}, {"x": 132, "text": "TWO-STAGE FRAMEWORK FOR VISUALIZATION OF CLUSTERED HIGH-DIMENSIONAL DATA"}, {"x": 133, "text": "In this chapter, we will discuss dimension reduction methods for 2D visualization of high dimensional clustered data."}, {"x": 142, "text": "To make these signatures useful they often need to be transformed into a lower dimension (i.e., 2D or 3D) for a variety of visual representations such as scatter plots."}, {"x": 143, "text": "Many researchers in this community have used a wide assortment of dimension reduction techniques, e.g., self-organizing map (SOM) <83>, principal component analysis (PCA) <75>, multidimensional scaling (MDS) <45>, etc."}, {"x": 153, "text": "However, there are at least two others: outlier and macro structure visualization."}, {"x": 155, "text": "Certain techniques (e.g., PCA) tend to show outliers more readily, however tend to compress the reduced space at the expense of showcasing the outliers."}, {"x": 156, "text": "Other techniques (e.g., SOM) maximize space usage well, but do so at the expense of masking or even hiding those outliers."}, {"x": 163, "text": "The focus of this chapter is the fundamental characteristics of dimension reduction techniques for visualizing high dimensional data in the form of a 2D scatter plot when the data has cluster structure."}, {"x": 165, "text": "To this end, supervised dimension reduction methods that incorporate cluster information such as linear discriminant analysis (LDA) <60> or orthogonal centroid method (OCM) <71> can be naturally considered."}, {"x": 167, "text": "For example, in LDA, the minimum reduced dimension that preserves the cluster struc- ture quality measure defined as a trace maximization problem is one less than the number of clusters in the data in general <68, 67>."}, {"x": 171, "text": "A similar situation may occur when using PCA for visualizing the data not having a cluster structure."}, {"x": 172, "text": "Even though PCA finds the principal axes that maximally capture the variance of the data, when the resulting 2-dimensional representation of the data maintains only a small fraction of the total variance, the relationships of the data in 2 dimension are likely to be highly inconsistent with those in the original dimension."}, {"x": 182, "text": "Specifically, we propose several two-stage methods utilizing linear dimension reduction methods such as LDA, orthogonal centroid method (OCM), and principal component analysis (PCA), and we present their theoretical justifications by mod- eling the optimization criteria for which each method provides the optimal solution."}, {"x": 183, "text": "Also, we illustrate and compare the effectiveness of the proposed methods by show- ing empirical visualization on synthetic and real-world data sets.Although nonlinear dimension reduction methods such as MDS or other manifold learning methods such as isometric feature mapping <125> and locally linear embedding <111> may also be utilized for the effective 2D visualization of high dimensional data, our focus in this chapter is on linear methods."}, {"x": 187, "text": "In Section 2.3, LDA, OCM, and PCA are described based on a unified framework of the scatter matrices and their trace optimization problems."}, {"x": 190, "text": "2.3 Dimension Reduction as Trace Optimization Problem"}, {"x": 192, "text": "Suppose a dimension reducing linear transformation GT  Rlm maps an m- dimensional data vector x to a vector z in an l-dimensional space (m > l):"}, {"x": 193, "text": "Let Ni denote the set of column indices that belong to cluster i, and ni the size of Ni."}, {"x": 196, "text": "(8) and (9), trace(Sb) can be viewed as the squared sum of the pairwise distances between cluster centroids as well as that of the distances between each centroid and the global centroid."}, {"x": 198, "text": "High quality clusters usually have small trace(Sw) and large trace(Sb), relating to the small variance within each cluster and the large distances between clusters."}, {"x": 199, "text": "Subsequently, dimension reduction methods may be intended to maximize trace(GT SbG) and minimize trace(GT SwG) in the reduced dimensional space."}, {"x": 201, "text": "On the other hand, regardless of cluster dependent terms, Sw and Sb, the trace of the total scatter matrix St can be maximized as"}, {"x": 202, "text": "Jt(G) = max trace(GT StG), (13) GT G=I"}, {"x": 203, "text": "which turns out to be the criterion of PCA."}, {"x": 205, "text": "(12) and (13), without the constraint, GT G = I, Jb(G) and Jt(G) can become arbitrarily large."}, {"x": 206, "text": "In what follows, LDA, OCM, and PCA are discussed based on such maximization criteria, and their properties relevant to visualization are identified."}, {"x": 207, "text": "2.3.1 Linear Discriminant Analysis (LDA)"}, {"x": 208, "text": "Conceptually, in LDA, we are looking for a dimension reducing transformation that keeps the between-cluster relationship as remote as possible by maximizing trace(GT SbG) while keeping the within cluster relationship as compact as possible by minimizing trace(GTSwG)."}, {"x": 210, "text": "(11), the criterion of LDA can be written as"}, {"x": 211, "text": "Jb/w(G) = max trace((GT SwG)1(GT SbG))."}, {"x": 213, "text": "trace((GT S G)1(GT S G))  trace(S1S ), (15) wbwb"}, {"x": 217, "text": "(14) with respect to G to zero, which gives the first order optimality condition, it can be shown that the solution of LDA, where we denote it as GLDA, has the columns which are the leading generalized eigenvectors u of the generalized eigenvalue problem,"}, {"x": 219, "text": "(16) Since the rank of Sb is at most k1, LDA achieves the upper bound of trace((GT SwG)1"}, {"x": 222, "text": "2.3.2 Orthogonal Centroid Method (OCM)"}, {"x": 223, "text": "Orthogonal centroid method (OCM) <71> focuses only on maximizing trace(GT SbG) under the constraint of GT G = I. The criterion of OCM is shown as"}, {"x": 224, "text": "Jb(G) = max trace(GT SbG)."}, {"x": 225, "text": "(18) GT G=I"}, {"x": 227, "text": "trace(GT SbG)  trace(Sb), (19)"}, {"x": 228, "text": "which means the cluster structure quality measured by trace(Sb) cannot be increased after dimension reduction."}, {"x": 230, "text": "(18) can be obtained by setting the columns of G as the leading eigenvectors of Sb."}, {"x": 231, "text": "Since Sb has at most k  1 nonzero eigenvalues, the upper bound of trace(GT SbG) in Eq."}, {"x": 233, "text": "trace(GT SbG) = trace(Sb) for l  k  1."}, {"x": 236, "text": "(20) indicates trace(Sb) is preserved between the original and the reduced dimen- sional spaces."}, {"x": 237, "text": "An advantage of OCM is that it achieves an upper bound of trace(GT SbG) more efficiently by using QR decomposition, avoiding the eigendecomposition."}, {"x": 238, "text": "The algo- rithm of OCM is as follows."}, {"x": 240, "text": "Then the reduced QR decomposition <61> of C is computed for C = QkR where Qk  Rmk with QTk Qk = I and R  Rkk is upper triangular."}, {"x": 241, "text": "The solution of OCM, GOCM, is found as"}, {"x": 245, "text": "Finally, OCM achieves trace(GTOCM SbGOCM ) = trace(Sb), where l = k."}, {"x": 247, "text": "(3) and (4), one can prove that each pair- wise distance between cluster centroids is also preserved in the reduced dimensional space obtained by OCM."}, {"x": 248, "text": "Another important property of OCM is that by projecting data into the subspace spanned by the centroids, the order of similarities between any particular point and centroids are preserved in terms of Euclidean norm and cosine similarity measure <71, 67>."}, {"x": 250, "text": "2.3.3 Principal Component Analysis (PCA)"}, {"x": 252, "text": "The criterion of PCA can be written as"}, {"x": 253, "text": "Jt(G) = max trace(GT StG)."}, {"x": 254, "text": "(21) GT G=I"}, {"x": 256, "text": "trace(GT StG)  trace(St), (22)"}, {"x": 260, "text": "Since the rank of St is at most min(m, n), PCA achieves the upper bound of trace(GTStG) in Eq."}, {"x": 261, "text": "(22) for any l such that l  min(m, n), i.e., trace(GTP CAStGP CA) = trace(St) for l  min(m, n)."}, {"x": 262, "text": "In many applications of PCA, however, l is usually chosen as a fixed value less than the ranke of St for the purpose of dimension reduction or noise reduction."}, {"x": 263, "text": "This noisy subspace corresponds to the smallest eigenvectors of St, and they are removed by PCA for better representation of the data."}, {"x": 264, "text": "Although St is related to Sb and Sw as in Eq."}, {"x": 266, "text": "That is, unlike LDA and OCM, PCA ignores the cluster structure represented by Sb and/or Sw, which is why PCA is considered as an unsupervised dimension reduction method."}, {"x": 267, "text": "Usually, PCA assumes that the global centroid is zero by subtracting the empirical mean of the data from each data vector."}, {"x": 268, "text": "The centered data can be represented as A  ceT , where e is n-dimensional vector whose components are all 1s."}, {"x": 271, "text": "GPCA =arg min GGT(AceT)(AceT), G, GT G=Il"}, {"x": 272, "text": "where the matrix norm    is either a Frobenius norm or a Euclidean norm."}, {"x": 279, "text": "In the first stage, a dimension reducing linear transformation GT  Rlm maps an m-dimensional data vector x to a vector y in the l-dimensional space (l  m):"}, {"x": 286, "text": "(25) Such consecutive dimension reductions performed by GT followed by HT can be"}, {"x": 290, "text": "In the next section, discussion will be focused on various ways for choosing the first stage dimension reducing transformation G and the second stage dimension transfor- mation H with a purpose to construct combined dimension reducing transformation V T = HT GT for 2-dimensional visualization according to various optimization crite- ria."}, {"x": 292, "text": "All the proposed two-stage methods start from one of the supervised dimension re-duction methods such as LDA or OCM that are designed for clustered data."}, {"x": 293, "text": "In the first stage (by GT  Rlm in Eq."}, {"x": 294, "text": "(24)), the dimension is reduced by LDA or OCM to the smallest dimension that satisfies Eq."}, {"x": 299, "text": "(21), i.e., that of PCA."}, {"x": 301, "text": "(21) gives the best approximation of the first-stage results that minimize the difference in terms of Frobenius/Euclidean norm."}, {"x": 305, "text": "In this method, LDA is applied in the first stage, and trace(S1S ) is preserved in the wb"}, {"x": 312, "text": "2.5.2 LDA followed by PCA"}, {"x": 313, "text": "In this method, LDA is applied in the first stage, and trace(S1Sb) is preserved in w the l-dimensional space where l = k  1."}, {"x": 314, "text": "In the second stage, PCA is applied in order to obtain the best approximation of the l-dimensional first-stage results in terms of Frobenius/Euclidean norm."}, {"x": 316, "text": "2.5.3 OCM followed by PCA"}, {"x": 317, "text": "In this method, OCM is applied in the first stage, and trace(Sb) is preserved in the l-dimensional space where l = k."}, {"x": 318, "text": "In the second stage, PCA is applied in order to obtain the best approximation of the l-dimensional first-stage results in terms of Frobenius/Euclidean norm."}, {"x": 322, "text": "2.5.4 Rank-2 PCA on Sb"}, {"x": 330, "text": "(36) has been used in one of the successful visual analytic systems, IN-SPIRE, for 2D representation of document data <138>."}, {"x": 334, "text": "2.6.1 Regularization on LDA for undersampled data"}, {"x": 335, "text": "In undersampled cases, the LDA criterion shown in Eq."}, {"x": 338, "text": "proposed a universal algorithmic framework of LDA using the generalized sin- gular value decomposition (LDA/GSVD) <68, 67>."}, {"x": 339, "text": "Specifically, for the case when m  n  k, which is usual for most undersampled problems, LDA/GSVD gives the solution for G such that GT SwG = 0 while maintaining the maximum value of trace(GT SbG)."}, {"x": 340, "text": "This solution makes sense since LDA criterion is formulated to min- imize trace(GTSwG)."}, {"x": 342, "text": "On the contrary, the fact that LDA makes GTSwG = 0 can be viewed as an advantage for visualization purposes since LDA has the capability to fully minimize trace(GT Sw G)."}, {"x": 343, "text": "By means of regularization on Sw one can control trace(GT Sw G), which determines the scatter of the data points within each cluster."}, {"x": 344, "text": "In regularized LDA which was originally proposed to avoid the singularity of Sw in classification context, Sw is replaced by a nonsingular matrix Sw + I where I is an identity matrix, and  is a control parameter."}, {"x": 347, "text": "Such manipulation of  can be exploited in a visualization context because one can choose an appropriate value of  so that the second-stage method such as PCA, which maximizes trace(GT StG) = trace(GT SbG + GT SwG), does not focus too much on trace(GT SwG)."}, {"x": 350, "text": "The data sets tested are composed of one artificially-generated Gaussian-mixture dataset (GAUSSIAN) and three real-world text data sets (MEDLINE, NEWSGROUPS, and REUTERS) that are clustered based on their topics."}, {"x": 355, "text": "The GAUSSIAN data set is a randomly generated Gaussian mixture with 10 clusters."}, {"x": 357, "text": "In its visualization shown in Fig."}, {"x": 359, "text": "The MEDLINE data set is a document corpus related to medical science from the"}, {"x": 364, "text": "where the letters in parentheses are used in the visualization shown in Fig."}, {"x": 366, "text": "The NEWSGROUPS data set <11> is a collection of newsgroup documents, and originally composed of 20 topics."}, {"x": 371, "text": "where the letters in parentheses are used in the visualization shown in Fig."}, {"x": 373, "text": "The REUTERS data set <11> is the document collection that appeared in the Reuters newswire in 1987, and originally composed of hundreds of topics."}, {"x": 378, "text": "where the letters in parentheses are used in the visualization shown in Fig."}, {"x": 382, "text": "5 is the example of applying OCM+PCA to the MEDLINE data set with and without data centering."}, {"x": 383, "text": "Once the MEDLINE data set is encoded as a term-document matrix, every component has a non-negative value, which results in the global centroid that is significantly far from the origin."}, {"x": 384, "text": "Then performing PCA without data centering might give the first principal axis as the one reflecting the global centroid rather than that discriminating clusters."}, {"x": 386, "text": "5, the former, which corresponds to the first principal axis, does not help in showing the cluster structure clearly, and only the vertical axis, which corresponds to the second principal axis from PCA, discriminates clusters."}, {"x": 391, "text": "In all cases, LDA-based methods show cluster structures more clearly than OCM- based methods."}, {"x": 392, "text": "This proves the effectiveness of LDA that considers both within- and between-cluster measures while OCM only takes into account the latter."}, {"x": 394, "text": "As a result, in the NEWSGROUPS dataset, such a wide within-cluster variance significantly deteriorates the cluster structure visualization even if OCM still"}, {"x": 396, "text": "In the MEDLINE and the REUTERS data sets, all of the four methods produce"}, {"x": 398, "text": "However, we have controlled the within-cluster variance in LDA-based methods using the regularization term I. In addition, the fact that rank- 2 LDA and LDA+PCA produce almost identical results indicates that GTLDAStGLDA is dominated by GTLDASbGLDA after LDA is applied in the first stage as we expected."}, {"x": 400, "text": "Rank-2 LDA represents each cluster most compactly by minimizing the within- cluster radii both in the first and the second stage."}, {"x": 402, "text": "As can be seen in the two LDA-based methods applied to the NEWGROUPS data set, while rank-2 LDA minimizes the within-cluster radii, it also places the centroids closer to each other as compared to those in LDA+PCA."}, {"x": 403, "text": "Due to this effect, which one is preferable between rank-2 LDA and LDA+PCA depends on the data set to be visualized."}, {"x": 404, "text": "Overall, OCM+PCA and Rank-2 PCA on Sb show similar results."}, {"x": 405, "text": "It means GT SbG  GT StG in that the difference between two methods lies in whether PCA is applied to GT SbG or GT StG in the second stage."}, {"x": 406, "text": "Since performing PCA on GT SbG is computationally more efficient than PCA on GT StG, Rank-2 PCA on Sb can be a good alternative to OCM+PCA in case efficient computation is important."}, {"x": 408, "text": "In Fig."}, {"x": 411, "text": "In Fig."}, {"x": 414, "text": "(a), and those of sci.crypt (y) and sci.med (d) are closely located respectively in LDA-based methods."}, {"x": 417, "text": "In Fig."}, {"x": 420, "text": "According to our results, LDA-based methods are shown to be superior to OCM-based methods since both within- and between-cluster relationships are taken into account religion.misc (r), those of comp.sys.ibm."}, {"x": 422, "text": "Especially, combined with PCA in the second stage, LDA+PCA achieves a clear discrimination between clusters as well as the best approximation of the results of LDA when the distance between data is measured in terms of Frobenius/Euclidean norm."}, {"x": 431, "text": "CHAPTER III EFFICIENT UPDATING OF COMPUTATIONAL METHODS DUE TO PARAMETER CHANGES"}, {"x": 432, "text": "One of the most widely-used nonlinear data embedding methods is ISOMAP."}, {"x": 433, "text": "Based on a manifold learning framework, ISOMAP has a parameter k or o that controls how many edges a neighborhood graph has."}, {"x": 435, "text": "When ISOMAP is used to visualize data, users might want to test different parameter values in order to gain various insights about data, but such interaction between humans and such visual- izations requires reasonably efficient updating, even for large-scale data."}, {"x": 436, "text": "To tackle these problems, we propose an efficient updating algorithm for ISOMAP with pa- rameter changes, called p-ISOMAP."}, {"x": 443, "text": "Among the most recent nonlinear dimension reduction methods, isometric feature mapping (ISOMAP) has shown its effectiveness in capturing the underlying manifold structure in the reduced dimensional space by being successfully applied to synthetic data such as Swiss roll data and real-world data such as facial image data <125>."}, {"x": 444, "text": "ISOMAP shares the basic idea with a traditional technique, classical multidimen- sional scaling (MDS)."}, {"x": 445, "text": "Classical MDS first constructs the pairwise similarity matrix, which is usually measured by the Euclidean distance, and computes the reduced dimensional mapping that maximally preserves such a similarity matrix in a given reduced dimension."}, {"x": 446, "text": "ISOMAP differs from classical MDS in that it constructs the pairwise similarity matrix based on the geodesic distance estimated by the short- est path in the neighborhood graph of data."}, {"x": 447, "text": "The neighborhood graph is formed by having vertices as data points and setting each edge weight between the nodes as their Euclidean distance only if at least one node is one of the k-nearest neighbors (k-NN) of the other node (k-ISOMAP) or if their Euclidean distance is smaller than o (o-ISOMAP)."}, {"x": 448, "text": "Hence, ISOMAP has an either parameter k or o to construct the neighborhood graph."}, {"x": 449, "text": "This chapter focuses on the algorithm and applications of the dynamic updating of ISOMAP when the value of k or o varies."}, {"x": 450, "text": "It is generally known that in ISOMAP, if k or o is too small, the graph becomes sparse, resulting in infinite geodesic distances between some pairs of data points, and if k or o is too large, it is prone to short circuit the true geometry of the manifold."}, {"x": 453, "text": "However, running ISOMAP repeatedly using different parameter values for k or o may be time-consuming since it involves computationally intensive processes such as the all-pairs shortest path computation and the eigendecomposition, whose complexity is usually O(n3) in which n is the number of data points.1"}, {"x": 454, "text": "In practice, there is also often no guarantee of the existence of the underlying well-defined manifold structure in data, and thus, one may not be sure if manifold learning methods such as ISOMAP are suitable for the data at hand."}, {"x": 455, "text": "Even so, one may still want to try ISOMAP or another manifold learning method in order to see if it serves ones purpose."}, {"x": 458, "text": "This statement also holds true even when we use just a single dimension reduc- tion method, e.g., ISOMAP, while we test its various parameter values."}, {"x": 459, "text": "In short, visualizations using ISOMAP with different parameter values for k or o can provide us with various aspects of our data."}, {"x": 460, "text": "In instances of the Swiss roll and toroidal helix data sets shown in Fig."}, {"x": 465, "text": "In this sense, it is worthwhile for users to test different parameter values in ISOMAP to visualize data in various ways."}, {"x": 466, "text": "1The complexity of the (all-pairs) shortest path computation depends on the algorithm."}, {"x": 467, "text": "Floyd- Warshall algorithm requires O(n3) computations while Dijkstras algorithm does O(|e|nlogn) com- putations <13> in which |e| is the number of edges."}, {"x": 471, "text": "Motivated by the above men- tioned cases, we propose p-ISOMAP, an efficient dynamic updating algorithm for ISOMAP when the parameter value changes."}, {"x": 472, "text": "Given the ISOMAP result from a par- ticular parameter value, our proposed algorithm updates the previous result to obtain another ISOMAP result of the same data with a new parameter value instead of re- computing ISOMAP for different parameter values from scratch."}, {"x": 474, "text": "In addition, we demonstrate several visualization examples by varying the parameters in ISOMAP, which not only show the interesting aspects of the tested data but also help us thoroughly understand the behavior of ISOMAP in terms of parameter values."}, {"x": 476, "text": "Section 3.2 briefly introduces ISOMAP and its algorithm, and Section 3.3 discusses previous work related to p- ISOMAP."}, {"x": 477, "text": "Section 3.4 describes the algorithmic details and the complexity analysis of p-ISOMAP, and Section 3.5 presents not only the experimental results that compare the computation times of ISOMAP and p-ISOMAP but also interesting visualization examples of real-world data using p-ISOMAP."}, {"x": 483, "text": ", n, ISOMAP assumes a lower dimensional manifold structure in which the data are embedded."}, {"x": 484, "text": "It yields the m-dimensional representation of xi as yi  Rm (m  M) such that the Euclidean distance between yi and yj approximates their geodesic distance along the underlying manifold as much as possible."}, {"x": 485, "text": "Such an approximation builds on the classical MDS framework, but unlike MDS, ISOMAP has the capability of handling nonlinearity existing in the original space since a geodesic distance reflects an arbitrary curvilinear shape of the manifold."}, {"x": 486, "text": "On input, ISOMAP takes a data matrix X =   x1 x2  xn    RMn, a reduced dimension m, and a parameter k or o."}, {"x": 489, "text": "Neighborhood graph construction."}, {"x": 490, "text": "ISOMAP first computes the pairwise Eu- clidean distance matrix, DX  Rnn, in which DX (i, j) is the Euclidean dis- tance between xi and xj."}, {"x": 491, "text": "Then it determines the set of neighbors for each point either by k-nearest neighbors or by those within a fixed radius o. Be- tween a point xi and each of its neighbors xj, an edge e(i, j) is assigned with a weight equivalent to their Euclidean distance, and in this way, ISOMAP forms a weighted undirected neighborhood graph G = (V, E), where the vertices in V correspond to the data points xis."}, {"x": 494, "text": "In the second step, ISOMAP estimates the pair- wise geodesic distance based on the shortest path length for every vertex pair along the neighborhood graph G, which is represented as a matrix DG  Rnn in which DG(i, j) is the shortest path length between xi and xj in G."}, {"x": 497, "text": "The final step performs classical MDS on DG, producing m-dimensional data embedding, Y =   y1 y2  yn    Rmn."}, {"x": 498, "text": "First, the pairwise geodesic distance matrix DG is converted to an inner product matrix BG as avoid ambiguity about which changes of neighbors in a directed graph cause actual edge changes in an undirected one in which we have to actually compute the shortest paths."}, {"x": 499, "text": "The detailed procedure of the neighborhood graph update are described in Algorithm 1."}, {"x": 501, "text": "3.4.1.1 Time Complexity"}, {"x": 502, "text": "In ISOMAP, the time complexity in constructing a neighborhood graph is as fol- lows."}, {"x": 503, "text": "It starts with a sort operation for a given data set whose time complexity is O(n2 log n)."}, {"x": 505, "text": "In p-ISOMAP, the time complexity required in the neighborhood graph update is bounded by O(n  maxi |ei|), in which |ei| is the number of inserted/removed edges associated with xi."}, {"x": 506, "text": "3.4.2 Shortest Path Update"}, {"x": 507, "text": "The shortest path update stage, which is one of the most computationally inten- sive steps in p-ISOMAP, takes the input as either A or D and updates the shortest path length matrix DG."}, {"x": 508, "text": "In order to facilitate this process, p-ISOMAP maintains and updates the information about the shortest path itself with a minimal memory requirement in the form of a predecessor matrix P  Rnn, in which P(i, j) stores the node index immediately preceding xj in the shortest path from xi to xj.3 For instance, if the shortest path from x1 to x2 is composed of x1  x4  x3  x2, then we set P (1, 2) = 3."}, {"x": 514, "text": "3.4.2.3 Time Complexity"}, {"x": 515, "text": "When a parameter increases, Algorithm 2 requires the time complexity of O(|A|nq"}, {"x": 516, "text": "maxi,a |T (i; a)|) in which maxi,a |T (i; a)| is the maximum number of nodes in sub-"}, {"x": 521, "text": "For a decreasing parameter, the time complexity of Algorithm 3"}, {"x": 522, "text": "requires O(n2) computations since it visits every vertex pair exactly once."}, {"x": 526, "text": "Then, the complexity of Algorithm 4 is represented"}, {"x": 527, "text": "as O(n  maxi(|E | log |Vd(i)| + (|E |)) in which E = <e(xa, xb)  G|xa, xb  Vd(i)> iii"}, {"x": 532, "text": "Let us denote the updated DG after the shortest path update described in Section 4.2 as Dnew."}, {"x": 548, "text": "The original ISOMAP uses the Lanczos G algorithm <61>, which is an iterative method that is appropriate for solving the first few leading eigenvalue/vector pairs."}, {"x": 549, "text": "The Lanczos algorithm iteratively refines the solution in the Krylov subspace that grows from an initial vector by multiplying itTable 6: Computation time in seconds required to determine the optimal k value by minimizing residual variances."}, {"x": 554, "text": "The performance of the Lanczos GG"}, {"x": 555, "text": "algorithm largely depends on how quickly such a Krylov subspace covers that spanned"}, {"x": 557, "text": "Another characteristic of the Lanczos algorithm is that the"}, {"x": 559, "text": "In other words, when the Krylov subspace becomes k dimensions, the first leading"}, {"x": 568, "text": "As a result, we can expect the Lanczos algorithm to terminate mm"}, {"x": 571, "text": "In this section, we present an empirical comparison between the computation times of ISOMAP and those of p-ISOMAP using both synthetic and real-world data sets."}, {"x": 573, "text": "In our experiments, we used the code of ISOMAP provided by the original author.4 However, the original code does not take advantage of sparse graphs, so we compared p-ISOMAP with an improved version of ISOMAP that runs Dijkstras algorithm in C++ with a sparse representation of the graph."}, {"x": 574, "text": "p-ISOMAP was implemented mainly in MATLAB except for the shortest path update part, which runs in C++."}, {"x": 575, "text": "In both ISOMAP and p-ISOMAP, the eigendecomposition was done by MATLAB built- in function eigs, which performs the Lanczos algorithm by using Fortran library"}, {"x": 577, "text": "Throughout all experiments, we used the ISOMAP parameter as k, where the neighborhood graph is constructed by k-NN, since we can easily bound |A| or |D| by O(nk) in which k = knew  k. All the experiments were done using MATLAB 7.7.0 on Windows Vista 64bit with 3.0GHz CPU with a 4.0GB memory."}, {"x": 579, "text": "To compare the computation times between ISOMAP and p-ISOMAP, we tested two synthetic data sets (Rand and Swiss roll) and two real-world data sets (Pendigits and Medline)."}, {"x": 583, "text": "Finally, Medline data set6 is a document corpus related to medical science from the National Institutes of Health, and it has 2,500 documents encoded in 22,095- dimensional space."}, {"x": 584, "text": "Table 5 compares computation times of ISOMAP with those of p-ISOMAP for each data set."}, {"x": 585, "text": "In most cases, p-ISOMAP runs significantly faster than ISOMAP."}, {"x": 586, "text": "However, as the number of vertex pairs whose shortest paths need to be updated increases, the computational advantage of p-ISOMAP over ISOMAP gradually vanishes."}, {"x": 587, "text": "Nonetheless, except for Swiss roll data set, which involves a large number of the shortest path update even with a slight parameter change, most data sets require only about 10-40% the shortest path update for a reasonable parameter change, e.g., within 5."}, {"x": 590, "text": "We selected Rand data since it was the most suitable one to clearly observe its behaviors."}, {"x": 596, "text": "As we can see, p-ISOMAP scales well in terms of the number of data compared to ISOMAP."}, {"x": 601, "text": "This is mainly because the original Dijkstras algorithm used in ISOMAP needs more computations as the graph gets denser while p-ISOMAP depends only on |A|, |D|, or correspondingly |F|, which probably does not increase over different initial k values."}, {"x": 604, "text": "3.5.2 Knowledge Discovery via Visualization using p-ISOMAP"}, {"x": 606, "text": "To be specific, we show how ISOMAP with different parameters can discover various knowledge about data and how the information acquired through visualization can facilitate traditional data mining problems such as a classification task."}, {"x": 607, "text": "p-ISOMAP was used to efficiently update ISOMAP results throughout all the visualization experiments."}, {"x": 614, "text": "In their visualizations shown in Figs."}, {"x": 623, "text": "Several interesting visualization examples of these data based on p-ISOMAP are shown in Figs."}, {"x": 630, "text": "Regarding a comparison between Figs."}, {"x": 635, "text": "8(d) connects almost all the data points between each other, which would reflect the Euclidean distances in the original space just like MDS does."}, {"x": 642, "text": "9 is not visualized in a well-clustered form by ISOMAP because it is usually difficult to find a well-defined manifold structure with few meaningful dimensions for document data."}, {"x": 650, "text": "10(a) to 10(f) is shown similar to that of Swiss roll data set from Fig."}, {"x": 656, "text": "In short, ISOMAP with a small parameter value tends to unroll the curved manifold due to geodesic paths, but that with a large parameter better shows its curvature itself."}, {"x": 659, "text": "In addition, the clusters 3 and 6 appears to overlap for a certain range of k between 9 and 11 as shown in Figs."}, {"x": 664, "text": "Based on this observation, we examined some sample data from each cluster and found out such subclusters are due to the different way to write 5.9 From the examples in these two subclusters shown in Figs."}, {"x": 676, "text": "10(d), some data in the cluster 0 seems to deviate from its major line-shaped data in Figs."}, {"x": 677, "text": "10(a)-(c)."}, {"x": 690, "text": "In this chapter, we proposed p-ISOMAP, an efficient algorithmic framework to dynam- ically update ISOMAP embedding for varying parameter values."}, {"x": 695, "text": "ITERATION-WISE INTEGRATION FRAMEWORK OF COMPUTATIONAL METHODS"}, {"x": 699, "text": "As a way to tackle this problem, this chapter presents PIVE, a Per-Iteration Visualization Environment for supporting real-time interactive visualization with computational methods."}, {"x": 702, "text": "We show the effectiveness of PIVE in terms of real-time visualization and interaction capabilities by customizing various dimension reduction methods such as principal component analysis, multidimensional scaling, and t-distributed stochastic neighborhood embedding, and clustering methods such as k-means and latent Dirichlet allocation."}, {"x": 707, "text": "More recently, latent Dirichlet allocation (LDA) <20>, a popular method for document topic modeling, has been adopted in a wide variety of visual analytics systems for document analysis <134, 50, 88>."}, {"x": 711, "text": "Consequently, in many domain areas, people still resort to only a few basic computational methods such as principal component analysis (PCA) <74> and multi- dimensional scaling (MDS) <45> for dimension reduction, hierarchical clustering and k-means <19> for clustering, etc."}, {"x": 727, "text": "Instead, we propose an alternative approach called PIVE (Per- Iteration Visualization Environment for supporting real-time interactive visualization with computational methods), which visualizes the intermediate result per iteration as soon as they become available."}, {"x": 740, "text": "Realizations of PIVE with various well-known computational methods (PCA,"}, {"x": 741, "text": "MDS, t-SNE, k-means, and LDA) in established visual analytics systems"}, {"x": 753, "text": "4.2.1 Efficient Interactive Visualization"}, {"x": 756, "text": "For example, Fisher et al."}, {"x": 776, "text": "<5> has visualized the streaming documents using a GPU-accelerated force-directed layout technique."}, {"x": 779, "text": "4.2.2 User Interaction with Computational Methods"}, {"x": 784, "text": "4.3 Per-Iteration Visualization Environment (PIVE)"}, {"x": 794, "text": "For example, the output of a dimension reduction method, e.g., PCA, can map data items onto the coordinates of the screen space, and the output of clustering can be used to color-code each group of data clusters."}, {"x": 825, "text": "Modern computers are usually equipped with at least two or more cores on the CPU."}, {"x": 838, "text": "For example, even if the data is a very high-dimensional, say, in the hundreds of thousands of dimensions, such as is the case in text data, the dimension reduction outputs would only be two-dimensional representations assum- ing they are visualized in a 2D space."}, {"x": 865, "text": "4.4.1 Principal Component Analysis (PCA)"}, {"x": 866, "text": "PCA <75> is a well-known dimension reduction method that captures the maximal variance in the data via a linear projection."}, {"x": 867, "text": "PCA is mainly based on the method called eigendecomposition, the algorithms of which are categorized into two different methods, the QR algorithm and the Lanczos algorithm <61>."}, {"x": 868, "text": "Basically, the Lanczos algorithm approximates a given data matrix by a much smaller one in the Krylov subspace <61>, the dimension of which iteratively expands, and efficiently solves the eigendecomposition on the latter matrix."}, {"x": 869, "text": "Due to the nature that this matrix well-approximates the largest eigenvectors of the original one, the Lanczos algorithm performs much faster than the QR algorithm in visual analytics in which only a few dimensions are needed."}, {"x": 870, "text": "We customize the Lanczos-based PCA implementation of FodavaTestbed so that the results for each iteration are dynamically visualized."}, {"x": 871, "text": "4.4.2 Multidimensional Scaling (MDS)"}, {"x": 873, "text": "Given the ideal distance ij between xi and xj, MDS solves"}, {"x": 875, "text": "A Euclidean distance xi  xj2 is usually used for dij."}, {"x": 878, "text": "We customize MDS in FodavaTestbed by extracting the xis at each iteration from the MDS implementation."}, {"x": 879, "text": "4.4.2.1 User Interaction Capabilities"}, {"x": 880, "text": "Additionally, while the results for each iteration of MDS are visualized in a scatter plot, we support the interaction capability that enables users to move the data points by mouse via drag-and-drop, similar to the Prefuse force-directed layout."}, {"x": 881, "text": "Then, during the MDS iterations, their new positions in the screen space are translated back to the MDS output coordinates, xis."}, {"x": 883, "text": "In terms of how MDS behaves due to these changes, we provide two different capabilities: soft vs."}, {"x": 885, "text": "The soft placement continues iterations without any changes in MDS behaviors."}, {"x": 886, "text": "It is equivalent to restarting MDS with the intermediate result at the particular iteration as the initial values for xis."}, {"x": 889, "text": "Note that, however, even though their values do not change, other data points are still influenced by these fixed points, and in this sense, our approach is a semi-supervised MDS that reflects user interventions."}, {"x": 890, "text": "When using the semi-supervised MDS, an important advantage of the proposed framework is that users can immediately check the effects of these interactions via the iteration-wise visualization."}, {"x": 892, "text": "4.4.3 t-Distributed Stochastic Neighbor Embedding (t-SNE)"}, {"x": 897, "text": "In practice, however, t- SNE does not provide a clear stopping criterion, and thus it typically iterates several hundred times by default for any data set, which usually takes a significant amount of time."}, {"x": 898, "text": "We customize the t-SNE in FodavaTestbed in a similar manner to the way we altered MDS."}, {"x": 899, "text": "4.4.3.1 User Interaction Capabilities"}, {"x": 900, "text": "Likewise, we provide both the soft and hard placement interactions for t-SNE, as discussed in MDS."}, {"x": 901, "text": "Although the algorithm details are different, the overall iterative procedure turns out to be quite similar to MDS."}, {"x": 910, "text": "(a) PCA criteria values and out- put changes"}, {"x": 912, "text": "Figure 13: The behavior for each iteration of PCA and its visualization snapshots."}, {"x": 913, "text": "In (a), the red lines represent the PCA criteria value, the lower-dimensional variance in PCA."}, {"x": 914, "text": "The blue lines are the Euclidean distances of the lower-dimensional outputs between the current and the previous iterations, and the black lines are the Euclidean distances of the lower-dimensional outputs between the current and the final itera- tions."}, {"x": 918, "text": "4.4.4.1 User Interaction Capabilities"}, {"x": 921, "text": "On a split/merge interaction, similar to the soft placement in MDS and t-SNE, k-means restarts with the intermediate cluster memberships that reflect split/merged clusters, involving dynamic changes in a k-means parameter which represents the number of clusters."}, {"x": 924, "text": "4.4.5 Latent Dirichlet Allocation (LDA)"}, {"x": 927, "text": "The iterations of LDA basically update these two outputs alternately."}, {"x": 935, "text": "Finally, LDA, which is a sampling-based approach, shows a significantly different"}, {"x": 950, "text": "The NSF-awarded abstract data have been used."}, {"x": 954, "text": "16 shows an interesting interaction which involves moving a data point in t- SNE."}, {"x": 955, "text": "Given some overlapping clusters in a particular visualization generated by t- SNE (Fig."}, {"x": 1005, "text": "The ability to filter noisy documents has been an appealing interaction for LDA in iVisClustering."}, {"x": 1007, "text": "By removing them and re-running LDA, iVis- Cluster generally obtains significantly clearer topics."}, {"x": 1013, "text": "We have presented PIVE (Per-Iteration Visualization Environment for supporting real-time interactive visualization with computational methods)."}, {"x": 1019, "text": "We have seen this kinds of limitations when using LDA under PIVE due to the random nature of the used LDA algorithm."}, {"x": 1023, "text": "TESTBED: AN INTERACTIVE VISUAL TESTBED SYSTEM FOR VARIOUS DIMENSION REDUCTION AND CLUSTERING METHODS"}, {"x": 1024, "text": "Many of the modern data sets such as text and image data can be represented in high- dimensional vector spaces and have benefited from computational methods that uti- lize advanced computational methods."}, {"x": 1030, "text": "The Testbed system enables users to apply various dimension reduction and clustering methods with different settings, visually compare the results from different algorithmic methods to obtain rich knowledge for the data and tasks at hand, and eventually choose the most appropriate path for a collection of algorithms and param- eters."}, {"x": 1031, "text": "Using various data sets such as documents, images, and others that are already encoded in vectors, we demonstrate how the Testbed system can support these tasks."}, {"x": 1036, "text": "Given high-dimensional data, understanding and analyzing these data become more challenging."}, {"x": 1043, "text": "Among various methods, two main ones, dimension reduction and clustering, play an essential role in visual analytics of large-scale high-dimensional data owing to their nature to reduce the numbers of features and data items into manageable sizes, respectively."}, {"x": 1045, "text": "In addition, it allows visualization of high-dimensional data in the form of a 2D/3D scatter plot in which one can obtain insight about data relationships with respect to the geometric locations of data."}, {"x": 1058, "text": "The main contributions of the proposed Testbed system are as follows."}, {"x": 1061, "text": "Additionally, the Testbed system facilitates easy comparisons between different dimension reduction and clustering results by com- putationally aligning them."}, {"x": 1062, "text": "Finally, the Testbed system is implemented in a highly modular way so that new data types and dimension reduction/clustering methods can be easily integrated to the current system."}, {"x": 1063, "text": "Note that even though the Testbed system can be used by anyone who wants to apply various methods to their own data, some background knowledge about machine learning and data mining would be of great help in fully utilizing the Testbed system via understanding the data and the applied methods simultaneously."}, {"x": 1064, "text": "For example, machine learning researchers/developers, who wish to easily plug in and visually evaluate their own methods in practical data analysis scenarios, would be able to receive significant benefit from the Testbed system."}, {"x": 1067, "text": "Section 5.3 describes the details of the Testbed system as well as several main computational methods used in the system."}, {"x": 1068, "text": "Section 5.4 shows various usage scenarios of the Testbed system, and finally Section 5.5 presents conclusions along with future work."}, {"x": 1069, "text": "Figure 22: 2D Scatter plots obtained by two dimension reduction methods, MDS (left) and LDA (right), for a facial image data set."}, {"x": 1074, "text": "5.2.1 Dimension Reduction and Clustering for Visualization"}, {"x": 1076, "text": "Numerous dimension reduction methods have been proposed, among which the most commonly used dimension reduction methods include principal component analysis (PCA) <75>, multidimensional scaling (MDS) <45>, and linear discriminant analysis (LDA) <60, 68>."}, {"x": 1077, "text": "In addition to traditional data analysis problems, they have also been widely utilized in visualization due to their capability of representing high-dimensional data as a form of scatter plots in 2D/3D space."}, {"x": 1078, "text": "In a scatter plot, each data item is represented as a point and its 2D/3D coordinate is determined from the dimension- reduced representation."}, {"x": 1081, "text": "For instance, the recently proposed man- ifold learning algorithms, e.g., isometric feature mapping (ISOMAP) <125>, locally linear embedding (LLE) <111>, and Laplacian Eigenmaps (LE) <17>, try to preserve the relationships between the local neighborhood rather than global relationships."}, {"x": 1082, "text": "These methods have been successfully applied to the data that originally have a low- dimensional manifold structure, and often they demonstrated their capability to reveal such a manifold structure in 2D/3D visualizations."}, {"x": 1086, "text": "In practice, however, it is not easy to obtain much insight from the 2D/3D scatter plot generated by them for a large number of data items."}, {"x": 1088, "text": "22 is a visualization example of such a dimension reduction method, MDS, for a facial image data set."}, {"x": 1094, "text": "Some representative supervised methods include LDA <60> and orthogonal centroid method (OCM) <102>."}, {"x": 1096, "text": "22 is an example of LDA visualization."}, {"x": 1097, "text": "This figure visualizes the data as groups of items computed by LDA based on the given cluster labels, and one can obtain better insight about the overall data structure at the cluster level over the individual data level."}, {"x": 1098, "text": "Representing the cluster structure has been one of the main concerns in many studies on dimension reduction and 2D/3D scatter plot visualizations even when unsupervised dimension reduction methods are used."}, {"x": 1100, "text": "For instance, a recently proposed dimension reduction method, t-distributed stochastic neighborhood embedding (t-SNE) <130>, shows its capability of grouping data and revealing the true cluster structure in 2D scatter plot visualizations."}, {"x": 1107, "text": "Widely-used methods include k-means clustering, spectral clustering <96>, and Gaussian mixture models."}, {"x": 1108, "text": "Recently, more advanced methods such as non-negative matrix factorization (NMF) <79> and latent Dirichlet allocation <20> have shown their successful applications in image segmentation and document topic modeling, etc."}, {"x": 1112, "text": "For instance, in recent applications of latent Dirichlet allocation for document topic modeling, while several coherent topic clusters have been successfully revealed for the document data, many other topics often seem unclear to understand."}, {"x": 1115, "text": "5.2.2 Visual Analytic Systems using Dimension Reduction and Cluster- ing"}, {"x": 1117, "text": "In this section, several systems such as IN-SPIRE <138>, Jigsaw <121>, GGobi <43>, iPCA <72>, and WEKA <65> are discussed."}, {"x": 1119, "text": "Given a set of documents, IN-SPIRE first encodes them as high-dimensional vectors using a bag- of-words model."}, {"x": 1120, "text": "Then it applies k-means clustering with a pre-defined number of clusters."}, {"x": 1121, "text": "PCA is computed on cluster centroids and applied to the entire data, which gives 2D coordinates of document data."}, {"x": 1122, "text": "Based on these 2D coordinates, a galaxy view similar to a scatter plot is shown to users with a keyword summary for each cluster placed at the cluster centroid."}, {"x": 1123, "text": "Owing to the simple algorithms adopted such as PCA and k-means, IN-SPIRE can deal with a fairly large amount of data, but it provides only a limited number of interaction capabilities to change the algorithms and their settings."}, {"x": 1125, "text": "The main information that Jigsaw utilizes for visualization is named entities such as person name and location and their co-occurrences between documents."}, {"x": 1126, "text": "Automatic named-entity extraction is one of the key computational components in the analysis in Jigsaw."}, {"x": 1127, "text": "The named-entity extraction can be viewed as dimension reduction that reduces the number of keywords out of the entire vocabulary."}, {"x": 1128, "text": "Users can modify the list of named-entities by manually adding/removing them."}, {"x": 1129, "text": "Jigsaw also provides a cluster view by using the k-means algorithm and visualizes the resulting clusters as groups of documents as well as their keyword summary."}, {"x": 1130, "text": "Jigsaw also supports basic interactions with clustering such as changing the number of clusters and providing seed documents."}, {"x": 1132, "text": "It mainly uses a 2D scatter plot, where the two dimensions are generated by grand tour <10>."}, {"x": 1135, "text": "Another system, iPCA <72>, which also takes high-dimensional data as an input, utilizes PCA as the main visualization technique."}, {"x": 1136, "text": "One of the main advantages of iPCA is that beyond 2D/3D scatter plots, it visualizes the reduced-dimensional data in a higher dimension than 2D or 3D via parallel coordinates."}, {"x": 1137, "text": "In general, dimension reduction from the original high-dimensional space to 2D/3D space introduces signif- icant information loss."}, {"x": 1139, "text": "Another aspect of iPCA is that it visualizes the PCA basis vectors in addition to the data items."}, {"x": 1142, "text": "Finally, WEKA <65> is mainly a library of various machine learning algorithms for high-dimensional data with several interaction capabilities."}, {"x": 1144, "text": "In addition, WEKA provides simple types of visualizations such as histograms, scatter plots, etc."}, {"x": 1145, "text": "Although WEKA is similar to our Testbed system in that it provides flexible algorithm choices and settings, most of its visualizations and interactions are focused on the used methods rather than data exploration."}, {"x": 1146, "text": "For example, WEKA does not support any interactions from its visualizations such as filtering operations and raw data access."}, {"x": 1149, "text": "In this respect, the Testbed system provides the unique capability of bringing a variety of algorithms along with full control to practical visual analytics scenarios."}, {"x": 1150, "text": "5.3 Testbed System"}, {"x": 1151, "text": "In this Section, we describe the Testbed system1 in detail."}, {"x": 1157, "text": "23, the Testbed system mainly has two parts: the computational and the interactive visualization parts."}, {"x": 1158, "text": "At the computational part, the Testbed system is composed of 1. vector encoding, 2. pre-processing, 3. clustering, and 4. dimension reduction."}, {"x": 1159, "text": "At the interactive visualization part, the Testbed provides the following interactive visualization modules: 1. parallel coordinates, 2. the scatter plot, 3. the cluster label view, and 4. the original data viewer."}, {"x": 1160, "text": "The basic workflow of the Testbed system is as follows."}, {"x": 1168, "text": "The Testbed system can generate as many visualization sets as needed depending on different specifications of dimension reduction and clustering, and users can ex- plore a certain visualization set and compare between different visualization sets."}, {"x": 1169, "text": "To facilitate an easy comparison between different visualization results, the Testbed sys- tem offers the capability of aligning the different clustering and dimension reduction outputs."}, {"x": 1170, "text": "In addition, users can highlight and/or filter out certain clusters/data items and look into the details of the selected data items in the original data viewer."}, {"x": 1174, "text": "The Testbed system can take various types of data such as text documents, images, and pre-encoded vectors in a comma-separated-values (CSV) file format."}, {"x": 1175, "text": "For docu- ment and image data, the Testbed system provides built-in vector encoding modules."}, {"x": 1176, "text": "For instance, the Testbed system supports bag-of-words encoding for document data in a sparse matrix form with stop word removal and stemming."}, {"x": 1183, "text": "In addition, for text documents, we provide options of 1. removing the terms appearing in less than a user-specified number and 2. applying the term-frequency- inverse-document-frequency (TF-IDF) weighting scheme."}, {"x": 1185, "text": "The Testbed system maintains multiple instances of different pre-processed vector sets, and users can interactively generate and/or choose one of them and proceed to perform its clustering and dimension reduction."}, {"x": 1189, "text": "The Testbed system currently provides the following clustering methods: 1. k-means, 2. agglomerative hierarchical clustering <66>, 3."}, {"x": 1190, "text": "Gaussian mixture models, and 4."}, {"x": 1195, "text": "5.3.2.4 Dimension Reduction"}, {"x": 1197, "text": "The Testbed system provides both super- vised and unsupervised dimension reduction methods, as discussed in Section 5.2.1."}, {"x": 1199, "text": "The currently available dimension reduction methods in the system include su- pervised ones such as 1."}, {"x": 1200, "text": "LDA, 2."}, {"x": 1201, "text": "OCM, 3. centroid method (CM) <102>, 4. two-stage methods (TSTG) <35>, 5. discriminative neighborhood metric learning (DNML) <133>, and 6. kernel LDA <101>, and unsupervised ones such as 7."}, {"x": 1202, "text": "PCA, 8. metric and non- metric MDS, 9."}, {"x": 1203, "text": "Sammon mapping <113>, 10."}, {"x": 1205, "text": "LLE, 12. local tangent space alignment (LTSA) <148>, 13. maximum variance unfolding (MVU) <135>, 14."}, {"x": 1206, "text": "LE, 15. diffusion maps (DM) <42>, 16. t-SNE, and 17."}, {"x": 1211, "text": "5.3.3.1 Parallel Coordinates View"}, {"x": 1212, "text": "Given an output from the computational part, i.e., lower-dimensional representations of data items and their cluster labels, the Testbed system takes a natural way to visualize the lower-dimensional data in parallel coordinates with a color coding based on the cluster labels (Fig."}, {"x": 1214, "text": "In this view, the Testbed system supports zoom- in/out via mouse wheel scroll and data selection via mouse drag-and-drop."}, {"x": 1215, "text": "5.3.3.2 Scatter Plot View"}, {"x": 1217, "text": "Due to these limitations, the Testbed system visualizes data in a 2D scatter plot (Fig."}, {"x": 1220, "text": "In addition, the Testbed system shows cluster centroids and ellipses, which summarize how the data within each class are distributed, and these features can be turned on/off via check boxes shown in the upper left part of the view."}, {"x": 1223, "text": "5.3.3.3 Cluster Label View"}, {"x": 1231, "text": "Cur- rently, the Testbed system provides three different original data viewers depending on the data type, e.g., text documents, images, and pre-encoded vectors (Fig."}, {"x": 1242, "text": "Each of these three views is created as an individual tab in its corresponding location, and multiple views are maintained flexibly in the Testbed system, as shown in Fig."}, {"x": 1247, "text": "Between different views with such a flexible layout, the Testbed system supports a brushing-and-linking capability."}, {"x": 1248, "text": "In the current Testbed system, if certain data items/custers are selected in one view, the corresponding data items in all the other views are highlighted as well."}, {"x": 1251, "text": "In addition to the above-described multi-view management and brushing-and-linking capability, the Testbed system provides a more active means to facilitate easy com- parison between visualization sets composed of different clustering and dimension reduction results."}, {"x": 1254, "text": "To align the two different clustering results, the Testbed system performs the Hungarian algorithm <85>."}, {"x": 1255, "text": "Given two different cluster assignments of the same data items, the Hungarian algorithm finds the best pairwise matchings between their clus- ter indices so that the number of common data items within matching cluster pairs is maximized."}, {"x": 1256, "text": "Once the Hungarian algorithm finishes, the Testbed system changes the cluster indices and colors of the second visualization set according to the matching clusters of the first visualization set."}, {"x": 1258, "text": "On the other hand, the Testbed system handles the alignment of dimension re- duction results via Procrustes analysis <69, 53>."}, {"x": 1259, "text": "Although there exist many advanced methods to align the two sets of vectors <30>, we chose Procrustes analysis due to its computational efficiency."}, {"x": 1260, "text": "Procrustes analysis transforms the second set of vectors via a rigid transformation, which allows only translation, rotation, and reflection, so that their Euclidean distances to the corresponding data vectors in the first set are mini- mized."}, {"x": 1261, "text": "Currently, instead of aligning the entire dimensions, the Testbed aligns only the two dimensions selected in the scatter plot view so that the alignment between the two scatter plot views are maximized."}, {"x": 1264, "text": "The current Testbed system is mainly implemented in JAVA to achieve various GUI and interaction capabilities."}, {"x": 1265, "text": "In order to support flexible window management, Net- Beans Rich Client Platform and IDE2 are used."}, {"x": 1266, "text": "Most of the internal computational methods are, however, written in MATLAB."}, {"x": 1267, "text": "There are several reasons of using MATLAB codes instead of porting them to JAVA."}, {"x": 1268, "text": "First of all, in many cases, the source codes of advanced computational methods are readily available in MATLAB due to its simplicity for matrix computations."}, {"x": 1270, "text": "Furthermore, MATLAB provides highly optimized matrix computations."}, {"x": 1271, "text": "For in- stance, MATLAB, by default, auto-identifies the parallelizable subroutines in the code and runs full CPU cores even in a single PC."}, {"x": 1273, "text": "For example, the k-means function in MATLAB provides various options for a distance metric to be used (Euclidean, city block, co- sine, and correlation), and a seed initialization (random, uniform, pilot-clustered, and user-selected seeds)."}, {"x": 1274, "text": "Due to these reasons, the current Testbed system interface with the computational methods via a custom JAVA library file created by MATLAB.3"}, {"x": 1276, "text": "In terms of the extensibility of the Testbed system, we designed it in a completely modular way so that it can easily accept new data types and clustering/dimension reduction methods."}, {"x": 1277, "text": "For instance, if one wants to use the Testbed system for a speech data type, one needs to implement only the encoding module, the possible pre-processing options specific to the speech data type, and the original data viewer that can play audio data."}, {"x": 1278, "text": "Otherwise, by performing vector encoding separately and putting the encoded vectors as an input to the system, one can easily utilize the full capability of the Testbed."}, {"x": 1279, "text": "Adding new dimension reduction/clustering methods is also a simple process."}, {"x": 1282, "text": "Therefore, whether the implemen- tation of a new method is written in MATLAB or JAVA, as long as it deals with two-dimensional double array type as an input and an output, it can be easily inte- grated into the current Testbed system without having to modify the entire system."}, {"x": 1285, "text": "To show how the Testbed system can be utilized in various visual analytics scenarios, we use three different data sets: 1."}, {"x": 1299, "text": "The InfoVisVAST data set6 is a document corpus of paper abstracts in IEEE Infovis (1995-2010) and VAST (2006-2010) conferences."}, {"x": 1301, "text": "5.4.2 Parallel Coordinates: Guiding beyond Two Leading Dimensions"}, {"x": 1303, "text": "Unlike these previous approaches, the Testbed system first visualizes the reduced-dimensional data in the parallel coordinates view, and then two of these dimensions are interactively selected for the scatter plot view."}, {"x": 1309, "text": "24, given the 10-dimensional results of PCA, the scatter plot view of (1, 2)-dimensions mixes up all the clusters together."}, {"x": 1311, "text": "5http://www.wisdom.weizmann.ac.il/ vision/FaceBase 6 http://www.cc.gatech.edu/gvu/ii/jigsaw/datafiles.html and 14 at dimensions 3 and 4, respectively, the scatter plot view of these dimensions turns out to give a well-clustered view."}, {"x": 1312, "text": "Such an observation is surprising because PCA is an unsupervised method, which does not take into account label information."}, {"x": 1315, "text": "Trying various methods/settings on a given data set and comparing between different visualizations is in the heart of the Testbed system, and the alignment functionality of the system supports this process."}, {"x": 1326, "text": "In this example, the cluster labels are unchanged for all data items in the three fig- ures, but two different dimension reduction methods, TSTG and ISOMAP, are used."}, {"x": 1329, "text": "For example, the cluster 4 is shown to be close to the cluster 8 in TSTG, which is not the case in ISOMAP."}, {"x": 1330, "text": "Any data items in the cluster 6 are not located close to the cluster 7 in ISOMAP, but some data items between the two clusters overlap in TSTG."}, {"x": 1332, "text": "5.4.4 Dimension Reduction: Supporting Multiple Perspectives"}, {"x": 1338, "text": "On the contrary, an unsupervised method, ISOMAP, may reveal different aspects of data."}, {"x": 1345, "text": "26(a), one can see that the cluster 5 (the digit 4) of the Pendigits data set moves from the top left near the cluster 10 (the digit 9) towards the cluster 7 (the digit 6) as the ISOMAP parameter k increases."}, {"x": 1346, "text": "In general, ISOMAP with a smaller k value focuses more on preserving the local neighborhood relationships by making non-neighborhood distances longer."}, {"x": 1350, "text": "5.4.5 Clustering: Combining Knowledge from Different Clustering"}, {"x": 1352, "text": "The Testbed system can remedy this problem by enabling users to perform different clustering methods and obtain more meaningful clusters by comparing between them."}, {"x": 1354, "text": "27 shows the scatter plot views of TSTG with the cluster labels obtained by two different clustering methods, k-means and NMF."}, {"x": 1376, "text": "As we perform brushing-and-linking on this cluster, it turns out that this cluster mainly corresponds partially to the clusters 1, 6, and 7 of the NMF clustering."}, {"x": 1378, "text": "On the other hand, in the NMF clustering, the cluster 4 seems to be clearly re- lated to multi-variate/multi-dimensional data visualization."}, {"x": 1379, "text": "By brushing-and-linking on this cluster, we found it corresponds mostly to the clusters 4 and 7 of the k- means clustering, which makes sense based on their keyword summaries although they are relatively more ambiguous than the cluster 4 of the NMF clustering."}, {"x": 1382, "text": "As shown in these cases, one can apply different clustering methods and take full advantage of them by visually analyzing them in the Testbed system."}, {"x": 1386, "text": "The Testbed system provides full control of these methods with interactive visual access to their results."}, {"x": 1394, "text": "To be specific, the currently used algorithms do not change anything in the reference view, and the Procrustes analysis does not change internal relationships within each visualization at all."}, {"x": 1405, "text": "Figure 24: The 10-dimensional results of PCA for the Weizmann facial image data set."}, {"x": 1410, "text": "In all three figures, ISOMAP is used with the same parameter values."}, {"x": 1412, "text": "For the Pendigits data set, the first figure uses TSTG and the other two use ISOMAP with the same parameter values."}, {"x": 1416, "text": "(a) The scatter plots for the Pendigits data set generated by ISOMAP with different parameter values, k =12, 20, 30, and 50, respectively."}, {"x": 1422, "text": "Figure 26: The effects of a parameter change in ISOMAP."}, {"x": 1423, "text": "Figure 27: The scatter plot view of two different clustering, k-means and NMF, using TSTG for the InfoVisVAST data set."}, {"x": 1424, "text": "The right figure is aligned with respect to the left one for both clustering and dimension reduction."}, {"x": 1425, "text": "CHAPTER VI IVISCLASSIFIER: AN INTERACTIVE VISUAL CLASSIFICATION SYSTEM USING SUPERVISED DIMENSION REDUCTION"}, {"x": 1426, "text": "We present an interactive visual analytics system for classification, iVisClassifier, based on a supervised dimension reduction method, linear discriminant analysis (LDA)."}, {"x": 1427, "text": "Given high-dimensional data and associated cluster labels, LDA gives their reduced dimensional representation, which provides a good overview about the cluster struc- ture."}, {"x": 1428, "text": "Instead of a single two- or three-dimensional scatter plot, iVisClassifier fully interacts with all the reduced dimensions obtained by LDA through parallel coor- dinates and a scatter plot."}, {"x": 1438, "text": "Figure 28: 2D Scatter plots obtained by two dimension reduction methods, LDA and PCA, for artificial Gaussian mixture data with 7 clusters and 1000 original dimen- sions."}, {"x": 1458, "text": "To resolve this issue, we choose the classification method based on linear discriminant analysis (LDA) <60>, one of the supervised dimension reduction methods."}, {"x": 1459, "text": "Unlike other unsupervised methods such as multidimensional scaling (MDS) and principal component analysis (PCA), which only use data, supervised ones also involve additional information such as cluster labels associated in the data."}, {"x": 1460, "text": "In case of LDA, it maximally discriminates different clusters while keeping the relationship among data within each cluster tight in the reduced dimensional space."}, {"x": 1461, "text": "This behavior of LDA has two advantages for interactive classification systems."}, {"x": 1462, "text": "The first advantage is that LDA is able to visualize the data so that their cluster structure can be well exposed."}, {"x": 1464, "text": "28, LDA reveals the cluster structure better than PCA, and through LDA, users can easily find the cluster relationship and explore the data based on it."}, {"x": 1465, "text": "The other advantage is that the reduced dimensional representation of the data by LDA does not require a sophisticated classification algorithm in general since the data is already transformed to a well-clustered form, and such a transformation would map an unseen data item to a nearby area of its true cluster."}, {"x": 1466, "text": "Thus, after applying LDA, a simple classification algorithm such as k-nearest neighbors <51> can be performed, which has been successfully applied to many areas <16, 123>."}, {"x": 1469, "text": "The first contribu- tion of iVisClassifier lies in its emphasis on interpretation of and interaction with LDA for data understanding."}, {"x": 1470, "text": "Then, iVisClassifier features the ability to let users cooperate with the LDA visualization for the classification process."}, {"x": 1471, "text": "To show the usefulness of iVisClassifier, we present facial recognition examples, where LDA-based classification works well."}, {"x": 1474, "text": "Section 6.3 briefly introduces LDA and its use of the regularization in visualization, and Section 6.4 describes the details of iVisClassifier."}, {"x": 1481, "text": "<143, 142> proposed a visual hierarchical dimension reduction method, which groups dimensions and visualizes data by using the subset of dimensions obtained from each group."}, {"x": 1483, "text": "A user-driven visualization approach using MDS was proposed in <136>."}, {"x": 1491, "text": "Even with such technical advances, people still prefer traditional methods such as PCA, MDS, and self-organizing maps (SOM) because the state-of-the-art methods tend not to work universally for various types of data and they often lack inter- pretability."}, {"x": 1492, "text": "Motivated by this, a recently proposed system called iPCA <72> enables users to interact with PCA and its visualization results in the form of scatter plots and parallel coordinates."}, {"x": 1493, "text": "Our system shares a lot in common with iPCA in that users can play with LDA via scatter plots and parallel coordinates."}, {"x": 1495, "text": "6.3 Linear Discriminant Analysis"}, {"x": 1496, "text": "In this section, we briefly introduce LDA and skip rigorous mathematical derivations due to a page limit."}, {"x": 1497, "text": "For more technical details about LDA and its use in visualization, refer to our previous work <35>."}, {"x": 1500, "text": "By projecting the data onto such a linear subspace, LDA puts cluster centroids as remote to each other as possible (by maximizing the weighted sum, B, of squared distances between cluster centroids, as shown in Fig."}, {"x": 1503, "text": "Due to this characteristic, LDA can highlight the cluster relationship as shown in Fig."}, {"x": 1504, "text": "28(a), as opposed to other dimension reduction methods such as PCA."}, {"x": 1505, "text": "In LDA, this simultaneous optimization is formulated as a generalized eigenvalue problem that maximizes B while keeping its minimum value of W. Theoretically, the objective function value of LDA cannot exceed that in the original space, and such an upper bound is achieved as long as at least k  1 dimensions are allowed in LDA, where k is the number of clusters."}, {"x": 1506, "text": "Due to this characteristic, LDA usually reduces the data (a) Maximization of distances be-(b) Minimization of approximate tween cluster centroids cluster radii"}, {"x": 1510, "text": "Although LDA can reduce the data dimension down to k  1 dimensions without"}, {"x": 1511, "text": "compromising its maximum objective function value, it is often not enough to use for 2D or 3D visualization purposes."}, {"x": 1513, "text": "In iVisClassifier, we adopt the former strategy so that we can easily interpret the dimension reduction step while interacting with all the LDA reduced dimensions."}, {"x": 1514, "text": "6.3.2 Regularization to Control the Cluster Radius"}, {"x": 1515, "text": "In regularized LDA, a scalar multiple of an identity matrix I is added to the within- scatter matrix Sw, the trace of which represents W.1 It was applied to LDA <58> in order to circumvent a singularity problem when the data matrix has more dimensions than the number of data items, i.e., an undersampled case."}, {"x": 1517, "text": "On the other hand, a unified algorithmic framework of LDA using the generalized"}, {"x": 1518, "text": "1Instead of W, the LDA formulation uses Sw, which is then replaced with Sw + I by regular- ization."}, {"x": 1522, "text": "singular value decomposition (LDA/GSVD) was proposed <68>, which broadens the applicability of LDA regardless of the singularity."}, {"x": 1523, "text": "For undersampled data, e.g., text and image data, LDA/GSVD can fully minimize the cluster radii, making them all equal to zero."}, {"x": 1525, "text": "Although it makes sense in terms of the LDA criteria, it does not keep any information to visualize at an individual data level."}, {"x": 1528, "text": "In an extreme case, when we sufficiently increase the regularization parameter , Sw is almost ignored in the minimization term, i.e., Sw + I  I, so that LDA focuses only on maximizing B without minimizing W. Mathematically, this case is equivalent to applying PCA on the cluster centroids <35>."}, {"x": 1531, "text": "Therefore, we reduce the data matrix size by applying either QR decomposition for undersampled cases or Cholesky decomposition for the other cases before running LDA."}, {"x": 1533, "text": "Then, the GSVD-based LDA algorithm is performed on this reduced matrix much efficiently."}, {"x": 1544, "text": "Once the data matrix whose columns represent data items is obtained, LDA is per- formed on this matrix with its associated labels."}, {"x": 1545, "text": "Users can recompute LDA with different regularization parameter values  through a horizontal slide bar interface until the data within each cluster are adequately scattered."}, {"x": 1546, "text": "As described in Section 3, LDA reduces the data dimension to k1 where k is the number of clusters."}, {"x": 1547, "text": "Just as the reduced dimensions in PCA are in an order to preserve the most variance, those in LDA are also in an order of preserving the most value of the LDA criterion."}, {"x": 1549, "text": "With this in mind, we visualize LDA results in four different ways: parallel coordinates (Fig."}, {"x": 1552, "text": "31C), and 2D scatter plots (Fig."}, {"x": 1558, "text": "However, LDA can deal with both problems effectively in the following ways."}, {"x": 1559, "text": "First, with a manageable number of clusters, k, LDA reduces the number of dimensions to k  1, without losing any information on the cluster structure based on the LDA criterion."}, {"x": 1560, "text": "In addition, in terms of the number of data items, LDA plays the role of data reduction for undersampled cases since it can represent all the data items within each cluster as a single point by setting  = 0, which in turn visualizes the entire data as k items."}, {"x": 1561, "text": "The dimension-reduced data by LDA may suffer the same scalability problem when the number of clusters and/or the regularization parameter  increases."}, {"x": 1562, "text": "Nonetheless, in most cases, LDA significantly alleviates the clutter in parallel coordinates in that dealing with a large number of clusters is not practical and that users can always start their analysis with  = 0."}, {"x": 1572, "text": "For instance, even though the dimension reduction result is given by LDA, users may need to know the meaning behind each dimension and the reasons why those dimensions maximize the LDA cri- terion."}, {"x": 1573, "text": "Without such information, users cannot readily understand why certain data points look like outliers or certain clusters are prominent in the LDA result."}, {"x": 1574, "text": "Follow- ing this motivation, we provide users with the meaning of each reduced dimension of LDA in the following way."}, {"x": 1575, "text": "First of all, LDA is a linear method where each reduced dimension is represented as a linear combination of those in the original space."}, {"x": 1578, "text": "Based on this idea, we reconstruct the LDA basis in the original data domain, e.g., an image in our case."}, {"x": 1580, "text": "For example, pixel values in an image have a certain specification that they have to be all integers between 0 and 255 while the LDA basis is real-valued with positive and negative signs mixed."}, {"x": 1581, "text": "In the past, several heuristics to handle this issue were used in the context of PCA by mapping basis vectors to grayscale images <129, 131> by taking either its absolute value or adding the minimum value."}, {"x": 1584, "text": "In this way, we obtain the reconstructed images of LDA basis vectors as shown in Fig."}, {"x": 1586, "text": "6.4.2.3 Heat maps"}, {"x": 1599, "text": "The scatter plot visualizes data points in the two user-selected reduced dimensions of LDA with a zoom-in/out functionality."}, {"x": 1602, "text": "Our scatter plot view given by LDA allows users to interactively explore the data in view of the overall cluster structure in the following senses: 1. which data points are outliers or representative points in their corresponding clusters, 2. which data points are outliers or representative points in their corresponding clusters, 3. how widely the data points within a cluster are distributed and accordingly, which clusters have potential subclusters, and 4. which data points overlap between different clusters."}, {"x": 1612, "text": "When the new data point falls into a cluttered region where many different clusters overlap, users can select or filter out some data or clusters and recompute LDA with this subset of data including the new point, which we call a computational zoom-in process."}, {"x": 1613, "text": "In other words, LDA takes into account the selected clusters and/or those corresponding to the selected data, which requires a much smaller number of dimensions than k  1 for LDA to fully discriminate the selected clusters."}, {"x": 1615, "text": "On completing the visually-supported classification process, users can assign a label to the new data item and optionally include the newly labeled data in future LDA computations, which is initiated only when users want to recompute them."}, {"x": 1616, "text": "The reason we do not force users to include every new data in LDA computations is that users confidence level of the assigned label may not be high enough for some reason such as noise."}, {"x": 1640, "text": "Next, let us look at the heat maps of the LDA dimensions shown in Figs."}, {"x": 1643, "text": "In addition, the heat maps in the LDA dimensions have mostly blue-colored elements, i.e., almost zero, except for a few rows and columns, which indicates that each of the LDA dimensions tends to discriminate only a few clusters."}, {"x": 1645, "text": "35 shows the image reconstruction of the first six LDA bases for both data sets."}, {"x": 1647, "text": "This indicates that the forehead part is the most prominent factor for facial recognition based on LDA in our data."}, {"x": 1655, "text": "6.5.2 Interactive Classification"}, {"x": 1673, "text": "Once some of the new data are assigned their labels, users can recompute LDA by taking into account the newly labeled data."}, {"x": 1675, "text": "39 shows the distributions of the new data whose label is 0 before and after LDA recomputation with a newly labeled data item."}, {"x": 1676, "text": "As we can see, the rest of the unseen data in cluster 0 becomes closer to its centroid after LDA recomputation, which indicates that the updated LDA dimensions potentially better discriminates the unseen data."}, {"x": 1679, "text": "Our system enables users to explore high-dimensional data through LDA, which is a supervised dimension reduction method."}, {"x": 1683, "text": "Finally, we showed that iVisClassifier can efficiently support a user-driven classification process by reducing humans search space, e.g., recomputing LDA with a user-selected subset of data and mutual filtering in parallel coordinates and the scatter plot."}, {"x": 1689, "text": "Finally, the computation of LDA can be burdensome for user interactions when we have a large-scale data."}, {"x": 1690, "text": "Novel interactions with LDA provided by iVisClassifier motivate the new types of dynamic updating algorithms based on the previous LDA results in various situations."}, {"x": 1691, "text": "For instance, updating the LDA results when changing the regularization parameter value has not been studied before."}, {"x": 1697, "text": "The LDA results are represented in 29 dimensions."}, {"x": 1699, "text": "The LDA basis vectors are reconstructed in the original data domain, which in this case is an image."}, {"x": 1700, "text": "(C) Heat map view."}, {"x": 1702, "text": "The leftmost one is computed from the original space, and the rest from each of the LDA dimensions."}, {"x": 1710, "text": "The interfaces for unseen data visualize them one by one, interactively classify them, and finally update the LDA model."}, {"x": 1711, "text": "A horizontal slide bar for the regularization parameter value in LDA controls the scattering of each cluster."}, {"x": 1717, "text": "Figure 33: Heat map view of the pairwise cluster distances of the Weizmann data set."}, {"x": 1719, "text": "Figure 34: Heat map view of the pairwise cluster distances of the SCface data set."}, {"x": 1722, "text": "Figure 35: Reconstructed images of the first six LDA bases."}, {"x": 1726, "text": "Figure 37: Interactive classification by computational zoom-in."}, {"x": 1727, "text": "Recursive visualiza- tion by recomputing LDA for interactively selected subsets of data guides a new point into its corresponding cluster."}, {"x": 1730, "text": "Figure 38: Interactive classification by mutual filtering."}, {"x": 1734, "text": "Figure 39: Effects of LDA recomputation when including a newly labeled point in the existing data."}, {"x": 1736, "text": "CHAPTER VII VISIRR: AN INTERACTIVE VISUAL INFORMATION RETRIEVAL AND RECOMMENDER SYSTEM FOR LARGE-SCALE DOCUMENT DATA"}, {"x": 1760, "text": "By seeing this as an information retrieval (IR) problem, the focus in this chapter is on the long tail, or recall (making sure that few relevant documents are missed), while in web search the focus is generally on the quicker gratification of precision (making sure the first page of hits or so contain very relevant documents)."}, {"x": 1765, "text": "In the context of visual analytics, document analyses have long been one of the main areas studied."}, {"x": 1766, "text": "Visual analytics systems for document data, such as IN-SPIRE <138> and JIGSAW <121>, can help to give an overall understanding about a set of documents as well as revealing their intra-set relationships that would have been difficult and time-consuming without the help of interactive visualization."}, {"x": 1768, "text": "As one of the milestones to fill this gap, we present a novel document visual an- alytics system called VisIRR, an interactive Visual Information Retrieval and Recommendation for document data, which effectively combines traditional query- based information retrieval and personalized recommendation."}, {"x": 1770, "text": "40, VisIRR adopts a scatter plot as a main visualization form similar to IN- SPIRE."}, {"x": 1771, "text": "In other words, the documents to be visualized are first clustered into several groups via a clustering algorithm and then projected to a 2D space via a dimension reduction algorithm."}, {"x": 1774, "text": "Advanced clustering and dimension reduction techniques: As core computational modules, VisIRR adopts state-of-the-art techniques such as nonnegative matrix factorization (NMF) for clustering and linear discriminant analysis (LDA) for dimension reduction."}, {"x": 1775, "text": "These techniques give the results with a much better quality as well as with a faster computational time than traditional methods including k-means, principal component analysis (PCA), and multidimensional scaling."}, {"x": 1780, "text": "To integrate all these capabilities into a mature visual analytics system, we incor- porate various building blocks for front-end GUIs and back-end computational al- gorithms."}, {"x": 1784, "text": "Section 7.3 explains the front-end GUI modules and comprehensive usage scenarios that highlight the key capabilities of the system."}, {"x": 1794, "text": "The Pacific Northwest National Labs SPIRE system (and IN-SPIRE follow-on) uses clustering to extract common themes, and includes several visualization components <138>."}, {"x": 1795, "text": "Its Themescape compo- nent is an abstract 3D landscape depiction of a document space, with arrangements of hills and valleys representing the relatively strength of various themes in the docu- ment corpus and how those themes interrelate."}, {"x": 1796, "text": "Other systems have used this general clusters-in-landscapes (both 2D and 3D) as well <116, 21, 6>."}, {"x": 1797, "text": "iVisClustering <88> is an interactive document clustering system focused on the user interactions to improve cluster quality based on an advanced technique called latent Dirichlet allocation <20>."}, {"x": 1798, "text": "On the other hand, rather than providing user interactions customized to a partic- ular clustering technique, the Testbed system <32> offers a wide variety of clustering algorithms and easy comparisons between them via an alignment process VisIRR has adopted."}, {"x": 1802, "text": "A relatively early example is the Envision digital library, which includes a visualization system that places documents in a 2D grid according to user-selectable attributes <97>."}, {"x": 1806, "text": "A recent survey <4, 63> distinguishes between the visualization of a single document (e.g., tag clouds) and a document collection and between time- (e.g., TIARA <134>) and network-oriented collection systems."}, {"x": 1815, "text": "Several commercial tools are targeted to this problem, with a variety of automated and visual features."}, {"x": 1817, "text": "The Microsoft Academic Search system from Microsoft Research <2> is a similar offering that also includes more advanced network- style visualization of authorship connections as well as various ways of examining topical, institutional, and venue trends and rankings."}, {"x": 1821, "text": "The Action Science Explorer (ASE) <52> focuses on co-citation network visualization, with document clus- ters created manually or by heuristics <95>."}, {"x": 1823, "text": "The FacetAtlas system <24> automatically clusters document collections using a Kernel density estimation algorithm and provides multi- faceted links between document nodes (rather than just keyword or author searches as in VisIRR)."}, {"x": 1831, "text": "The Query Bar at the top (Fig."}, {"x": 1833, "text": "The Scatter Plot view (with document details shown in the lower table) (Fig."}, {"x": 1843, "text": "These recommended documents are also visualized in the Scatter Plot view as rectangles while the query-retrieved documents are shown as circles."}, {"x": 1847, "text": "VisIRR has been implemented using a modified version of the ArnetMiner dataset, which contains approximately 430,000 academic research articles from a variety of disciplines and venues (primarily conferences, journals and books), as will be described in detail in Section 7.4."}, {"x": 1850, "text": "The user starts by issuing queries from the Query Toolbar."}, {"x": 1852, "text": "Once documents are retrieved due to this query, the clustering and dimension reduction steps are performed to generate the Scatter plot view (Fig."}, {"x": 1856, "text": "From the Scatter plot view, the user can drill down to a cluster of interest, e.g., the clusters about gene expression data (the top right), and image analysis (the top left)."}, {"x": 1858, "text": "(a) Default cluster summary (b) Distinct cluster summary"}, {"x": 1864, "text": "7.3.2.2 Drilling Down via Computational Zoom-in"}, {"x": 1878, "text": "Suppose the user wanted to focus on those recently published in 2008 or later and thus created another filter from the Query Toolbar in conjunction with the previous keyword query disease."}, {"x": 1882, "text": "(a) An unaligned view (b) A reference view (c) An aligned view"}, {"x": 1896, "text": "Among the retrieved documents, suppose the user found a document Au- tomatic tool for Alzheimers disease diagnosis using PCA and Bayesian classification rules interesting and assigned the document a 5-star rating(highly-like) by right- clicking the corresponding data point in the Scatter Plot View."}, {"x": 1900, "text": "From the list of recommended documents shown in the lower table, the user could obtain an idea that the research about Alzheimers disease mainly involves an image analysis, clustering, classification, etc."}, {"x": 1902, "text": "In the Scatter Plot view, the user can see these recommended documents at the upper left corner around the rated document and its nearby clusters."}, {"x": 1905, "text": "From its own cluster summary and visualization, the user could see that the documents directly related to Alzheimers disease are mainly shown in the bottom half while the upper half in the Scatter Plot view, shows documents mainly related to image analysis such as content-based image retrieval, clustering, etc."}, {"x": 1906, "text": "7.3.2.5 Citation- and Co-authorship-based Recommendation"}, {"x": 1907, "text": "Now, among the recommended documents, the user chose another document Auto- matic Classification System for the Diagnosis of Alzheimer Disease Using Component- Based SVM Aggregations and assigned it a 5-star rating."}, {"x": 1910, "text": "(b) A visualization of recommended doc- uments"}, {"x": 1911, "text": "Figure 44: Citation-based recommendation results obtained by assigning a 5-star rating to the paper, Automatic Classification System for the Diagnosis of Alzheimer Disease Using Component-Based SVM Aggregations."}, {"x": 1913, "text": "(a) A visualization of retrieved and rec-(b) A visualization of only the recom- ommended documents mended documents"}, {"x": 1914, "text": "Figure 45: Co-authorship-based recommendation results based on the paper, Auto- matic Classification System for the Diagnosis of Alzheimer Disease Using Component- Based SVM Aggregations."}, {"x": 1925, "text": "In addition, to better show the direct co-authorship relationships from the rated paper, the user turned on the Edges checkbox by selecting the edge type as Co-authorship in the Label panel."}, {"x": 1931, "text": "From this visualization, the user could gain an insight that the authors of the rated paper have written the papers, other than Alzheimers disease-related papers (the green cluster on the right), in the four areas corresponding to blind source separation, gene expression, speech processing, and neural networks."}, {"x": 1932, "text": "This potentially indicates that the user, who was originally interested in Alzheimers disease diagnosis, could expand his/her research by following the ways the authors of the rated paper have published in other domains."}, {"x": 1933, "text": "7.4 Data Collection / Ingestion"}, {"x": 1936, "text": "To this end, VisIRR begins with the ArnetMiner data set, which is composed of about half a million academic papers, books, etc."}, {"x": 1939, "text": "To clean up the data, we utilized the Microsoft Academic Search APIs.3 Specifically, we used a title of each document as a query in order to obtain the full information about the document from the Microsoft Academic Search API, which fills the missing values and rectifies the inconsistencies."}, {"x": 1945, "text": "2The used data is available as DBLP-Citation-network V5 at http://arnetminer.org/ citation."}, {"x": 1949, "text": "For efficient and flexible query support, we have encoded the original data as a SQL database including full-text search capabilities on a title, keywords, an abstract, and a venue fields."}, {"x": 1952, "text": "In this way, VisIRR can retrieve the vector representations of documents using their document IDs in the time complexity of O(1)."}, {"x": 1953, "text": "7.4.2.2 Vector Representation"}, {"x": 1964, "text": "Since maintaining all the pairwise information requires O(n2) storage where n is the total number of documents, we identified the fixed number (10 in our case) of the most similar documents for each document and kept only the edges between them."}, {"x": 1967, "text": "Since citation and co- authorship graphs are typically sparse, we stored all these edge information."}, {"x": 1968, "text": "For each graph, VisIRR maintains the mappings from an individual document to a list of edges in terms of the destination document and its edge value so that it can retrieve the edge information for particular documents in the time complexity of O(1)."}, {"x": 1974, "text": "(a) Maximization of dis-(b) Minimization of ap- (c) LDA (d) PCA tances between clusterproximate cluster radii centroids"}, {"x": 1975, "text": "Figure 46: A high-level idea of LDA and a comparison example betwee LDA and PCA."}, {"x": 1976, "text": "A different color corresponds to a different cluster, and c1 and c2 are the cluster centroids."}, {"x": 1978, "text": "(c) and (d) show an example 2D scatter plots obtained by PCA and LDA, respectively, for artificial Gaussian mixture data with 7 clusters and 1,000 original dimensions."}, {"x": 1979, "text": "From a comparison between them, LDA is shown to reveal a much clearer cluster structure than PCA in a 2D space."}, {"x": 1985, "text": "The computa- tional complexity of this process is O(n  nnew) where n and nnew are the numbers of the existing and the new documents, respectively."}, {"x": 1993, "text": "VisIRR adopts a state-of-the-art technique called nonnegative matrix factorization (NMF) <80>, which have shown superior performances in document clustering over traditional methods such as k-means <81, 141>."}, {"x": 1994, "text": "Given a nonnegative matrix X  Rmn, and an integer k  min(m, n), NMF finds a lower-rank approximation given by"}, {"x": 1995, "text": "X  WH, (40) where W  Rmk and H  Rkn are nonnegative factors."}, {"x": 1996, "text": "NMF can be formulated using the Frobenius norm as for the i-th document, and by taking the index the value of which is the largest, the cluster index of the document can be obtained."}, {"x": 1997, "text": "The specific NMF algorithm we have used is based on a recently proposed block principal pivoting algorithm <82>,4 which is found to be one of the fastest and reliable algorithms."}, {"x": 1998, "text": "Although not reported, we have conducted an extensive amount of com- parison of NMF against traditional clustering techniques such as k-means, and we found that NMF mostly gives semantically more meaningful clusters than any other methods while requiring a significantly faster computational time."}, {"x": 1999, "text": "7.5.2 Dimension Reduction"}, {"x": 2000, "text": "Given high-dimensional vector representations of documents, dimension reduction computes their 2D representations so that they can be visualized in a scatter plot (Fig."}, {"x": 2003, "text": "VisIRR adopts an advanced dimension reduction method called linear discriminant analysis (LDA) <68>."}, {"x": 2004, "text": "Unlike traditional methods such as principal component analysis and multidi- mensional scaling, LDA explicitly utilizes additional cluster label information, which are taken from the clustering module, associated with the input high-dimensional vectors."}, {"x": 2005, "text": "Using this information, LDA tries to preserve the cluster structure in the low-dimensional space such that the dimension-reduced result can clearly reveal the underlying cluster structure in the input data."}, {"x": 2007, "text": "46, LDA has an advantage over most traditional methods such as PCA and MDS in that it can provide a clear cluster structure in the data when the cluster label information is given."}, {"x": 2008, "text": "Furthermore, VisIRR provides a slider interface for controlling how compactly each cluster is represented by using regularization on LDA, which enables users to"}, {"x": 2013, "text": "In VisIRR, users can create multiple scatter plots for (1) new parameter values, e.g., the number of clusters in NMF, a regularization value in LDA, and (2) a new set of data from different queries or arbitrary selection by users."}, {"x": 2016, "text": "On the other hand, by aligning dimension reduction results, users can expect that the same data point is located in a similar position in the 2D space between different scatter plots."}, {"x": 2017, "text": "To align different clustering results, VisIRR utilizes the Hungarian algorithm <85>."}, {"x": 2018, "text": "Given two sets of cluster assignments for the same set of documents, the Hungarian algorithm finds the optimal pairwise matching of cluster indices between the two sets so that the number of common data items within matching cluster pairs can be maximized."}, {"x": 2021, "text": "The alignment of different dimension reduction results is based on Procrustes analysis <69, 53>, which best maps one result to the other with only a rotation matrix."}, {"x": 2022, "text": "In addition, VisIRR extends the original Procrustes analysis by incorporating trans- lation and isotropic scaling factors as well."}, {"x": 2023, "text": "That is, given two reduced-dimensional matrices X, Y Rmn, where m is the number of dimensions and n is the number of data points, VisIRR solves"}, {"x": 2035, "text": "In de- tail, given an input graph W  RNN between N documents, where each column of W is normalized such that its sum is equal to one, and a user preference vector p  RN1, where the i-th component pi is the preference value, VisIRR computes the An intuitive explanation of this formulation is that the preference value piof node i is propagated to its neighbor nodes with the corresponding weights specified in the graph W at the first iteration, and then the resulting values are then propagated again with the same graph W with the scale factor (1  ) at the next iteration, and so on."}, {"x": 2045, "text": "The system is mainly implemented in JAVA for front-end UI and rendering modules, which are partly based on the FODAVA Testbed system <32>."}, {"x": 2046, "text": "NetBeans Rich Client Platform and IDE5 have been used for flexible window management."}, {"x": 2047, "text": "The back-end computational modules NMF and LDA are originally written in MATLAB but we have wrapped them into a JAVA library by using a Matlab built-in functionality called Javabuilder.6 Since the library made in this manner is self-contained, VisIRR does not require an actual Matlab to be installed."}, {"x": 2048, "text": "For querying and accessing with the database, we have used H2 library.7"}, {"x": 2056, "text": "It is not an experimental design as it includes no control condition, so we cannot and do not make any relative claims about VisIRRs effectiveness compared to other research or commercial alternatives (e.g., Google Scholar)."}, {"x": 2062, "text": "Finally, we deployed a version of the IBM Computer System Usabil- ity Questionnaire (CSUQ) <90> along with a few other subjective assessment questions specific to VisIRR."}, {"x": 2063, "text": "The system was installed on a workstation with dual 2.5GHz Intel Xeon processors and 128GB RAM running 64-bit Windows 7, though the Java VM memory limit was set to only 8 GB."}, {"x": 2067, "text": "As such, they all had experience doing academic literature searches using online resources such as Google, Google Scholar, the IEEE/ACM digital libraries, etc."}, {"x": 2068, "text": "We asked participants to self-rate their familiarity with information visualization and ubiquitous computing literature; all self-rated 4 or less on a 7-point Likert scale for information visualization and 6 of the 7 did so for ubiquitous computing."}, {"x": 2070, "text": "The VisIRR system was instrumented to log the UI actions shown in Table 9."}, {"x": 2078, "text": "All users made at least 9 distinct document ratings (again, across all tasks), and interestingly did so relatively evenly from different portions of the UI (the recom- mended, rating and query lists, and the scatterplot)."}, {"x": 2082, "text": "On the subjective CSUQ, scores were generally 5 or higher, with the lowest rated scores coming on the questions The system has all the functions and capabilities I expect it to have; The system gives error messages that clearly tell me how to fix problems; and Whenever I make a mistake using the system, I recover easily and quickly."}, {"x": 2087, "text": "Of course, we would hypothesize that rating-based refinement is more productive since it does require less user expertise at generating useful keyword sequences; at least one user agreed, saying that VisIRR ... is definitely much better than blindly searching Google Scholar or basic search engines using just a few keywords."}, {"x": 2091, "text": "In addition, VisIRR directly tackles a large-scale docu- ment corpus via efficient data management and new data updating as well as a suite of state-of-the-art computational methods such as NMF, LDA, and graph diffusion- based recommendation."}, {"x": 2110, "text": "The proposed two-stage framework enables various com- binations and their interpretations of several well-known supervised and unsu- pervised dimension reduction methods to obtain appropriate 2D/3D represen- tations of high-dimensional data."}, {"x": 2117, "text": "The presented framework and several applications of this idea in existing visual analytics systems, such as Jigsaw, iVisClustering, and the Testbed system, shows the effectiveness of the proposed framework using widely-used computational methods such as PCA, MDS, t-SNE, k-means, and latent Dirichlet allocation <31>."}, {"x": 2121, "text": "The Testbed system brings a wide variety of tradi- tional and state-of-the-art dimension reduction and clustering methods to vi- sual analytics."}, {"x": 2122, "text": "The Testbed system provides full control of these methods with interactive visual access to their results."}, {"x": 2131, "text": "Figure 47: Hierarchical precision refinement of PCA computational results."}, {"x": 2137, "text": "VisIRR is well-engineered to handle large-scale data and streaming data and utilizes the state-of-the-art clustering and dimension reduction methods such as NMF and LDA."}, {"x": 2162, "text": "In principle, as the number of data items increases, the algorithm complexity cannot be more efficient than O(n), which assumes that every data item is processed at least once."}, {"x": 2164, "text": "The notion of a fixed number of available pixels can turn the algorithm complexity into O(1) in the sense that we can visualize only a specific number of data items at most."}, {"x": 2170, "text": "In addition, in the case of dimension reduction, suppose PCA has been computed on a subset of data."}, {"x": 2171, "text": "Then, the rest of the data can be projected onto the same space via a linear transformation matrix given by PCA, which is a much more efficient process than computing PCA on the entire data set."}, {"x": 2176, "text": "For example in dimension reduction, MDS gives the reduced-dimensional re- sult that best preserves the original pairwise dimension reduction under the low- dimensional space within a given dimension."}, {"x": 2182, "text": "48 indicates that MDS significantly distorts the original data relationships by severely decreasing particular pairwise distances while some others are almost preserved."}, {"x": 2183, "text": "Even though MDS sup- posedly gives the best result in preserving the pairwise distances, one would not be able to trust the result considering such a significant distortion."}, {"x": 2186, "text": "For instance, one could come up with a new criterion for an alternative method to MDS so that distance losses can be evenly distributed throughout all the pairwise distances."}], "characters": [{"name": "Interactivity", "offsets": [84840, 193740, 200369, 200729], "paragraph_occurrences": [263, 547, 569, 571], "sentence_occurrences": [753, 1655, 1726, 1730], "affiliation": "light", "frequency": 4, "id": "Interactivity"}, {"name": "Self-organizing map", "offsets": [19561, 21096], "paragraph_occurrences": [47, 48], "sentence_occurrences": [143, 156], "affiliation": "light", "frequency": 2, "id": "Self-organizing_map"}, {"name": "Uniform-cost search", "offsets": [54355, 63268, 66073], "paragraph_occurrences": [182, 225, 231], "sentence_occurrences": [467, 573, 601], "affiliation": "light", "frequency": 3, "id": "Dijkstra's_algorithm"}, {"name": "Principal component analysis", "offsets": [8382, 19602, 20946, 23401, 23473, 25327, 26559, 28482, 28618, 31905, 31909, 32020, 32389, 32559, 32786, 32970, 33046, 33119, 33341, 35908, 36889, 37254, 37661, 31875, 32502, 40469, 43551, 43987, 46065, 46179, 46251, 46304, 46326, 47351, 78269, 83716, 90060, 98901, 98931, 98935, 99058, 99720, 104115, 104329, 104401, 104455, 122895, 129734, 130054, 131846, 132488, 152665, 153107, 163252, 166836, 170099, 170658, 174573, 174839, 176164, 179228, 181315, 185503, 207486, 222798, 233093, 233228, 233556, 233759, 237242, 253602, 255210, 260094, 260244, 260303], "paragraph_occurrences": [14, 47, 48, 53, 53, 55, 57, 65, 66, 90, 90, 91, 95, 96, 96, 97, 97, 98, 98, 115, 123, 126, 130, 90, 95, 140, 158, 158, 166, 166, 166, 166, 166, 173, 246, 255, 275, 297, 297, 297, 298, 300, 316, 318, 318, 318, 372, 383, 383, 386, 386, 442, 443, 479, 491, 492, 492, 499, 499, 503, 512, 520, 526, 584, 620, 645, 646, 646, 646, 657, 696, 701, 709, 709, 709], "sentence_occurrences": [57, 143, 155, 171, 172, 182, 187, 203, 206, 250, 250, 252, 260, 262, 263, 266, 266, 267, 268, 299, 314, 318, 322, 250, 261, 347, 384, 386, 404, 405, 406, 406, 406, 422, 711, 740, 794, 865, 865, 865, 866, 870, 910, 912, 913, 913, 1076, 1120, 1123, 1135, 1139, 1309, 1312, 1405, 1438, 1459, 1464, 1491, 1492, 1504, 1528, 1547, 1581, 1775, 1896, 1974, 1975, 1978, 1979, 2007, 2117, 2131, 2170, 2171, 2171], "affiliation": "light", "frequency": 75, "id": "Principal_component_analysis"}, {"name": "Fortran", "offsets": [63581], "paragraph_occurrences": [225], "sentence_occurrences": [575], "affiliation": "light", "frequency": 1, "id": "Fortran"}, {"name": "Common Fig", "offsets": [41598, 42088, 42684, 43214, 46526, 46621, 46960], "paragraph_occurrences": [144, 148, 152, 156, 167, 168, 171], "sentence_occurrences": [357, 364, 371, 378, 408, 411, 417], "affiliation": "light", "frequency": 7, "id": "Common_fig"}, {"name": "Reuters", "offsets": [40802, 42696, 44952, 42766], "paragraph_occurrences": [142, 153, 162, 153], "sentence_occurrences": [350, 373, 396, 373], "affiliation": "light", "frequency": 4, "id": "Reuters"}, {"name": "Centroid", "offsets": [29874], "paragraph_occurrences": [76], "sentence_occurrences": [222], "affiliation": "light", "frequency": 1, "id": "Centroid"}, {"name": "Williams' p&nbsp;+&nbsp;1 algorithm", "offsets": [59554], "paragraph_occurrences": [194], "sentence_occurrences": [508], "affiliation": "light", "frequency": 1, "id": "Williams'_p_+_1_algorithm"}, {"name": "Ambiguity", "offsets": [4637], "paragraph_occurrences": [11], "sentence_occurrences": [34], "affiliation": "light", "frequency": 1, "id": "Ambiguity"}, {"name": "Document", "offsets": [201315], "paragraph_occurrences": [574], "sentence_occurrences": [1736], "affiliation": "light", "frequency": 1, "id": "Document"}, {"name": "Tiara", "offsets": [212161], "paragraph_occurrences": [592], "sentence_occurrences": [1806], "affiliation": "light", "frequency": 1, "id": "Tiara"}, {"name": "Oil cleansing method", "offsets": [28609, 29891, 29923, 30025, 30666, 30817, 31115, 31287, 31529, 31563, 32965, 35371, 35491, 37097, 37132, 140087], "paragraph_occurrences": [66, 76, 77, 77, 84, 84, 84, 87, 88, 89, 97, 115, 115, 125, 126, 408], "sentence_occurrences": [206, 222, 223, 223, 237, 238, 241, 245, 247, 248, 266, 292, 294, 316, 317, 1200], "affiliation": "light", "frequency": 16, "id": "Oil_cleansing_method"}, {"name": "Norm", "offsets": [31733, 33696, 36051, 37009, 37374], "paragraph_occurrences": [89, 102, 115, 123, 126], "sentence_occurrences": [248, 272, 301, 314, 318], "affiliation": "light", "frequency": 5, "id": "Norm_(mathematics)"}, {"name": "Sinclair Broadcast Group", "offsets": [28136, 28941, 29134, 29969, 30061, 30132, 30440, 30515, 30717, 39133, 40515, 46108, 46200, 46261], "paragraph_occurrences": [62, 68, 69, 77, 78, 80, 81, 82, 84, 139, 140, 166, 166, 166], "sentence_occurrences": [199, 208, 211, 223, 224, 227, 231, 233, 237, 339, 347, 405, 405, 406], "affiliation": "light", "frequency": 14, "id": "Sinclair_Broadcast_Group"}, {"name": "Graphics processing unit", "offsets": [87971], "paragraph_occurrences": [267], "sentence_occurrences": [776], "affiliation": "light", "frequency": 1, "id": "Graphics_processing_unit"}, {"name": "Java version history", "offsets": [129343, 130235, 130342, 130566, 130780, 130941], "paragraph_occurrences": [382, 383, 384, 384, 384, 384], "sentence_occurrences": [1117, 1123, 1125, 1126, 1128, 1129], "affiliation": "light", "frequency": 6, "id": "Java_version_history"}, {"name": "Heat", "offsets": [185940], "paragraph_occurrences": [527], "sentence_occurrences": [1586], "affiliation": "light", "frequency": 1, "id": "Heat"}, {"name": "User", "offsets": [90209, 130703, 136778, 181010], "paragraph_occurrences": [275, 384, 395, 520], "sentence_occurrences": [794, 1127, 1170, 1544], "affiliation": "light", "frequency": 4, "id": "User_(computing)"}, {"name": "Grounded theory", "offsets": [39074, 39130, 39672, 39736, 40496, 40512, 40521, 40563, 46105, 46113, 46197, 46207, 46258, 46311], "paragraph_occurrences": [139, 139, 140, 140, 140, 140, 140, 140, 166, 166, 166, 166, 166, 166], "sentence_occurrences": [339, 339, 342, 343, 347, 347, 347, 347, 405, 405, 405, 405, 406, 406], "affiliation": "light", "frequency": 14, "id": "Grounded_theory"}, {"name": "Salesforce.com", "offsets": [205676, 253456], "paragraph_occurrences": [581, 696], "sentence_occurrences": [1766, 2117], "affiliation": "light", "frequency": 2, "id": "Salesforce.com"}, {"name": "Graphical user interface", "offsets": [147610, 208317, 208577, 242582, 246296, 247446], "paragraph_occurrences": [428, 586, 586, 670, 677, 681], "sentence_occurrences": [1264, 1780, 1784, 2045, 2070, 2078], "affiliation": "light", "frequency": 6, "id": "Graphical_user_interface"}, {"name": "Mathematical optimization", "offsets": [27019], "paragraph_occurrences": [58], "sentence_occurrences": [190], "affiliation": "light", "frequency": 1, "id": "Mathematical_optimization"}, {"name": "MEDLINE", "offsets": [40777, 41652, 43301, 43359, 44936, 64084, 64521], "paragraph_occurrences": [142, 145, 158, 158, 162, 228, 228], "sentence_occurrences": [350, 359, 382, 383, 396, 579, 583], "affiliation": "light", "frequency": 7, "id": "MEDLINE"}, {"name": "Matrix norm", "offsets": [33676, 36041, 36999, 37364, 47544, 235661], "paragraph_occurrences": [102, 115, 123, 126, 173, 653], "sentence_occurrences": [272, 301, 314, 318, 422, 1996], "affiliation": "light", "frequency": 6, "id": "Matrix_norm"}, {"name": "tf\u2013idf", "offsets": [138115], "paragraph_occurrences": [401], "sentence_occurrences": [1183], "affiliation": "light", "frequency": 1, "id": "Tf\u2013idf"}, {"name": "Iteration", "offsets": [15550, 75758, 81177, 89120, 112461], "paragraph_occurrences": [38, 243, 250, 272, 354], "sentence_occurrences": [115, 699, 727, 784, 1013], "affiliation": "light", "frequency": 5, "id": "Iteration"}, {"name": "Gigabyte", "offsets": [245346], "paragraph_occurrences": [676], "sentence_occurrences": [2063], "affiliation": "light", "frequency": 1, "id": "Gigabyte"}, {"name": "Regularization", "offsets": [38605, 177385], "paragraph_occurrences": [138, 508], "sentence_occurrences": [334, 1514], "affiliation": "light", "frequency": 2, "id": "Regularization_(mathematics)"}, {"name": "t-distributed stochastic neighbor embedding", "offsets": [102627, 108025, 108107], "paragraph_occurrences": [310, 336, 336], "sentence_occurrences": [897, 954, 955], "affiliation": "light", "frequency": 3, "id": "T-distributed_stochastic_neighbor_embedding"}, {"name": "Latent Dirichlet allocation", "offsets": [7713, 76552, 77210, 77239, 83745, 105568, 105597, 105601, 105857, 106466, 111628, 111960, 113613, 113665, 127452, 128203, 195597, 195728, 195870, 195922, 196180, 196839, 197747, 197847, 197954, 198015, 198523, 198587, 198849, 199453, 199528, 200123, 200462, 201002, 210605, 253632], "paragraph_occurrences": [14, 243, 245, 245, 256, 323, 323, 323, 324, 328, 352, 352, 355, 355, 379, 379, 550, 550, 550, 550, 552, 552, 555, 555, 555, 555, 556, 556, 556, 556, 556, 566, 569, 573, 589, 696], "sentence_occurrences": [54, 702, 707, 707, 741, 924, 924, 924, 927, 935, 1005, 1007, 1019, 1019, 1108, 1112, 1673, 1675, 1676, 1676, 1679, 1683, 1689, 1690, 1690, 1691, 1697, 1699, 1702, 1710, 1711, 1722, 1727, 1734, 1797, 2117], "affiliation": "light", "frequency": 36, "id": "Latent_Dirichlet_allocation"}, {"name": "ICO", "offsets": [149726], "paragraph_occurrences": [433], "sentence_occurrences": [1279], "affiliation": "light", "frequency": 1, "id": "ICO_(file_format)"}, {"name": "Euclidean space", "offsets": [47554, 104482, 104612, 147053, 148675], "paragraph_occurrences": [173, 318, 318, 426, 430], "sentence_occurrences": [422, 914, 914, 1260, 1273], "affiliation": "light", "frequency": 5, "id": "Euclidean_space"}, {"name": "2D computer graphics", "offsets": [206700, 210410, 211402], "paragraph_occurrences": [582, 589, 591], "sentence_occurrences": [1771, 1796, 1802], "affiliation": "light", "frequency": 3, "id": "2D_computer_graphics"}, {"name": "Sammon mapping", "offsets": [140313], "paragraph_occurrences": [408], "sentence_occurrences": [1202], "affiliation": "light", "frequency": 1, "id": "Sammon_mapping"}, {"name": "Shortest path problem", "offsets": [58964], "paragraph_occurrences": [193], "sentence_occurrences": [506], "affiliation": "light", "frequency": 1, "id": "Shortest_path_problem"}, {"name": "Integral", "offsets": [75252], "paragraph_occurrences": [242], "sentence_occurrences": [695], "affiliation": "light", "frequency": 1, "id": "Integral"}, {"name": "Cornelius Lanczos", "offsets": [61590, 62040], "paragraph_occurrences": [213, 214], "sentence_occurrences": [548, 554], "affiliation": "light", "frequency": 2, "id": "Cornelius_Lanczos"}, {"name": "Naive Bayes classifier", "offsets": [222806], "paragraph_occurrences": [620], "sentence_occurrences": [1896], "affiliation": "light", "frequency": 1, "id": "Naive_Bayes_classifier"}, {"name": "Data mining", "offsets": [66552], "paragraph_occurrences": [232], "sentence_occurrences": [604], "affiliation": "light", "frequency": 1, "id": "Data_mining"}, {"name": "SQL", "offsets": [229674], "paragraph_occurrences": [638], "sentence_occurrences": [1949], "affiliation": "light", "frequency": 1, "id": "SQL"}, {"name": "National Institutes of Health", "offsets": [64592], "paragraph_occurrences": [228], "sentence_occurrences": [583], "affiliation": "light", "frequency": 1, "id": "National_Institutes_of_Health"}, {"name": "Component-based software engineering", "offsets": [224453, 225220, 224814], "paragraph_occurrences": [623, 628, 626], "sentence_occurrences": [1907, 1914, 1911], "affiliation": "light", "frequency": 3, "id": "Component-based_software_engineering"}, {"name": "Science", "offsets": [214476], "paragraph_occurrences": [595], "sentence_occurrences": [1821], "affiliation": "light", "frequency": 1, "id": "Science_(journal)"}, {"name": "Microsoft Research", "offsets": [213858], "paragraph_occurrences": [594], "sentence_occurrences": [1817], "affiliation": "light", "frequency": 1, "id": "Microsoft_Research"}, {"name": "Flow Java", "offsets": [242563, 242862], "paragraph_occurrences": [670, 670], "sentence_occurrences": [2045, 2047], "affiliation": "light", "frequency": 2, "id": "Java_(programming_language)"}, {"name": "H2", "offsets": [243110], "paragraph_occurrences": [670], "sentence_occurrences": [2048], "affiliation": "light", "frequency": 1, "id": "H2_(DBMS)"}, {"name": "Microsoft Academic Search", "offsets": [213820, 228397, 228556], "paragraph_occurrences": [594, 633, 633], "sentence_occurrences": [1817, 1939, 1939], "affiliation": "light", "frequency": 3, "id": "Microsoft_Academic_Search"}, {"name": "Reduction", "offsets": [114194], "paragraph_occurrences": [357], "sentence_occurrences": [1023], "affiliation": "light", "frequency": 1, "id": "Reduction_(complexity)"}, {"name": "System", "offsets": [165186], "paragraph_occurrences": [487], "sentence_occurrences": [1425], "affiliation": "light", "frequency": 1, "id": "System"}, {"name": "Vector space", "offsets": [230245], "paragraph_occurrences": [639], "sentence_occurrences": [1953], "affiliation": "light", "frequency": 1, "id": "Vector_space"}, {"name": "Kernel", "offsets": [140501], "paragraph_occurrences": [408], "sentence_occurrences": [1206], "affiliation": "light", "frequency": 1, "id": "Kernel_(operating_system)"}, {"name": "Big O notation", "offsets": [58668, 60388, 58868, 60639, 230232, 232343, 231624, 258959, 259218], "paragraph_occurrences": [192, 203, 192, 206, 638, 642, 642, 708, 708], "sentence_occurrences": [503, 522, 505, 527, 1952, 1968, 1964, 2162, 2164], "affiliation": "light", "frequency": 9, "id": "Big_O_notation"}, {"name": "QR algorithm", "offsets": [99191, 99614], "paragraph_occurrences": [298, 299], "sentence_occurrences": [867, 869], "affiliation": "light", "frequency": 2, "id": "QR_algorithm"}, {"name": "Stochastic", "offsets": [101891], "paragraph_occurrences": [308], "sentence_occurrences": [892], "affiliation": "light", "frequency": 1, "id": "Stochastic"}, {"name": "Internet Explorer", "offsets": [214484], "paragraph_occurrences": [595], "sentence_occurrences": [1821], "affiliation": "light", "frequency": 1, "id": "Internet_Explorer"}, {"name": "Parameter", "offsets": [48721], "paragraph_occurrences": [175], "sentence_occurrences": [431], "affiliation": "light", "frequency": 1, "id": "Parameter"}, {"name": "Bachelor of Environmental Studies", "offsets": [220947, 224591, 224994], "paragraph_occurrences": [616, 625, 627], "sentence_occurrences": [1882, 1910, 1913], "affiliation": "light", "frequency": 3, "id": "Bachelor's_degree"}, {"name": "Nickel", "offsets": [27332, 27413], "paragraph_occurrences": [61, 61], "sentence_occurrences": [193, 193], "affiliation": "light", "frequency": 2, "id": "Nickel"}, {"name": "Outlier", "offsets": [20723], "paragraph_occurrences": [48], "sentence_occurrences": [153], "affiliation": "light", "frequency": 1, "id": "Outlier"}, {"name": "Google Scholar", "offsets": [213684, 244096, 245872, 249478], "paragraph_occurrences": [594, 673, 677, 684], "sentence_occurrences": [1815, 2056, 2067, 2087], "affiliation": "light", "frequency": 4, "id": "Google_Scholar"}, {"name": "Minimum-variance unbiased estimator", "offsets": [140434], "paragraph_occurrences": [408], "sentence_occurrences": [1205], "affiliation": "light", "frequency": 1, "id": "Minimum-variance_unbiased_estimator"}, {"name": "Two-dimensional space", "offsets": [18179, 19364, 22055, 25830, 239471], "paragraph_occurrences": [45, 47, 51, 55, 663], "sentence_occurrences": [133, 142, 163, 183, 2023], "affiliation": "light", "frequency": 5, "id": "Two-dimensional_space"}, {"name": "Cholesky decomposition", "offsets": [179473], "paragraph_occurrences": [514], "sentence_occurrences": [1531], "affiliation": "light", "frequency": 1, "id": "Cholesky_decomposition"}, {"name": "Three-dimensional space", "offsets": [123190, 132140], "paragraph_occurrences": [373, 386], "sentence_occurrences": [1077, 1137], "affiliation": "light", "frequency": 2, "id": "Three-dimensional_space_(mathematics)"}, {"name": "India", "offsets": [38195, 129327, 129577, 130071, 205657, 206552, 209992], "paragraph_occurrences": [134, 382, 383, 383, 581, 582, 589], "sentence_occurrences": [330, 1117, 1119, 1123, 1766, 1770, 1794], "affiliation": "light", "frequency": 7, "id": "India"}, {"name": "Hungarian algorithm", "offsets": [146049, 146142, 146320, 238531, 238629], "paragraph_occurrences": [425, 425, 425, 662, 662], "sentence_occurrences": [1254, 1255, 1256, 2017, 2018], "affiliation": "light", "frequency": 5, "id": "Hungarian_algorithm"}, {"name": "Algorithm", "offsets": [58340, 59999, 60368, 60610], "paragraph_occurrences": [190, 198, 202, 205], "sentence_occurrences": [499, 515, 521, 526], "affiliation": "light", "frequency": 4, "id": "Algorithm"}, {"name": "Rich client platform", "offsets": [147702, 242674], "paragraph_occurrences": [428, 670], "sentence_occurrences": [1265, 2046], "affiliation": "light", "frequency": 2, "id": "Rich_client_platform"}, {"name": "Foreach loop", "offsets": [232098], "paragraph_occurrences": [642], "sentence_occurrences": [1967], "affiliation": "light", "frequency": 1, "id": "Foreach_loop"}, {"name": "Automatic transmission", "offsets": [130470], "paragraph_occurrences": [384], "sentence_occurrences": [1125], "affiliation": "light", "frequency": 1, "id": "Automatic_transmission"}, {"name": "Embedding", "offsets": [101911], "paragraph_occurrences": [308], "sentence_occurrences": [892], "affiliation": "light", "frequency": 1, "id": "Embedding"}, {"name": "T.I.", "offsets": [60058, 60086], "paragraph_occurrences": [199, 199], "sentence_occurrences": [516, 516], "affiliation": "light", "frequency": 2, "id": "T.I."}, {"name": "Java virtual machine", "offsets": [245307], "paragraph_occurrences": [676], "sentence_occurrences": [2063], "affiliation": "light", "frequency": 1, "id": "Java_virtual_machine"}, {"name": "Dimension", "offsets": [11980, 14623, 18093, 19370, 114184, 117895, 123270, 124074, 124526, 125940, 131939, 132033, 139377, 165210, 236292], "paragraph_occurrences": [20, 35, 44, 47, 357, 362, 373, 374, 375, 377, 386, 386, 406, 487, 655], "sentence_occurrences": [80, 107, 132, 142, 1023, 1045, 1078, 1082, 1086, 1098, 1136, 1136, 1195, 1425, 1999], "affiliation": "light", "frequency": 15, "id": "Dimension"}, {"name": "Cluster analysis", "offsets": [114208, 122589, 129031, 157500], "paragraph_occurrences": [357, 371, 381, 451], "sentence_occurrences": [1023, 1074, 1115, 1350], "affiliation": "light", "frequency": 4, "id": "Cluster_analysis"}, {"name": "Digital Bibliography & Library Project", "offsets": [229121], "paragraph_occurrences": [636], "sentence_occurrences": [1945], "affiliation": "light", "frequency": 1, "id": "DBLP"}, {"name": "Windows 7", "offsets": [245285], "paragraph_occurrences": [676], "sentence_occurrences": [2063], "affiliation": "light", "frequency": 1, "id": "Windows_7"}, {"name": "Usenet newsgroup", "offsets": [40786, 42100, 44743], "paragraph_occurrences": [142, 149, 160], "sentence_occurrences": [350, 366, 394], "affiliation": "light", "frequency": 3, "id": "Usenet_newsgroup"}, {"name": "Citation", "offsets": [224256], "paragraph_occurrences": [622], "sentence_occurrences": [1906], "affiliation": "light", "frequency": 1, "id": "Citation"}, {"name": "QR decomposition", "offsets": [30748, 30987, 179430], "paragraph_occurrences": [84, 84, 514], "sentence_occurrences": [237, 240, 1531], "affiliation": "light", "frequency": 3, "id": "QR_decomposition"}, {"name": "Computer", "offsets": [75277], "paragraph_occurrences": [242], "sentence_occurrences": [695], "affiliation": "light", "frequency": 1, "id": "Computer"}, {"name": "Heat map", "offsets": [198687, 199827, 199981], "paragraph_occurrences": [556, 561, 563], "sentence_occurrences": [1700, 1717, 1719], "affiliation": "light", "frequency": 3, "id": "Heat_map"}, {"name": "Non-negative matrix factorization", "offsets": [127438, 138957, 157958, 159939, 160252, 160611, 164976, 207224, 235336, 235524, 235629, 235824, 236069, 236150, 237843, 242785, 250109, 255878], "paragraph_occurrences": [379, 404, 452, 469, 470, 470, 486, 584, 651, 652, 653, 654, 654, 654, 661, 670, 686, 703], "sentence_occurrences": [1108, 1190, 1354, 1376, 1378, 1379, 1423, 1774, 1993, 1994, 1995, 1997, 1998, 1998, 2013, 2047, 2091, 2137], "affiliation": "light", "frequency": 18, "id": "Non-negative_matrix_factorization"}, {"name": "Application programming interface", "offsets": [228423, 228582], "paragraph_occurrences": [633, 633], "sentence_occurrences": [1939, 1939], "affiliation": "light", "frequency": 2, "id": "Application_programming_interface"}, {"name": "National Science Foundation", "offsets": [107823], "paragraph_occurrences": [334], "sentence_occurrences": [950], "affiliation": "light", "frequency": 1, "id": "National_Science_Foundation"}, {"name": "Visual system", "offsets": [201247], "paragraph_occurrences": [574], "sentence_occurrences": [1736], "affiliation": "light", "frequency": 1, "id": "Visual_system"}, {"name": "Local tangent space alignment", "offsets": [140389], "paragraph_occurrences": [408], "sentence_occurrences": [1205], "affiliation": "light", "frequency": 1, "id": "Local_tangent_space_alignment"}, {"name": "Time complexity", "offsets": [58491, 59956, 61861], "paragraph_occurrences": [191, 197, 213], "sentence_occurrences": [501, 514, 549], "affiliation": "light", "frequency": 3, "id": "Time_complexity"}, {"name": "Antimony", "offsets": [27606, 27960, 28341, 29671, 30144, 30213, 30362, 30371, 30528, 30575, 31329, 32854, 33019, 37177, 37668], "paragraph_occurrences": [61, 62, 63, 73, 80, 81, 81, 81, 82, 83, 87, 97, 97, 126, 130], "sentence_occurrences": [196, 198, 201, 219, 227, 228, 230, 231, 233, 236, 245, 264, 266, 317, 322], "affiliation": "light", "frequency": 15, "id": "Antimony"}, {"name": "Weka", "offsets": [129384, 132841, 133096, 133189, 133406], "paragraph_occurrences": [382, 387, 387, 387, 387], "sentence_occurrences": [1117, 1142, 1144, 1145, 1146], "affiliation": "light", "frequency": 5, "id": "Weka_(machine_learning)"}, {"name": "Alzheimer's disease", "offsets": [222763, 223327, 224042, 227391, 224429, 224790, 225196, 227132], "paragraph_occurrences": [620, 621, 621, 629, 623, 626, 628, 629], "sentence_occurrences": [1896, 1900, 1905, 1932, 1907, 1911, 1914, 1931], "affiliation": "light", "frequency": 8, "id": "Alzheimer's_disease"}, {"name": "Arnetminer", "offsets": [217063, 227731], "paragraph_occurrences": [603, 632], "sentence_occurrences": [1847, 1936], "affiliation": "light", "frequency": 2, "id": "Arnetminer"}, {"name": "Prefuse", "offsets": [100637], "paragraph_occurrences": [305], "sentence_occurrences": [880], "affiliation": "light", "frequency": 1, "id": "Prefuse"}, {"name": "Lanczos algorithm", "offsets": [61727, 62183, 62686, 63554, 99212, 99250, 99566], "paragraph_occurrences": [213, 216, 222, 225, 298, 299, 299], "sentence_occurrences": [549, 557, 568, 575, 867, 868, 869], "affiliation": "light", "frequency": 7, "id": "Lanczos_algorithm"}, {"name": "Procrustes analysis", "offsets": [146721, 146841, 146897, 162185, 239195, 239334], "paragraph_occurrences": [426, 426, 426, 475, 663, 663], "sentence_occurrences": [1258, 1259, 1259, 1394, 2021, 2022], "affiliation": "light", "frequency": 6, "id": "Procrustes_analysis"}, {"name": "Cartesian coordinate system", "offsets": [11977, 14620, 117892, 122138, 123187, 123267, 124071, 124523, 125937, 126392, 129815, 129862, 131184, 131936, 132027, 132137, 141695, 177002, 181726, 233527, 233768, 236406, 238431], "paragraph_occurrences": [20, 35, 362, 368, 373, 373, 374, 375, 377, 377, 383, 383, 385, 386, 386, 386, 413, 507, 520, 646, 646, 656, 661], "sentence_occurrences": [80, 107, 1045, 1069, 1077, 1078, 1082, 1086, 1098, 1100, 1121, 1122, 1132, 1136, 1136, 1137, 1217, 1511, 1552, 1978, 1979, 2000, 2016], "affiliation": "light", "frequency": 23, "id": "Cartesian_coordinate_system"}, {"name": "Guanosine triphosphate", "offsets": [32491], "paragraph_occurrences": [95], "sentence_occurrences": [261], "affiliation": "light", "frequency": 1, "id": "Guanosine_triphosphate"}, {"name": "Europe", "offsets": [57065], "paragraph_occurrences": [188], "sentence_occurrences": [490], "affiliation": "light", "frequency": 1, "id": "Europe"}, {"name": "Microsoft Windows", "offsets": [148501], "paragraph_occurrences": [430], "sentence_occurrences": [1271], "affiliation": "light", "frequency": 1, "id": "Microsoft_Windows"}, {"name": "Information retrieval", "offsets": [201254, 204490, 206287], "paragraph_occurrences": [574, 579, 582], "sentence_occurrences": [1736, 1760, 1768], "affiliation": "light", "frequency": 3, "id": "Information_retrieval"}, {"name": "Parallel coordinates", "offsets": [140821, 151568], "paragraph_occurrences": [410, 439], "sentence_occurrences": [1211, 1301], "affiliation": "light", "frequency": 2, "id": "Parallel_coordinates"}, {"name": "Automation", "offsets": [224737], "paragraph_occurrences": [626], "sentence_occurrences": [1911], "affiliation": "light", "frequency": 1, "id": "Automation"}, {"name": "Viewer Access Satellite Television", "offsets": [151414], "paragraph_occurrences": [438], "sentence_occurrences": [1299], "affiliation": "light", "frequency": 1, "id": "Viewer_Access_Satellite_Television"}, {"name": "MATLAB", "offsets": [63372, 63503, 63828, 147809, 147851, 148001, 148312, 148381, 148609, 148944, 150097, 242823, 242886, 243028], "paragraph_occurrences": [225, 225, 226, 429, 429, 429, 430, 430, 430, 430, 433, 670, 670, 670], "sentence_occurrences": [574, 575, 577, 1266, 1267, 1268, 1270, 1271, 1273, 1274, 1282, 2047, 2047, 2047], "affiliation": "light", "frequency": 14, "id": "MATLAB"}, {"name": "Graph theory", "offsets": [27235, 28133, 28160, 28423, 28437, 28532, 28938, 29122, 29131, 29188, 29197, 29728, 29966, 29998, 30058, 30071, 30129, 30437, 30512, 30714, 32059, 32072, 32123, 33632, 34155, 34247, 34769, 35137, 35435], "paragraph_occurrences": [60, 62, 62, 64, 64, 65, 68, 69, 69, 70, 70, 73, 77, 77, 78, 78, 80, 81, 82, 84, 92, 92, 94, 101, 106, 106, 110, 113, 115], "sentence_occurrences": [192, 199, 199, 202, 202, 205, 208, 211, 211, 213, 213, 219, 223, 223, 224, 225, 227, 231, 233, 237, 253, 254, 256, 271, 279, 279, 286, 290, 293], "affiliation": "light", "frequency": 29, "id": "Graph_theory"}, {"name": "Google", "offsets": [245864], "paragraph_occurrences": [677], "sentence_occurrences": [2067], "affiliation": "light", "frequency": 1, "id": "Google"}, {"name": "Novel", "offsets": [172951, 197823], "paragraph_occurrences": [496, 555], "sentence_occurrences": [1481, 1689], "affiliation": "light", "frequency": 2, "id": "Novel"}, {"name": "Data collection", "offsets": [227540], "paragraph_occurrences": [630], "sentence_occurrences": [1933], "affiliation": "light", "frequency": 1, "id": "Data_collection"}, {"name": "Comma-Separated Values", "offsets": [137097], "paragraph_occurrences": [398], "sentence_occurrences": [1174], "affiliation": "light", "frequency": 1, "id": "Comma-separated_values"}, {"name": "Philippines", "offsets": [59370], "paragraph_occurrences": [194], "sentence_occurrences": [508], "affiliation": "light", "frequency": 1, "id": "Philippines"}, {"name": "Dimensionality reduction", "offsets": [11597, 26990, 117652, 122565, 129007, 155440], "paragraph_occurrences": [20, 58, 362, 371, 381, 448], "sentence_occurrences": [78, 190, 1043, 1074, 1115, 1332], "affiliation": "light", "frequency": 6, "id": "Dimensionality_reduction"}, {"name": "Default", "offsets": [218178], "paragraph_occurrences": [606], "sentence_occurrences": [1858], "affiliation": "light", "frequency": 1, "id": "Default_(computer_science)"}, {"name": "Classical antiquity", "offsets": [50710], "paragraph_occurrences": [179], "sentence_occurrences": [444], "affiliation": "light", "frequency": 1, "id": "Classical_antiquity"}, {"name": "O(n)", "offsets": [234622], "paragraph_occurrences": [647], "sentence_occurrences": [1985], "affiliation": "light", "frequency": 1, "id": "O(n)"}, {"name": "Support vector machine", "offsets": [224470, 224830, 225237], "paragraph_occurrences": [623, 626, 628], "sentence_occurrences": [1907, 1911, 1914], "affiliation": "light", "frequency": 3, "id": "Support_vector_machine"}, {"name": "Switzerland", "offsets": [50540, 53598, 64028, 65024, 70818], "paragraph_occurrences": [178, 181, 228, 228, 236], "sentence_occurrences": [443, 460, 579, 587, 650], "affiliation": "light", "frequency": 5, "id": "Switzerland"}, {"name": "Label", "offsets": [142524, 226551], "paragraph_occurrences": [415, 629], "sentence_occurrences": [1223, 1925], "affiliation": "light", "frequency": 2, "id": "Label"}, {"name": "Multidimensional scaling", "offsets": [19639, 25677, 50705, 50720, 50980, 56653, 56679, 69598, 78311, 83720, 99828, 99854, 99858, 100057, 100295, 100365, 100475, 100684, 100767, 100922, 101081, 101126, 101572, 101640, 102892, 103027, 103142, 105118, 122200, 122932, 124684, 170060, 173105, 260953, 261472, 261635, 262045], "paragraph_occurrences": [47, 55, 179, 179, 179, 187, 187, 235, 246, 255, 301, 301, 301, 302, 303, 303, 305, 305, 305, 305, 305, 305, 306, 307, 310, 312, 312, 321, 368, 372, 375, 492, 496, 712, 712, 712, 713], "sentence_occurrences": [143, 183, 444, 445, 446, 485, 485, 635, 711, 740, 871, 871, 871, 873, 878, 878, 880, 881, 881, 883, 885, 886, 889, 890, 898, 900, 901, 921, 1069, 1076, 1088, 1459, 1483, 2176, 2182, 2183, 2186], "affiliation": "light", "frequency": 37, "id": "Multidimensional_scaling"}, {"name": "Information visualization", "offsets": [15560, 75768, 89130, 151390], "paragraph_occurrences": [38, 243, 272, 438], "sentence_occurrences": [115, 699, 784, 1299], "affiliation": "light", "frequency": 4, "id": "Information_visualization"}, {"name": "Human\u2013computer interaction", "offsets": [88385, 100392, 102904, 104897], "paragraph_occurrences": [269, 304, 311, 320], "sentence_occurrences": [779, 879, 899, 918], "affiliation": "light", "frequency": 4, "id": "Human\u2013computer_interaction"}, {"name": "Floyd\u2013Warshall algorithm", "offsets": [54295], "paragraph_occurrences": [182], "sentence_occurrences": [466], "affiliation": "light", "frequency": 1, "id": "Floyd\u2013Warshall_algorithm"}, {"name": "Medal bar", "offsets": [215630], "paragraph_occurrences": [600], "sentence_occurrences": [1831], "affiliation": "light", "frequency": 1, "id": "Medal_bar"}, {"name": "FaceBase", "offsets": [152874], "paragraph_occurrences": [443], "sentence_occurrences": [1311], "affiliation": "light", "frequency": 1, "id": "FaceBase"}, {"name": "IEEE Fellow", "offsets": [151385, 245888], "paragraph_occurrences": [438, 677], "sentence_occurrences": [1299, 2067], "affiliation": "light", "frequency": 2, "id": "Institute_of_Electrical_and_Electronics_Engineers"}, {"name": "Normal distribution", "offsets": [40730, 41325, 41367, 127339, 166856, 233598], "paragraph_occurrences": [142, 144, 144, 379, 491, 646], "sentence_occurrences": [350, 355, 355, 1107, 1438, 1978], "affiliation": "light", "frequency": 6, "id": "Normal_distribution"}, {"name": "Holotype", "offsets": [10617, 12691, 156995, 222976, 238812], "paragraph_occurrences": [16, 23, 450, 620, 662], "sentence_occurrences": [71, 85, 1346, 1896, 2018], "affiliation": "light", "frequency": 5, "id": "Holotype"}, {"name": "Ronald Fisher", "offsets": [85100], "paragraph_occurrences": [264], "sentence_occurrences": [756], "affiliation": "light", "frequency": 1, "id": "Ronald_Fisher"}, {"name": "Scatter plot", "offsets": [122141, 141261, 166769, 217675, 217885, 222958, 223614, 224125], "paragraph_occurrences": [368, 412, 491, 605, 605, 620, 621, 621], "sentence_occurrences": [1069, 1215, 1438, 1852, 1856, 1896, 1902, 1905], "affiliation": "light", "frequency": 8, "id": "Scatter_plot"}, {"name": "Questionnaire", "offsets": [245075], "paragraph_occurrences": [675], "sentence_occurrences": [2062], "affiliation": "light", "frequency": 1, "id": "Questionnaire"}, {"name": "Scattering", "offsets": [215801, 216618], "paragraph_occurrences": [600, 601], "sentence_occurrences": [1833, 1843], "affiliation": "light", "frequency": 2, "id": "Scattering"}, {"name": "Kernel density estimation", "offsets": [214770], "paragraph_occurrences": [595], "sentence_occurrences": [1823], "affiliation": "light", "frequency": 1, "id": "Kernel_density_estimation"}, {"name": "Fig", "offsets": [67750, 68327, 69046, 71747, 72209, 73359, 73374], "paragraph_occurrences": [235, 235, 235, 236, 236, 236, 236], "sentence_occurrences": [614, 623, 630, 659, 664, 676, 677], "affiliation": "light", "frequency": 7, "id": "Ficus"}, {"name": "IBM", "offsets": [245043], "paragraph_occurrences": [675], "sentence_occurrences": [2062], "affiliation": "light", "frequency": 1, "id": "IBM"}, {"name": "Discriminant", "offsets": [175189], "paragraph_occurrences": [500], "sentence_occurrences": [1495], "affiliation": "light", "frequency": 1, "id": "Discriminant"}, {"name": "Statistical classification", "offsets": [165171], "paragraph_occurrences": [487], "sentence_occurrences": [1425], "affiliation": "light", "frequency": 1, "id": "Statistical_classification"}, {"name": "JavaScript", "offsets": [147586, 147891, 148915, 150107], "paragraph_occurrences": [428, 429, 430, 433], "sentence_occurrences": [1264, 1267, 1274, 1282], "affiliation": "light", "frequency": 4, "id": "JavaScript"}, {"name": "Pacific Northwest National Laboratory", "offsets": [209942], "paragraph_occurrences": [589], "sentence_occurrences": [1794], "affiliation": "light", "frequency": 1, "id": "Pacific_Northwest_National_Laboratory"}, {"name": "Recommender system", "offsets": [201280], "paragraph_occurrences": [574], "sentence_occurrences": [1736], "affiliation": "light", "frequency": 1, "id": "Recommender_system"}, {"name": "GGobi", "offsets": [129357, 131066], "paragraph_occurrences": [382, 384], "sentence_occurrences": [1117, 1130], "affiliation": "light", "frequency": 2, "id": "GGobi"}, {"name": "Visualization", "offsets": [177008], "paragraph_occurrences": [507], "sentence_occurrences": [1511], "affiliation": "light", "frequency": 1, "id": "Visualization_(computer_graphics)"}, {"name": "Analytics", "offsets": [128984], "paragraph_occurrences": [381], "sentence_occurrences": [1115], "affiliation": "light", "frequency": 1, "id": "Analytics"}, {"name": "Windows Vista", "offsets": [63844], "paragraph_occurrences": [226], "sentence_occurrences": [577], "affiliation": "light", "frequency": 1, "id": "Windows_Vista"}, {"name": "Gaussian function", "offsets": [3610], "paragraph_occurrences": [6], "sentence_occurrences": [21], "affiliation": "light", "frequency": 1, "id": "Gaussian_function"}, {"name": "Chapter", "offsets": [165122], "paragraph_occurrences": [486], "sentence_occurrences": [1424], "affiliation": "light", "frequency": 1, "id": "Chapter_(religion)"}, {"name": "Software framework", "offsets": [18052, 75264], "paragraph_occurrences": [44, 242], "sentence_occurrences": [132, 695], "affiliation": "light", "frequency": 2, "id": "Software_framework"}, {"name": "Toolbar", "offsets": [217474, 220508], "paragraph_occurrences": [605, 615], "sentence_occurrences": [1850, 1878], "affiliation": "light", "frequency": 2, "id": "Toolbar"}, {"name": "Audio Units", "offsets": [222742], "paragraph_occurrences": [620], "sentence_occurrences": [1896], "affiliation": "light", "frequency": 1, "id": "Audio_Units"}, {"name": "Association for Computing Machinery", "offsets": [245897], "paragraph_occurrences": [677], "sentence_occurrences": [2067], "affiliation": "light", "frequency": 1, "id": "Association_for_Computing_Machinery"}, {"name": "Linear discriminant analysis", "offsets": [22376, 22800, 25254, 26545, 28604, 28771, 28792, 29080, 29502, 29689, 32957, 35364, 35484, 36261, 36725, 36760, 28741, 38623, 38675, 38865, 38923, 39030, 39170, 39542, 39625, 39829, 44399, 44511, 45088, 45174, 45284, 45456, 45733, 45799, 45980, 46752, 47137, 47485, 122215, 122977, 125490, 125586, 125667, 140080, 140232, 165395, 165459, 165707, 166828, 169931, 170235, 170404, 170494, 170612, 170675, 170842, 171093, 171651, 171752, 171889, 172116, 174999, 175248, 175348, 175621, 176042, 176171, 176359, 176492, 176559, 176852, 177356, 177444, 177586, 177893, 177937, 178304, 178369, 178453, 178687, 179121, 180949, 181030, 181210, 181375, 181436, 181607, 182230, 182344, 182459, 182524, 182763, 182910, 179711, 184149, 184261, 184419, 184522, 184561, 185067, 185358, 185894, 187189, 187551, 189120, 189236, 189395, 189726, 189864, 192286, 192445, 192582, 192700, 193033, 233085, 233184, 233220, 233326, 233564, 233698, 236678, 236784, 236965, 237183, 237492, 237874], "paragraph_occurrences": [51, 52, 55, 57, 66, 67, 68, 68, 72, 73, 97, 115, 115, 118, 122, 123, 67, 138, 139, 139, 139, 139, 139, 140, 140, 140, 160, 160, 163, 163, 163, 165, 165, 165, 165, 170, 173, 173, 368, 372, 376, 376, 376, 408, 408, 488, 488, 488, 491, 492, 492, 492, 492, 492, 492, 492, 492, 493, 493, 493, 494, 499, 501, 501, 503, 503, 503, 503, 503, 503, 506, 507, 509, 509, 510, 511, 512, 512, 512, 512, 512, 520, 520, 520, 520, 520, 520, 522, 522, 522, 522, 522, 522, 514, 525, 525, 525, 525, 526, 526, 526, 526, 531, 532, 535, 535, 535, 536, 536, 543, 543, 543, 544, 544, 645, 646, 646, 646, 646, 646, 656, 657, 657, 657, 658, 661], "sentence_occurrences": [165, 167, 182, 187, 206, 207, 208, 210, 217, 219, 266, 292, 294, 305, 312, 313, 207, 334, 335, 338, 338, 339, 340, 342, 342, 344, 391, 392, 398, 398, 398, 400, 402, 402, 403, 414, 420, 422, 1069, 1076, 1094, 1096, 1097, 1199, 1201, 1426, 1427, 1428, 1438, 1458, 1460, 1461, 1462, 1464, 1464, 1465, 1466, 1469, 1470, 1471, 1474, 1493, 1496, 1497, 1500, 1503, 1505, 1505, 1505, 1506, 1510, 1513, 1515, 1515, 1517, 1518, 1522, 1522, 1523, 1525, 1528, 1544, 1545, 1546, 1547, 1547, 1549, 1558, 1559, 1559, 1560, 1561, 1562, 1533, 1572, 1572, 1573, 1574, 1575, 1578, 1580, 1584, 1599, 1602, 1612, 1613, 1613, 1615, 1616, 1640, 1643, 1643, 1645, 1647, 1974, 1975, 1975, 1976, 1978, 1979, 2003, 2004, 2005, 2007, 2008, 2013], "affiliation": "light", "frequency": 126, "id": "Linear_discriminant_analysis"}, {"name": "Generalized singular value decomposition", "offsets": [38927, 39034, 178308, 178457, 179700], "paragraph_occurrences": [139, 139, 512, 512, 514], "sentence_occurrences": [338, 339, 1522, 1523, 1533], "affiliation": "light", "frequency": 5, "id": "Generalized_singular_value_decomposition"}, {"name": "Visual analytics", "offsets": [5680, 11091, 114429, 116421, 205605], "paragraph_occurrences": [12, 19, 358, 361, 581], "sentence_occurrences": [41, 75, 1024, 1036, 1765], "affiliation": "light", "frequency": 5, "id": "Visual_analytics"}, {"name": "NetBeans", "offsets": [147691, 242665], "paragraph_occurrences": [428, 670], "sentence_occurrences": [1265, 2045], "affiliation": "light", "frequency": 2, "id": "NetBeans"}, {"name": "IBM System i", "offsets": [248314], "paragraph_occurrences": [682], "sentence_occurrences": [2082], "affiliation": "light", "frequency": 1, "id": "IBM_System_i"}, {"name": "Xeon", "offsets": [245234], "paragraph_occurrences": [676], "sentence_occurrences": [2063], "affiliation": "light", "frequency": 1, "id": "Xeon"}, {"name": "Trace", "offsets": [27013], "paragraph_occurrences": [58], "sentence_occurrences": [190], "affiliation": "light", "frequency": 1, "id": "Trace_(linear_algebra)"}, {"name": "Euclidean distance", "offsets": [50806, 51258, 51385, 56483, 57403, 57129, 100145], "paragraph_occurrences": [179, 179, 179, 187, 188, 188, 303], "sentence_occurrences": [445, 447, 447, 484, 491, 490, 875], "affiliation": "light", "frequency": 7, "id": "Euclidean_distance"}, {"name": "Central processing unit", "offsets": [63876, 94083], "paragraph_occurrences": [226, 285], "sentence_occurrences": [577, 825], "affiliation": "light", "frequency": 2, "id": "Central_processing_unit"}, {"name": "Atlantic Reporter", "offsets": [96187], "paragraph_occurrences": [287], "sentence_occurrences": [838], "affiliation": "light", "frequency": 1, "id": "Atlantic_Reporter"}, {"name": "RAND", "offsets": [64019, 64093, 65365], "paragraph_occurrences": [228, 228, 228], "sentence_occurrences": [579, 579, 590], "affiliation": "light", "frequency": 3, "id": "RAND_Corporation"}, {"name": "Visual perception", "offsets": [165164], "paragraph_occurrences": [487], "sentence_occurrences": [1425], "affiliation": "light", "frequency": 1, "id": "Visual_perception"}, {"name": "Likert scale", "offsets": [246083], "paragraph_occurrences": [677], "sentence_occurrences": [2068], "affiliation": "light", "frequency": 1, "id": "Likert_scale"}, {"name": "Down feather", "offsets": [218744], "paragraph_occurrences": [609], "sentence_occurrences": [1864], "affiliation": "light", "frequency": 1, "id": "Down_feather"}, {"name": "Supervised learning", "offsets": [165199], "paragraph_occurrences": [487], "sentence_occurrences": [1425], "affiliation": "light", "frequency": 1, "id": "Supervised_learning"}, {"name": "Double-sided RAM", "offsets": [245266], "paragraph_occurrences": [676], "sentence_occurrences": [2063], "affiliation": "light", "frequency": 1, "id": "Random-access_memory"}, {"name": "Testbed", "offsets": [1841, 16272, 16843, 17013, 17499, 115326, 115779, 120199, 120808, 120961, 121154, 121356, 121650, 121914, 122043, 133212, 133889, 134039, 134086, 134612, 134738, 134892, 135089, 136189, 136510, 136966, 137149, 137224, 138354, 138797, 139693, 140975, 141150, 141659, 142038, 143279, 144564, 145051, 145124, 145614, 146021, 146354, 146647, 147198, 147546, 148846, 149086, 149270, 149643, 150246, 150354, 151890, 153506, 157673, 160963, 161301, 210752, 242645, 253488, 253848, 253992], "paragraph_occurrences": [3, 39, 40, 40, 41, 358, 358, 365, 365, 365, 366, 366, 366, 367, 367, 387, 388, 389, 390, 392, 392, 392, 393, 395, 395, 398, 398, 398, 402, 404, 407, 411, 411, 413, 414, 418, 421, 422, 422, 424, 425, 425, 426, 426, 428, 430, 432, 432, 432, 433, 436, 440, 445, 452, 471, 473, 589, 670, 696, 698, 698], "sentence_occurrences": [10, 119, 122, 123, 126, 1030, 1031, 1058, 1061, 1062, 1063, 1063, 1064, 1067, 1068, 1145, 1149, 1150, 1151, 1157, 1158, 1159, 1160, 1168, 1169, 1174, 1175, 1176, 1185, 1189, 1197, 1212, 1214, 1217, 1220, 1231, 1242, 1247, 1248, 1251, 1254, 1256, 1258, 1261, 1264, 1274, 1276, 1277, 1278, 1282, 1285, 1303, 1315, 1352, 1382, 1386, 1798, 2045, 2117, 2121, 2122], "affiliation": "light", "frequency": 61, "id": "Testbed"}, {"name": "3D computer graphics", "offsets": [210148, 210417, 252748], "paragraph_occurrences": [589, 589, 694], "sentence_occurrences": [1795, 1796, 2110], "affiliation": "light", "frequency": 3, "id": "3D_computer_graphics"}, {"name": "Krylov subspace", "offsets": [61785, 62098, 62322, 99330], "paragraph_occurrences": [213, 215, 218, 299], "sentence_occurrences": [549, 555, 559, 868], "affiliation": "light", "frequency": 4, "id": "Krylov_subspace"}, {"name": "Stage", "offsets": [18046], "paragraph_occurrences": [44], "sentence_occurrences": [132], "affiliation": "light", "frequency": 1, "id": "Stage_(stratigraphy)"}, {"name": "Recurrent neural network", "offsets": [57790, 59356, 241009], "paragraph_occurrences": [189, 194, 667], "sentence_occurrences": [494, 508, 2035], "affiliation": "light", "frequency": 3, "id": "Recurrent_neural_network"}, {"name": "Isomap", "offsets": [48802, 48849, 49117, 49450, 50368, 50608, 50950, 51440, 51596, 51665, 52256, 52734, 52810, 53398, 53480, 54168, 55056, 55106, 55226, 55309, 55567, 55693, 55809, 55889, 56080, 56318, 56684, 56858, 57030, 57440, 57615, 58509, 61574, 62889, 63089, 63251, 63447, 63648, 63963, 64729, 64832, 64974, 65714, 66101, 66734, 66982, 69992, 71329, 74762, 123684, 140338, 154759, 155226, 155307, 155956, 156816, 156857, 163735, 163899, 164260, 164895], "paragraph_occurrences": [176, 176, 176, 176, 178, 178, 179, 179, 180, 180, 180, 181, 181, 181, 181, 181, 184, 184, 184, 184, 184, 184, 185, 185, 185, 187, 187, 187, 188, 188, 189, 192, 213, 225, 225, 225, 225, 226, 228, 228, 228, 228, 231, 231, 233, 233, 236, 236, 240, 374, 408, 447, 447, 447, 449, 450, 450, 480, 481, 483, 485], "sentence_occurrences": [432, 433, 435, 436, 443, 443, 445, 448, 449, 450, 453, 454, 455, 458, 459, 465, 471, 472, 472, 472, 474, 474, 476, 476, 477, 483, 485, 486, 489, 491, 494, 502, 548, 571, 573, 573, 575, 577, 579, 584, 585, 586, 596, 601, 606, 607, 642, 656, 690, 1081, 1203, 1326, 1329, 1330, 1338, 1345, 1346, 1410, 1412, 1416, 1422], "affiliation": "light", "frequency": 61, "id": "Isomap"}, {"name": "Data General", "offsets": [57786, 57803, 57934, 58046, 59165, 60957], "paragraph_occurrences": [189, 189, 190, 190, 194, 210], "sentence_occurrences": [494, 494, 497, 498, 507, 532], "affiliation": "light", "frequency": 6, "id": "Data_General"}]}