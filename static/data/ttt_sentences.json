{"paragraph_scenes_info": [{"x": 0, "text": "witter sentiment classification has attracted increasing research interest in recent years (Jiang et al., 2011; Hu et al., 2013).The objective is to classify the sentiment polarity of a tweet as positivenegative or neutral.The majority of existing approaches follow Pang et al.(2002) and employ machine learning algorithms to build classifiers from tweets with manually annotated sentiment polarity.Under this direction, most studies focus on designing effective features to obtain better classification performance.For example, Mohammad et al.(2013) build the top-performed system in the Twitter sentiment classification track of SemEval 2013 (Nakov et al., 2013), using diverse sentiment lexicons and a variety of hand-crafted features."}, {"x": 3, "text": "We apply SSWE as features in a supervised learning framework for Twitter sentiment classi- fication, and evaluate it on the benchmark dataset in SemEval 2013.In the task of predicting positive/negative polarity of tweets, our method yields 84.89% in macro-F1 by only using SSWE as feature, which is comparable to the top-performed system based on hand-crafted features (84.70%).After concatenating the SSWE feature with existing feature set, we push the state-of-the-art to 86.58% in macro-F1.The quality of SSWE is also directly evaluated by measuring the word similarity in the embedding space for sentiment lexicons.In the accuracy of polarity consistency between each sentiment word and its top N closest words, SSWE outperforms existing word embedding learning algorithms."}, {"x": 6, "text": "To our knowledge, this is the first work that exploits word embedding for Twitter sentiment classification.We report the results that the SSWE feature performs comparably with hand-crafted features in the top-performed system in SemEval 2013;"}], "scenes": [["Twitter", "SemEval"], ["Twitter", "SemEval"], ["Twitter"], ["SemEval"]], "chapters": [{"text": "2 Related Work", "sentence_id": "s_31", "sentence_rank": "31", "paragraph_id": "p_8", "paragraph_rank": 8}], "all_paragraphs": [{"paragraph_info": {"end": 738, "start": 0, "text": "witter sentiment classification has attracted increasing research interest in recent years (Jiang et al., 2011; Hu et al., 2013).The objective is to classify the sentiment polarity of a tweet as positivenegative or neutral.The majority of existing approaches follow Pang et al.(2002) and employ machine learning algorithms to build classifiers from tweets with manually annotated sentiment polarity.Under this direction, most studies focus on designing effective features to obtain better classification performance.For example, Mohammad et al.(2013) build the top-performed system in the Twitter sentiment classification track of SemEval 2013 (Nakov et al., 2013), using diverse sentiment lexicons and a variety of hand-crafted features.", "rank": 0, "paragraph_comparative_number": 3, "entities": [], "id": "p_0"}, "sentences": [{"end": 129, "text": "witter sentiment classification has attracted increasing research interest in recent years (Jiang et al., 2011; Hu et al., 2013).", "rank": 0, "start": 0, "IsComparative": "0", "id": "st_0"}, {"end": 223, "text": "The objective is to classify the sentiment polarity of a tweet as positivenegative or neutral.", "rank": 1, "start": 129, "IsComparative": "1", "id": "st_1"}, {"end": 277, "text": "The majority of existing approaches follow Pang et al.", "rank": 2, "start": 223, "IsComparative": "0", "id": "st_2"}, {"end": 399, "text": "(2002) and employ machine learning algorithms to build classifiers from tweets with manually annotated sentiment polarity.", "rank": 3, "start": 277, "IsComparative": "0", "id": "st_3"}, {"end": 516, "text": "Under this direction, most studies focus on designing effective features to obtain better classification performance.", "rank": 4, "start": 399, "IsComparative": "1", "id": "st_4"}, {"end": 544, "text": "For example, Mohammad et al.", "rank": 5, "start": 516, "IsComparative": "0", "id": "st_5"}, {"end": 738, "text": "(2013) build the top-performed system in the Twitter sentiment classification track of SemEval 2013 (Nakov et al., 2013), using diverse sentiment lexicons and a variety of hand-crafted features.", "rank": 6, "start": 544, "IsComparative": "1", "id": "st_6"}]}, {"paragraph_info": {"end": 2036, "start": 738, "text": "Feature engineering is important but laborintensive.It is therefore desirable to discover explanatory factors from the data and make the learning algorithms less dependent on extensive feature engineering (Bengio, 2013).For the task of sentiment classification, an effective feature learning method is to compose the representation of a sentence (or document) from the representations of the words or phrases it contains (Socher et al., 2013b; Yessenalina and Cardie, 2011).Accordingly, it is a crucial step to learn the word representation (or word embedding), which is a dense, low-dimensional and real-valued vector for a word.Although existing word embedding learning algorithms (Collobert et al., 2011; Mikolov et al., 2013) are intuitive choices, they are not effective enough if directly used for sentiment classi- fication.The most serious problem is that traditional methods typically model the syntactic context of words but ignore the sentiment information of text.As a result, words with opposite polarity, such as good and bad, are mapped into close vectors.It is meaningful for some tasks such as pos-tagging (Zheng et al., 2013) as the two words have similar usages and grammatical roles, but it becomes a disaster for sentiment analysis as they have the opposite sentiment polarity.", "rank": 1, "paragraph_comparative_number": 3, "entities": [], "id": "p_1"}, "sentences": [{"end": 790, "text": "Feature engineering is important but laborintensive.", "rank": 7, "start": 738, "IsComparative": "0", "id": "st_7"}, {"end": 958, "text": "It is therefore desirable to discover explanatory factors from the data and make the learning algorithms less dependent on extensive feature engineering (Bengio, 2013).", "rank": 8, "start": 790, "IsComparative": "0", "id": "st_8"}, {"end": 1212, "text": "For the task of sentiment classification, an effective feature learning method is to compose the representation of a sentence (or document) from the representations of the words or phrases it contains (Socher et al., 2013b; Yessenalina and Cardie, 2011).", "rank": 9, "start": 958, "IsComparative": "1", "id": "st_9"}, {"end": 1368, "text": "Accordingly, it is a crucial step to learn the word representation (or word embedding), which is a dense, low-dimensional and real-valued vector for a word.", "rank": 10, "start": 1212, "IsComparative": "1", "id": "st_10"}, {"end": 1569, "text": "Although existing word embedding learning algorithms (Collobert et al., 2011; Mikolov et al., 2013) are intuitive choices, they are not effective enough if directly used for sentiment classi- fication.", "rank": 11, "start": 1368, "IsComparative": "0", "id": "st_11"}, {"end": 1714, "text": "The most serious problem is that traditional methods typically model the syntactic context of words but ignore the sentiment information of text.", "rank": 12, "start": 1569, "IsComparative": "0", "id": "st_12"}, {"end": 1809, "text": "As a result, words with opposite polarity, such as good and bad, are mapped into close vectors.", "rank": 13, "start": 1714, "IsComparative": "0", "id": "st_13"}, {"end": 2036, "text": "It is meaningful for some tasks such as pos-tagging (Zheng et al., 2013) as the two words have similar usages and grammatical roles, but it becomes a disaster for sentiment analysis as they have the opposite sentiment polarity.", "rank": 14, "start": 1809, "IsComparative": "1", "id": "st_14"}]}, {"paragraph_info": {"end": 2964, "start": 2036, "text": "In this paper, we propose learning sentimentspecific word embedding (SSWE) for sentiment analysis.We encode the sentiment information in-to the continuous representation of words, so that it is able to separate good and bad to opposite ends of the spectrum.To this end, we extend the existing word embedding learning algorithm (Collobert et al., 2011) and develop three neural networks to effectively incorporate the supervision from sentiment polarity of text (e.g.sentences or tweets) in their loss functions.We learn the sentiment-specific word embedding from tweets, leveraging massive tweets with emoticons as distant-supervised corpora without any manual annotations.These automatically collected tweets contain noises so they cannot be directly used as gold training data to build sentiment classifiers, but they are effective enough to provide weakly supervised signals for training the sentimentspecific word embedding.", "rank": 2, "paragraph_comparative_number": 2, "entities": [], "id": "p_2"}, "sentences": [{"end": 2134, "text": "In this paper, we propose learning sentimentspecific word embedding (SSWE) for sentiment analysis.", "rank": 15, "start": 2036, "IsComparative": "0", "id": "st_15"}, {"end": 2293, "text": "We encode the sentiment information in-to the continuous representation of words, so that it is able to separate good and bad to opposite ends of the spectrum.", "rank": 16, "start": 2134, "IsComparative": "0", "id": "st_16"}, {"end": 2502, "text": "To this end, we extend the existing word embedding learning algorithm (Collobert et al., 2011) and develop three neural networks to effectively incorporate the supervision from sentiment polarity of text (e.g.", "rank": 17, "start": 2293, "IsComparative": "1", "id": "st_17"}, {"end": 2547, "text": "sentences or tweets) in their loss functions.", "rank": 18, "start": 2502, "IsComparative": "1", "id": "st_18"}, {"end": 2709, "text": "We learn the sentiment-specific word embedding from tweets, leveraging massive tweets with emoticons as distant-supervised corpora without any manual annotations.", "rank": 19, "start": 2547, "IsComparative": "0", "id": "st_19"}, {"end": 2964, "text": "These automatically collected tweets contain noises so they cannot be directly used as gold training data to build sentiment classifiers, but they are effective enough to provide weakly supervised signals for training the sentimentspecific word embedding.", "rank": 20, "start": 2709, "IsComparative": "0", "id": "st_20"}]}, {"paragraph_info": {"end": 3741, "start": 2964, "text": "We apply SSWE as features in a supervised learning framework for Twitter sentiment classi- fication, and evaluate it on the benchmark dataset in SemEval 2013.In the task of predicting positive/negative polarity of tweets, our method yields 84.89% in macro-F1 by only using SSWE as feature, which is comparable to the top-performed system based on hand-crafted features (84.70%).After concatenating the SSWE feature with existing feature set, we push the state-of-the-art to 86.58% in macro-F1.The quality of SSWE is also directly evaluated by measuring the word similarity in the embedding space for sentiment lexicons.In the accuracy of polarity consistency between each sentiment word and its top N closest words, SSWE outperforms existing word embedding learning algorithms.", "rank": 3, "paragraph_comparative_number": 3, "entities": [], "id": "p_3"}, "sentences": [{"end": 3122, "text": "We apply SSWE as features in a supervised learning framework for Twitter sentiment classi- fication, and evaluate it on the benchmark dataset in SemEval 2013.", "rank": 21, "start": 2964, "IsComparative": "0", "id": "st_21"}, {"end": 3342, "text": "In the task of predicting positive/negative polarity of tweets, our method yields 84.89% in macro-F1 by only using SSWE as feature, which is comparable to the top-performed system based on hand-crafted features (84.70%).", "rank": 22, "start": 3122, "IsComparative": "1", "id": "st_22"}, {"end": 3457, "text": "After concatenating the SSWE feature with existing feature set, we push the state-of-the-art to 86.58% in macro-F1.", "rank": 23, "start": 3342, "IsComparative": "1", "id": "st_23"}, {"end": 3583, "text": "The quality of SSWE is also directly evaluated by measuring the word similarity in the embedding space for sentiment lexicons.", "rank": 24, "start": 3457, "IsComparative": "0", "id": "st_24"}, {"end": 3741, "text": "In the accuracy of polarity consistency between each sentiment word and its top N closest words, SSWE outperforms existing word embedding learning algorithms.", "rank": 25, "start": 3583, "IsComparative": "1", "id": "st_25"}]}, {"paragraph_info": {"end": 3816, "start": 3741, "text": "The major contributions of the work presented in this paper are as follows.", "rank": 4, "paragraph_comparative_number": 0, "entities": [], "id": "p_4"}, "sentences": [{"end": 3816, "text": "The major contributions of the work presented in this paper are as follows.", "rank": 26, "start": 3741, "IsComparative": "0", "id": "st_26"}]}, {"paragraph_info": {"end": 3969, "start": 3816, "text": "We develop three neural networks to learn sentiment-specific word embedding (SSWE) from massive distant-supervised tweets without any manual annotations;", "rank": 5, "paragraph_comparative_number": 0, "entities": [], "id": "p_5"}, "sentences": [{"end": 3969, "text": "We develop three neural networks to learn sentiment-specific word embedding (SSWE) from massive distant-supervised tweets without any manual annotations;", "rank": 27, "start": 3816, "IsComparative": "0", "id": "st_27"}]}, {"paragraph_info": {"end": 4211, "start": 3969, "text": "To our knowledge, this is the first work that exploits word embedding for Twitter sentiment classification.We report the results that the SSWE feature performs comparably with hand-crafted features in the top-performed system in SemEval 2013;", "rank": 6, "paragraph_comparative_number": 2, "entities": [], "id": "p_6"}, "sentences": [{"end": 4076, "text": "To our knowledge, this is the first work that exploits word embedding for Twitter sentiment classification.", "rank": 28, "start": 3969, "IsComparative": "1", "id": "st_28"}, {"end": 4211, "text": "We report the results that the SSWE feature performs comparably with hand-crafted features in the top-performed system in SemEval 2013;", "rank": 29, "start": 4076, "IsComparative": "1", "id": "st_29"}]}, {"paragraph_info": {"end": 4361, "start": 4211, "text": "We release the sentiment-specific word embedding learned from 10 million tweets, which can be adopted off-the-shell in other sentiment analysis tasks.", "rank": 7, "paragraph_comparative_number": 0, "entities": [], "id": "p_7"}, "sentences": [{"end": 4361, "text": "We release the sentiment-specific word embedding learned from 10 million tweets, which can be adopted off-the-shell in other sentiment analysis tasks.", "rank": 30, "start": 4211, "IsComparative": "0", "id": "st_30"}]}, {"paragraph_info": {"end": 4375, "start": 4361, "text": "2 Related Work", "rank": 8, "paragraph_comparative_number": 0, "entities": [], "id": "p_8"}, "sentences": [{"end": 4375, "text": "2 Related Work", "rank": 31, "start": 4361, "IsComparative": "0", "id": "st_31"}]}], "sentence_scenes_info": [{"x": 6, "text": "(2013) build the top-performed system in the Twitter sentiment classification track of SemEval 2013 (Nakov et al., 2013), using diverse sentiment lexicons and a variety of hand-crafted features."}, {"x": 21, "text": "We apply SSWE as features in a supervised learning framework for Twitter sentiment classi- fication, and evaluate it on the benchmark dataset in SemEval 2013."}, {"x": 28, "text": "To our knowledge, this is the first work that exploits word embedding for Twitter sentiment classification."}, {"x": 29, "text": "We report the results that the SSWE feature performs comparably with hand-crafted features in the top-performed system in SemEval 2013;"}], "characters": [{"name": "Twitter Inc.", "offsets": [589, 3029, 4043], "paragraph_occurrences": [0, 3, 6], "sentence_occurrences": [6, 21, 28], "affiliation": "light", "frequency": 3, "id": "Twitter"}, {"name": "SemEval", "offsets": [631, 3109, 4198], "paragraph_occurrences": [0, 3, 6], "sentence_occurrences": [6, 21, 29], "affiliation": "light", "frequency": 3, "id": "SemEval"}]}